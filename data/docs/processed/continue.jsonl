
{"source":"github","repo":"continue","path":"core/diff/test-examples/README.md","content":"# Diff algorithm tests\n\nTests are specified as\n\n```\n<CODE BEFORE>\n\n---\n\n<CODE AFTER>\n\n---\n\n<EXPECTED DIFF>\n```\n\n`---` is the delimeter, and surrounding whitespace will be trimmed.\n\nThe expected diff can be generated with the `displayDiff` function.\n\nWe make this explicit instead of comparing to the output of `myersDiff` in case the output from that is either unattainable or not exactly what we want.\n\nIn order to generate the expected diff, you can first leave it empty and then run the test. The test will catch this and write the _computed_ diff to the test file. It is up to you to correct this to the expected diff.\n"}
{"source":"github","repo":"continue","path":"core/nextEdit/README.md","content":"# Next Edit Prediction\n\n## What is it?\n\n## How is it different from autocomplete?\n\n## Where are we right now?\n\n- Users can decide to switch between autocomplete and next edit.\n- Next edit triggers at the same time autocomplete is triggered, via vscode's inline completion provider.\n- The following happens after the trigger:\n  - User's current cursor position is captured.\n  - We define an editable range, ¬±5 lines from the current cursor position.\n  - User's most recent edit is captured as a unified diff. (this is currently buggy)\n  - This is sent to the model, which returns a new editable range with next edit predictions.\n  - We display this new editable range in a SVG decoration.\n  - User can either tab to accept or esc to reject.\n    - On accept, the old editable range will be replaced by the new editable region. The cursor will be moved to the last line containing some change.\n    - On reject, nothing happens.\n\n## What needs to be worked on?\n\n- User edit captures.\n- Find a better way to trigger next edit (this links back to the diff capture problem).\n  - We can see that next edit triggers as soon as the user accepts a change. This is because autocomplete runs the same way.\n  - I think autocomplete has some filter logic that doesn't display the ghost text under some conditions, which I am guessing are the following:\n    - The model does not have any more completions to create.\n    - The prediction at the cursor location has been cached.\n- JetBrains integration.\n"}
{"source":"github","repo":"continue","path":"core/indexing/README.md","content":"# Indexing\n\nContinue uses a tagging system along with content addressing to ensure that nothing needs to be indexed twice. When you change branches, Continue will only re-index the files that are newly modified and that we don't already have a copy of. This system can be used across many different \"artifacts\" just by implementing the `CodebaseIndex` class.\n\n_artifact_: something that is generated by indexing and then saved to be used later (e.g. emeddings, full-text search index, or a table of top-level code snippets in each file)\n\n_cacheKey_: a key that determines whether two files can be considered the same to avoid re-indexing (always hash of file contents at this point)\n\n_`CodebaseIndex`_: a class that makes it easy to use the indexing system to help you generate a new artifact\n\nThe indexing process does the following:\n\n1. Check the modified timestamps of all files in the repo (this may seem extreme, but checking timestamps is significantly faster than actually reading a file. Git does the same thing.)\n2. Compare these to a \"catalog\" (stored in SQLite) of the last time that we indexed each of these files to get a list of files to \"add\" or \"remove\". If the file exists in the repo but not in the catalog, then we must \"add\" the file. If it exists in the catalog but not the repo, we must \"remove\" the file. If it exists in both and was modified after last indexed, then we must update the file. In this case we also add it to the \"add\" list.\n3. For each file to \"add\", check whether it was indexed on another branch. Here we use a SQLite table that acts as a cache for indexed files. If we find an entry in this table for a file with the same cacheKey, then we only need to add a tag to this entry for the current branch (\"addTag\"). Otherwise, we must \"compute\" the artifact.\n4. For each file in \"remove\", check whether it was indexed on another branch. If we find only one entry with the same cacheKey (presumably this should be the entry for the current branch, or something has gone wrong), then this entry should be removed and there will be no more branches that need the artifact, so we want to \"delete\" it. If there is more than one tag on this artifact, then we should just remove the tag for this branch (\"removeTag\").\n5. After having calculated these four lists of files (\"compute\", \"delete\", \"addTag\", \"removeTag\"), we pass them to the `CodebaseIndex` so that it can update whatever index-specific storage it might have. Many of them use SQLite and/or LanceDB. The `CodebaseIndex` implements a method called \"update\" that accepts the four lists and yields progress updates as it iterates over the lists. These progress updates are used to officially mark a file as having been indexed, so that if the extension is closed mid-indexing we don't falsely record progress.\n\n## Existing `CodebaseIndex`es\n\nAll indexes must be returned by `getIndexesToBuild` in [`CodebaseIndexer.ts`](./CodebaseIndexer.ts) if they are to be used.\n\n`CodeSnippetsCodebaseIndex`: uses tree-sitter queries to get a list of functions, classes, and other top-level code objects in each file\n`FullTextSearchCodebaseIndex`: creates a full-text search index using SQLite FTS5\n`ChunkCodebaseIndex`: chunks files recursively by code structure, for use in other embeddings providers like `LanceDbIndex`\n`LanceDbIndex`: calculates embeddings for each chunk and adds them to the LanceDB vector database, with metadata going into SQLite. Note that for each branch, a unique table is created in LanceDB.\n\n## Known problems\n\n- `FullTextSearchCodebaseIndex` doesn't differentiate between tags (branch, repo), so results may come from any branch/repo. LanceDB does this by creating separate tables for each tag (see `tableNameForTag`). The chunk index does this with a second table\n"}
{"source":"github","repo":"continue","path":"core/vendor/README.md","content":"# Vendored node_modules\n\n- transformers.js: to avoid the sharp dependency, which isn't used and has native dependencies\n"}
{"source":"github","repo":"continue","path":"core/vendor/modules/@xenova/transformers/README.md","content":"<p align=\"center\">\n    <br/>\n    <picture> \n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-dark.svg\" width=\"500\" style=\"max-width: 100%;\">\n        <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-light.svg\" width=\"500\" style=\"max-width: 100%;\">\n        <img alt=\"transformers.js javascript library logo\" src=\"https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-light.svg\" width=\"500\" style=\"max-width: 100%;\">\n    </picture>\n    <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://www.npmjs.com/package/@xenova/transformers\">\n        <img alt=\"NPM\" src=\"https://img.shields.io/npm/v/@xenova/transformers\">\n    </a>\n    <a href=\"https://www.npmjs.com/package/@xenova/transformers\">\n        <img alt=\"NPM Downloads\" src=\"https://img.shields.io/npm/dw/@xenova/transformers\">\n    </a>\n    <a href=\"https://www.jsdelivr.com/package/npm/@xenova/transformers\">\n        <img alt=\"jsDelivr Hits\" src=\"https://img.shields.io/jsdelivr/npm/hw/@xenova/transformers\">\n    </a>\n    <a href=\"https://github.com/xenova/transformers.js/blob/main/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/github/license/xenova/transformers.js?color=blue\">\n    </a>\n    <a href=\"https://huggingface.co/docs/transformers.js/index\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers.js/index.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n</p>\n\nState-of-the-art Machine Learning for the web. Run ü§ó Transformers directly in your browser, with no need for a server!\n\nTransformers.js is designed to be functionally equivalent to Hugging Face's [transformers](https://github.com/huggingface/transformers) python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as:\n\n- üìù **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n- üñºÔ∏è **Computer Vision**: image classification, object detection, and segmentation.\n- üó£Ô∏è **Audio**: automatic speech recognition and audio classification.\n- üêô **Multimodal**: zero-shot image classification.\n\nTransformers.js uses [ONNX Runtime](https://onnxruntime.ai/) to run models in the browser. The best part about it, is that you can easily [convert](#convert-your-models-to-onnx) your pretrained PyTorch, TensorFlow, or JAX models to ONNX using [ü§ó Optimum](https://github.com/huggingface/optimum#onnx--onnx-runtime).\n\nFor more information, check out the full [documentation](https://huggingface.co/docs/transformers.js).\n\n## Quick tour\n\nIt's super simple to translate from existing code! Just like the python library, we support the `pipeline` API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library.\n\n<table>\n<tr>\n<th width=\"440px\" align=\"center\"><b>Python (original)</b></th>\n<th width=\"440px\" align=\"center\"><b>Javascript (ours)</b></th>\n</tr>\n<tr>\n<td>\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\npipe = pipeline('sentiment-analysis')\n\nout = pipe('I love transformers!')\n# [{'label': 'POSITIVE', 'score': 0.999806941}]\n```\n\n</td>\n<td>\n\n```javascript\nimport { pipeline } from \"@xenova/transformers\";\n\n// Allocate a pipeline for sentiment-analysis\nlet pipe = await pipeline(\"sentiment-analysis\");\n\nlet out = await pipe(\"I love transformers!\");\n// [{'label': 'POSITIVE', 'score': 0.999817686}]\n```\n\n</td>\n</tr>\n</table>\n\nYou can also use a different model by specifying the model id or path as the second argument to the `pipeline` function. For example:\n\n```javascript\n// Use a different model for sentiment-analysis\nlet pipe = await pipeline(\n  \"sentiment-analysis\",\n  \"Xenova/bert-base-multilingual-uncased-sentiment\",\n);\n```\n\n## Installation\n\nTo install via [NPM](https://www.npmjs.com/package/@xenova/transformers), run:\n\n```bash\nnpm i @xenova/transformers\n```\n\nAlternatively, you can use it in vanilla JS, without any bundler, by using a CDN or static hosting. For example, using [ES Modules](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules), you can import the library with:\n\n```html\n<script type=\"module\">\n  import { pipeline } from \"https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0\";\n</script>\n```\n\n## Examples\n\nWant to jump straight in? Get started with one of our sample applications/templates:\n\n| Name                                | Description                               | Links                                                                                                                                                     |\n| ----------------------------------- | ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Whisper Web                         | Speech recognition w/ Whisper             | [code](https://github.com/xenova/whisper-web), [demo](https://huggingface.co/spaces/Xenova/whisper-web)                                                   |\n| Doodle Dash                         | Real-time sketch-recognition game         | [blog](https://huggingface.co/blog/ml-web-games), [code](https://github.com/xenova/doodle-dash), [demo](https://huggingface.co/spaces/Xenova/doodle-dash) |\n| Code Playground                     | In-browser code completion website        | [code](./examples/code-completion/), [demo](https://huggingface.co/spaces/Xenova/ai-code-playground)                                                      |\n| Semantic Image Search (client-side) | Search for images with text               | [code](./examples/semantic-image-search-client/), [demo](https://huggingface.co/spaces/Xenova/semantic-image-search-client)                               |\n| Semantic Image Search (server-side) | Search for images with text (Supabase)    | [code](./examples/semantic-image-search/), [demo](https://huggingface.co/spaces/Xenova/semantic-image-search)                                             |\n| Vanilla JavaScript                  | In-browser object detection               | [video](https://scrimba.com/scrim/cKm9bDAg), [code](./examples/vanilla-js/), [demo](https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector)     |\n| React                               | Multilingual translation website          | [code](./examples/react-translator/), [demo](https://huggingface.co/spaces/Xenova/react-translator)                                                       |\n| Text to speech (client-side)        | In-browser speech synthesis               | [code](./examples/text-to-speech-client/), [demo](https://huggingface.co/spaces/Xenova/text-to-speech-client)                                             |\n| Browser extension                   | Text classification extension             | [code](./examples/extension/)                                                                                                                             |\n| Electron                            | Text classification application           | [code](./examples/electron/)                                                                                                                              |\n| Next.js (client-side)               | Sentiment analysis (in-browser inference) | [code](./examples/next-client/), [demo](https://huggingface.co/spaces/Xenova/next-example-app)                                                            |\n| Next.js (server-side)               | Sentiment analysis (Node.js inference)    | [code](./examples/next-server/), [demo](https://huggingface.co/spaces/Xenova/next-server-example-app)                                                     |\n| Node.js                             | Sentiment analysis API                    | [code](./examples/node/)                                                                                                                                  |\n| Demo site                           | A collection of demos                     | [code](./examples/demo-site/), [demo](https://xenova.github.io/transformers.js/)                                                                          |\n\nCheck out the Transformers.js [template](https://huggingface.co/new-space?template=static-templates%2Ftransformers.js) on Hugging Face to get started in one click!\n\n## Custom usage\n\nBy default, Transformers.js uses [hosted pretrained models](https://huggingface.co/models?library=transformers.js) and [precompiled WASM binaries](https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0/dist/), which should work out-of-the-box. You can customize this as follows:\n\n### Settings\n\n```javascript\nimport { env } from \"@xenova/transformers\";\n\n// Specify a custom location for models (defaults to '/models/').\nenv.localModelPath = \"/path/to/models/\";\n\n// Disable the loading of remote models from the Hugging Face Hub:\nenv.allowRemoteModels = false;\n\n// Set location of .wasm files. Defaults to use a CDN.\nenv.backends.onnx.wasm.wasmPaths = \"/path/to/files/\";\n```\n\nFor a full list of available settings, check out the [API Reference](https://huggingface.co/docs/transformers.js/api/env).\n\n### Convert your models to ONNX\n\nWe recommend using our [conversion script](https://github.com/xenova/transformers.js/blob/main/scripts/convert.py) to convert your PyTorch, TensorFlow, or JAX models to ONNX in a single command. Behind the scenes, it uses [ü§ó Optimum](https://huggingface.co/docs/optimum) to perform conversion and quantization of your model.\n\n```bash\npython -m scripts.convert --quantize --model_id <model_name_or_path>\n```\n\nFor example, convert and quantize [bert-base-uncased](https://huggingface.co/bert-base-uncased) using:\n\n```bash\npython -m scripts.convert --quantize --model_id bert-base-uncased\n```\n\nThis will save the following files to `./models/`:\n\n```\nbert-base-uncased/\n‚îú‚îÄ‚îÄ config.json\n‚îú‚îÄ‚îÄ tokenizer.json\n‚îú‚îÄ‚îÄ tokenizer_config.json\n‚îî‚îÄ‚îÄ onnx/\n    ‚îú‚îÄ‚îÄ model.onnx\n    ‚îî‚îÄ‚îÄ model_quantized.onnx\n```\n\nFor the full list of supported architectures, see the [Optimum documentation](https://huggingface.co/docs/optimum/main/en/exporters/onnx/overview).\n\n## Supported tasks/models\n\nHere is the list of all tasks and architectures currently supported by Transformers.js.\nIf you don't see your task/model listed here or it is not yet supported, feel free\nto open up a feature request [here](https://github.com/xenova/transformers.js/issues/new/choose).\n\nTo find compatible models on the Hub, select the \"transformers.js\" library tag in the filter menu (or visit [this link](https://huggingface.co/models?library=transformers.js)).\nYou can refine your search by selecting the task you're interested in (e.g., [text-classification](https://huggingface.co/models?pipeline_tag=text-classification&library=transformers.js)).\n\n### Tasks\n\n#### Natural Language Processing\n\n| Task                                                                                                   | ID                                            | Description                                                                                    | Supported?                                                                                                                                                                                                                        |\n| ------------------------------------------------------------------------------------------------------ | --------------------------------------------- | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Conversational](https://huggingface.co/tasks/conversational)                                          | `conversational`                              | Generating conversational text that is relevant, coherent and knowledgable given a prompt.     | ‚ùå                                                                                                                                                                                                                                |\n| [Fill-Mask](https://huggingface.co/tasks/fill-mask)                                                    | `fill-mask`                                   | Masking some of the words in a sentence and predicting which words should replace those masks. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FillMaskPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=fill-mask&library=transformers.js)                              |\n| [Question Answering](https://huggingface.co/tasks/question-answering)                                  | `question-answering`                          | Retrieve the answer to a question from a given text.                                           | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.QuestionAnsweringPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=question-answering&library=transformers.js)            |\n| [Sentence Similarity](https://huggingface.co/tasks/sentence-similarity)                                | `sentence-similarity`                         | Determining how similar two texts are.                                                         | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=feature-extraction&library=transformers.js)            |\n| [Summarization](https://huggingface.co/tasks/summarization)                                            | `summarization`                               | Producing a shorter version of a document while preserving its important information.          | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=summarization&library=transformers.js)                     |\n| [Table Question Answering](https://huggingface.co/tasks/table-question-answering)                      | `table-question-answering`                    | Answering a question about information from a given table.                                     | ‚ùå                                                                                                                                                                                                                                |\n| [Text Classification](https://huggingface.co/tasks/text-classification)                                | `text-classification` or `sentiment-analysis` | Assigning a label or class to a given text.                                                    | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextClassificationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=text-classification&library=transformers.js)          |\n| [Text Generation](https://huggingface.co/tasks/text-generation#completion-generation-models)           | `text-generation`                             | Producing new text by predicting the next word in a sequence.                                  | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextGenerationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=text-generation&library=transformers.js)                  |\n| [Text-to-text Generation](https://huggingface.co/tasks/text-generation#text-to-text-generation-models) | `text2text-generation`                        | Converting one text sequence into another text sequence.                                       | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.Text2TextGenerationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=text2text-generation&library=transformers.js)        |\n| [Token Classification](https://huggingface.co/tasks/token-classification)                              | `token-classification` or `ner`               | Assigning a label to each token in a text.                                                     | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TokenClassificationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=token-classification&library=transformers.js)        |\n| [Translation](https://huggingface.co/tasks/translation)                                                | `translation`                                 | Converting text from one language to another.                                                  | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TranslationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=translation&library=transformers.js)                         |\n| [Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification)                      | `zero-shot-classification`                    | Classifying text into classes that are unseen during training.                                 | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotClassificationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-classification&library=transformers.js) |\n\n#### Vision\n\n| Task                                                                                          | ID                     | Description                                                                                                                                                                             | Supported?                                                                                                                                                                                                                 |\n| --------------------------------------------------------------------------------------------- | ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Depth Estimation](https://huggingface.co/tasks/depth-estimation)                             | `depth-estimation`     | Predicting the depth of objects present in an image.                                                                                                                                    | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DepthEstimationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=depth-estimation&library=transformers.js)         |\n| [Image Classification](https://huggingface.co/tasks/image-classification)                     | `image-classification` | Assigning a label or class to an entire image.                                                                                                                                          | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageClassificationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=image-classification&library=transformers.js) |\n| [Image Segmentation](https://huggingface.co/tasks/image-segmentation)                         | `image-segmentation`   | Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageSegmentationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=image-segmentation&library=transformers.js)     |\n| [Image-to-Image](https://huggingface.co/tasks/image-to-image)                                 | `image-to-image`       | Transforming a source image to match the characteristics of a target image or a target image domain.                                                                                    | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToImagePipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=image-to-image&library=transformers.js)              |\n| [Mask Generation](https://huggingface.co/tasks/mask-generation)                               | `mask-generation`      | Generate masks for the objects in an image.                                                                                                                                             | ‚ùå                                                                                                                                                                                                                         |\n| [Object Detection](https://huggingface.co/tasks/object-detection)                             | `object-detection`     | Identify objects of certain defined classes within an image.                                                                                                                            | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ObjectDetectionPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=object-detection&library=transformers.js)         |\n| [Video Classification](https://huggingface.co/tasks/video-classification)                     | n/a                    | Assigning a label or class to an entire video.                                                                                                                                          | ‚ùå                                                                                                                                                                                                                         |\n| [Unconditional Image Generation](https://huggingface.co/tasks/unconditional-image-generation) | n/a                    | Generating images with no condition in any context (like a prompt text or another image).                                                                                               | ‚ùå                                                                                                                                                                                                                         |\n\n#### Audio\n\n| Task                                                                                      | ID                                  | Description                                          | Supported?                                                                                                                                                                                                                                |\n| ----------------------------------------------------------------------------------------- | ----------------------------------- | ---------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Audio Classification](https://huggingface.co/tasks/audio-classification)                 | `audio-classification`              | Assigning a label or class to a given audio.         | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AudioClassificationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=audio-classification&library=transformers.js)                |\n| [Audio-to-Audio](https://huggingface.co/tasks/audio-to-audio)                             | n/a                                 | Generating audio from an input audio source.         | ‚ùå                                                                                                                                                                                                                                        |\n| [Automatic Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition) | `automatic-speech-recognition`      | Transcribing a given audio into text.                | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AutomaticSpeechRecognitionPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&library=transformers.js) |\n| [Text-to-Speech](https://huggingface.co/tasks/text-to-speech)                             | `text-to-speech` or `text-to-audio` | Generating natural-sounding speech given text input. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextToAudioPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=text-to-audio&library=transformers.js)                               |\n\n#### Tabular\n\n| Task                                                                          | ID  | Description                                                         | Supported? |\n| ----------------------------------------------------------------------------- | --- | ------------------------------------------------------------------- | ---------- |\n| [Tabular Classification](https://huggingface.co/tasks/tabular-classification) | n/a | Classifying a target category (a group) based on set of attributes. | ‚ùå         |\n| [Tabular Regression](https://huggingface.co/tasks/tabular-regression)         | n/a | Predicting a numerical value given a set of attributes.             | ‚ùå         |\n\n#### Multimodal\n\n| Task                                                                                                                                      | ID                               | Description                                                                                                                   | Supported?                                                                                                                                                                                                                                   |\n| ----------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Document Question Answering](https://huggingface.co/tasks/document-question-answering)                                                   | `document-question-answering`    | Answering questions on document images.                                                                                       | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DocumentQuestionAnsweringPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=document-question-answering&library=transformers.js)      |\n| [Feature Extraction](https://huggingface.co/tasks/feature-extraction)                                                                     | `feature-extraction`             | Transforming raw data into numerical features that can be processed while preserving the information in the original dataset. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=feature-extraction&library=transformers.js)                       |\n| [Image-to-Text](https://huggingface.co/tasks/image-to-text)                                                                               | `image-to-text`                  | Output text from a given image.                                                                                               | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToTextPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=image-to-text&library=transformers.js)                                  |\n| [Text-to-Image](https://huggingface.co/tasks/text-to-image)                                                                               | `text-to-image`                  | Generates images from input text.                                                                                             | ‚ùå                                                                                                                                                                                                                                           |\n| [Visual Question Answering](https://huggingface.co/tasks/visual-question-answering)                                                       | `visual-question-answering`      | Answering open-ended questions based on an image.                                                                             | ‚ùå                                                                                                                                                                                                                                           |\n| [Zero-Shot Audio Classification](https://huggingface.co/learn/audio-course/chapter4/classification_models#zero-shot-audio-classification) | `zero-shot-audio-classification` | Classifying audios into classes that are unseen during training.                                                              | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotAudioClassificationPipeline)<br>[(models)](https://huggingface.co/models?other=zero-shot-audio-classification&library=transformers.js)        |\n| [Zero-Shot Image Classification](https://huggingface.co/tasks/zero-shot-image-classification)                                             | `zero-shot-image-classification` | Classifying images into classes that are unseen during training.                                                              | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotImageClassificationPipeline)<br>[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&library=transformers.js) |\n| [Zero-Shot Object Detection](https://huggingface.co/tasks/zero-shot-object-detection)                                                     | `zero-shot-object-detection`     | Identify objects of classes that are unseen during training.                                                                  | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotObjectDetectionPipeline)<br>[(models)](https://huggingface.co/models?other=zero-shot-object-detection&library=transformers.js)                |\n\n#### Reinforcement Learning\n\n| Task                                                                          | ID  | Description                                                                                                                                | Supported? |\n| ----------------------------------------------------------------------------- | --- | ------------------------------------------------------------------------------------------------------------------------------------------ | ---------- |\n| [Reinforcement Learning](https://huggingface.co/tasks/reinforcement-learning) | n/a | Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback. | ‚ùå         |\n\n### Models\n\n1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\n1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su√°rez\\*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\n1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\n1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of G√∂ttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L√ºddecke and Alexander Ecker.\n1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\n1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.\n1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv√© J√©gou.\n1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv√© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.\n1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ren√© Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models. **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.\n1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.\n1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\n1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\n1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\n1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.\n1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\n1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.\n1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.\n1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by J√∂rg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\n1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by The [Mistral AI](https://mistral.ai) team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.\n1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with the paper [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.\n1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\n1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.\n1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/) by the MosaicML NLP Team.\n1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\n1. **[Nougat](https://huggingface.co/docs/transformers/model_doc/nougat)** (from Meta AI) released with the paper [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic.\n1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.\n1. **[Phi](https://huggingface.co/docs/transformers/main/model_doc/phi)** (from Microsoft) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\n1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n1. **[SigLIP](https://huggingface.co/docs/transformers/main/model_doc/siglip)** (from Google AI) released with the paper [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer.\n1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of W√ºrzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\n1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.\n1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)** (from HUST-VL) released with the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\n1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.\n1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\n1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n"}
{"source":"github","repo":"continue","path":"core/rules.md","content":"# `core` rules\n\nWhenever a new protocol message is added to the `protocol/` directory, check the following:\n\n- It's type is defined correctly\n- If it is a message from webview to core or vice versa:\n  - It has been added to `core/protocol/passThrough.ts`\n  - It has been added to `extensions/intellij/src/main/kotlin/com/github/continuedev/continueintellijextension/constants/MessageTypes.kt`\n- It is implemented in either `core/core.ts` (for messages to the core), in a `useWebviewListener` (for messages to the gui), or in `VsCodeMessenger.ts` for VS Code or `IdeProtocolClient.kt` for JetBriains (for messages to the IDE).\n- It does not duplicate functionality from another message type that already exists.\n"}
{"source":"github","repo":"continue","path":".continue/rules/personality.md","content":"---\nname: Personality Rules\ndescription: Conversational and personality guidelines\n---\n\nWhen the user challenges your output or asks a question, don't be overly-amiable (e.g. responding \"You're right\" all the time). Focus on correctness and be willing to tell the user they are wrong.\n"}
{"source":"github","repo":"continue","path":".continue/rules/llm-specificity.md","content":"---\nglobs: core/llm/llms/**/*.{ts,test.ts}\ndescription: Tailor recommendations for LLM code based on which specific LLM is being used.\n---\n\n# LLM Model Specificity\n\n- Refer to the file name and names of big classes to determine which LLM is being used in a file.\n- Ground all observations and recommendations with knowledge of that LLM.\n- Consider items such as context length, architecture, speed, and such.\n- Pay attention to the parent classes in these files.\n"}
{"source":"github","repo":"continue","path":".continue/rules/navigating-responses.md","content":"If the user's request is vague and you don't have enough information to confidently answer, you can either\n\n- In Chat OR Agent mode, stop and ask questions to clarify before proceeding\n- In Agent mode, use tools to discover more information, starting with the glob/grep tools if available.\n"}
{"source":"github","repo":"continue","path":".continue/rules/pure-function-unit-tests.md","content":"Always create comprehensive unit tests for new pure functions. Tests should cover normal cases, edge cases, and boundary conditions.\n"}
{"source":"github","repo":"continue","path":".continue/rules/gui-link-opening.md","content":"---\nglobs: gui/**/*\ndescription: Ensures consistent URL opening behavior in GUI components using the\n  IDE messenger pattern\nalwaysApply: false\n---\n\n# GUI Link Opening\n\nWhen adding functionality to open external links in GUI components, use `ideMessenger.post(\"openUrl\", url)` where `ideMessenger` is obtained from `useContext(IdeMessengerContext)`\n"}
{"source":"github","repo":"continue","path":".continue/rules/migrate-styled-components-to-tailwind.md","content":"---\nalwaysApply: false\n---\n\nConvert all of the styled components in this file into tailwind CSS. If a variable is used that is not already in @theme.ts and @tailwind.config.cjs, then you should figure out where it comes from and try adding that so it can be used. Wherever a function is called to interpolate a value, you can just use inline `styles={{ ... }}`. For ternaries, you could use @cn.ts.\n"}
{"source":"github","repo":"continue","path":".continue/rules/typescript-enum-usage.md","content":"---\nglobs: \"**/*.{ts,tsx}\"\nalwaysApply: false\n---\n\nUse enums instead of simple string unions when possible in TypeScript code\n"}
{"source":"github","repo":"continue","path":".continue/rules/no-any-types.md","content":"---\nglobs: \"**/*.{ts,tsx}\"\n---\n\nAvoid using the `any` type wherever possible. Use unknown or find the correct type. The only acceptable place to use any is when typecasting for test mocks, and even then it's better to avoid and provide a proper mock.\n"}
{"source":"github","repo":"continue","path":".continue/rules/programming-principles.md","content":"---\nname: Programming principles\ndescription: Guidelines for coding fundamentals in this project\n---\n\nUse functional programming paradigms whenever possible. Modifying existing classes or creating singletons where needed is acceptable. but otherwise, use functions.\n"}
{"source":"github","repo":"continue","path":".continue/rules/intellij-plugin-test-execution.md","content":"---\nname: IntelliJ Plugin Test Execution\ndescription: Guidelines for running IntelliJ plugin tests with Gradle\nalwaysApply: false\nglobs: extensions/intellij/**/*Test.kt\n---\n\nRun IntelliJ plugin tests using Gradle with the fully qualified test class or method name:\n\n## Run test class\n\n```bash\n./gradlew test --tests \"com.github.continuedev.continueintellijextension.unit.ApplyToFileHandlerTest\"\n```\n\n## Run specific test method\n\n```bash\n./gradlew test --tests \"com.github.continuedev.continueintellijextension.unit.ApplyToFileHandlerTest.should*\"\n```\n"}
{"source":"github","repo":"continue","path":".continue/rules/mintlify-formatting.md","content":"# Mintlify Documentation Formatting Rules\n\n## Component Formatting\n\nWhen working with Mintlify documentation components (Card, Info, Tip, Note, Warning, etc.), follow these formatting guidelines:\n\n### Bullet Points and Lists\n\n1. **Always add a blank line** after the opening component tag and before the closing tag\n2. **Indent content** by 2 spaces within components\n3. **Use proper list formatting** with each item on its own line:\n   - Start lists on a new line after introductory text\n   - Use `-` for unordered lists\n   - Maintain consistent indentation\n\n### Examples\n\n#### ‚úÖ Correct Formatting:\n\n```mdx\n<Card title=\"Example\" icon=\"icon-name\">\n\n  This is the content with proper formatting:\n  - First bullet point\n  - Second bullet point\n  - Third bullet point\n\n</Card>\n```\n\n```mdx\n<Info>\n\n  Important information here:\n  - Point one\n  - Point two\n  - Point three\n\n</Info>\n```\n\n#### ‚ùå Incorrect Formatting:\n\n```mdx\n<Card title=\"Example\" icon=\"icon-name\">\n  This is wrong: - All bullets - On one line - Bad formatting\n</Card>\n```\n\n### Component-Specific Rules\n\n1. **Card Components**: Always include blank lines and proper indentation\n2. **Info/Tip/Note/Warning**: Format lists as bullet points, not inline\n3. **CardGroup**: Each Card within should follow the same formatting rules\n4. **Code Blocks**: Within components, maintain proper indentation\n\n### Links in Lists\n\nWhen including links in bullet points:\n```mdx\n- [Link Text](url): Description of the link\n```\n\n### Nested Components\n\nFor nested components, maintain proper indentation levels:\n```mdx\n<CardGroup>\n  <Card title=\"First Card\">\n\n    Content here:\n    - Bullet one\n    - Bullet two\n\n  </Card>\n\n  <Card title=\"Second Card\">\n\n    More content:\n    - Another bullet\n    - Final bullet\n\n  </Card>\n</CardGroup>\n```\n\n## Application\n\nThese rules apply to all `.mdx` files in the `docs/` directory, particularly:\n- Guide documents\n- Cookbook documents\n- Reference documentation\n- Any Mintlify-powered documentation\n\n## Automation Note\n\nWhen using Continue or other AI assistants to generate or modify documentation:\n- Always format Mintlify components according to these rules\n- Review generated content for proper formatting\n- Apply these rules consistently across all documentation"}
{"source":"github","repo":"continue","path":".continue/rules/colors.md","content":"---\nname: Extension Color Themes\ndescription: Guidelines for using theme colors in GUI components\nalwaysApply: false\nglobs: \"gui/**/*.tsx\"\n---\n\nWhen adding colors to components, use tailwind color classes.\nDo NOT use explicit colors like text-gray-400. Instead, use theme colors.\n\n## Available theme colors\n\n### Normal text\n\n- `foreground`, `description`, `description-muted`\n\n### Other text, icons, etc\n\n- `success`, `warning`, `error`, `accent`, `link`\n\n### General components\n\n- `background`, `border`, `border-focus`\n\n### Specific components\n\n#### Button\n\n- `primary`, `primary-foreground`, `primary-hover`\n- `secondary`, `secondary-foreground`, `secondary-hover`\n\n#### Input\n\n- `input`, `input-foreground`, `input-border`, `input-placeholder`\n\n#### Badge\n\n- `badge`, `badge-foreground`\n\n#### List/Dropdown items\n\n- `list-hover`, `list-active`, `list-active-foreground`\n\n#### Code Editor\n\n- `editor`, `editor-foreground`\n\n## Usage examples\n\nAny of these colors can be used in tailwind classes:\n\n- `bg-primary`\n- `text-success`\n- `border-error`\n- `hover:bg-list-hover`\n\n## Excluded colors\n\nThe following less-used colors are excluded from this guide:\n\n- Command (only used by tip-tap): `command`, `command-foreground`, `command-border`, `command-border-focus`\n- Find widget colors: `find-match`, `find-match-selected`\n- `table-oddRow`\n"}
{"source":"github","repo":"continue","path":".continue/rules/vs-code-commands-helper-functions.md","content":"---\nglobs: \"extensions/vscode/src/commands.ts\"\n---\n\nWhen adding new commands to the commands map, always create a separate helper function for the command logic instead of defining it inline. Follow the pattern of existing commands like `streamInlineEdit` - define the helper function below the commands map, then call it from within the command entry.\n"}
{"source":"github","repo":"continue","path":".continue/rules/documentation-standards.md","content":"---\nglobs: docs/\\*_/_.{md,mdx}\ndescription: This style guide should be used as a reference for maintaining consistency across all Continue documentation\nalwaysApply: false\n---\n\n# Continue Documentation Style Guide\n\n## Overview\n\n## Writing Tone & Voice\n\n### Conversational and Direct\n\n- Follow Mintlify documentation standards\n- Use simple, conversational language that gets straight to the point\n- Avoid overly technical jargon when simpler terms work\n- Write as if speaking directly to the developer using the tool\n- Keep paragraphs concise and scannable\n\n**Example:**\n‚úÖ \"You send it a question, and it replies with an answer\"\n‚ùå \"The system processes user queries and generates corresponding responses\"\n\n### Helpful and Instructional\n\n- Focus on helping users accomplish their goals\n- Use active voice and imperative mood for instructions\n- Assume users want to get things done quickly\n- Include relevant Admonition components for tips, warnings, and info\n\n**Example:**\n‚úÖ \"Press cmd/ctrl + L to begin a new session\"\n‚ùå \"A new session can be initiated by pressing cmd/ctrl + L\"\n\n### Practical and Task-Oriented\n\n- Emphasize what users can accomplish with each feature\n- Lead with benefits and use cases before diving into mechanics\n- Keep explanations grounded in real-world scenarios\n\n## Content Structure\n\n### Page Organization\n\n1. **Visual Introduction**: Lead with GIFs or images showing the feature in action\n2. **Purpose Statement**: Brief explanation of what the feature does and when to use it\n3. **Step-by-Step Instructions**: Clear, actionable steps with keyboard shortcuts\n4. **Platform-Specific Notes**: Separate sections for VS Code and JetBrains when needed\n5. **Additional Tips**: Advanced usage or troubleshooting notes\n\n### Section Headers\n\n- Use consistent heading hierarchy starting with h2 (##)\n- Include YAML frontmatter with title, description, and keywords\n- Use action-oriented headers that describe what users will do\n- Format: \"Verb + object\" (e.g., \"Type a request and press enter\")\n- Keep headers concise but descriptive\n- Use title case\n\n**Examples:**\n‚úÖ \"Highlight code and activate\"\n‚úÖ \"Accept or reject changes\"\n‚úÖ \"Switch between different models\"\n\n### Lists and Steps\n\n- Use numbered lists for sequential steps\n- Use bullet points for feature lists or options\n- Keep list items parallel in structure\n- Start action items with verbs\n\n## Technical Writing Standards\n\n### Code and Keyboard Shortcuts\n\n- Use `backticks` for inline code elements\n- Format keyboard shortcuts consistently: `cmd/ctrl + L`\n- Always provide shortcuts for Mac/Windows/Linux\n- Use code blocks for configuration examples with proper syntax highlighting\n\n### Cross-References\n\n- Link to related sections using descriptive anchor text\n- Use relative links to other documentation pages\n- Format: `[descriptive text](/path/to/page)`\n\n### Platform Differences\n\n- Always address both VS Code and JetBrains when applicable\n- Use clear subheadings to separate platform-specific instructions\n- Lead with the more common platform (typically VS Code) when both are covered\n\n## Language Conventions\n\n### Terminology\n\n- **Consistent Terms**: Use the same terms throughout (e.g., \"LLM\" not \"AI model\" in some places)\n- **Product Names**: Capitalize product names correctly (VS Code, JetBrains, Continue)\n- **Feature Names**: Use consistent capitalization for Continue features (Chat, Edit, Agent, Autocomplete)\n\n### Abbreviations\n\n- Spell out acronyms on first use, then use abbreviation consistently\n- Common abbreviations: LLM, IDE, API, URL\n\n### Pronouns\n\n- Use \"you\" to address the user directly\n- Use \"it\" to refer to the tool/model\n- Avoid \"we\" unless referring to the Continue team\n"}
{"source":"github","repo":"continue","path":".continue/rules/css-units.md","content":"---\nglobs: \"gui/**/*.tsx\"\n---\n\nYou should try to use the `rem` CSS unit whenever possible for scalability instead of `px`.\n"}
{"source":"github","repo":"continue","path":".continue/rules/unit-testing-rules.md","content":"---\nname: Unit Testing Rules\ndescription: Guidelines for unit testing in this project\nalwaysApply: false\n---\n\nFor unit testing in this project:\n\n## 1. Testing frameworks\n\nThe project uses Vitest and Jest for testing. Prefer Vitest.\n\n## 2. Test execution location\n\nRun tests from within the specific package directory (e.g., `cd core && ..`).\n\n## 3. Vitest tests\n\n- Test files follow the pattern `*.vitest.ts`\n- Run tests using `vitest` from within the specific package/module directory:\n  ```bash\n  cd [directory] && vitest -- [test file path]\n  ```\n\n## 4. Jest tests\n\n- Test files follow the pattern `*.test.ts`\n- Run tests using `npm test` from within the specific package/module directory:\n  ```bash\n  cd [directory] && npm test -- [test file path]\n  ```\n- The test script uses experimental VM modules via NODE_OPTIONS flag\n\n## 5. Test structure\n\n- Write tests as top-level `test()` functions - DO NOT use `describe()` blocks\n- Include the function name being tested in the test description for clarity\n"}
{"source":"github","repo":"continue","path":".continue/rules/new-protocol-message.md","content":"---\nname: New protocol message\ndescription: Create a new protocol message from core/gui/ide to core/gui/ide\nalwaysApply: false\n---\n\nCreate a new protocol message by taking the following steps:\n\n## 1. Define the message type\n\nAdd your new message type definition to the appropriate file in the `protocol/` directory with correct TypeScript typing.\n\n## 2. Check for duplicates\n\nVerify that no existing message type already provides the same functionality.\n\n## 3. Add to passThrough (if webview ‚Üî core)\n\nIf your message is between webview and core, add it to `core/protocol/passThrough.ts`.\n\n## 4. Add to IntelliJ constants (if webview ‚Üî core)\n\nIf your message is between webview and core, add it to `extensions/intellij/src/main/kotlin/com/github/continuedev/continueintellijextension/constants/MessageTypes.kt`.\n\n## 5. Implement the message handler\n\nImplement the message in the appropriate location:\n\n- **Messages to core**: `core/core.ts`\n- **Messages to GUI**: `useWebviewListener` hook\n- **Messages to VS Code IDE**: `VsCodeMessenger.ts`\n- **Messages to JetBrains IDE**: `IdeProtocolClient.kt`\n\n## 6. Test the implementation\n\nVerify that your message works correctly in both VS Code and JetBrains IDEs if applicable.\n"}
{"source":"github","repo":"continue","path":".continue/rules/documentation-description-rule.md","content":"---\nglobs: docs/**/*.{md,mdx}\ndescription: This rule applies to all documentation files to ensure consistent\n  SEO optimization and improve discoverability. It helps users and search\n  engines understand the content of each page before reading it.\n---\n\nEvery file in the docs folder must include a 'description' field in its frontmatter that accurately summarizes the content of the page in 100-160 characters. The description should be concise, keyword-rich, and explain what users will learn or accomplish from the page.\n"}
{"source":"github","repo":"continue","path":".continue/rules/bigger-picture-description-rules.md","content":"---\nname: Bigger Picture Description Rules\ndescription: Guidelines for explaining how code works in context\nalwaysApply: false\n---\n\nWhen a user asks how a certain part of the code works:\n\n## 1. Describe what the code does in isolation\n\nExplain the functionality of the code without considering its interactions with other parts of the codebase.\n\n## 2. Describe interactions with other parts of the codebase\n\nIf the code interacts with other parts of the codebase, describe how the code is imported and used in other parts of the codebase.\n\n## 3. Include parent function for clarity\n\nWhen describing each use-case, include the parent function for clarity.\n"}
{"source":"github","repo":"continue","path":".continue/rules/dev-data-guide.md","content":"---\nalwaysApply: false\n---\n\n# Continue Development Data (Dev Data) Guide\n\n## Overview\n\nDevelopment data (dev data) captures detailed information about how developers interact with LLM-aided development tools. Unlike basic telemetry, dev data includes lots of details into the complete software development workflow, including code context, user interactions, and development patterns.\n\n## Core Architecture\n\n### Primary Implementation Files\n\n- **`/core/data/log.ts`**: Main `DataLogger` class - singleton for event logging and remote transmission\n- **`/packages/config-yaml/src/schemas/data/`**: Schema definitions for all event types\n\n### Storage Locations\n\n- **Default storage**: `~/.continue/dev_data/`\n- **Event files**: `~/.continue/dev_data/{version}/{eventName}.jsonl`\n\n## Event Types and Schemas\n\n### Core Event Types\n\n1. **`tokensGenerated`**: LLM token usage tracking\n2. **`autocomplete`**: Code completion interactions\n3. **`chatInteraction`**: Chat-based development assistance\n4. **`editInteraction`**: Code editing sessions\n5. **`editOutcome`**: Results of edit operations\n6. **`nextEditOutcome`**: Next Edit feature outcomes\n7. **`chatFeedback`**: User feedback on AI responses\n8. **`toolUsage`**: Tool interaction statistics\n9. **`quickEdit`**: Quick edit functionality usage\n\n### Schema Versioning\n\n- **Version 0.1.0**: Initial schema implementation\n- **Version 0.2.0**: Current schema with expanded fields and metadata\n- **Schema files**: Located in `/packages/config-yaml/src/schemas/data/`\n\n### Base Schema Structure\n\nAll events inherit from a base schema (`/packages/config-yaml/src/schemas/data/base.ts`):\n\n```typescript\n{\n  eventName: string,\n  schema: string,\n  timestamp: string,\n  userId: string,\n  userAgent: string,\n  selectedProfileId: string\n}\n```\n\n## Key Integration Points\n\n### Autocomplete System\n\n- **File**: `/core/autocomplete/util/AutocompleteLoggingService.ts`\n- **Purpose**: Tracks code completion acceptance/rejection, timing, and cache hits\n- **Integration**: Called from autocomplete engine when completions are shown/accepted\n\n### Chat Interface\n\n- **Integration**: Chat interactions logged through `DataLogger.logDevData()`\n- **Data**: Includes prompts, responses, context, and user feedback\n- **Privacy**: Can be configured to exclude code content\n\n### Edit Features\n\n- **Files**: `/extensions/vscode/src/extension/EditOutcomeTracker.ts`, `/core/nextEdit/NextEditLoggingService.ts`\n- **Purpose**: Track edit suggestions, acceptance rates, and outcomes\n- **Integration**: Embedded in edit workflow to capture user decisions\n\n### LLM Token Tracking\n\n- **File**: `/core/llm/index.ts`\n- **Purpose**: Track token usage across all LLM providers\n- **Storage**: SQLite database for efficient querying and reporting\n\n## Configuration and Customization\n\n### Configuration Structure\n\nDev data is configured through `data` blocks in your Continue config:\n\n```yaml\ndata:\n  - name: \"Local Development Data\"\n    destination: \"file:///Users/developer/.continue/dev_data\"\n    schema: \"0.2.0\"\n    level: \"all\"\n    events: [\"autocomplete\", \"chatInteraction\", \"editOutcome\"]\n\n  - name: \"Team Analytics\"\n    destination: \"https://analytics.yourcompany.com/api/events\"\n    schema: \"0.2.0\"\n    level: \"noCode\"\n    apiKey: \"your-api-key-here\"\n    events: [\"tokensGenerated\", \"toolUsage\"]\n```\n\n### Configuration Options\n\n- **`destination`**: Where to send data (`file://` for local, `http://`/`https://` for remote)\n- **`schema`**: Schema version to use (`\"0.1.0\"` or `\"0.2.0\"`)\n- **`level`**: Data detail level (`\"all\"` includes code, `\"noCode\"` excludes code content)\n- **`events`**: Array of event types to collect\n- **`apiKey`**: Authentication for remote endpoints\n\n### Privacy Controls\n\n- **`\"all\"` level**: Includes code content (prefixes, suffixes, completions)\n- **`\"noCode\"` level**: Excludes code content, only metadata and metrics\n- **Local-first**: Data is always stored locally, remote transmission is optional\n\n## Making Changes to Dev Data\n\n### Adding New Event Types\n\n1. **Create schema**: Add new event schema in `/packages/config-yaml/src/schemas/data/`\n2. **Update index**: Add to schema aggregator in `/packages/config-yaml/src/schemas/data/index.ts`\n3. **Implement logging**: Add logging calls in relevant service files\n4. **Update version**: Consider schema version bump if breaking changes\n\n### Modifying Existing Events\n\n1. **Schema changes**: Update schema files in `/packages/config-yaml/src/schemas/data/`\n2. **Backward compatibility**: Ensure changes don't break existing data consumers\n3. **Version management**: Increment schema version for breaking changes\n4. **Test thoroughly**: Validate schema changes with existing data\n\n### Adding New Logging Points\n\n1. **Import DataLogger**: `import { DataLogger } from \"core/data/log\"`\n2. **Log events**: Call `DataLogger.getInstance().logDevData(eventName, data)`\n3. **Follow patterns**: Use existing logging services as examples\n4. **Validate data**: Ensure logged data matches schema requirements\n\n### Debugging Dev Data Issues\n\n1. **Check local storage**: Verify files are being created in `~/.continue/dev_data/`\n2. **Validate schemas**: Ensure event data matches expected schema format\n3. **Review configuration**: Check `data` blocks in Continue config\n4. **Test endpoints**: Verify remote endpoints are reachable and accepting data\n\n## Best Practices\n\n### When Adding New Events\n\n- Follow existing naming conventions for event types\n- Include sufficient context for analysis without oversharing sensitive data\n- Consider privacy implications and respect user configuration levels\n- Add appropriate error handling and logging\n\n### When Modifying Schemas\n\n- Maintain backward compatibility when possible\n- Document schema changes thoroughly\n- Consider impact on existing data consumers\n- Test with real development data\n\n### When Integrating Logging\n\n- Use the singleton pattern: `DataLogger.getInstance()`\n- Log events at appropriate points in user workflow\n- Respect user privacy settings and configuration\n- Handle errors gracefully without disrupting user experience\n\n## Common Patterns\n\n### Service-Based Logging\n\nMost dev data logging follows a service pattern:\n\n```typescript\nexport class FeatureLoggingService {\n  private dataLogger = DataLogger.getInstance();\n\n  logFeatureUsage(data: FeatureUsageData) {\n    this.dataLogger.logDevData(\"featureUsage\", data);\n  }\n}\n```\n\n### Event-Driven Logging\n\nEvents are typically logged at key interaction points:\n\n```typescript\n// When user accepts autocomplete\nonAutocompleteAccepted(completion: CompletionData) {\n  AutocompleteLoggingService.getInstance().logAutocompleteAccepted(completion);\n}\n```\n\nThis guide provides the foundation for understanding and working with Continue's dev data system. Always prioritize user privacy and follow established patterns when making changes.\n"}
{"source":"github","repo":"continue","path":".continue/rules/continue-specificity.md","content":"---\nglobs: /**/*.\ndescription: General questions about code completion should be answered specific to Continue\n---\n\n# Continue Specificity\n\n- In chat mode, if the user asks generally about code completion or developer tools, answer specifically regarding Continue and not other similar software.\n- Keep all suggestions and comments concentrated on Continue, unless the user asks otherwise. If the user does this, answer with no particular specificity to Continue.\n"}
{"source":"github","repo":"continue","path":".continue/rules/github-pr-documentation-updater.md","content":"---\nname: Update docs from GitHub PR\ndescription: Provide a PR link to update docs based on\nalwaysApply: false\n---\n\nWhen provided with a GitHub PR URL, use the GitHub CLI (`gh pr view <PR_URL> --json title,body,commits,files,additions,deletions,changedFiles`) to fetch comprehensive PR details including title, description, commits, changed files, and diff statistics. Analyze the PR content to identify what documentation in the `docs` folder needs to be updated, created, or modified. Update relevant documentation files to reflect the changes, new features, bug fixes, or improvements described in the PR. Ensure documentation changes are accurate, well-structured, and maintain consistency with existing docs formatting and style.\n"}
{"source":"github","repo":"continue","path":".continue/rules/test-running-guide.md","content":"---\nglobs: [\"gui/**/*\", \"core/**/*\"]\ndescription: Provides test running instructions for GUI and core folders\nalwaysApply: false\n---\n\nWhen working with test files, use the following commands to run tests:\n\nGUI folder tests:\n\n- Run all tests: `cd gui && npm test`\n\nCore folder tests:\n\n- Run Jest tests: `cd core && npm test`\n- Run Vitest tests: `cd core && npm run vitest`\n\nTest file patterns:\n\n- GUI: _.test.ts or_.test.tsx files use Vitest\n- Core: _.test.ts files use Jest,_.vitest.ts files use Vitest\n\nBest practices:\n\n- We are transitioning to vitest, so use that when creating new tests\n"}
{"source":"github","repo":"continue","path":".continue/rules/overeager.md","content":"---\nname: Don't be overeager\n---\n\nAvoid over-eagerly adding new features. You should solve the problem at hand and then can propose further work.\n"}
{"source":"github","repo":"continue","path":".continue/prompts/sub-agent-background.md","content":"---\nname: Sub Agent Background Prompt\ndescription: Start a subagent using the continue cli in the background\ninvokable: true\n---\n\n# Continue Sub Agent Background Prompt\n\nTake the prompt provided by the user and using the terminal tool run the following command in the background:\n\ncn -p \"{{prompt}}\"\n"}
{"source":"github","repo":"continue","path":".continue/prompts/sub-agent-foreground.md","content":"---\nname: Sub Agent Foreground Prompt\ndescription: Start a subagent using the continue cli in the foreground\ninvokable: true\n---\n\n# Continue Sub Agent Foreground Prompt\n\nTake the prompt provided by the user and using the terminal tool run the following command in the foreground:\n\ncn -p \"{{prompt}}\"\n"}
{"source":"github","repo":"continue","path":"SECURITY.md","content":"# Security Policy\n\n## Reporting a Vulnerability\n\nIf you discover a security vulnerability, please do not open a public issue. Instead, please report it by emailing security@continue.dev. We will be highly responsive to all security concerns and ask that you give us sufficient time to investigate and address the vulnerability before disclosing it publicly.\n\nPlease include the following details in your report:\n\n- A description of the vulnerability\n- Steps to reproduce the issue\n- Your assessment of the potential impact\n- Any possible mitigations\n\n## Contact\n\nFor any other questions or concerns related to security, please contact us at security@continue.dev.\n"}
{"source":"github","repo":"continue","path":"binary/README.md","content":"# Continue Core Binary\n\nThe purpose of this folder is to package Typescript code in a way that can be run from any IDE or platform. We first bundle with `esbuild` and then package into binaries with `pkg`.\n\nThe `pkgJson/package.json` contains instructions for building with pkg, and needs to be in a separte folder because there is no CLI flag for the assets option (it must be in a package.json), and pkg doesn't recognize any name other than package.json, but if we use the same package.json with dependencies in it, pkg will automatically include these, significantly increasing the binary size.\n\nThe build process is otherwise defined entirely in `build.js`.\n\n### List of native modules\n\n- sqlite3/build/Release/node_sqlite3.node (\\*)\n- @lancedb/\\*\\*\n- esbuild?\n- @esbuild?\n- onnxruntime-node?\n\n### List of dynamically imported modules\n\n- posthog-node\n- @octokit/rest\n- esbuild\n\n### List of .wasm files\n\n- tree-sitter.wasm\n- tree-sitter-wasms/\n\n(\\*) = need to download for each platform manually\n\n## Debugging\n\nTo debug the binary with IntelliJ, set `useTcp` to `true` in `CoreMessenger.kt`, and then in VS Code run the \"Core Binary\" debug script. Instead of starting a subprocess for the binary and communicating over stdin/stdout, the IntelliJ extension will connect over TCP to the server started from the VS Code window. You can place breakpoints anywhere in the `core` or `binary` folders.\n\n## Building\n\n```bash\nnpm run build\n```\n\n## Testing\n\n```bash\nnpm run test\n```\n"}
{"source":"github","repo":"continue","path":"CODE_OF_CONDUCT.md","content":"# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\neducation, socio-economic status, nationality, personal appearance, race,\nreligion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n- Using welcoming and inclusive language\n- Being respectful of differing viewpoints and experiences\n- Gracefully accepting constructive criticism\n- Focusing on what is best for the community\n- Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n- The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n- Trolling, insulting/derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at hi@continue.dev. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n"}
{"source":"github","repo":"continue","path":"manual-testing-sandbox/readme.md","content":"The sole purpose of this folder is to open it when debugging the extension.\nIt is not used by the extension itself.\nYou can add more files that can be useful when manually testing the extension.\n"}
{"source":"github","repo":"continue","path":"manual-testing-sandbox/nested-folder/rules.md","content":"HELLO WORLD THIS IS A RULE\n"}
{"source":"github","repo":"continue","path":"CONTRIBUTING.md","content":"# Contributing to Continue\n\n## Table of Contents\n\n- [Contributing to Continue](#contributing-to-continue)\n  - [Table of Contents](#table-of-contents)\n- [‚ù§Ô∏è Ways to Contribute](#Ô∏è-ways-to-contribute)\n  - [üëã Continue Contribution Ideas](#-continue-contribution-ideas)\n  - [üêõ Report Bugs](#-report-bugs)\n  - [‚ú® Suggest Enhancements](#-suggest-enhancements)\n  - [üìñ Updating / Improving Documentation](#-updating--improving-documentation)\n    - [Running the Documentation Server Locally](#running-the-documentation-server-locally)\n      - [Method 1: NPM Script](#method-1-npm-script)\n      - [Method 2: VS Code Task](#method-2-vs-code-task)\n  - [üßë‚Äçüíª Contributing Code](#-contributing-code)\n    - [Environment Setup](#environment-setup)\n      - [Pre-requisites](#pre-requisites)\n      - [Fork the Continue Repository](#fork-the-continue-repository)\n      - [VS Code](#vs-code)\n        - [Debugging](#debugging)\n      - [JetBrains](#jetbrains)\n    - [Our Git Workflow](#our-git-workflow)\n    - [Development Workflow](#development-workflow)\n    - [Formatting](#formatting)\n    - [Theme Colors](#theme-colors)\n    - [Testing](#testing)\n    - [Review Process](#review-process)\n    - [Getting Help](#getting-help)\n  - [Contributing new LLM Providers/Models](#contributing-new-llm-providersmodels)\n    - [Adding an LLM Provider](#adding-an-llm-provider)\n    - [Adding Models](#adding-models)\n  - [üìê Continue Architecture](#-continue-architecture)\n    - [Continue VS Code Extension](#continue-vs-code-extension)\n    - [Continue JetBrains Extension](#continue-jetbrains-extension)\n  - [Contributor License Agreement](#contributor-license-agreement-cla)\n\n# ‚ù§Ô∏è Ways to Contribute\n\n## üëã Continue Contribution Ideas\n\n[This GitHub project board](https://github.com/orgs/continuedev/projects/2) is a list of ideas for how you can\ncontribute to Continue. These aren't the only ways, but are a great starting point if you are new to the project. You\ncan also browse the list\nof [good first issues](https://github.com/continuedev/continue/issues?q=is:issue%20state:open%20label:good-first-issue).\n\n## üêõ Report Bugs\n\nIf you find a bug, please [create an issue](https://github.com/continuedev/continue/issues) to report it! A great bug\nreport includes:\n\n- A description of the bug\n- Steps to reproduce\n- What you expected to happen\n- What actually happened\n- Screenshots or videos\n\n## ‚ú® Suggest Enhancements\n\nContinue is quickly adding features, and we'd love to hear which are the most important to you. The best ways to suggest\nan enhancement are:\n\n- Create an issue\n\n  - First, check whether a similar proposal has already been made\n  - If not, [create an issue](https://github.com/continuedev/continue/issues)\n  - Please describe the enhancement in as much detail as you can, and why it would be useful\n\n- Join the [Continue Discord](https://discord.gg/NWtdYexhMs) and tell us about your idea in the `#feedback` channel\n\n## üìñ Updating / Improving Documentation\n\nContinue is continuously improving, but a feature isn't complete until it is reflected in the documentation! If you see\nsomething out-of-date or missing, you can help by clicking \"Edit this page\" at the bottom of any page\non [docs.continue.dev](https://docs.continue.dev).\n\n### Running the Documentation Server Locally\n\nYou can run the documentation server locally using either of the following methods:\n\n#### Method 1: NPM Script\n\n1. Open your terminal and navigate to the `docs` subdirectory of the project. The `docusaurus.config.js` file you'll see\n   there is a sign you're in the right place.\n\n2. Run the following command to install the necessary dependencies for the documentation server:\n\n   ```bash\n   npm install\n   ```\n\n3. Run the following command to start the documentation server:\n\n   ```bash\n   npm run start\n   ```\n\n#### Method 2: VS Code Task\n\n1. Open VS Code in the root directory of the project.\n\n2. Open the VS Code command pallet (`cmd/ctrl+shift+p`) and select `Tasks: Run Task`.\n\n3. Look for the `docs:start` task and select it.\n\nThis will start a local server and you can see the documentation rendered in your default browser, typically accessible\nat `http://localhost:3000`.\n\n## üßë‚Äçüíª Contributing Code\n\nWe welcome contributions from developers of all experience levels - from first-time contributors to seasoned open source\nmaintainers. While we aim to maintain high standards for reliability and maintainability, our goal is to keep the\nprocess as welcoming and straightforward as possible.\n\n### Environment Setup\n\n#### Pre-requisites\n\nYou should have Node.js version 20.19.0 (LTS) or higher installed. You can get it\non [nodejs.org](https://nodejs.org/en/download) or, if you are using NVM (Node Version Manager), you can set the correct\nversion of Node.js for this project by running the following command in the root of the project:\n\n```bash\nnvm use\n```\n\nThen, install Vite globally\n\n```bash\nnpm i -g vite\n```\n\n#### Fork the Continue Repository\n\n1. Go to the [Continue GitHub repository](https://github.com/continuedev/continue) and fork it to your GitHub account.\n\n2. Clone your forked repository to your local machine. Use: `git clone https://github.com/YOUR_USERNAME/continue.git`\n\n3. Navigate to the cloned directory and make sure you are on the main branch. Create your feature/fix branch from there,\n   like so: `git checkout -b 123-my-feature-branch`\n\n4. Send your pull request to the main branch.\n\n#### VS Code\n\n1. Open the VS Code command pallet (`cmd/ctrl+shift+p`) and select `Tasks: Run Task` and then select\n   `install-all-dependencies`\n\n2. Start debugging:\n\n   1. Switch to Run and Debug view\n   2. Select `Launch extension` from drop down\n   3. Hit play button\n   4. This will start the extension in debug mode and open a new VS Code window with it installed\n      1. The new VS Code window with the extension is referred to as the _Host VS Code_\n      2. The window you started debugging from is referred to as the _Main VS Code_\n\n3. To package the extension, run `npm run package` in the `extensions/vscode` directory, select `Tasks: Run Task` and\n   then select `vscode-extension:package`. This will generate `extensions/vscode/build/continue-{VERSION}.vsix`, which\n   you can install by right-clicking and selecting \"Install Extension VSIX\".\n\n##### Debugging\n\n**Breakpoints** can be used in both the `core` and `extensions/vscode` folders while debugging, but are not currently\nsupported inside of `gui` code.\n\n**Hot-reloading** is enabled with Vite, so if you make any changes to the `gui`, they should be automatically reflected\nwithout rebuilding. In some cases, you may need to refresh the _Host VS Code_ window to see the changes.\n\nSimilarly, any changes to `core` or `extensions/vscode` will be automatically included by just reloading the _Host VS\nCode_ window with cmd/ctrl+shift+p \"Reload Window\".\n\n#### JetBrains\n\nSee [`intellij/CONTRIBUTING.md`](./extensions/intellij/CONTRIBUTING.md) for the JetBrains extension.\n\n### Our Git Workflow\n\nWe keep a single permanent branch: `main`. When we are ready to create a \"pre-release\" version, we create a tag on the\n`main` branch titled `v1.3.x-vscode`, which automatically triggers the workflow\nin [preview.yaml](./.github/workflows/preview.yaml), which builds and releases a version of the VS Code extension. When\na release has been sufficiently tested, we will create a new release titled `v1.2.x-vscode`, triggering a similar\nworkflow in [main.yaml](./.github/workflows/main.yaml), which will build and release a main release of the VS Code\nextension. Any hotfixes can be made by creating a feature branch from the tag for the release in question. This workflow\nis well explained by <http://releaseflow.org>.\n\n### What makes a good PR?\n\nTo keep the Continue codebase clean and maintainable, we expect the following from our own team and all contributors:\n\n- Open a new issue or comment on an existing one before writing code. This ensures your proposed changes are aligned\n  with the project direction\n- Keep changes focused. Multiple unrelated fixes should be opened as separate PRs\n- Write or update tests for new functionality\n- Update relevant documentation in the `docs` folder\n- **For new features**: Include a short screen recording or screenshot demonstrating the new functionality. This makes it much easier for us as contributors to review and understand your changes. See [this PR](https://github.com/continuedev/continue/pull/6455) as a good example\n- Open a PR against the `main` branch. Make sure to fill in the PR template\n\n### Formatting\n\nContinue uses [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode) to format\nJavaScript/TypeScript. Please install the Prettier extension in VS Code and enable \"Format on Save\" in your settings.\n\n### Theme Colors\n\nContinue has a set of named theme colors that we map to extension colors and tailwind classes, which can be found in [gui/src/styles/theme.ts](gui/src/styles/theme.ts)\n\nGuidelines for using theme colors:\n\n- Use Tailwind colors whenever possible. If developing in VS Code, download the [Tailwind CSS Intellisense extension](https://marketplace.visualstudio.com/items?itemName=bradlc.vscode-tailwindcss) for great suggestions\n- Avoid using any explicit classes and CSS variables outside the theme (e.g. `text-yellow-400`)\n\nGuidelines for adding/updating theme colors:\n\n- Choose sensible VS Code variables to add/update in [gui/src/styles/theme.ts](gui/src/styles/theme.ts) (see [here](https://code.visualstudio.com/api/references/theme-color) and [here](https://www.notion.so/1fa1d55165f78097b551e3bc296fcf76?pvs=25) for inspiration)\n- Choose sensible JetBrains named colors to add/update in `GetTheme.kt` (flagship LLMs can give you good suggestions to try)\n- Update `tailwind.config.js` if needed\n- Use the Theme Test Page to check colors. This can be accessed by going to `Settings` -> `Help` -> `Theme Test Page` in dev/debug mode.\n\n### Testing\n\nWe have a mix of unit, functional, and e2e test suites, with a primary focus on functional testing. These tests run on\neach pull request. If your PR causes one of these tests to fail, we will ask you to resolve the issue before we\nmerge.\n\nWhen contributing, please update or create the appropriate tests to help verify the correctness of your implementation.\n\n### Review Process\n\n- **Initial Review** - A maintainer will be assigned as primary reviewer\n- **Feedback Loop** - The reviewer may request changes. We value your work, but also want to ensure the code is\n  maintainable and follows our patterns.\n- **Approval & Merge** - Once the PR is approved, it will be merged into the `main` branch.\n\n### Getting Help\n\nJoin [#contribute on Discord](https://discord.gg/vapESyrFmJ) to engage with maintainers and other contributors.\n\n## Contributing New LLM Providers/Models\n\n### Adding an LLM Provider\n\nContinue has support for more than a dozen different LLM \"providers\", making it easy to use models running on OpenAI,\nOllama, Together, LM Studio, Msty, and more. You can find all of the existing\nproviders [here](https://github.com/continuedev/continue/tree/main/core/llm/llms), and if you see one missing, you can\nadd it with the following steps:\n\n1. Create a new file in the `core/llm/llms` directory. The name of the file should be the name of the provider, and it\n   should export a class that extends `BaseLLM`. This class should contain the following minimal implementation. We\n   recommend viewing pre-existing providers for more details. The [LlamaCpp Provider](./core/llm/llms/LlamaCpp.ts) is a\n   good simple example.\n2. Add your provider to the `LLMs` array in [core/llm/llms/index.ts](./core/llm/llms/index.ts).\n3. If your provider supports images, add it to the `PROVIDER_SUPPORTS_IMAGES` array\n   in [core/llm/autodetect.ts](./core/llm/autodetect.ts).\n4. Add a documentation page for your provider in [\n   `docs/customize/model-providers/more`](./docs/customize/model-providers/more). This should show an example\n   of configuring your provider in `config.yaml` and explain what options are available.\n\n### Adding Models\n\nWhile any model that works with a supported provider can be used with Continue, we keep a list of recommended models\nthat can be automatically configured from the UI or `config.json`. The following files should be updated when adding a\nmodel:\n\n- [AddNewModel page](./gui/src/pages/AddNewModel/configs/) - This directory defines which model options are shown in the\n  side bar model selection UI. To add a new model:\n  1. Add a `ModelPackage` entry for the model into [configs/models.ts](./gui/src/pages/AddNewModel/configs/models.ts),\n     following the lead of the many examples near the top of the file\n  2. Add the model within its provider's array\n     to [configs/providers.ts](./gui/src/pages/AddNewModel/configs/providers.ts) (add provider if needed)\n- LLM Providers: Since many providers use their own custom strings to identify models, you'll have to add the\n  translation from Continue's model name (the one you added to `index.d.ts`) and the model string for each of these\n  providers: [Ollama](./core/llm/llms/Ollama.ts), [Together](./core/llm/llms/Together.ts),\n  and [Replicate](./core/llm/llms/Replicate.ts). You can find their full model lists\n  here: [Ollama](https://ollama.ai/library), [Together](https://docs.together.ai/docs/inference-models), [Replicate](https://replicate.com/collections/streaming-language-models).\n- [Prompt Templates](./core/llm/autodetect.ts) - In this file you'll find the `autodetectTemplateType` function. Make\n  sure that for the model name you just added, this function returns the correct template type. This is assuming that\n  the chat template for that model is already built in Continue. If not, you will have to add the template type and\n  corresponding edit and chat templates.\n\n## Contributor License Agreement (CLA)\n\nWe require all contributors to accept the CLA and have made it as easy as commenting on your PR:\n\n1. Open your pull request.\n2. Paste the following comment (or reply `recheck` if you‚Äôve signed before):\n\n   ```text\n   I have read the CLA Document and I hereby sign the CLA\n   ```\n\n3. The CLA-Assistant bot records your signature in the repo and marks the status check as passed.\n"}
{"source":"github","repo":"continue","path":"CLA.md","content":"# Individual Contributor License Agreement (v1.0, Continue)\n\n_Based on the Apache Software Foundation Individual CLA v 2.2._\n\nBy commenting **‚ÄúI have read the CLA Document and I hereby sign the CLA‚Äù**\non a Pull Request, **you (‚ÄúContributor‚Äù) agree to the following terms** for any\npast and future ‚ÄúContributions‚Äù submitted to **Continue (the ‚ÄúProject‚Äù)**.\n\n---\n\n## 1. Definitions\n\n- **‚ÄúContribution‚Äù** ‚Äì any original work of authorship submitted to the Project\n  (code, documentation, designs, etc.).\n- **‚ÄúYou‚Äù / ‚ÄúYour‚Äù** ‚Äì the individual (or legal entity) posting the acceptance\n  comment.\n\n## 2. Copyright License\n\nYou grant **Continue Dev, Inc.** and all recipients of software distributed by the\nProject a perpetual, worldwide, non‚Äëexclusive, royalty‚Äëfree, irrevocable\nlicense to reproduce, prepare derivative works of, publicly display, publicly\nperform, sublicense, and distribute Your Contributions and derivative works.\n\n## 3. Patent License\n\nYou grant **Continue Dev, Inc.** and all recipients of the Project a perpetual,\nworldwide, non‚Äëexclusive, royalty‚Äëfree, irrevocable (except as below) patent\nlicense to make, have made, use, sell, offer to sell, import, and otherwise\ntransfer Your Contributions alone or in combination with the Project.\n\nIf any entity brings patent litigation alleging that the Project or a\nContribution infringes a patent, the patent licenses granted by You to that\nentity under this CLA terminate.\n\n## 4. Representations\n\n1. You are legally entitled to grant the licenses above.\n2. Each Contribution is either Your original creation or You have authority to\n   submit it under this CLA.\n3. Your Contributions are provided **‚ÄúAS IS‚Äù** without warranties of any kind.\n4. You will notify the Project if any statement above becomes inaccurate.\n\n## 5. Miscellany\n\nThis Agreement is governed by the laws of the **State of California**, USA,\nexcluding its conflict‚Äëof‚Äëlaws rules. If any provision is held unenforceable,\nthe remaining provisions remain in force.\n"}
{"source":"github","repo":"continue","path":"README.md","content":"<div align=\"center\">\n\n![Continue logo](media/readme.png)\n\n</div>\n\n<h1 align=\"center\">Continue</h1>\n\n<div align=\"center\">\n\n<a target=\"_blank\" href=\"https://opensource.org/licenses/Apache-2.0\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://docs.continue.dev\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/Continue-docs-%23BE1B55.svg?logo=data:image/svg%2bxml;base64,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\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://changelog.continue.dev\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/changelog-%96EFF3\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://discord.gg/vapESyrFmJ\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/discord-join-continue.svg?labelColor=191937&color=6F6FF7&logo=discord\" style=\"height: 22px;\" />\n</a>\n\n<p></p>\n\n<div align=\"center\">\n\n**Ship faster with Continuous AI**\n\n**The future of coding isn't writing more code. It's delegating the boring parts, so you can build the interesting stuff**\n\n</div>\n\nGet started in [Mission Control](https://hub.continue.dev/agents), [CLI (Headless Mode)](https://docs.continue.dev/cli/quick-start#headless-mode), or [CLI (TUI mode)](https://docs.continue.dev/cli/quick-start#tui-mode)\n\n### Quick Install\n\n```bash\nnpm i -g @continuedev/cli\ncn\n```\n\n## Cloud Agents\n\nSet workflows to run automatically on [PR opens](https://docs.continue.dev/guides/continuous-ai#pattern-2-the-pr-review-agent), [schedules](https://docs.continue.dev/guides/continuous-ai#pattern-1-the-async-triage-bot), or [any event trigger](https://docs.continue.dev/cli/quick-start#headless-mode)\n\n![Cloud Agents](docs/images/background-agent.gif)\n\n## CLI Agents\n\nWatch workflows execute in real-time and approve decisions step-by-step from your [terminal](https://docs.continue.dev/cli/quick-start#tui-mode)\n\n![CLI Agents](docs/images/cli-agent.gif)\n\n## IDE Agents\n\nTrigger workflows from [VS Code](https://marketplace.visualstudio.com/items?itemName=Continue.continue) or [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension)‚Äîlet agents handle the refactoring while you keep coding\n\n![IDE Agents](docs/images/agent.gif)\n\n</div>\n\n## Contributing\n\nRead the [contributing guide](https://github.com/continuedev/continue/blob/main/CONTRIBUTING.md), and\njoin [#contribute on Discord](https://discord.gg/vapESyrFmJ).\n\n## License\n\n[Apache 2.0 ¬© 2023-2024 Continue Dev, Inc.](./LICENSE)\n"}
{"source":"github","repo":"continue","path":"sync/src/README.md","content":"# Codebase Indexing\n\nThis is a small Rust library for efficiently keeping a codebase index up to date.\n\n### How it works\n\n> Important definition: a _tag_ is a (workspace, branch, provider_id) pair that uniquely identifies an index. Since we use content-based addressing within the index, much of the data is shared for efficiency.\n\nThe output of the sync_results function is a list of 4 lists of tuples. Each tuple contains a file path and a hash of the file contents. The 4 lists are:\n\n1. Compute: Files that need to be newly computed or updated\n2. Delete: Files that need to be deleted from the index\n3. Add label: Files that exist in the index but need to have a label added for a new tag\n4. Remove label: Files that exist in the index but need to have a label removed\n\nThe labels help us filter when retrieving results from an index like Meilisearch or Chroma. All ids of the items in these indices are the hash of the file contents (possibly plus a chunk index at the end).\n\nThe first time, a Merkle tree of the codebase folder is constructed, ignoring any files in .gitignore or .continueignore. Every file found will be returned as needing to be computed added to the index.\n\nThereafter, the following steps are performed:\n\n1. Load the previously computed merkle tree for the tag\n2. Compute the current merkle tree of the codebase\n3. Update the .last_sync file with current timestamp\n4. Save the new tree to disk\n5. Compute the diff of the trees, which tells you which files have been a) added or b) removed\n6. For each file added:\n   - If in the global cache, append it to `add_label`\n   - Otherwise, append it to `compute`\n7. For each file removed:\n   - If in the global cache, but only in rev_tags for this tag, append it to `delete`\n   - If in global cache for more than this tag, append it to `remove_label`\n   - Otherwise, ignore. This should never happen.\n8. Return (compute, delete, add_label, remove_label)\n\n### Files created\n\nSeveral files are stored and updated on disk in the ~/.continue/index folder to keep track of indexed files:\n\n- `~/.continue/index/tags/<dir>/<branch>/<provider_id>/merkle_tree` - the last computed Merkle tree of the codebase for a given tag\n- `~/.continue/index/tags/<dir>/<branch>/<provider_id>/.last_sync` - the last time the tag was synced\n- The index cache contains a list of hashes that have already been computed both in general and per tag. These are always kept in sync.\n  - `~/.continue/index/.index_cache` - contains the global cache (flat file of hashes)\n  - `~/.continue/index/tags/<dir>/<branch>/<provider_id>/.index_cache` - contains the tag-specific cache (flat file of hashes)\n  - `~/.continue/index/rev_tags` - contains a mapping from hash to tags that the hash is currently indexed for. This is a directory of files, where each file is prefixed with the first 2 characters of the hash. The file is a JSON mapping from hash to list of tags.\n\n### Files\n\n- `lib.rs` contains just the top-level function that is called by the Python bindings\n- `sync/merkle.rs` contains the Merkle tree implementation (for building and comparing trees)\n- `sync/mod.rs` contains the main sync logic, which handles maintenance of the on-disk database of which hashes are included in which tags\n\n### Current limitations:\n\n- Only handles local files, so is not currently being used in situations where the Continue server is on a different machine from the IDE or the workspace (Remote SSH, WSL, or a Continue server being run for a team).\n- Currently not using stat to check for recent changes to files, is instaed re-calculating the entire Merkle tree on every IDE reload. This is fine for now since it only takes 0.2 seconds on the Continue codebase, but is a quick improvement we can make later.\n"}
{"source":"github","repo":"continue","path":"packages/llm-info/README.md","content":"# @continuedev/llm-info\n\nA lightweight package providing information about various Large Language Models (LLMs), including embedding, reranking, and other models.\n\nWhereas @continuedev/openai-adapters is responsible for translation between API types, @continuedev/llm-info is concerned with\n\n- Templates\n- Capabilities (e.g. tools, images, streaming, predicted outputs, etc.)\n- Model aliases\n\nand openai-adapters might depend on llm-info for some of these things.\n\n### Goal\n\nWe know we are done when the steps required to add support for a new model in Continue are exactly\n\n1. editing a single LlmInfo object, and\n2. adding it to the supporting ModelProviders.\n\n### Code structure\n\nThe two primary types are LlmInfo and ModelProvider\n\nModels are defined on their own in the `models` directory. They can be grouped however makes sense.\n\nProviders are defined in the `providers` directory, with all models that they support in their `models` attribute. It's important that models are tied to providers, because the model might have slightly different attributes (e.g. context length) per provider. Define as much as possible in the base object, and then spread to update for the specific providers as needed.\n\n### Where to use llm-info\n\n- Replace autodetect.ts\n- See usage in `BaseLLM` constructor, and finish the job of using llm-info everywhere relevant.\n- Replace `gui/pages/AddNewModel/configs/[providers/models].ts`\n"}
{"source":"github","repo":"continue","path":"packages/openai-adapters/README.md","content":"# OpenAI Adapters\n\nOpenAI adapters convert an OpenAI-compatible request to a request for another API and back.\n\nThey are purely a translation layer, and are not concerned with:\n\n- Templates\n- Whether a model supports tools, images, etc.\n- Dynamically changing API base for model\n- Keeping track of system message (it will always be the first message with a systemMesage)\n- Keeping a private variable with anything that is already passed through the OpenAI request body\n- Appending \"/\" to the apiBase (but this is TODO)\n- Batching embeddings (yes, it requires some knowledge of max batch size, but it's more important to maintain 1 req = 1 req)\n- Using streamChat for streamComplete and vice-versa if one isn't defined\n\nThe goal is for this to change as infrequently as possible. It should only require updating when the actual API format changes.\n\nThey are concerned with:\n\n- Converting model aliases\n- Cache behavior\n- max stop words\n- use legacy completions endpoint?\n- anything else that couldn't possibly be guess by the client since it won't know the endpoint behind the proxy\n\n## Supported APIs\n\n- [x] Anthropic\n- [ ] AskSage\n- [x] Azure\n- [x] Bedrock\n- [ ] Bedrock Import\n- [x] Cerebras\n- [ ] Cloudflare\n- [x] Cohere\n- [x] DeepInfra\n- [x] Deepseek\n- [ ] Flowise\n- [x] Function Network\n- [x] Gemini\n- [x] Groq\n- [ ] HuggingFace Inference API\n- [ ] HuggingFace TGI\n- [x] Kindo\n- [x] LMStudio\n- [x] LlamaCpp\n- [x] Llamafile\n- [x] Msty\n- [x] Mistral\n- [x] Nvidia\n- [x] Nebius\n- [x] OpenRouter\n- [x] OpenAI\n- [ ] !Ollama\n- [x] OVHCLoud\n- [ ] Replicate\n- [ ] SageMaker\n- [x] SambaNova\n- [x] Scaleway\n- [ ] Silicon Flow\n- [x] TextGen Web UI\n- [x] Together\n- [x] Novita AI\n- [x] Vllm\n- [x] Vertex AI\n- [x] Voyage AI\n- [x] WatsonX\n- [x] xAI\n- [x] Fireworks\n- [x] Moonshot\n"}
{"source":"github","repo":"continue","path":"packages/openai-adapters/VERCEL_AI_SDK.md","content":"# Vercel AI SDK Integration\n\n## Why\n\n### The Problem\n\nThe openai-adapters package contains ~50 provider implementations, each maintaining:\n\n- Custom SSE parsing and chunk buffering\n- Provider-specific streaming protocols\n- Tool call serialization logic\n- Type definitions that must stay in sync with provider APIs\n- Error handling for provider-specific edge cases\n\nThis represents significant maintenance burden:\n\n- Provider API changes require manual updates\n- Streaming bugs must be debugged per-provider\n- New features need implementation across all providers\n- Type safety requires constant vigilance\n\n### The Solution\n\nThe [Vercel AI SDK](https://sdk.vercel.ai/) is a well-maintained, provider-agnostic abstraction that:\n\n- Handles streaming protocols automatically\n- Tracks provider API changes\n- Provides unified tool calling interface\n- Maintains type safety\n- Offers comprehensive error handling\n\nBy offloading this work to Vercel, we reduce ~60% of our maintenance burden while preserving all existing functionality.\n\n## How It Works\n\n### Feature-Flagged Implementation\n\nThe integration is feature-flagged for gradual rollout:\n\n```typescript\n// Enable for OpenAI\nUSE_VERCEL_AI_SDK_OPENAI=true cn\n\n// Enable for Anthropic\nUSE_VERCEL_AI_SDK_ANTHROPIC=true cn\n```\n\nWhen disabled (default), the original implementation is used. When enabled, Vercel AI SDK handles the provider communication while we maintain the same external API.\n\n### Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Consumer (CLI / Core IDE)                  ‚îÇ\n‚îÇ  - Uses openai-adapters package             ‚îÇ\n‚îÇ  - No code changes required                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                   ‚îÇ\n                   ‚îÇ Same API (BaseLlmApi)\n                   ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  openai-adapters Package                    ‚îÇ\n‚îÇ                                              ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n‚îÇ  ‚îÇ OpenAIApi  ‚îÇ         ‚îÇAnthropicApi‚îÇ     ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n‚îÇ        ‚îÇ                       ‚îÇ             ‚îÇ\n‚îÇ   Feature Flag?           Feature Flag?     ‚îÇ\n‚îÇ        ‚îÇ                       ‚îÇ             ‚îÇ\n‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n‚îÇ    ‚îÇ  Yes   ‚îÇ              ‚îÇ  Yes  ‚îÇ        ‚îÇ\n‚îÇ    ‚ñº        ‚îÇ              ‚ñº       ‚îÇ         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ        ‚îÇ\n‚îÇ  ‚îÇ Vercel ‚îÇ ‚îÇ            ‚îÇ Vercel ‚îÇ‚îÇ        ‚îÇ\n‚îÇ  ‚îÇ   SDK  ‚îÇ ‚îÇ            ‚îÇ   SDK  ‚îÇ‚îÇ        ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ        ‚îÇ\n‚îÇ             ‚îÇ                      ‚îÇ         ‚îÇ\n‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îò                  ‚îå‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n‚îÇ         ‚îÇ No                   ‚îÇ No          ‚îÇ\n‚îÇ         ‚ñº                      ‚ñº             ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n‚îÇ  ‚îÇ  Original  ‚îÇ        ‚îÇ  Original  ‚îÇ       ‚îÇ\n‚îÇ  ‚îÇ    SDK     ‚îÇ        ‚îÇ    SDK     ‚îÇ       ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Custom Fetch Preservation\n\nAll RequestOptions (headers, proxy, SSL certs, timeout) are fully preserved:\n\n```typescript\n// Only use customFetch when RequestOptions are present\nconst hasRequestOptions =\n  config.requestOptions &&\n  (config.requestOptions.headers ||\n    config.requestOptions.proxy ||\n    config.requestOptions.caBundlePath ||\n    config.requestOptions.clientCertificate ||\n    config.requestOptions.extraBodyProperties);\n\nthis.openaiProvider = createOpenAI({\n  apiKey: config.apiKey ?? \"\",\n  baseURL:\n    this.apiBase !== \"https://api.openai.com/v1/\" ? this.apiBase : undefined,\n  fetch: hasRequestOptions ? customFetch(config.requestOptions) : undefined,\n});\n```\n\nWhen no custom options are needed, we use native fetch for Web Streams API compatibility.\n\n### Format Conversion\n\nVercel AI SDK uses different formats than OpenAI. We handle conversion transparently:\n\n**Tool Format:**\n\n```typescript\n// OpenAI format (input)\n{\n  type: \"function\",\n  function: {\n    name: \"readFile\",\n    description: \"Read a file\",\n    parameters: { /* JSON Schema */ }\n  }\n}\n\n// Vercel format (converted)\n{\n  readFile: {\n    description: \"Read a file\",\n    parameters: aiJsonSchema({ /* JSON Schema */ })\n  }\n}\n```\n\n**Stream Events:**\n\n```typescript\n// Vercel AI SDK emits various event types\nfor await (const part of stream.fullStream) {\n  switch (part.type) {\n    case 'text-delta':\n      yield chatChunk({ content: part.textDelta });\n      break;\n    case 'tool-call':\n      yield chatChunkFromDelta({ delta: { tool_calls: [...] } });\n      break;\n    case 'finish':\n      yield usageChatChunk({ usage: part.usage });\n      break;\n    // Filter out events with no OpenAI equivalent\n    case 'step-start':\n    case 'step-finish':\n    case 'tool-result':\n      continue;\n  }\n}\n```\n\nAll conversion logic lives in shared utilities:\n\n- `convertToolsToVercelFormat()` - Tool conversion\n- `convertVercelStream()` - Stream event conversion\n- `convertOpenAIMessagesToVercel()` - Message conversion\n\n### Automatic Fallbacks\n\nThe implementation intelligently falls back to original SDK when needed:\n\n**1. OpenAI Responses Endpoint (o1/o3 models)**\n\n```typescript\nif (this.shouldUseResponsesEndpoint(body.model)) {\n  // Use responses endpoint instead of Vercel SDK\n  const response = await this.responsesNonStream(body, signal);\n  return responseToChatCompletion(response);\n}\n```\n\n**2. Anthropic Multi-turn Tool Conversations**\n\n```typescript\n// Vercel SDK manages tool call lifecycle internally\n// Can't handle pre-existing tool results in history\nconst hasToolMessages = body.messages.some((msg) => msg.role === \"tool\");\n\nif (this.useVercelSDK && this.anthropicProvider && !hasToolMessages) {\n  yield * this.chatCompletionStreamVercel(body, signal);\n  return;\n}\n\n// Fall back to original for tool conversations\nyield * this.handleStreamResponse(response, body.model);\n```\n\n## Implementation Details\n\n### Files Modified\n\n**Core Implementation:**\n\n- `src/apis/OpenAI.ts` - Added Vercel SDK branch with feature flag\n- `src/apis/Anthropic.ts` - Added Vercel SDK branch with fallback logic\n\n**Shared Utilities (new):**\n\n- `src/convertToolsToVercel.ts` - Tool format conversion\n- `src/vercelStreamConverter.ts` - Stream event conversion\n- `src/openaiToVercelMessages.ts` - Message format conversion\n\n**Tests (new):**\n\n- `src/test/vercel-sdk.test.ts` - 28 integration tests\n- `src/test/convertToolsToVercel.test.ts` - 8 unit tests\n- `src/test/vercelStreamConverter.test.ts` - 17 unit tests\n- `src/test/multi-turn-tools.test.ts` - Multi-turn conversation test\n- `src/test/cli-tools.test.ts` - CLI tool compatibility tests\n\n### OpenAI Implementation\n\n```typescript\nexport class OpenAIApi implements BaseLlmApi {\n  private openaiProvider?: ReturnType<typeof createOpenAI>;\n  private useVercelSDK: boolean;\n\n  constructor(protected config: OpenAIConfig) {\n    this.useVercelSDK = process.env.USE_VERCEL_AI_SDK_OPENAI === \"true\";\n\n    if (this.useVercelSDK) {\n      this.openaiProvider = createOpenAI({\n        apiKey: config.apiKey ?? \"\",\n        baseURL:\n          this.apiBase !== \"https://api.openai.com/v1/\"\n            ? this.apiBase\n            : undefined,\n        fetch: hasRequestOptions\n          ? customFetch(config.requestOptions)\n          : undefined,\n      });\n    }\n\n    // Always create original client for fallback\n    this.openai = new OpenAI({\n      /* ... */\n    });\n  }\n\n  async *chatCompletionStream(body, signal) {\n    if (\n      this.useVercelSDK &&\n      this.openaiProvider &&\n      !this.shouldUseResponsesEndpoint(body.model)\n    ) {\n      const model = this.openaiProvider(body.model);\n      const vercelTools = convertToolsToVercelFormat(body.tools);\n\n      const stream = await streamText({\n        model,\n        messages: body.messages,\n        tools: vercelTools,\n        // ... other parameters\n      });\n\n      yield* convertVercelStream(stream.fullStream, { model: body.model });\n      return;\n    }\n\n    // Fall back to original implementation\n    const response = await this.openai.chat.completions.create(body);\n    for await (const result of response) {\n      yield result;\n    }\n  }\n}\n```\n\n### Anthropic Implementation\n\n```typescript\nexport class AnthropicApi implements BaseLlmApi {\n  private anthropicProvider?: ReturnType<typeof createAnthropic>;\n  private useVercelSDK: boolean;\n\n  constructor(protected config: AnthropicConfig) {\n    this.useVercelSDK = process.env.USE_VERCEL_AI_SDK_ANTHROPIC === \"true\";\n\n    if (this.useVercelSDK) {\n      this.anthropicProvider = createAnthropic({\n        apiKey: config.apiKey ?? \"\",\n        baseURL:\n          this.apiBase !== \"https://api.anthropic.com/v1/\"\n            ? this.apiBase\n            : undefined,\n        fetch: hasRequestOptions\n          ? customFetch(config.requestOptions)\n          : undefined,\n      });\n    }\n  }\n\n  async *chatCompletionStream(body, signal) {\n    // Check for tool messages in history\n    const hasToolMessages = body.messages.some((msg) => msg.role === \"tool\");\n\n    if (this.useVercelSDK && this.anthropicProvider && !hasToolMessages) {\n      const vercelMessages = convertOpenAIMessagesToVercel(body.messages);\n      const model = this.anthropicProvider(body.model);\n      const vercelTools = convertToolsToVercelFormat(body.tools);\n\n      const stream = await streamText({\n        model,\n        messages: vercelMessages,\n        tools: vercelTools,\n        // ... other parameters\n      });\n\n      for await (const chunk of convertVercelStream(stream.fullStream, {\n        model: body.model,\n      })) {\n        yield chunk;\n      }\n      return;\n    }\n\n    // Fall back to original implementation\n    const response = await customFetch(config.requestOptions)(\n      new URL(\"messages\", this.apiBase),\n      { method: \"POST\", body: JSON.stringify(this._convertBody(body)), signal },\n    );\n    yield* this.handleStreamResponse(response, body.model);\n  }\n}\n```\n\n## What's Preserved\n\n### 100% Backward Compatibility\n\n- ‚úÖ Same `BaseLlmApi` interface\n- ‚úÖ Same input/output formats\n- ‚úÖ All existing tests pass (191 tests)\n- ‚úÖ Zero consumer code changes\n- ‚úÖ Custom fetch (headers, proxy, SSL, timeout)\n- ‚úÖ Request options infrastructure\n- ‚úÖ Provider factory logic\n- ‚úÖ Error handling\n- ‚úÖ Usage tracking (token counts)\n\n### What Still Uses Original Implementation\n\n- FIM completion (not supported by Vercel SDK)\n- Reranking (not supported by Vercel SDK)\n- Embeddings (not supported by Vercel SDK)\n- OpenAI responses endpoint (for o1/o3 models)\n- Anthropic multi-turn tool conversations (lifecycle managed by Vercel SDK)\n\n## Benefits\n\n### Maintenance Reduction\n\n**Offloaded to Vercel (~60%):**\n\n- ‚úÖ SSE parsing and chunk buffering\n- ‚úÖ Provider API versioning\n- ‚úÖ Tool call serialization\n- ‚úÖ Provider-specific error codes\n- ‚úÖ Type safety maintenance\n- ‚úÖ Streaming protocol handling\n- ‚úÖ Backpressure management\n\n**Still Maintained (~40%):**\n\n- Format translation (simplified by Vercel SDK's normalized interface)\n- Custom features (responses endpoint, FIM, reranking)\n- RequestOptions infrastructure\n- Provider factory logic\n\n### Code Quality\n\n**Immediate:**\n\n- Shared utilities eliminate duplication\n- Better test coverage (57 new tests)\n- Cleaner separation of concerns\n\n**After Full Rollout (~300 LOC reduction):**\n\n- Complete elimination of SSE parsing\n- No more provider-specific chunking\n- Simplified error handling\n- Reduced type maintenance\n\n### Rollback Strategy\n\n**Immediate (< 1 minute):**\n\n```bash\nUSE_VERCEL_AI_SDK_OPENAI=false cn\n```\n\n**Short-term (< 1 hour):**\n\n```json\n\"@continuedev/openai-adapters\": \"1.31.0\"\n```\n\n**Long-term:**\nRevert commits (feature flags preserve old code)\n\n## Testing\n\n### Run All Tests\n\n```bash\nnpm test\n```\n\n### Test with Vercel SDK Enabled\n\n```bash\nUSE_VERCEL_AI_SDK_OPENAI=true cn\nUSE_VERCEL_AI_SDK_ANTHROPIC=true cn\n```\n\n### Test Scenarios\n\n1. **Basic chat** - Simple questions, streaming responses\n2. **Tool calls** - First turn tool usage\n3. **Multi-turn tools** - Conversation with tool results (Anthropic falls back)\n4. **Custom options** - Proxy, headers, SSL certificates\n5. **Error handling** - Invalid API key, network errors\n6. **Responses endpoint** - o1/o3 models (OpenAI falls back)\n\nAll scenarios should work identically with flags enabled/disabled.\n\n## Known Limitations\n\n### Anthropic Multi-turn Tool Conversations\n\n**Behavior:** Automatically falls back to original implementation when `role: \"tool\"` messages exist in history.\n\n**Why:** Vercel AI SDK manages tool call lifecycle internally and doesn't support resuming from pre-existing tool conversations.\n\n**Impact:** Minimal - tool calls still work, just uses original implementation for subsequent turns.\n\n### OpenAI Responses Endpoint\n\n**Behavior:** o1/o3 models bypass Vercel SDK and use responses endpoint.\n\n**Why:** These models have unique message handling requirements.\n\n**Impact:** None - automatic fallback is transparent.\n\n### Not Yet Migrated\n\n- Other 48 providers (Gemini, Mistral, Azure, Bedrock, etc.)\n- FIM completion\n- Reranking\n- Embeddings\n\n## Future Work\n\n### High Priority\n\n1. Migrate additional providers (Gemini, Mistral)\n2. Performance monitoring and optimization\n3. Enhanced logging for fallback decisions\n\n### Medium Priority\n\n4. Investigate Anthropic prompt caching with Vercel SDK\n5. Verify vision/image support with Vercel SDK\n6. Add metrics collection\n\n### Low Priority\n\n7. Migrate remaining providers\n8. Add inline documentation\n9. Create usage examples\n\n## Summary\n\nThe Vercel AI SDK integration:\n\n- **Reduces maintenance burden by ~60%**\n- **Preserves 100% backward compatibility**\n- **Enables easy rollback at any point**\n- **Improves code quality and test coverage**\n- **Benefits both CLI and Core IDE**\n- **No consumer code changes required**\n\nThe implementation is production-ready and awaiting Phase 3 validation testing.\n"}
{"source":"github","repo":"continue","path":"packages/config-yaml/CHANGELOG.md","content":"## [1.0.95](https://github.com/continuedev/continue/compare/@continuedev/config-yaml@1.0.94...@continuedev/config-yaml@1.0.95) (2025-06-22)\n\n### Bug Fixes\n\n- avoid minor ([7186c04](https://github.com/continuedev/continue/commit/7186c04418abd61f151d25bf03733f2f5d371b48))\n\n# 1.0.0 (2025-06-22)\n\n### Bug Fixes\n\n- :adhesive_bandage: allow GGML to use api.openai.com ([db19f6b](https://github.com/continuedev/continue/commit/db19f6bc98285d8ea45b4db16f619dffbec7c3db))\n- :adhesive_bandage: skip indexing really large files ([b773fe6](https://github.com/continuedev/continue/commit/b773fe6d7a0b489a658139ea5fc958abd46a20b2))\n- :ambulance: catch error from meilisearch client.health ([00775f5](https://github.com/continuedev/continue/commit/00775f54e6c3fa8044a996ea1a7cf0f2205735dd))\n- :ambulance: class_name hotfix ([83b0417](https://github.com/continuedev/continue/commit/83b0417f6c8c579d0ea5a0f689eceb822fe7a04d))\n- :ambulance: fix import of run from **main** ([ebfe428](https://github.com/continuedev/continue/commit/ebfe428b7f70de66bc5692cca1db7cd10ef4b997))\n- :ambulance: hotfix and package.json seo experiment ([42024ef](https://github.com/continuedev/continue/commit/42024effba73673b4080c25806c21293b5daad3e))\n- :ambulance: load global ~/.continue/assistants ([4b55cfb](https://github.com/continuedev/continue/commit/4b55cfb2e8ad4803855020111d0cff6a38ad79d5))\n- :ambulance: logging to file causing problems with starting server ([8b95ef7](https://github.com/continuedev/continue/commit/8b95ef7de258de8498b328d9e6107a95f57f8d2c))\n- :ambulance: specify packagePath for vsix ([512ccfd](https://github.com/continuedev/continue/commit/512ccfda670abb6132e9cd720280a472e53e3326))\n- :arrow_up: upgrade openai python package ([19cbe2c](https://github.com/continuedev/continue/commit/19cbe2cebae8e2155b6b4375c6a96a3b25e87615))\n- :art: many small improvements ([28f5d7b](https://github.com/continuedev/continue/commit/28f5d7bedab05a8b061e4e7ee9055a5403786bbc))\n- :bookmark: update extension version ([05b9642](https://github.com/continuedev/continue/commit/05b96420bda2da0c725cacc8141d87449eaf9e9c))\n- :bookmark: update version ([afae160](https://github.com/continuedev/continue/commit/afae1600255714d0a4f18f892d3e7b5e1d921962))\n- :bookmark: update version ([9aee2cc](https://github.com/continuedev/continue/commit/9aee2cc44c461ce0e001185af85352e78522bab5))\n- :bookmark: update version to try again ([1905f31](https://github.com/continuedev/continue/commit/1905f319470c02ee414498b9101b6e64b4b15d65))\n- :bookmark: v3 -> v4 of upload-artifact ([d24862a](https://github.com/continuedev/continue/commit/d24862a6fd22e4eac1b2ca27ce7bf029f0d8fa4a))\n- :bug: a few minor fixes ([c918bb3](https://github.com/continuedev/continue/commit/c918bb3af5ec4a4a409eb0a3add27951b00c3c59))\n- :bug: a handful of bug fixes ([e1325c0](https://github.com/continuedev/continue/commit/e1325c0153becb95b454810d9461efd7d3624a6a))\n- :bug: a number of small fixes + disable summaries ([a975560](https://github.com/continuedev/continue/commit/a9755603c3a2c0b3afe809f77a63824c77c6419e))\n- :bug: access highlighted_code through context_manager ([1afb37b](https://github.com/continuedev/continue/commit/1afb37b5bb901d95c493039591b9243cd2cdd6f7))\n- :bug: add data file for ca_bundle ([b82d83f](https://github.com/continuedev/continue/commit/b82d83f79389897ed5f05eb9b5e8daf9cf64ee6f))\n- :bug: Add requestOptions to YAML config for mcp ([81c20c1](https://github.com/continuedev/continue/commit/81c20c11dbbce2eccefd00364c5b74b298b2f24f))\n- :bug: add server/exe to .vscodeignore insteading of manually removing ([e8ebff1](https://github.com/continuedev/continue/commit/e8ebff1e6b07dfaafff81ee7013bb019cbfe2075))\n- :bug: additional fixes to ssh /edit ([4428acd](https://github.com/continuedev/continue/commit/4428acdd6f372c3724a908fafb1c793e0eae4096))\n- :bug: allow end/home keys to work ([615d30e](https://github.com/continuedev/continue/commit/615d30e3dce92a9993b0e93b044faadf228529b1))\n- :bug: allow None for timeout ([ff3de11](https://github.com/continuedev/continue/commit/ff3de1184737f1124090d384b877a30550b60869))\n- :bug: another hotfix - don't destructure selectors ([534304a](https://github.com/continuedev/continue/commit/534304a2a4f9abfc221a961f279d1b43d14b6d33))\n- :bug: another windows fix in typegen.js ([f38c8fb](https://github.com/continuedev/continue/commit/f38c8fb8b33a705ed4eb4d2e0974060ebb88afd3))\n- :bug: async with Client (meilisearch) ([9a0cd64](https://github.com/continuedev/continue/commit/9a0cd644dcb5ff46817a6ea686a6de0fb764c960))\n- :bug: attempting to fix mkdir ([c1a8097](https://github.com/continuedev/continue/commit/c1a8097f0a7f3cddb0aebac26e6197ffef186972))\n- :bug: automigrate between short/long imports ([eecc2b5](https://github.com/continuedev/continue/commit/eecc2b57c5c5a144abfc0623102438e902c4aeba))\n- :bug: avoid removing disallowed file windows ([19a3266](https://github.com/continuedev/continue/commit/19a3266b6f14186bd0839fac8b2a04b5a29f32e7))\n- :bug: bug when highlighting code prior to context_manager creation ([74a52c8](https://github.com/continuedev/continue/commit/74a52c8399b3ccf2d2100b088b79e65c6ca6ad7e))\n- :bug: bug where old server doesn't get updated ([bb776a0](https://github.com/continuedev/continue/commit/bb776a03df3e6a39a1726b781ea33c2ccebd5343))\n- :bug: catch error when workspace uri isn't defined ([fc9eb30](https://github.com/continuedev/continue/commit/fc9eb3051fd5a7c9cad57b5d6cd93374bd8210fb))\n- :bug: change for/backwardslash decoding scheme ([a3a05fe](https://github.com/continuedev/continue/commit/a3a05fee312ad7c04d2abb0e186da55c7d061462))\n- :bug: chmod for linux as well as mac ([089def0](https://github.com/continuedev/continue/commit/089def08c58120f78df78c10027639802ad8f77d))\n- :bug: clear all other selector destrucuring ([145642f](https://github.com/continuedev/continue/commit/145642f1eaf01d5809dabd79e7f64f234124683e))\n- :bug: Codebase Indexing was not starting on load ([e391d58](https://github.com/continuedev/continue/commit/e391d583041c54edb3fa0836eb9186c61e6b063d))\n- :bug: compatibility with python 3.8 ([275ad6f](https://github.com/continuedev/continue/commit/275ad6f72dafdfacffd9c9b5cc4847135a30f425))\n- :bug: convert to correct path sep in wsl URIs ([1b2341a](https://github.com/continuedev/continue/commit/1b2341a0113fadf8c8d23097ef1041d3e3088e84))\n- :bug: correct path sep for ssh-remote files ([b9bd8c1](https://github.com/continuedev/continue/commit/b9bd8c1848eaf38d5d15694a1ecae67f14566214))\n- :bug: correction to ContinueConfig serialization model ([b8aba4b](https://github.com/continuedev/continue/commit/b8aba4bc96d3b064012a40d837d5191cae20037e))\n- :bug: correctly generate uris for remote ([ab31cb1](https://github.com/continuedev/continue/commit/ab31cb15fae74592f49c2ceadc8d7810228fa7e2))\n- :bug: ctrl+c for windows overriding copy ([c3925c0](https://github.com/continuedev/continue/commit/c3925c04d981d2abc1e21cf72d6e77d165420a73))\n- :bug: custom escaping instead of URI for diff paths ([da3970e](https://github.com/continuedev/continue/commit/da3970e00061b7a223d23f51bd53012666d324dc))\n- :bug: default to counting chars if tiktoken blocked ([7006dbb](https://github.com/continuedev/continue/commit/7006dbb3e38a837a2580a516791874f6815ac25f))\n- :bug: don't fail on disconnected websocket ([0876610](https://github.com/continuedev/continue/commit/08766100cdb3638b3300ae4b700f8ec2af6b9a8a))\n- :bug: don't log stdout to console ([ee4701d](https://github.com/continuedev/continue/commit/ee4701dc45cd540728302ca8a09e9b7ce842597f))\n- :bug: don't open continue automatically ([8b76f51](https://github.com/continuedev/continue/commit/8b76f518313c20f13dda605931c9929ef58a7a22))\n- :bug: don't override context length param in OpenAI ([b2a6d07](https://github.com/continuedev/continue/commit/b2a6d07ea99be1f9288ee21477edc0874e780cad))\n- :bug: ebusy and logging bug fixes ([3d61469](https://github.com/continuedev/continue/commit/3d614690cd825ac5580074ecdc22f660455204f1))\n- :bug: fix \"code\" keyerror prior to context_manager.start ([866b16c](https://github.com/continuedev/continue/commit/866b16c3a9c9d88a7b90aa8a43610fc4884ab123))\n- :bug: fix /edit in ssh, pinyin input in combobox ([cda1be4](https://github.com/continuedev/continue/commit/cda1be46625abd8f44962cceeded04c8c47d9f65))\n- :bug: fix >c_d.png file path ([a9bc4e2](https://github.com/continuedev/continue/commit/a9bc4e26263faef8598dd8aa2aec7949c75ab70c))\n- :bug: fix 2 model config bugs ([b144d21](https://github.com/continuedev/continue/commit/b144d21b48a94aa8c203469eb7667bd22fc4e243))\n- :bug: fix 404 from undefined gif ([b467371](https://github.com/continuedev/continue/commit/b4673712e1a6a5b435125004a9b51498207fb7b6))\n- :bug: fix automigration ([ec41f55](https://github.com/continuedev/continue/commit/ec41f553c24d5f4b5bc4e601c989b1936d67ae1a))\n- :bug: fix azure openai bug for 07 version ([a8e69a0](https://github.com/continuedev/continue/commit/a8e69a02e6897689a1727fb7542ed5684b1348e2))\n- :bug: fix broken docs link ([210a02e](https://github.com/continuedev/continue/commit/210a02ef02341a98b4ed18095b2d656a7b994bd9))\n- :bug: fix bugs when selecting code to edit ([fa34214](https://github.com/continuedev/continue/commit/fa34214012d14385d231a1ac4f16006aaf4331fb))\n- :bug: fix ci to only upload from linux x64, not alpine ([4e1a5b1](https://github.com/continuedev/continue/commit/4e1a5b1fb3f96edc95b0938265da980e98566d56))\n- :bug: fix cmd+m bug ([38e8272](https://github.com/continuedev/continue/commit/38e827243ceff3732cd0f260e7a3bd4941a96bc5))\n- :bug: fix command enter, stop streaming on reject ([8e15ec3](https://github.com/continuedev/continue/commit/8e15ec3c2c1490d4a7d6371f877368376fd64e8a))\n- :bug: fix command enter, stop streaming on reject ([19b3886](https://github.com/continuedev/continue/commit/19b38863c21656526e0729776682430e0fa277da))\n- :bug: fix config.py import paths ([97861bf](https://github.com/continuedev/continue/commit/97861bf4117bbc36f8f87797a9ca60e6336f82cc))\n- :bug: fix context length bug for /edit ([d103263](https://github.com/continuedev/continue/commit/d103263030ad52debe73bd131c71bbf17f545956))\n- :bug: fix dialog links ([4c84e69](https://github.com/continuedev/continue/commit/4c84e6945a7c2018622eceb54e7fb54de193b03a))\n- :bug: fix for --meilisearch-url flag ([e2798c5](https://github.com/continuedev/continue/commit/e2798c5bb62eeb2a3bc8f5baee18f9d64ee86563))\n- :bug: fix for Azure OpenAI model names ([bcec2a0](https://github.com/continuedev/continue/commit/bcec2a0870d0ef649961b6c91ec866b612680b9e))\n- :bug: fix for edit=None in highlightedCode update ([247d3e9](https://github.com/continuedev/continue/commit/247d3e9a41ff8d9fe2da6386bfb0d0eb063b071c))\n- :bug: fix for lmstudio defaults ([0012922](https://github.com/continuedev/continue/commit/00129229cd881d6b910a4b01db68e702cdd63a40))\n- :bug: fix for windows drive difference bug ([d69c6d4](https://github.com/continuedev/continue/commit/d69c6d4f3729374ab40fcebc861e67f2da100ad9))\n- :bug: fix ggml bug ([1a75475](https://github.com/continuedev/continue/commit/1a75475c681053494984664ef1179171fe2a5d83))\n- :bug: fix headers for openai.;y ([44fe0c9](https://github.com/continuedev/continue/commit/44fe0c94a55a753ff5d6c3da6b63db4a5c70d780))\n- :bug: fix height bug after cmd+shift+R ([a7cb092](https://github.com/continuedev/continue/commit/a7cb0929bd064f73a1e3e49ba8dd6b6b7de387f4))\n- :bug: fix history.timeline indexing bug ([01ed2c7](https://github.com/continuedev/continue/commit/01ed2c7eb2d3417b2c190eea105008372f49a7c6))\n- :bug: fix huggingface tgi ([5316180](https://github.com/continuedev/continue/commit/5316180394d48d9877cda0cb3d7c3c6de9995d12))\n- :bug: fix import in run.py ([4cf1f75](https://github.com/continuedev/continue/commit/4cf1f75518053f9df174d5ab90c426124f85ecfa))\n- :bug: fix inability to copy/paste when ipynb is open ([850c8ae](https://github.com/continuedev/continue/commit/850c8aea7f3d9c46ff8e98bde936b92282376dae))\n- :bug: fix incorrect imports in default config file ([374bdd0](https://github.com/continuedev/continue/commit/374bdd037792825bf984026da12d4100ffebcac2))\n- :bug: fix keyboard shortcut for debugging ([c021958](https://github.com/continuedev/continue/commit/c021958ae893a9683352ba99e5c6301e38331492))\n- :bug: fix meilisearch empty body content-type bug ([598e243](https://github.com/continuedev/continue/commit/598e243fd292dd8851865ab1c3915ca55f4992cc))\n- :bug: fix missing path import ([5bfe68e](https://github.com/continuedev/continue/commit/5bfe68ea7f7e90e3cb1c3101360cf959b336a857))\n- :bug: fix model changing bug ([fd4a4dc](https://github.com/continuedev/continue/commit/fd4a4dcf004bea86d982ffffb66b4e3cb38193a6))\n- :bug: fix overriding of system message ([8444e76](https://github.com/continuedev/continue/commit/8444e76b7232fbddb62d3626de13653ae332d168))\n- :bug: fix paths ([b893c95](https://github.com/continuedev/continue/commit/b893c956fe75a9e45f06129290d043737f5c1007))\n- :bug: fix reducers for user input queue ([1a36a3c](https://github.com/continuedev/continue/commit/1a36a3c02acaf6bf29d4153c113217517b832942))\n- :bug: fix replicate to work with models requiring prompt input ([84ec574](https://github.com/continuedev/continue/commit/84ec574e182ec441e95d13c3543a934e0a036228))\n- :bug: fix serialization bug for context_providers ([2799249](https://github.com/continuedev/continue/commit/27992499af977baeb9124d9ab35ffec6d36a298a))\n- :bug: fix set_system_message ([084fdac](https://github.com/continuedev/continue/commit/084fdac3992f58dcf11241e7e5c2d5efa784ce0d))\n- :bug: fix ssh /edit by checking for file through vscode fs ([417d45c](https://github.com/continuedev/continue/commit/417d45ccddc2f434d7467e4f17113783996653dd))\n- :bug: fix telemetry bug ([042bc5a](https://github.com/continuedev/continue/commit/042bc5ac76800ee66e603ef23b2bb857fafe053e))\n- :bug: Fix the generating animation ([2f402b4](https://github.com/continuedev/continue/commit/2f402b4a1227ade5a4ba70f770974627b586e930))\n- :bug: fix timeout type ([e1a0290](https://github.com/continuedev/continue/commit/e1a0290d5a699e30464f1e682cb11c6aa119bd59))\n- :bug: fix togetherAI model json parsing ([deb291c](https://github.com/continuedev/continue/commit/deb291c1b225425cba543dd3b4c5557089abfb59))\n- :bug: fix undefined.filter bug ([9b58278](https://github.com/continuedev/continue/commit/9b582781ab0aceaaf1cff7432fed92fa6c205aae))\n- :bug: fix usages of LLM.complete ([f057ee4](https://github.com/continuedev/continue/commit/f057ee4d619b834dc245065d13417a86b44dc61b))\n- :bug: fix when multiple cursor ranges are selected ([f9c145c](https://github.com/continuedev/continue/commit/f9c145c9667e0cd9adb7f9b645f7abf12f7cf2a2))\n- :bug: fix yaml syntax error ([11c7cec](https://github.com/continuedev/continue/commit/11c7cecc107ef9f2571926055dbd80495fe0f8b2))\n- :bug: fixes for a few context_providers ([41b3233](https://github.com/continuedev/continue/commit/41b3233693c34cd81c872a1e7279721b5f640d60))\n- :bug: fixes to templating messages ([c56e24d](https://github.com/continuedev/continue/commit/c56e24d2a5f2b40702e4b495fa3f28d554eaa3ab))\n- :bug: fixing bugs with ggml ([87409c3](https://github.com/continuedev/continue/commit/87409c31832ccb707abbf134843323c9eb6e1183))\n- :bug: fixing issues with creating markdown files ([5c1c2d6](https://github.com/continuedev/continue/commit/5c1c2d626ffed786d00c79aadef26fa5718ca43d))\n- :bug: fixing small UI details ([088b7b8](https://github.com/continuedev/continue/commit/088b7b803866817aaedce6b61834f1ce5de7a7c2))\n- :bug: force kill old server with taskkill on windows ([b1b7d13](https://github.com/continuedev/continue/commit/b1b7d13dbf5b9f6ada28a5ef22ea6857d3b0bcb6))\n- :bug: ftc fix ([f5f10ef](https://github.com/continuedev/continue/commit/f5f10efee3402e117c34b6f0de4bf2fd7d2819c1))\n- :bug: gpt-4-32k in CHAT_MODELS ([b0445cd](https://github.com/continuedev/continue/commit/b0445cd5fc4538c8a9c4f3e76be0f3d724c99818))\n- :bug: handle when vscode workspace not open ([73c6827](https://github.com/continuedev/continue/commit/73c6827d02ff62313184e3745fd94c7591c98b61))\n- :bug: hotfix for user_input_queue.map ([610c576](https://github.com/continuedev/continue/commit/610c576cc9df72716c5e65838f805b15431011ea))\n- :bug: install python-virtualenv on linux, fix git hash files error ([6f0e634](https://github.com/continuedev/continue/commit/6f0e6340bb22ee150ef4b7996750f4c63c0bc2a7))\n- :bug: install python-virtualenv on linux, fix git hash files error ([7fa98ff](https://github.com/continuedev/continue/commit/7fa98ffe843320ddc63794a497a2d44570e005c3))\n- :bug: kill server before trying to delete exe on windows ([286fb0e](https://github.com/continuedev/continue/commit/286fb0e20e48859f129ccf568d03248805bcbc61))\n- :bug: let context providers work without meilisearch ([0f86a69](https://github.com/continuedev/continue/commit/0f86a69e4a83458db2e20e404c26dac2e02355cf))\n- :bug: llamacpp fix indexing max_tokens ([90590ab](https://github.com/continuedev/continue/commit/90590ab4e06fbc3fa721f73a4a922136946a756f))\n- :bug: make sure server_version.txt exists ([17806d9](https://github.com/continuedev/continue/commit/17806d932502adbf974ccd93a670e57b78be9a08))\n- :bug: make typegen.js windows compatible ([dc06228](https://github.com/continuedev/continue/commit/dc0622848b648ba27e7110b9b900673bb668ab4c))\n- :bug: MAX_TOKENS_FOR_MODEL bug fix, more testing ([1c288f7](https://github.com/continuedev/continue/commit/1c288f7749747c6b1908ae16c977f80e5597d2ca))\n- :bug: meilisearch fixes ([0de6e19](https://github.com/continuedev/continue/commit/0de6e1985d0e97ede5e19e7752a6be7cd2a5818d))\n- :bug: more reliable download with request ([fdb036b](https://github.com/continuedev/continue/commit/fdb036bcecec891adaf99d73101c458fc4087406))\n- :bug: more reliable setup of meilisearch ([12a8ae1](https://github.com/continuedev/continue/commit/12a8ae1c47f111b9f36633c96b26e8642c5ff223))\n- :bug: now progress bar when api_key entered ([bf82c6f](https://github.com/continuedev/continue/commit/bf82c6fd16a6777f0a9bb68ce4879d7bab9019bb))\n- :bug: number of bug fixes ([b9bdf58](https://github.com/continuedev/continue/commit/b9bdf5894c1c68b60d1919ae07b0f5909b00dec2))\n- :bug: numerous small fixes ([0940d75](https://github.com/continuedev/continue/commit/0940d756dec3b98071ae5e5a12966e02420b3cd2))\n- :bug: patch for ocassional 0 choices from older azure versions ([5c09b80](https://github.com/continuedev/continue/commit/5c09b8077588a447d6eaac9b7f624571be3ddb1d))\n- :bug: permissions for pypi-deployment step ([b237850](https://github.com/continuedev/continue/commit/b237850c4b64435e26dfb5f12275a16a93e556a8))\n- :bug: post-merge fixes ([96379a7](https://github.com/continuedev/continue/commit/96379a7bf5b576a2338142b10932d98cbc865d59))\n- :bug: remove empty grammar from llama_cpp_args ([e5bbe3b](https://github.com/continuedev/continue/commit/e5bbe3bc4d59b6f35db1ce1b94be14244c11c766))\n- :bug: replace hardcoded path for config file ([4fe9ace](https://github.com/continuedev/continue/commit/4fe9ace518bcdcf79999ce9938ba01b218d355e4))\n- :bug: require socksio ([bba5e5e](https://github.com/continuedev/continue/commit/bba5e5e5b1da2dd924aa2632e38d4bb702bbbdd9))\n- :bug: separately load ctx provs, fix filetree ([d8e821e](https://github.com/continuedev/continue/commit/d8e821e422678fd4248b472c7f3e67a32ecfefb5))\n- :bug: set api_keys in config.py, fix spawn error handling ([6823307](https://github.com/continuedev/continue/commit/68233071dd0d97a353a66fe5627d69f97a389ca8))\n- :bug: set export display in same step as linux npm test ([c3d62c5](https://github.com/continuedev/continue/commit/c3d62c5ae203aaca32583f75a7e80dfd9f196e11))\n- :bug: small bug fix ([32d1149](https://github.com/continuedev/continue/commit/32d1149692c26eb966693f03db6d9cf496ba57a4))\n- :bug: small bug fixes ([bc75ff2](https://github.com/continuedev/continue/commit/bc75ff294a2b5ec5eef5f77aff72aaa0c7f4a3f2))\n- :bug: small fixes, update troubleshooting docs ([51fc07c](https://github.com/continuedev/continue/commit/51fc07cf6441d6330ce64e45e56e8f333ca309ed))\n- :bug: solve EBUSY by polling ([9417973](https://github.com/continuedev/continue/commit/941797359f6554ac16a2e478047aabd5cbc0404b))\n- :bug: ssh compatibility by reading from vscode.workspace.fs ([e5f5630](https://github.com/continuedev/continue/commit/e5f56308c5fd87695278682b2a36ca60df0db863))\n- :bug: start meilisearch in parallel to server ([e4c1bb4](https://github.com/continuedev/continue/commit/e4c1bb4bedbe426d090f4bb2b8819ad935c5b3fb))\n- :bug: stop streaming on rejection ([8d05fc2](https://github.com/continuedev/continue/commit/8d05fc2bb5c5df617800c1abcf43bb03c574482f))\n- :bug: stop streaming on rejection ([9fc831e](https://github.com/continuedev/continue/commit/9fc831e7587cce99c8a6f2e56905c25068c8cab6))\n- :bug: streaming url_decode for Ollama ([3690101](https://github.com/continuedev/continue/commit/3690101b790f91c749f208693aaffc00b9fa2a42))\n- :bug: templating fix for queued LLM ([5c6609a](https://github.com/continuedev/continue/commit/5c6609ab5fa3a69cd0e3e8e61df643fcce1ecb47))\n- :bug: temporarily disable lsp before fixing w/ vscode ([98f340b](https://github.com/continuedev/continue/commit/98f340bd97cba6f30cfe55d47419e3925b9dc679))\n- :bug: temporarily remove replicate altogether ([bd79c00](https://github.com/continuedev/continue/commit/bd79c00e7790b92cfd8b8c8f8211b6c3d36e33a2))\n- :bug: test and fix small issues with GGML ([72e8332](https://github.com/continuedev/continue/commit/72e83325a8eb5032c448a5e891c157987921ced2))\n- :bug: timeout on blocking processes ([345b773](https://github.com/continuedev/continue/commit/345b7734d8c887d699d5038416d2a1f8193a33e9))\n- :bug: traceback fixes, remove replicate from hiddenimports ([45d9bab](https://github.com/continuedev/continue/commit/45d9bab5cea745573be7112d7130089c596c88fa))\n- :bug: try/except around starting meilisearch ([c867cd4](https://github.com/continuedev/continue/commit/c867cd40342d44901cf5277ded25f5dc5aaa4326))\n- :bug: update search path for ripgrep on windows ([e428dc5](https://github.com/continuedev/continue/commit/e428dc53cedf54f394a7cddfe8a7ce7fbf469bb9))\n- :bug: update the tip message for keyboard shortcut ([3558450](https://github.com/continuedev/continue/commit/355845002e178a618e9a792dd57b0649c3da8845))\n- :bug: urldecode ollama responses, make edit faster ([19050f8](https://github.com/continuedev/continue/commit/19050f83228b3e7f08a6aacd5bdd1804a8315e4a))\n- :bug: use certifi to set ca_bundle_path for openai ([3849420](https://github.com/continuedev/continue/commit/3849420948e491d5f84ac485169165d887751fd3))\n- :bug: use posthog-node, not -js ([88a8166](https://github.com/continuedev/continue/commit/88a8166476d38889fd4f9323472cc34a5226e05c))\n- :bug: use powershell remove-item ([64552dc](https://github.com/continuedev/continue/commit/64552dc881509c46aa14253ff94aee9d86ade256))\n- :bug: use windows equivalent of rm -rf ([8d19866](https://github.com/continuedev/continue/commit/8d198663e116c7c77b7e59015bc6032736f71f6e))\n- :bug: verify_ssl and ssl_context mutual exclusivity ([59b7453](https://github.com/continuedev/continue/commit/59b7453afed06418d4c171b65370a6a82f5a9221))\n- :bug: version patch in the publish step ([1936f72](https://github.com/continuedev/continue/commit/1936f725d226bea2e13d5d88c1dd7a9a02ddd259))\n- :bug: windowsHide on process spawn ([c3d31f0](https://github.com/continuedev/continue/commit/c3d31f00bb589df1c83308b7d9d69ed51c31341a))\n- :bug: write out npm run package as package.js ([4636c95](https://github.com/continuedev/continue/commit/4636c9590154d6b5995948003da212eb25003750))\n- :bug: write to local diff files ([6140d05](https://github.com/continuedev/continue/commit/6140d05e7d415d3334032c300ed593bdd181f7f5))\n- :children_crossing: add slash commands to default config ([58e5dc4](https://github.com/continuedev/continue/commit/58e5dc4a5c4fcbed25170b61fbd88d479c5aebcf))\n- :children_crossing: clear the dropdown after text input cleared ([23167a5](https://github.com/continuedev/continue/commit/23167a51d959fed5e4be057ceb9fff50cf34c6c8))\n- :children_crossing: don't order meilisearch results by contnet ([ab7a90a](https://github.com/continuedev/continue/commit/ab7a90a0972188dcc7b8c28b1263c918776ca19d))\n- :children_crossing: use default model in default config.py ([1bc5777](https://github.com/continuedev/continue/commit/1bc5777ed168e47e2ef2ab1b33eecf6cbd170a61))\n- :construction_worker: copy_metadata for replicate in run.spec ([b0426d8](https://github.com/continuedev/continue/commit/b0426d82a4871e9081367ad4e977b22f42db5a89))\n- :construction: working on fixing lsp ([1f95bb2](https://github.com/continuedev/continue/commit/1f95bb287846fc0501193d642420b574d9900857))\n- :fire: remove version from package.json ([27c0a40](https://github.com/continuedev/continue/commit/27c0a403de28345cd03c39ad46c02f68ff57b3a1))\n- :goal_net: catch errors when loading to meilisearch index ([7894c8e](https://github.com/continuedev/continue/commit/7894c8ed1517394aa00f6e496a97d9e27d204f5f))\n- :goal_net: display errors in SimpleChatStep ([72784f6](https://github.com/continuedev/continue/commit/72784f6f1161f0c5b647889c26089a8247111dc9))\n- :green_heart: cd extension before packaging ([0a6d72c](https://github.com/continuedev/continue/commit/0a6d72c099316e6cffa123d3ffa915f3fe13e770))\n- :green_heart: cleanup file ([951552d](https://github.com/continuedev/continue/commit/951552dd0eede1f8f255aeaf5d34a13ff0c7bfb7))\n- :green_heart: don't exclude jedi from pyinstaller ([a1328cb](https://github.com/continuedev/continue/commit/a1328cb5431f99cfe16b246ee4201b19530404e2))\n- :green_heart: fix build scripts ([ab799b0](https://github.com/continuedev/continue/commit/ab799b0b0133e926ea06a1a12c092f42b9e053a1))\n- :green_heart: fix copy statement to include.exe for windows ([36c3dfd](https://github.com/continuedev/continue/commit/36c3dfd51d319b9b9ad392988d13ef7f443e0937))\n- :green_heart: fix preview.yaml ([a736d62](https://github.com/continuedev/continue/commit/a736d62f0e8b9b80ab9a949fd1739fb9a3be26e1))\n- :green_heart: increase testing timeout to allow for fkill ([08b1cfd](https://github.com/continuedev/continue/commit/08b1cfdd2f6f456df7344c16f5d229a0ccfb841b))\n- :green_heart: install rosetta ([5b9ef10](https://github.com/continuedev/continue/commit/5b9ef102973c608bc409a7b9ec244a4be1494e96))\n- :green_heart: one last test ([758520f](https://github.com/continuedev/continue/commit/758520fe6b59d3330dec80ac07d05282d36e0058))\n- :green_heart: only upload once per binary ([490838a](https://github.com/continuedev/continue/commit/490838a8ad920a52ada7e85675aefd965e978d77))\n- :green_heart: package patch ([34f32ed](https://github.com/continuedev/continue/commit/34f32ed5f71055ea11d4332f18e77ceba5849631))\n- :green_heart: package:pre-release ([d3f21da](https://github.com/continuedev/continue/commit/d3f21da803e36b78f968c2216d9f93f90ebabd6a))\n- :green_heart: publish as pre-release! ([831bf5e](https://github.com/continuedev/continue/commit/831bf5e7f7c48dd80f71f1256f5597bd47bf22de))\n- :green_heart: publishing to depend on ALL tests ([a131c17](https://github.com/continuedev/continue/commit/a131c17326591e67a68faf6f96371ad8fc332b71))\n- :green_heart: pull origin main in main.yaml after pypi update ([6a9a079](https://github.com/continuedev/continue/commit/6a9a079914d94419183182cd0a5cc4439f2101ad))\n- :green_heart: remove \"patch\" from vsce publish command ([d8327ec](https://github.com/continuedev/continue/commit/d8327ec6f82058479bd294bfcdccaf3c2b54de0a))\n- :green_heart: remove npm_config_arch ([8c51664](https://github.com/continuedev/continue/commit/8c5166471b0d83b924d8bee1e0ca51822cc1bbdc))\n- :green_heart: remove version from apckage.json ([5438ce9](https://github.com/continuedev/continue/commit/5438ce94406baa0f7d131ecacadefc72912dca0d))\n- :green_heart: set permissions on apple silicon binary ([715cfed](https://github.com/continuedev/continue/commit/715cfed18747b6bc2e6d7bd7a977d249cc9066d5))\n- :green_heart: testing ([14062c5](https://github.com/continuedev/continue/commit/14062c5c385bb7ba80096bf7daf6b6a5568b0b54))\n- :green_heart: testing for failure to package dist in vsix ([19acf3b](https://github.com/continuedev/continue/commit/19acf3bb36c1e44274297c806b89b589ca02f5ba))\n- :green_heart: update permissions and version ([c4ed41c](https://github.com/continuedev/continue/commit/c4ed41c861573f4c9bdff1a21ca3e056cfdd766e))\n- :green_heart: update pylsp hidden import ([1e8ea65](https://github.com/continuedev/continue/commit/1e8ea654f5ad1e06bff2660b54a50955098703ba))\n- :green_heart: update pypi version, don't push from main ([208eb65](https://github.com/continuedev/continue/commit/208eb65f67ccc62ce6d683fd9bed2fe9524b2136))\n- :green_heart: use curl to download binary ([199e4b3](https://github.com/continuedev/continue/commit/199e4b3b99642ba5b1558132aa10119be1eeb525))\n- :heavy_plus_sign: add bs4 to requirements.txt ([8a1e6fb](https://github.com/continuedev/continue/commit/8a1e6fb4adec6e5febb2a0d78eb0b2a01bfa028b))\n- :heavy_plus_sign: add ripgrepy dependency to requirements.txt ([9801b50](https://github.com/continuedev/continue/commit/9801b50192ca661972d5b2997028db3cd0725fb7))\n- :heavy_plus_sign: hidden import for replicate ([b75555f](https://github.com/continuedev/continue/commit/b75555f106be3c7612e7c31818ff674485096e4f))\n- :heavy_plus_sign: include replicate in requirements.rtxt ([01b3f1f](https://github.com/continuedev/continue/commit/01b3f1ff8f4dd89ae79f15626ef5a3af2bc558c4))\n- :lipstick: don't display entirety of large tracebacks ([a74eda5](https://github.com/continuedev/continue/commit/a74eda56cfcafb5c463a74df564ced6f882f8d3e))\n- :lipstick: fix layout bugs ([b655781](https://github.com/continuedev/continue/commit/b6557810d70a7f341761d5018fa2835cc3a50af1))\n- :lipstick: fix UI problems in vscode light themes ([5e8866d](https://github.com/continuedev/continue/commit/5e8866da83f8a97cb8492f26e175b948d0282262))\n- :lipstick: logo alignment, better config failure description, patch ([c51ad53](https://github.com/continuedev/continue/commit/c51ad538deff06af6c9e5498b23e3536e18bfc4c))\n- :lipstick: nicer autoscroll ([88699ff](https://github.com/continuedev/continue/commit/88699ff909b026511da392bf2c0a96be02abc6fd))\n- :lipstick: small UI improvements ([ec4fb4d](https://github.com/continuedev/continue/commit/ec4fb4d9235151901c1f7367932ecc17ab55d8e4))\n- :lipstick: ui tweaks to history + scrollbars ([6e8885f](https://github.com/continuedev/continue/commit/6e8885fc2f7feb06ef6ac87d2d7688f9f33d15de))\n- :lipstick: update font size for input, remove first tutorial step ([73ff267](https://github.com/continuedev/continue/commit/73ff2678ad984c9d9082ec078a38450d5daa1376))\n- :lipstick: update light gray hex code ([e0e0482](https://github.com/continuedev/continue/commit/e0e0482f2af2eadd3df72fbdb6974c07ba11c527))\n- :lock: opt out of meilisearch analytics ([8db5b39](https://github.com/continuedev/continue/commit/8db5b39170229ba93b83f526e7fd80056e461c6a))\n- :loud_sound: better logging for ggml completion endpoint ([0459b0c](https://github.com/continuedev/continue/commit/0459b0c919903852254ac2cd081307788884cd84))\n- :loud_sound: fix logs to be sent from uvicorn ([d3b4103](https://github.com/continuedev/continue/commit/d3b4103cd2f639fc072b8a3269d7730478c8bb1c))\n- :loud_sound: websocket logging and horizontal scrollbar ([7bb0fe3](https://github.com/continuedev/continue/commit/7bb0fe34bbc8affce0c675b88ffb79a6b9985860))\n- :memo: escape <QUESTION> in docs ([7314e79](https://github.com/continuedev/continue/commit/7314e79ac5bc34936a2c3de0fd01aadbfe640e72))\n- :memo: fix deployent readme ([7a38025](https://github.com/continuedev/continue/commit/7a3802523c2e5ae136c39849e2fbb0d3e7bba63e))\n- :memo: remove reference duplicates for ctx providers ([043d695](https://github.com/continuedev/continue/commit/043d695198caed305fa6651918c3bbb2de87db36))\n- :memo: small fix in troubleshooting.md ([275a03b](https://github.com/continuedev/continue/commit/275a03b7f1e32f57bd68e501074aa80e0dbed40f))\n- :memo: use backup server links in docs ([815627b](https://github.com/continuedev/continue/commit/815627b167e4bf06308b51c6756e33c36b17b631))\n- :pencil2: Fix typo that was causing automatic version bumping not to work for intellij ([3daf2c7](https://github.com/continuedev/continue/commit/3daf2c7b23caf838b862c2d2791ae8655b761d12))\n- :rocket: fallback s3 bucket ([aa98080](https://github.com/continuedev/continue/commit/aa98080cb16c75d2b7d6d9771b97e63120052c62))\n- :safety_vest: more safely convert windows path to posix ([4309f9d](https://github.com/continuedev/continue/commit/4309f9def89c25611273d99db01e7cc477ad935e))\n- :white_check_mark: allow longer for python server to start in test ([d8f5f10](https://github.com/continuedev/continue/commit/d8f5f102f6f91487be0281316e581858ec4ca260))\n- :white_check_mark: allow longer wait in test ([40ec1a3](https://github.com/continuedev/continue/commit/40ec1a31a7cd37da8b75bbabf1f0d160bb7bec5d))\n- :zap: register vscode commands prior to server loading ([f7a3659](https://github.com/continuedev/continue/commit/f7a3659381f839b890f2c53086f7fedecf23d9ab))\n- :zap: update count_tokens method ([8214203](https://github.com/continuedev/continue/commit/82142033f935d6236620d82e31a70ea8f2fb243e))\n- 'inferenceConfig.stopSequences' failed to satisfy constraint: Member must have length less than or equal to 4 [#2538](https://github.com/continuedev/continue/issues/2538) ([90e994d](https://github.com/continuedev/continue/commit/90e994db3c106f9b63bd043203111e5e101071ba))\n- (very) small typo breaking the prompt file examples link! ([bd6f9b9](https://github.com/continuedev/continue/commit/bd6f9b969647ee8cf6138dca63538063414553bb))\n- `REPLACE INTO` code_snippet table ([bdb967d](https://github.com/continuedev/continue/commit/bdb967d7b3c18fa2e0237f3cc0e0ea7415102715))\n- üêõ Codebase Indexing still not work ([955ab93](https://github.com/continuedev/continue/commit/955ab93efc3f488127d16673b89f3900f75c2007))\n- üêõ typo in core.py ([#429](https://github.com/continuedev/continue/issues/429)) ([705324e](https://github.com/continuedev/continue/commit/705324ed2ef588b2885c0b03107b9e30ae358dae))\n- a bunch of bugs, commit residuals such as npm install pg ([6d74e6b](https://github.com/continuedev/continue/commit/6d74e6b1fedf18b11ea12ba38164fcd146fdba4b))\n- actions ([653ca20](https://github.com/continuedev/continue/commit/653ca205dfbe2f7fec94891c99053d393d3efd0c))\n- add .mvn/ to list of default ignored folders ([ec91021](https://github.com/continuedev/continue/commit/ec91021adfcbd49becc3fdfbee6784981238a9f4))\n- add 'rich' module to requirements.txt ([#612](https://github.com/continuedev/continue/issues/612)) ([5d21bdf](https://github.com/continuedev/continue/commit/5d21bdf2930b30723f1fd80b05d8c1c2ad589bb2))\n- add checkmark icon to indicate selected model in dropdown ([98a3219](https://github.com/continuedev/continue/commit/98a321939ea1ba551025b16dc09f13e7a5e980ca))\n- add code range for quick actions/fixes ([#1687](https://github.com/continuedev/continue/issues/1687)) ([9f2e9bc](https://github.com/continuedev/continue/commit/9f2e9bc2dff474447d8502e386bb0cc804730bb9))\n- add context provider ([ae888d9](https://github.com/continuedev/continue/commit/ae888d9e814cf10931222070ed1e2ae5438325fe))\n- Add context/getSymbolsForFiles endpoint in JetBrains and handle symbol retrieval errors ([ba1fdf7](https://github.com/continuedev/continue/commit/ba1fdf7bac07afe04e62bd659a87dc89800a302b))\n- Add ContinuePluginDisposable to avoid memory leaks in Jetbrains ([a48a150](https://github.com/continuedev/continue/commit/a48a150b2024fca63cb50fb09a914a391c0cfce5))\n- add Delphi/Pascal syntax highlighting support ([cfcb06b](https://github.com/continuedev/continue/commit/cfcb06b1763b17e9e6d150963e090523c372d5d6))\n- Add directory checks and optimize token usage ([79736c3](https://github.com/continuedev/continue/commit/79736c3108313204184bb7d833ac21e23341354f))\n- add focus to InputToolbar on click ([#703](https://github.com/continuedev/continue/issues/703)) ([6b17de4](https://github.com/continuedev/continue/commit/6b17de49ed0e991221baee986f9dbb758d55f291))\n- add hover effect and restrict clickable area for the history and more back buttons ([9662b18](https://github.com/continuedev/continue/commit/9662b18a1ff36d8eaf743003f4df501245a31be2))\n- add missing eos_token for qwen2.5-coder ([6a7eea2](https://github.com/continuedev/continue/commit/6a7eea27ca5d1ade790ce08dc1cf4eb7bbdeb585))\n- add missing import ([400be9b](https://github.com/continuedev/continue/commit/400be9b47f0b8ab52b4e32ca77339497c63c8f96))\n- add mistral model options to config schema ([d56b48c](https://github.com/continuedev/continue/commit/d56b48cad2fcb0bb8e4a3e0dd3e32ab7f704d762))\n- add Msty logo ([397dcf4](https://github.com/continuedev/continue/commit/397dcf4ccc663560a49ff41e5892a5c118bce02e))\n- add new UI breakpoints ([5105645](https://github.com/continuedev/continue/commit/5105645dedec92abd3fc613777f60fd2a638a0ab))\n- add unit tests for commandExtractor and improve multiline comment handling ([0d940ec](https://github.com/continuedev/continue/commit/0d940ec6d1377b38945bd6217040cd025f89a80c))\n- add uuid to chat msgs for deletions ([f63fd5d](https://github.com/continuedev/continue/commit/f63fd5d83cefbbe01721515a8f28f435353bd804))\n- addTag ([53f886f](https://github.com/continuedev/continue/commit/53f886fd40c7bca29de06156f0e713a9700cc189))\n- allow downloading missing embedder on Ollama, in VS Code ([1dc58c6](https://github.com/continuedev/continue/commit/1dc58c63ab174f579e860d778ae85e73f6fb8c9d))\n- always show close icon ([4faed0b](https://github.com/continuedev/continue/commit/4faed0bc17a2b79e24fe5133efc40868bccaa6aa))\n- apply actions in toolbar ([cd810eb](https://github.com/continuedev/continue/commit/cd810eb93939bccc5bc0ff1e5c54572c7d27a97d))\n- apply insertion bug ([ceb9277](https://github.com/continuedev/continue/commit/ceb92773e9a6e4da27aa2de8f871c07f1e23d1a7))\n- apply notch filename trunaction ([033ade0](https://github.com/continuedev/continue/commit/033ade024ed685ef66dad53e675449a47e2dc872))\n- artifact name ([85e3fbc](https://github.com/continuedev/continue/commit/85e3fbc1c73872b1dfd8aee1f1dcef26b9f37761))\n- artifact upload ([1039e77](https://github.com/continuedev/continue/commit/1039e777c8a081ed9362f29e20c7c6133f3b07cd))\n- attempt to fix [#485](https://github.com/continuedev/continue/issues/485) ([#498](https://github.com/continuedev/continue/issues/498)) ([1188dd7](https://github.com/continuedev/continue/commit/1188dd7e5f26ed57d034c927ba032739963b9abc))\n- auth ([72636d9](https://github.com/continuedev/continue/commit/72636d96c8d8bb303e66f2344f5eeef26f078816))\n- autocomplete label after selecting a query type context provider ([#1562](https://github.com/continuedev/continue/issues/1562)) ([6407458](https://github.com/continuedev/continue/commit/6407458d7626d52558ea760b79a03129caba14ff))\n- autocomplete logging + input layout ([c542e5d](https://github.com/continuedev/continue/commit/c542e5d6153078f42e8eaf4361a4e5bdf24d9bc2))\n- Autocomplete not working when lookup is selected ([346f8fe](https://github.com/continuedev/continue/commit/346f8fe191a96eab5903b9a9f5a1e7e5ea1e3ee2))\n- avoid parsing md rules as yaml ([6c24132](https://github.com/continuedev/continue/commit/6c24132fab5b1c828aeb2e162c0f07ab6c830d0d))\n- binary ([8f24197](https://github.com/continuedev/continue/commit/8f2419740044a5df2ca70905c2b9ecd0fef3389e))\n- broken build ([d09711a](https://github.com/continuedev/continue/commit/d09711ab5895aca1c91552e7d9140b3aa0535d77))\n- broken help center quickpick ([a83526d](https://github.com/continuedev/continue/commit/a83526d6a49df6589d107695f99c03d02b68629c))\n- broken imports ([5fd8d2c](https://github.com/continuedev/continue/commit/5fd8d2cae5926418116031dbab8ce9465bc4c8f9))\n- broken imports ([f60009c](https://github.com/continuedev/continue/commit/f60009c2ea7c1c1a980152e5aa3cda17c3f6e64d))\n- broken JB build ([0e2587c](https://github.com/continuedev/continue/commit/0e2587cc71f497141ccd5fc807624eac80f8f183))\n- broken link ([6a675d8](https://github.com/continuedev/continue/commit/6a675d8edce1e430faa32b5142554bf61b843e0a))\n- broken schema config ([0529ae8](https://github.com/continuedev/continue/commit/0529ae8e1e4f61d83f2a4c43c9b9d23a84192ec9))\n- browse functionality ([ef5d753](https://github.com/continuedev/continue/commit/ef5d7535f0860978baeb337930f266b1201a7a5c))\n- build issues with jetbrains ([267961f](https://github.com/continuedev/continue/commit/267961f5b67f8f4529917fc43ebc1df88db92b51))\n- bump extension tester version ([006e1e4](https://github.com/continuedev/continue/commit/006e1e4b3bad2a8545a492cbeeb3238a1bf3345b))\n- bump node.js dependencies ([18fd7fb](https://github.com/continuedev/continue/commit/18fd7fbe58a03dfda382f755e261c5f16e88f38e))\n- cache ([368d264](https://github.com/continuedev/continue/commit/368d264d221793b7f2439678cef9b8b032135529))\n- caching ([1e7aa1a](https://github.com/continuedev/continue/commit/1e7aa1a879c80c0acddf09fbe0d887b7ec9bd31d))\n- change from llama3.1-8b to llama3.1:8b ([ea060a3](https://github.com/continuedev/continue/commit/ea060a37567af620cbdb8d665beb3f36cfeada6d))\n- chat tests ([7be3bb4](https://github.com/continuedev/continue/commit/7be3bb4161e7a17f7b0ae6e8d3ce365479e70dde))\n- ci ([1f47f54](https://github.com/continuedev/continue/commit/1f47f5436dde900b04cf1b307efa64f0fecebd59))\n- ci ([f6f2633](https://github.com/continuedev/continue/commit/f6f263375be129e438ab46a3ddbbba0722d3eb24))\n- ci ([fa9f86b](https://github.com/continuedev/continue/commit/fa9f86b6fd9ddb8cfab7dc52772c9f5a0623d654))\n- CI checks + fix for broken config-yaml types ([2032a8c](https://github.com/continuedev/continue/commit/2032a8c31ce9f30a5996a0fa6566d840a1eae21b))\n- ci e2e tests ([ade32d4](https://github.com/continuedev/continue/commit/ade32d428c5ef0fc8f5daefd52571b95387002f3))\n- ci tests ([27d20eb](https://github.com/continuedev/continue/commit/27d20eb08b25b142a54641048670b036e201ea1d))\n- circular state in redux selector ([5921bfa](https://github.com/continuedev/continue/commit/5921bfac67237dd4ea34dc1aad7856b220adf062))\n- cleanup model select ([1933a29](https://github.com/continuedev/continue/commit/1933a29ec4536d4887130703f1b0f94c3cd3db8d))\n- Clear all diff blocks before streaming and ensure dispose is run on EDT ([41357cd](https://github.com/continuedev/continue/commit/41357cd00c72b61cd54b80aa3963d047c0bb4184))\n- Clipboard is not available in chat window ([ef26b7c](https://github.com/continuedev/continue/commit/ef26b7c80eb0cc503017df8b9abd634f868a9b89))\n- close extension after tests ([575a707](https://github.com/continuedev/continue/commit/575a70721b39e41bfe8b860824d5eb606ce29ba9))\n- close sidebar when cmd+l pressed on focus ([93c9507](https://github.com/continuedev/continue/commit/93c95075425179d27421c7b54d92c9235d914c3c))\n- Cloudflare Workers AI message handling ([7eff255](https://github.com/continuedev/continue/commit/7eff25595c6aa063db37fb2d5394447c8472ba20))\n- cmd+shft+l closes sidebar if focused ([#1638](https://github.com/continuedev/continue/issues/1638)) ([92b5c4c](https://github.com/continuedev/continue/commit/92b5c4ccf64a88c461bb32abc9ab02329651e6be)), closes [#1536](https://github.com/continuedev/continue/issues/1536) [#1456](https://github.com/continuedev/continue/issues/1456) [#1564](https://github.com/continuedev/continue/issues/1564) [#1576](https://github.com/continuedev/continue/issues/1576) [#1570](https://github.com/continuedev/continue/issues/1570) [#1582](https://github.com/continuedev/continue/issues/1582) [#1600](https://github.com/continuedev/continue/issues/1600) [#1618](https://github.com/continuedev/continue/issues/1618) [#1626](https://github.com/continuedev/continue/issues/1626) [#1637](https://github.com/continuedev/continue/issues/1637)\n- code automatically expands after pressing enter ([42c02be](https://github.com/continuedev/continue/commit/42c02bed5fb61f925eb829cd8e7e39425225cc30))\n- codestral template ([d56a33e](https://github.com/continuedev/continue/commit/d56a33ea06f7c9724ffaef86d71bbb8d31840fc8))\n- colocation ([fb28272](https://github.com/continuedev/continue/commit/fb282727e92237f6b14e28e3538e04e13b837f1e))\n- colocation ([e1d227c](https://github.com/continuedev/continue/commit/e1d227cbc3ea56e607fce5c5ce585c2b901f6029))\n- comment out auto apply on MFE ([f61476a](https://github.com/continuedev/continue/commit/f61476a4936f364b4d0adc679da067d5f3ee8411))\n- completions ([a2f62da](https://github.com/continuedev/continue/commit/a2f62dad3b4df8a9a243964584eed148835a077b))\n- config error handling ([170676a](https://github.com/continuedev/continue/commit/170676a46149a4e507f827aa566f4b1d39344e2e))\n- config-types \"useSuffix\" to \"useFileSuffix\" ([fa3afd5](https://github.com/continuedev/continue/commit/fa3afd52e766c298a9bfa62c57e8a9b6e695b1d6))\n- **continue:** update context for slash commands ([cfac639](https://github.com/continuedev/continue/commit/cfac639e44ff5ba13960bbed6756047a90ed93f2))\n- convert `walkDir` to an async generator ([#1783](https://github.com/continuedev/continue/issues/1783)) ([6824497](https://github.com/continuedev/continue/commit/68244977d377e1706a20892abc059bf5cb70bc71))\n- copy ([5e122b7](https://github.com/continuedev/continue/commit/5e122b76b3cf5f0f511101bb1b2cea1349fa3a7a))\n- **core:** delete indexed docs if re-indexing ([acea620](https://github.com/continuedev/continue/commit/acea620043d2c48826266d4f1f73eaf42819f0c6))\n- **core:** remove eslint config and fix errors ([#1457](https://github.com/continuedev/continue/issues/1457)) ([3e1c06b](https://github.com/continuedev/continue/commit/3e1c06b41b7b90a0cad026c0ba0433fb1be6d277))\n- **core:** use `TextDecoderStream` for stream response ([#1498](https://github.com/continuedev/continue/issues/1498)) ([09d256a](https://github.com/continuedev/continue/commit/09d256ad562eda0919b9fa2853176319bf4eda36))\n- correct formatting ([cbb5c21](https://github.com/continuedev/continue/commit/cbb5c21f591e078df186fd86865830cb8971be03))\n- correct llama2TemplateMessages ([#855](https://github.com/continuedev/continue/issues/855)) ([f95a54d](https://github.com/continuedev/continue/commit/f95a54d9c29247680839129c54eae912ba5ca85d))\n- correct package.json update ([02968fb](https://github.com/continuedev/continue/commit/02968fb1551c0a26af75340641312e1f9f5a1102))\n- correct typo in stop sequence ([0dac76e](https://github.com/continuedev/continue/commit/0dac76edf5f35226cf6b3e99c83a7c02273cc05c))\n- corrected typo in the info Alert paragraph ([be55052](https://github.com/continuedev/continue/commit/be55052d230b50635c43727127ee776e67bd4c20))\n- cursor focus issue causing unwanted return to text area ([#1086](https://github.com/continuedev/continue/issues/1086)) ([c54cd7a](https://github.com/continuedev/continue/commit/c54cd7a26dd2e4e8743be86c6ac22dd1c8dca774)), closes [#1078](https://github.com/continuedev/continue/issues/1078)\n- cursor position ([6284eee](https://github.com/continuedev/continue/commit/6284eeebef984ac1afd3846b606a7745c5dfb49d))\n- cycling through chat messages ([b3ac143](https://github.com/continuedev/continue/commit/b3ac14332e8bd89237dfa41847701e3378487145))\n- deepseek doesn't support 'https://api.deepseek.com/completions' URL currently. When user selecting code in editor, and then use 'CMD +I' command to let model to modify codes, 400 error happens. ([170b99d](https://github.com/continuedev/continue/commit/170b99d3261b7d2ef32ba9e03a51ac37bb1fb787))\n- delete all code_snippets ([b9114bb](https://github.com/continuedev/continue/commit/b9114bbdab90de0dd167c88b57bd96b27ad03524))\n- delete button default style ([5423d24](https://github.com/continuedev/continue/commit/5423d24c35c614b0970e472aafa1f1048bf0d970))\n- delete old docs index on force re-index ([#1778](https://github.com/continuedev/continue/issues/1778)) ([0d632ea](https://github.com/continuedev/continue/commit/0d632eabbfd512c813e65212c3f5765bbc3fc8c7))\n- dependencies ([384fb56](https://github.com/continuedev/continue/commit/384fb562e5aac023301d592a186982a4b575fe98))\n- dependencies ([8efb3be](https://github.com/continuedev/continue/commit/8efb3be1b1f2253f9a7fd3035d8f0055de8f426e))\n- dependencies ([cc063fd](https://github.com/continuedev/continue/commit/cc063fdd124d66515bdcb9e2c64f575b40cc99d8))\n- dependencies ([fccae87](https://github.com/continuedev/continue/commit/fccae8732e1a4f447ff0ab650e9ae717c2022ffc))\n- Desktop.browse on linux ([7a11446](https://github.com/continuedev/continue/commit/7a11446bfc5d4cb9c05f64f1377518456d30f530))\n- disable completions in the commit box ([2b611df](https://github.com/continuedev/continue/commit/2b611df53ba716c2b550ab3b275c571eed9d2c65))\n- disable indexing for unsupported CPUs ([a005aa7](https://github.com/continuedev/continue/commit/a005aa78ac75ac47be3e647e9e23b45eab7dafaf))\n- disable lancedb cpu check ([b496777](https://github.com/continuedev/continue/commit/b49677711b4340f57ba671bacda301138c2909fb))\n- Do not break completion for unsupported FS providers ([4d1a33f](https://github.com/continuedev/continue/commit/4d1a33fd794a7415b0cc3b8de1d349fbba5a01cb))\n- do not filter context providers if not in editMode ([b2182b9](https://github.com/continuedev/continue/commit/b2182b9c8568a38d3af179909fb305e1459f5276))\n- do not write to the cache if caching is disabled ([b29ed9d](https://github.com/continuedev/continue/commit/b29ed9d34aa7ae343a2fd0c767f315e46e9aa110))\n- docs and install dependencies task for new directory structure ([#551](https://github.com/continuedev/continue/issues/551)) ([6ef8ae6](https://github.com/continuedev/continue/commit/6ef8ae6d5ab7efb1e928ff6474b6ac7a804a34cc))\n- **docs:** contributing guide ([5a06dfe](https://github.com/continuedev/continue/commit/5a06dfe1fb865bed0ca76ff23816da6396a0b27d))\n- **docs:** correct openai api key name ([9cc25c1](https://github.com/continuedev/continue/commit/9cc25c18c9fff5fc6ac4d56153667b0e8311920c))\n- **DocsCrawler:** Add response.ok check and improve link splitting ([d613b5a](https://github.com/continuedev/continue/commit/d613b5a7da5ad15c8b4b7dfe949812045f69635c))\n- don't override embeddings model name with UNSPECIFIED ([bc79388](https://github.com/continuedev/continue/commit/bc79388634ebcfcf2e38aa240441f9939e9de85a))\n- don't return empty string ([403eb38](https://github.com/continuedev/continue/commit/403eb38b9e3e38c277daed4ac6a2b6ac54353570))\n- dont toggle models when toggling assistants ([5bd8dd4](https://github.com/continuedev/continue/commit/5bd8dd4cb759bbabc35bdc00c66d88e4a175f7a8))\n- double ssh retries ([c9e2ebc](https://github.com/continuedev/continue/commit/c9e2ebc4589f7a06e804daee7f2ce6d7b2b66ce2))\n- dropdown with the height of the sticky div ([#605](https://github.com/continuedev/continue/issues/605)) ([a8d6ff0](https://github.com/continuedev/continue/commit/a8d6ff04e43a81a3c76373469ffebdfcfa8ac9f2))\n- editor focus handling on click events ([#716](https://github.com/continuedev/continue/issues/716)) ([f1660a5](https://github.com/continuedev/continue/commit/f1660a533146c420253df6e2dec0e0d4a2a9ebf4))\n- Eliminate Uncaught TypeError in dev tools ([d342825](https://github.com/continuedev/continue/commit/d3428256543bcc5ab269df93c949b56df92793f2))\n- Empty file inserts incorrect content ([ba9027b](https://github.com/continuedev/continue/commit/ba9027baea44ca51d299804636771d1a084d5fa7))\n- empty last message ([d3f7795](https://github.com/continuedev/continue/commit/d3f7795e6133fe7452c0626e6a54fb229bacbd7b))\n- enable OSR for mac after validations ([db77b50](https://github.com/continuedev/continue/commit/db77b5020d9b93cb4a7504e213d81940dbee9eaa))\n- end thinking for reasoning models when request is complete ([5109685](https://github.com/continuedev/continue/commit/51096858ab1ddc2bee534c1ea848be2b4bdcfb3f))\n- ensure code preview uses one more backtick than in the selected code as a fence ([#742](https://github.com/continuedev/continue/issues/742)) ([9ce7770](https://github.com/continuedev/continue/commit/9ce7770066cf870d2d1f36883f5e867157373eda))\n- ensure template variables are only processed for stdio MCP servers ([99b1ab2](https://github.com/continuedev/continue/commit/99b1ab2ebe51b4678722703922ecca023afa911c))\n- Ensure that the state is cleared after refresh, and files->uris when the file changes ([abd8907](https://github.com/continuedev/continue/commit/abd890738111e51bf12a700f9e5e8e66a7c76235))\n- Ensure valid line index for highlighter ([f86c95e](https://github.com/continuedev/continue/commit/f86c95ebdbd1851bcbf0e6f22d18a348a8a154ed))\n- error ([4c7f918](https://github.com/continuedev/continue/commit/4c7f9182665a259fc6b9cf10c93e696e97400a2c))\n- error printing bug leading to uncaught err ([3af4d6b](https://github.com/continuedev/continue/commit/3af4d6b1edeac113499cc9626cea75a5083b1030))\n- exclude open-codestral-mamba from supported models ([c6591f8](https://github.com/continuedev/continue/commit/c6591f836ea73e1244a0e965111f7c599669d9fa))\n- **extensions:** schema for db ctx provider ([#1534](https://github.com/continuedev/continue/issues/1534)) ([6fdad55](https://github.com/continuedev/continue/commit/6fdad553b6ac7f64c1bb23617dbe313e16174a56))\n- failing azure llm test ([546b52a](https://github.com/continuedev/continue/commit/546b52a2df928687d56924f593108f5bb9235712))\n- failing test ([ec2a72d](https://github.com/continuedev/continue/commit/ec2a72dfabbe7bd9c220b18b55145f431e19f3bf))\n- failing test ([002ecc9](https://github.com/continuedev/continue/commit/002ecc9b519472e0968e9f227894f3143c87f1ec))\n- failing test ([07b6b0c](https://github.com/continuedev/continue/commit/07b6b0cbefe86f2efa5488a33060ee9eb9f20e4d))\n- failing tests ([88ccb74](https://github.com/continuedev/continue/commit/88ccb74f7e69cf816d27f15a318a42c11f6d2aa5))\n- failing tests ([cb2e0a7](https://github.com/continuedev/continue/commit/cb2e0a7c0e9330d9b79a3e9878c851b9eba92ade))\n- failing tests ([7b52ff7](https://github.com/continuedev/continue/commit/7b52ff74d4009cb85c32a5e9482bd3624931d21e))\n- failing tests ([8d29d41](https://github.com/continuedev/continue/commit/8d29d4158d0fe21b0c0c4722e8d23715acf9ebdf))\n- failing tests ([2b828b0](https://github.com/continuedev/continue/commit/2b828b0e7b455bf6b0f9ae5a6f6a705ab4cefe6a))\n- field name fix ([0de1a2f](https://github.com/continuedev/continue/commit/0de1a2ff8937452815311c806a038a7f6d75fdf8))\n- filepath replacements in prompt files ([#1939](https://github.com/continuedev/continue/issues/1939)) ([c0923a0](https://github.com/continuedev/continue/commit/c0923a0dd7ca3a6e9dcbd9b02bfc34250269d2ef))\n- fix [#709](https://github.com/continuedev/continue/issues/709) ([#712](https://github.com/continuedev/continue/issues/712)) ([9ae07cd](https://github.com/continuedev/continue/commit/9ae07cd711d011faaf9a79952c0e213a6aa72e87))\n- fix 100% indexing progress ([7f214ad](https://github.com/continuedev/continue/commit/7f214adcbdca33ebb7270f721fd57eeeb0bac1e8))\n- fixed misallignment in tests caused by linter ([15e93dd](https://github.com/continuedev/continue/commit/15e93dd5d0dc3b5dcd00504833beacbddbfe44f3))\n- format code ([1d000be](https://github.com/continuedev/continue/commit/1d000bec682d1d723bf6748f34c3dd86b684d775))\n- formatting ([9efd0e1](https://github.com/continuedev/continue/commit/9efd0e128ef464d45379e84abf15e98ad1b24d40))\n- formatting ([2284291](https://github.com/continuedev/continue/commit/22842919489ad272edfa182e9f8af3a26d9813ae))\n- formatting ([4d70d3d](https://github.com/continuedev/continue/commit/4d70d3dcbd646b1aa1cb6e586439459dec2db462))\n- free trial config.json updates ([905f064](https://github.com/continuedev/continue/commit/905f064543e71864eaeb4e039e5d55fe2da845fd))\n- fullscreen gui retains context when hidden, fixed fullscreen focusing ([#1582](https://github.com/continuedev/continue/issues/1582)) ([679e26d](https://github.com/continuedev/continue/commit/679e26d4a8a81402d57a64d819f347d9f56d42e0))\n- gemini tool calls with MCP ([9ca4ad4](https://github.com/continuedev/continue/commit/9ca4ad4afe04fbfc3a6e0e6d447dd81ff22b4369))\n- getting diff ([9431131](https://github.com/continuedev/continue/commit/9431131f1cbbc5fe742c033a8baf970241c63378))\n- **google:** remove unnecessary parameter ([#394](https://github.com/continuedev/continue/issues/394)) ([938c1db](https://github.com/continuedev/continue/commit/938c1db5b37d5332ff5d188f4fa79f3bc6b7549a))\n- grab PATH for mcp connections ([60ec3ac](https://github.com/continuedev/continue/commit/60ec3ac452f0b25d176728dfe6e83e4f34ade745))\n- grammar ([58feb0b](https://github.com/continuedev/continue/commit/58feb0b8622e5f64737594ccbfd94aff2e9687c5))\n- **gui:** ctx rendering w/ renderInlineAs: \"\" ([#1541](https://github.com/continuedev/continue/issues/1541)) ([8a92f43](https://github.com/continuedev/continue/commit/8a92f4312693549a1590e99c70152b843d59b119))\n- **gui:** passing style props on StyledTooltip ([41e1eb5](https://github.com/continuedev/continue/commit/41e1eb582bec8676fc973c3a2b76259344fbb317))\n- **gui:** typo ([3d53809](https://github.com/continuedev/continue/commit/3d5380908292bb4373a122a802f2011dd0283f62))\n- handle apiType for azure w/ proxy ([036e196](https://github.com/continuedev/continue/commit/036e19618b86600e4e9c5ad6e6ee5a98e55e15f1))\n- handle closed webview on quick edit ([#1942](https://github.com/continuedev/continue/issues/1942)) ([fe05db5](https://github.com/continuedev/continue/commit/fe05db541dafc45c9870d52d9f15e812e1ea90ad))\n- handle deleted blocks ([c1157db](https://github.com/continuedev/continue/commit/c1157dbf02f6750d385b28006c90202e6b159d5a))\n- Handle empty addedLines in revertDiff ([f280319](https://github.com/continuedev/continue/commit/f28031902f41c4ae9f7cab01398216272950b7d0))\n- handle line removal diff ([#1744](https://github.com/continuedev/continue/issues/1744)) ([6126eca](https://github.com/continuedev/continue/commit/6126eca35ab981c7b8cd6a56769dfe7cb9e69349))\n- handle optional params in EditSlashCommand ([#745](https://github.com/continuedev/continue/issues/745)) ([0141229](https://github.com/continuedev/continue/commit/014122957ff6e087b03def436b95797e59c6f6cb))\n- Handle status update in DocsService ([ac3df81](https://github.com/continuedev/continue/commit/ac3df814e45343b07f63c99e8710f4a6b185074e))\n- handle when dir might be a file path ([e43d3bd](https://github.com/continuedev/continue/commit/e43d3bda6aa833bfe686b8c84dfa64d43fa08748))\n- hardcode `fontSize` for tool call status msg ([f78969b](https://github.com/continuedev/continue/commit/f78969b147a8aecb5c9e5162d080cdafb3f4adb7))\n- hmr issues with non-react component exports ([0e55c08](https://github.com/continuedev/continue/commit/0e55c08be3ef5dcaec5305f41ae871ae6903b6c7))\n- hotkey for new session ([#572](https://github.com/continuedev/continue/issues/572)) ([1d3acd9](https://github.com/continuedev/continue/commit/1d3acd9f194802d78c99cd407bcb72543aa0ff19))\n- **HttpContextProvider:** support all possible local servers via a library ([c3b2552](https://github.com/continuedev/continue/commit/c3b2552ed5e47eef8dc2a691d5f76f30b61ceafd))\n- if rules ([a6dd5a8](https://github.com/continuedev/continue/commit/a6dd5a83dedcae099f320a2da9362490eadde299))\n- ignore CSV files ([#1972](https://github.com/continuedev/continue/issues/1972)) ([1272f34](https://github.com/continuedev/continue/commit/1272f3402a1fc12e5eb0566ad4a4aadc3db7dc06))\n- Implement RepoMapContextProvider ([48c442d](https://github.com/continuedev/continue/commit/48c442d1c687b15d5612786e87a09b016714abd6))\n- import posthog as type, use inline import ([57a84dc](https://github.com/continuedev/continue/commit/57a84dcfe16e3cdebea54f7a0b0fe354f0d8ad2b))\n- Improve error handling and ensure coroutines continue correctly in case of failure ([46bd573](https://github.com/continuedev/continue/commit/46bd573a4296bb8ce8dc4c43b7293ff575655e53))\n- improve textarea ux ([#1901](https://github.com/continuedev/continue/issues/1901)) ([228bc30](https://github.com/continuedev/continue/commit/228bc30e895b87e06c170775974c49e2781d9ae0))\n- include access to the Documents folder ([03839c3](https://github.com/continuedev/continue/commit/03839c30813f3242d9a1fce91f39af110d8d85a7))\n- incorporate feedback ([98efbbd](https://github.com/continuedev/continue/commit/98efbbd80f075928a45334f8cc4103221c2656c9))\n- incorrect scroll logic on inline edit ([ed3af3f](https://github.com/continuedev/continue/commit/ed3af3f3fc49f6678d8a1f940d06aa62bec1f4b5))\n- intellij freezing ([1f602e5](https://github.com/continuedev/continue/commit/1f602e53c29eb28d98754252823ad8f40f839d1b))\n- isRuleActive ([2b97efe](https://github.com/continuedev/continue/commit/2b97efe5c6d045b461c76068c0e06df298d95902))\n- issue 3732 ([ce5d664](https://github.com/continuedev/continue/commit/ce5d664f828de8681ee7129dfec3c16cad667c46))\n- **JB+GUI:** arrow keys in GUI on linux ([9110904](https://github.com/continuedev/continue/commit/9110904e7d4f727339666ce235388397b8c4ff99))\n- **jb:** meta keybinding issues ([b4dad78](https://github.com/continuedev/continue/commit/b4dad78a727feed0ce18769a8d3f9cfd00a3bd57))\n- **jb:** remove markup from `edit` model title ([f6d334a](https://github.com/continuedev/continue/commit/f6d334a190bf1132c33a68f472aaeb2e5389399f))\n- **jb:** set `setOffScreenRendering` conditionally ([fd66ff7](https://github.com/continuedev/continue/commit/fd66ff7d464d3138a127e2035bca88f6d42c4402))\n- **Jira Context:** add api abstraction ([cc9b960](https://github.com/continuedev/continue/commit/cc9b96038bb0b88156729d1c39e6eec1794fe76a))\n- jira ctx provider ([9d24d34](https://github.com/continuedev/continue/commit/9d24d3402f3e9f9937bfdff70fa5c4fa120b2dbf))\n- keyboard shortcuts test on linux ([e45aa16](https://github.com/continuedev/continue/commit/e45aa164c64a52936c7dd1c0467548aadf821639))\n- layout alignment ([#1779](https://github.com/continuedev/continue/issues/1779)) ([6e8fc12](https://github.com/continuedev/continue/commit/6e8fc1247c44c344a48db9758437cb7b3028591c))\n- lint ([f575721](https://github.com/continuedev/continue/commit/f575721980a596cdb08be8a2e472ca26e6012325))\n- lint ([3ba1153](https://github.com/continuedev/continue/commit/3ba1153243753682ca19b26031c2c3c73394b5f2))\n- lint ([03cd1a3](https://github.com/continuedev/continue/commit/03cd1a338f03fc1baa87feb02bd6e56111fc0b13))\n- lint ([747d380](https://github.com/continuedev/continue/commit/747d3806a99abf7f55823c2fbc06c74448ba3c1c))\n- linting error ([67ab058](https://github.com/continuedev/continue/commit/67ab05821191d75a201e516b725a679671f94734))\n- linux key ([9618327](https://github.com/continuedev/continue/commit/9618327cf45687f18aacdc04d62da86ce2d1e520))\n- linux test env ([9f2f9d8](https://github.com/continuedev/continue/commit/9f2f9d895b2fe6678f49e394e4657f044e2aef2a))\n- listbox z-index issue ([1d2bfcd](https://github.com/continuedev/continue/commit/1d2bfcd4b2ad7c3afef97348c55a1c668610ea07))\n- listDir ([a41a4bd](https://github.com/continuedev/continue/commit/a41a4bd2df5807bd216a4b23eabe8656ca098cdb))\n- **llm/Ollama:** changed buffer handling like in streamChat() ([f1f8967](https://github.com/continuedev/continue/commit/f1f8967b6c4b422428fcad0575c0b24ae78bf34a))\n- load last session when completing edits ([1e33400](https://github.com/continuedev/continue/commit/1e334008c08eb8796510ba6aaac434073c44b3b8))\n- local build script ([#810](https://github.com/continuedev/continue/issues/810)) ([76fe78a](https://github.com/continuedev/continue/commit/76fe78a1a3c05b0d6dd5ed6d15575e20e797cfa6))\n- log llm completition in edit command ([3094180](https://github.com/continuedev/continue/commit/30941800b04c59b9a45375d6934a4fb79cac0141))\n- logs ([e3cf75e](https://github.com/continuedev/continue/commit/e3cf75e0779d3bc02a468ea6551e2499979c08f7))\n- mac build issues ([9946a5c](https://github.com/continuedev/continue/commit/9946a5c05f85d20e2d766719a861eb0f8a55019c))\n- major fixes to getTheme ([c6002ba](https://github.com/continuedev/continue/commit/c6002ba2ac905bd5439173e6430676bdc3594a87))\n- make `env.apiVersion` required for azure openai ([4355618](https://github.com/continuedev/continue/commit/4355618079808bdfc3240022e79a0a9dd8457b02))\n- matching context code ([64fb928](https://github.com/continuedev/continue/commit/64fb928b8ff9c0f7753a8b8d374abc179de1f4d5))\n- mcp context provider bug with issue 2467 ([98f07be](https://github.com/continuedev/continue/commit/98f07bef818cf5acc4de2828cbeaa0b4b499fc53))\n- MDX compilation issue ([aca82d2](https://github.com/continuedev/continue/commit/aca82d2ef36a5bd57119343790875e2acf7819b3))\n- merge bugs ([009e578](https://github.com/continuedev/continue/commit/009e578416982a90af05539420a7137ccd2d03c1))\n- messages ([0b07020](https://github.com/continuedev/continue/commit/0b07020d7dd9bf59ca1d804668a215174273625b))\n- minor issues ([#581](https://github.com/continuedev/continue/issues/581)) ([570f61d](https://github.com/continuedev/continue/commit/570f61de5a404559b4ff69eb5cd2fa216e70e872))\n- missing arguments ([452b242](https://github.com/continuedev/continue/commit/452b2422d18c05dd28dbfa6fd127f4c3e0b0223c))\n- missing cd ([0d88b8f](https://github.com/continuedev/continue/commit/0d88b8f379d161962a72626fe2af583cce0f3fe0))\n- missing Content-Type header in OllamaEmbeddingsProvider ([#1855](https://github.com/continuedev/continue/issues/1855)) ([841294d](https://github.com/continuedev/continue/commit/841294d15f3ad6ea4a9f7cf277fbcb905d9bd978))\n- missing context items ([bdc2ec8](https://github.com/continuedev/continue/commit/bdc2ec8374ac9e7d14fc440ea80fa438005735e8))\n- missing env ([1ebd3a3](https://github.com/continuedev/continue/commit/1ebd3a31bab5ed31374518022c5f1b7658c379f2))\n- missing package ([e284cec](https://github.com/continuedev/continue/commit/e284cecd6584b14bee43794d0d238d64c734a10a))\n- missing useLegacyCompletionsEndpoint in yaml schema ([a4b4395](https://github.com/continuedev/continue/commit/a4b4395649e9fa7888f8204ddcf56b88632b5b90))\n- model name ([b1ec201](https://github.com/continuedev/continue/commit/b1ec2012c649ff29e84d6682234328e9c2fdb792))\n- models and mode are not synced ([57441da](https://github.com/continuedev/continue/commit/57441dad6696eeb6d4bd6d769aec06e389367a53))\n- modify access modifier public to private ([c36f233](https://github.com/continuedev/continue/commit/c36f23322265a0d9c28eb401acf49d76294a1249))\n- move autocomplete logic off of EDT ([c46083d](https://github.com/continuedev/continue/commit/c46083d4115a79938bc6ae2b9421f36803b0a992))\n- naming ([ba39842](https://github.com/continuedev/continue/commit/ba3984290ddb875608845904d31c5a5d57b1bf03))\n- naming ([42a03ad](https://github.com/continuedev/continue/commit/42a03ad020634a7c6be719a70bf40144582682dc))\n- naming ([fb1a75d](https://github.com/continuedev/continue/commit/fb1a75dcf33fa38863bda4f5b88c1a567aaf0269))\n- navigation bug ([fab5347](https://github.com/continuedev/continue/commit/fab53472a931b49d7f0c19af22263df6b54bc292))\n- not sure how that button fragment got there ([880f1de](https://github.com/continuedev/continue/commit/880f1de80d2bc33635cc9ebed01ae84e2c17c640))\n- number of completions to generate is not supported ([db90b09](https://github.com/continuedev/continue/commit/db90b0954b36e856258eed2b61af8bf107f13510))\n- old docs ([b52910b](https://github.com/continuedev/continue/commit/b52910b43351e5df8c07cfb27beca8fba5463074))\n- ollama tool use ([82ac684](https://github.com/continuedev/continue/commit/82ac6848cabca21122995460328b8f4420ae3222))\n- onboarding telemetry ([445dba3](https://github.com/continuedev/continue/commit/445dba3bb60654ac9c3f8d245f896d490574c3a1))\n- only alert free trial if using free trial ([f0e73ef](https://github.com/continuedev/continue/commit/f0e73ef63a8fd8bb7ce313f9ed5439c7c68969bf))\n- only perform lancedb cpu check for linux ([e0ee7d1](https://github.com/continuedev/continue/commit/e0ee7d1e446b18c05ed3166b0aa3b199ec021f1a))\n- open new session from any page ([258bbc6](https://github.com/continuedev/continue/commit/258bbc6b5cbbd3b897e8045df247bb2a1fc21f94))\n- overly eager suffix matching ([556a8c5](https://github.com/continuedev/continue/commit/556a8c5ee3f72311a384c64ba14fde7122e68741))\n- path ([d01f243](https://github.com/continuedev/continue/commit/d01f24309b490799014dca0fd03775c39745d660))\n- path ([3ecc89e](https://github.com/continuedev/continue/commit/3ecc89eaf55b970e1a494f88fed0850c4b85c565))\n- pathing issue in jb ([c33e390](https://github.com/continuedev/continue/commit/c33e390427e459a5426ee7669733c2b407649e40))\n- pause between tests to avoid db lock ([4cb84c1](https://github.com/continuedev/continue/commit/4cb84c11b09dac6eb0791e35b95ff6f571130391))\n- placeholder ([8ded13a](https://github.com/continuedev/continue/commit/8ded13a8f8e54df4951b547d468782117fb11b8b))\n- policy precedence ([de6a0ae](https://github.com/continuedev/continue/commit/de6a0ae36c43753e768bf0164dfbe1b938da790d))\n- preserve indentation in code blocks ([#1554](https://github.com/continuedev/continue/issues/1554)) ([574a392](https://github.com/continuedev/continue/commit/574a3923a6f97d8995561da3654aae3aca9ae439))\n- preserve system messages as user messages ([d60a1c1](https://github.com/continuedev/continue/commit/d60a1c1b1f3174cd501eca69746009bdeb4c6f7f))\n- prettier ([4d278e4](https://github.com/continuedev/continue/commit/4d278e479ae02f0d02ccff8db2448d6b38b3d3e5))\n- prevent autoscrolling on new tool msgs ([678a75f](https://github.com/continuedev/continue/commit/678a75f8f7b97b0f75407c6e1b1dd78c415d22eb))\n- Prevent duplicate context provider addition ([2d6eacb](https://github.com/continuedev/continue/commit/2d6eacbeab8eb89ddc32311ade4efe3ccb272bca))\n- Prevent multiple resumes in continuation ([1e96bfb](https://github.com/continuedev/continue/commit/1e96bfb9683fa0a45437f16b419be066917351cb))\n- Promise arrays being returned. Added a new definitions-only context provider. fix: typescript tagging. ([#835](https://github.com/continuedev/continue/issues/835)) ([00eeb00](https://github.com/continuedev/continue/commit/00eeb0024178953172941bba293808be4ca5eed2))\n- prompt format ([0f6d280](https://github.com/continuedev/continue/commit/0f6d280234c459e1935b62e5244f4577d64fac53))\n- promptblock parsing ([eb6735e](https://github.com/continuedev/continue/commit/eb6735ee25da93e01a7b09223960b8975b41d898))\n- Properly ignore ignored folders when generating /onboard context ([f7bca6a](https://github.com/continuedev/continue/commit/f7bca6a61d56b6e17b100294e605d0e2bbe0239b))\n- protocols ([c8a24c0](https://github.com/continuedev/continue/commit/c8a24c0c9040a0da02bdd215b77b0320c160256f))\n- qwen completions missing whitespaces ([75dce9c](https://github.com/continuedev/continue/commit/75dce9cbd14cc374d1ce4921522f3423202be7b1))\n- **reg:** :ambulance: remove log except when in NODE_ENV === \"test\" ([f60fd1c](https://github.com/continuedev/continue/commit/f60fd1c6188567c67d3d8285fd6c2c5d6acc5bfa))\n- regex ([05a3eec](https://github.com/continuedev/continue/commit/05a3eec21ed1c26aa4d9825e6be5e209cdb9563c))\n- reject logic ([5944fca](https://github.com/continuedev/continue/commit/5944fca7374851869e469ed1cd3f13056de5c8b3))\n- remove `any` from promptTemplate passing ([7834d26](https://github.com/continuedev/continue/commit/7834d266330625915542913f2d44ac0d80875d7e))\n- remove debugger statement ([3e4c75b](https://github.com/continuedev/continue/commit/3e4c75bf461c36c2200ac2922670bb76a612b292))\n- remove deugger ([3e8e29e](https://github.com/continuedev/continue/commit/3e8e29e8f2ce18e138df64441e436d3a07ffa45e))\n- remove duplicated context comments ([0c217a4](https://github.com/continuedev/continue/commit/0c217a40082c2c244de4f4f8c955d96314f130da))\n- remove gifs from media ([77d321f](https://github.com/continuedev/continue/commit/77d321ff82dcd34b55d0a0253eb45672be774fe1))\n- remove invalid schema ([2355eb6](https://github.com/continuedev/continue/commit/2355eb62a5c3c5034b528ab17e8ad7a0b661bc7c))\n- remove mismatch between last selected profile and current profile ([d9eb118](https://github.com/continuedev/continue/commit/d9eb118c514fc2a4364f532eb6438588fc9f712c))\n- remove some backup files generated by pkg if present ([#1287](https://github.com/continuedev/continue/issues/1287)) ([9f160ad](https://github.com/continuedev/continue/commit/9f160ad11a1d36bfc92d2da2a8ea31721652b3e4))\n- remove stale code ([cd97833](https://github.com/continuedev/continue/commit/cd97833be88ecaf3c2860dc41ca57c9e2b18ad48))\n- remove testing logic ([5df8177](https://github.com/continuedev/continue/commit/5df81770df526c69dd71f2421a9052deb17accf6))\n- remove tooltips when switching files ([200d291](https://github.com/continuedev/continue/commit/200d29103ea97db06283f2d4b5e0eb9b6afb86df))\n- remove unnecessary baseUrl ([5f159fb](https://github.com/continuedev/continue/commit/5f159fbd81c4b8680cbad8b3875aa54e2079928e))\n- remove unused argument ([7dd2c22](https://github.com/continuedev/continue/commit/7dd2c22632d50726271655d5c5d027a8219e6862))\n- remove unused imports ([1b82d39](https://github.com/continuedev/continue/commit/1b82d39ac5b8e0cfb990a4968b3e8bc81af768ce))\n- rename edit folder ([ab1ad0a](https://github.com/continuedev/continue/commit/ab1ad0ae48eeceee87043cae9a7862a1c4f70da0))\n- replace null with empty string in getSidebarContent function ([#822](https://github.com/continuedev/continue/issues/822)) ([50d1188](https://github.com/continuedev/continue/commit/50d1188479be206d1a4ed5dcd258d4101ae4f8ba))\n- report indexing errors in webview ([6a67514](https://github.com/continuedev/continue/commit/6a67514fcf108bd0eb6989777abaed02ce2e61e7))\n- reset config errors ([cc199c2](https://github.com/continuedev/continue/commit/cc199c28634d006d38416533b06f26e001cea3fe))\n- Resolve proxy error when adding Azure OpenAI model ([17dbfe6](https://github.com/continuedev/continue/commit/17dbfe6bfd38f67b7cb535da00fa626cdaf4f690))\n- Resolve type error in env comparison ([45eacf4](https://github.com/continuedev/continue/commit/45eacf4ebbd99a97eef32eef01d780a1f8c57b7a))\n- Resolved errors related to incorrect URIs ([0a7da20](https://github.com/continuedev/continue/commit/0a7da2067e8d8567084d05f69038b7ca95464d1f))\n- restore all e2e tests ([d3a6eeb](https://github.com/continuedev/continue/commit/d3a6eebeb7b5d96133d1c13e533913dc5ec2f19c))\n- restore cache ([91ea00d](https://github.com/continuedev/continue/commit/91ea00d284227f3875daf17591b15409004372ac))\n- restore comments ([067a3a8](https://github.com/continuedev/continue/commit/067a3a843baf5a0e12bd5b6f21f37a504836286c))\n- restore tests ([553c751](https://github.com/continuedev/continue/commit/553c751562afb736fb914549ba3d2c75e5a4b305))\n- return ctx items from prompt file ([84df7f4](https://github.com/continuedev/continue/commit/84df7f45bf91c70da79901c0a44cc3b156655f6f))\n- revert file changes ([7a44a38](https://github.com/continuedev/continue/commit/7a44a38c860cfbd0eeab35d8fdb3cb198edad680))\n- Revert filepath changes ([3df34b7](https://github.com/continuedev/continue/commit/3df34b725e873fcd42058474d83ef634e21d5cad))\n- revert launch.json change ([b3abfb8](https://github.com/continuedev/continue/commit/b3abfb80d71c16a0e51cc980d4262dbd218d6463))\n- reverted the url of llama.cpp back to 'completion'. ([e8523a2](https://github.com/continuedev/continue/commit/e8523a2b07c7a5d1539138ee8e4a4836fea7a36b))\n- rules ([bcae5b3](https://github.com/continuedev/continue/commit/bcae5b3cf210f1a8ced3b2c913961ed16aaf2ea9))\n- rules ([9e57412](https://github.com/continuedev/continue/commit/9e57412fe161a9451aaaa651a9f9ed2d7b6b5517))\n- rules ([f29110f](https://github.com/continuedev/continue/commit/f29110f98b1c71f4832d6b7ff0041e342e2a0c4a))\n- rules ([b39cb83](https://github.com/continuedev/continue/commit/b39cb836ad6180226d39abe85f5637a6e2dac44d))\n- sanitize lance table names ([5d3b2cb](https://github.com/continuedev/continue/commit/5d3b2cb5070505578cc16e67d2309a40a403bc94))\n- scaffolding for future language syntax highlighting ([647656c](https://github.com/continuedev/continue/commit/647656cd3d007c016bea372bdfcb5d69f9d1d52a))\n- scroll issues w/ code blocks ([#1688](https://github.com/continuedev/continue/issues/1688)) ([ceb8da0](https://github.com/continuedev/continue/commit/ceb8da0b20db70707a3443ff6f705bba6922488d))\n- scroll to the bottom on history change ([b94ccc6](https://github.com/continuedev/continue/commit/b94ccc6db557f705ba4f810ed56c55ce21f383dd))\n- selected profile id writing to global context in org profile rectification ([47755d1](https://github.com/continuedev/continue/commit/47755d1cba3b8c7cb5c0b9a19bf8ca0a92eec850))\n- selector for `isSingleRangeEdit` ([badeb1c](https://github.com/continuedev/continue/commit/badeb1c88b7e1f8eb711a8a2cfc8f341ac22f67e))\n- send feature flags ([2e3fdb3](https://github.com/continuedev/continue/commit/2e3fdb34e2e2358232cc12fcf0a0c2d16a3ef0cc))\n- serve to localhost ([7eccf2d](https://github.com/continuedev/continue/commit/7eccf2ddb9308111686251474fb79fa03dfc87d6))\n- set DEFAULT_CONTEXT_LENGTH to 8192 ([f163768](https://github.com/continuedev/continue/commit/f1637685c651180d1fc969c2f11712fda69eb9ee))\n- settings persistance ([56f3803](https://github.com/continuedev/continue/commit/56f38038b35cedc2aa1bca1b09780b21c811bd83))\n- setup ([6a93eae](https://github.com/continuedev/continue/commit/6a93eaeb0faf685dff216ecca850f9b2ef3cf2c3))\n- show error is GH sign in failed in vscode ([e92e052](https://github.com/continuedev/continue/commit/e92e052b16fb91173aa4dc85d4e80fa569b8e60a))\n- sign in ([fa5d2c5](https://github.com/continuedev/continue/commit/fa5d2c5aa2cdcc580486f81fde3ae69e9d3069e5))\n- sign in btn + colors ([1b669bf](https://github.com/continuedev/continue/commit/1b669bf80e2d81e4aa19c4d3f2dca730ddd6d4f8))\n- Skip duplicate checks at document end or blank text ([decd04e](https://github.com/continuedev/continue/commit/decd04eddd4965b9f76c7a9a3abd276d8d2e1692))\n- snippets in fim prefix ([51e1c42](https://github.com/continuedev/continue/commit/51e1c4216baa38d73141aca1d1e681eabf044b4b))\n- some typos ([#478](https://github.com/continuedev/continue/issues/478)) ([775d051](https://github.com/continuedev/continue/commit/775d051b4bcdbe07fbd38fae4d3e36e79234eb56))\n- spread tool degs ([4ddd275](https://github.com/continuedev/continue/commit/4ddd2755996012677c15069648158bd4334fa3dd))\n- sqlite binary build on fresh clone ([#1433](https://github.com/continuedev/continue/issues/1433)) ([d124aa0](https://github.com/continuedev/continue/commit/d124aa0534303b58b55bcc2de4ec460b4f499178))\n- stream ([c497517](https://github.com/continuedev/continue/commit/c49751713248e27f060b6eee11179e13b14c5adf))\n- system message ([c2707c6](https://github.com/continuedev/continue/commit/c2707c6988a9b6368f183f0602de0cc2872726da))\n- test ([7266efb](https://github.com/continuedev/continue/commit/7266efb4e8490accd69ad5818bd9c35fe6f59e5c))\n- test env path ([71a151f](https://github.com/continuedev/continue/commit/71a151f399d514497ee917eca521863d07b2a270))\n- test timing ([878e211](https://github.com/continuedev/continue/commit/878e2116d6a97c31cad97432d785aaf3b6deb196))\n- tests ([35c674c](https://github.com/continuedev/continue/commit/35c674c724c21c6dc0b699a37f868e68c62f1259))\n- tests ([cafb2e9](https://github.com/continuedev/continue/commit/cafb2e93c7a54b4a35ecc8a209f53982b3cc26d4))\n- tests ([b786591](https://github.com/continuedev/continue/commit/b786591415d79119845e01376bf5dc97c0676b23))\n- tests ([ff543e8](https://github.com/continuedev/continue/commit/ff543e8b321e4e48a088ba296ebac4504b97356b))\n- tests ([81acb44](https://github.com/continuedev/continue/commit/81acb446019a773ce2df98467e4dcb8e5c2a428e))\n- tests ([deea8f1](https://github.com/continuedev/continue/commit/deea8f1f44403b6d7d7bb4236ff5acb8d38bca01))\n- The selected item in inline chat is always the first one ([97bc074](https://github.com/continuedev/continue/commit/97bc07453ba067f0438192d8dc6cc1cc9a86d962))\n- timeout ([b9f161f](https://github.com/continuedev/continue/commit/b9f161f6d8b4e7400dff25912881f41f472a9137))\n- timeouts ([0bf9169](https://github.com/continuedev/continue/commit/0bf9169a968a42dfc083696b3e8be5f1997ad7e5))\n- timeouts ([0c4d9eb](https://github.com/continuedev/continue/commit/0c4d9eb819b9f1a41cddd7d0b84dc0e21ee372e9))\n- timeouts ([4c90f3b](https://github.com/continuedev/continue/commit/4c90f3bdd32761b7f34a6f05334dffc085805964))\n- tip tap undo and redo ([88e34fb](https://github.com/continuedev/continue/commit/88e34fb69caae3750621f9f93253a7425535928f))\n- tool use call ([c73f72f](https://github.com/continuedev/continue/commit/c73f72fbb379dae08aa41ac07608d9dc2aa8bc83))\n- transpile dynamic imports to cjs ([#1975](https://github.com/continuedev/continue/issues/1975)) ([1e3e8eb](https://github.com/continuedev/continue/commit/1e3e8ebe408de1bbb5f9fe0dc61fefef3b85ca69))\n- truncate `tagToString` to max filename len ([afb6bd0](https://github.com/continuedev/continue/commit/afb6bd0fb53505e29e961f5a78f0afe36c3b3071))\n- ts ignore declaration file issue w/ dbinfoz ([#1945](https://github.com/continuedev/continue/issues/1945)) ([477ecd6](https://github.com/continuedev/continue/commit/477ecd63acc85be25ec992d4329f6460c42c6785))\n- tsconfig ([8549981](https://github.com/continuedev/continue/commit/8549981e200e7e62f4a5895babc1c87c45b59db0))\n- type errors ([f976e54](https://github.com/continuedev/continue/commit/f976e54dc3b2b8072aef3ecadf259ee1e5299956))\n- type errors ([f575a36](https://github.com/continuedev/continue/commit/f575a3683c499f04766ac68af6b8e214ed2c3514))\n- type errors ([cd82f4c](https://github.com/continuedev/continue/commit/cd82f4c61fdff90c8ed0e845a3b2ee8b0ce1be9d))\n- type errors ([ea6b234](https://github.com/continuedev/continue/commit/ea6b2344d0b2cde7def58de549e604142a40a353))\n- type errors ([f19b6b8](https://github.com/continuedev/continue/commit/f19b6b8abdb4dfb64c804efee851dee0bb00471c))\n- type errors ([5c49505](https://github.com/continuedev/continue/commit/5c49505f2071504481c17976670d883efebddb4a))\n- type errors ([cdc9600](https://github.com/continuedev/continue/commit/cdc9600e689161877ff7416f63baa9dcbfa6c18a))\n- type errors ([7ee4d8d](https://github.com/continuedev/continue/commit/7ee4d8db505924a4d0edd5ef3ac49f4bb9a706b6))\n- type errors ([70ae585](https://github.com/continuedev/continue/commit/70ae58594cd136e16a53fcfde88a227dbec1104f))\n- type errors ([34489b1](https://github.com/continuedev/continue/commit/34489b14de0fc1c5cfb300a624b3b8571372e532))\n- type errors / formatting ([9dba473](https://github.com/continuedev/continue/commit/9dba4730b958e05ffad6e0bab74effefd228cd6c))\n- types ([7b9c5cc](https://github.com/continuedev/continue/commit/7b9c5cc4f062e2882706919b1effe34a460f45b8))\n- types ([82cad59](https://github.com/continuedev/continue/commit/82cad596c85d0ee0b17ceccdf1f6cca5811b8f94))\n- types ([5faecca](https://github.com/continuedev/continue/commit/5faecca09f27a7490b4050b7191197c74fe38299))\n- types ([be27552](https://github.com/continuedev/continue/commit/be27552e3330b3a18ecb1065b6ab7210a4375826))\n- types ([4b120e5](https://github.com/continuedev/continue/commit/4b120e543e7c2cec0d33714cfebe88800604fbfd))\n- types ([44212e4](https://github.com/continuedev/continue/commit/44212e45828bb72c035bf63de4c13342935dcf0f))\n- types ([72b253d](https://github.com/continuedev/continue/commit/72b253d14faf2d19832b0307fb4fba1e9aed11dd))\n- types ([a88f5d7](https://github.com/continuedev/continue/commit/a88f5d771ced366926c9d6d821c697e52847a2f9))\n- types ([41a7916](https://github.com/continuedev/continue/commit/41a79166aadc2bbbe80145d12bfeb1336d94c7e2))\n- typo ([be874ad](https://github.com/continuedev/continue/commit/be874addb463f7db81c43b52f6e69f9de1c11fa3))\n- **typo:** customize overview doc ([d4dfb69](https://github.com/continuedev/continue/commit/d4dfb699af1f5bffc923a5d850f877ed45ad74a6))\n- undefined in title ([dcdfa59](https://github.com/continuedev/continue/commit/dcdfa59db43ae4e864e5cb1ca04b6804cca64d2f))\n- unnecessary dependency ([ade83fd](https://github.com/continuedev/continue/commit/ade83fd0c6b3f2d3bb8175b6bdbdc0a6e9735c5b))\n- unnecessary scroll delay ([183ae03](https://github.com/continuedev/continue/commit/183ae03ee37ba6629201211e9a63fbcb04ebc527))\n- unset system message ([4480ee3](https://github.com/continuedev/continue/commit/4480ee3ac358e60182304e0e052c27ae45815e2f))\n- update adf2md package ([ecbc123](https://github.com/continuedev/continue/commit/ecbc1234f7419f7921b31354594e5841363c54f5))\n- Update Amazon Bedrock documentation and categorization ([b156d8f](https://github.com/continuedev/continue/commit/b156d8fd4c103e520ad720853752a4f9033ef68f))\n- update CodebaseIndexer path ([#1870](https://github.com/continuedev/continue/issues/1870)) ([00d19e6](https://github.com/continuedev/continue/commit/00d19e623f8abb04fc4a022d354a909e2087aa3c))\n- update imports in CodeSnippetsIndex.ts ([434e4ee](https://github.com/continuedev/continue/commit/434e4ee44b513db15d502c6a9d52ceb17fa805c3))\n- update install script ([91f87f8](https://github.com/continuedev/continue/commit/91f87f8fd0ca328a91c9fd033c58fd5d2db02892))\n- update intellij getDiff to return string[] ([1672d9f](https://github.com/continuedev/continue/commit/1672d9f864e99436b8f309af3529f2fda341172d))\n- update snippets index to upsert cmds ([676f0d9](https://github.com/continuedev/continue/commit/676f0d9335df7bf7f42aae3434847cc0032c3c0a))\n- update spacing ([a9d6597](https://github.com/continuedev/continue/commit/a9d6597a3fa054d093cb63fa5859e24b8f8d571b))\n- update streamId matching ([8a1ce0f](https://github.com/continuedev/continue/commit/8a1ce0f1f06e92b7bd140d7eabd8393dc5993fea))\n- update streamResponse function to use streaming decoding ([#1436](https://github.com/continuedev/continue/issues/1436)) ([2d1155d](https://github.com/continuedev/continue/commit/2d1155d7d4441042480bf928398b7a15a58d92d2))\n- uri utils ([f1026c6](https://github.com/continuedev/continue/commit/f1026c6a2cfdd9424097d2103a5ffc2216e402d9))\n- use `cmd` for windows MCP connections ([b8ca4cb](https://github.com/continuedev/continue/commit/b8ca4cb6ee91670e87ae331f5e4bab24a42adbe6))\n- use bolt icon for shortcuts ([ff2116b](https://github.com/continuedev/continue/commit/ff2116bacc4537e09792ce9817b68e9cdfacb93e))\n- use dir hash in `tagToString` ([9c2a780](https://github.com/continuedev/continue/commit/9c2a780fa986f5f98c3231a12971d3f16c7a8e74))\n- use exponential backoff when checking ollama ([ebbc2fd](https://github.com/continuedev/continue/commit/ebbc2fd55df1fc43e1eacceba042fa4c0bc38e77))\n- use ide scheme auth redirect ([bae74c1](https://github.com/continuedev/continue/commit/bae74c1b324b51c2e9b14b548f92557c75025f51))\n- use index when sending chat input ([9398945](https://github.com/continuedev/continue/commit/93989459e3806f1ba6d647036dc9db064bf30e38))\n- use introduction layout ([2b71869](https://github.com/continuedev/continue/commit/2b7186973695703a48daacea46c076231d308ddf))\n- Use MCP server_name from JSON config as display name ([481d60a](https://github.com/continuedev/continue/commit/481d60a74d9b7cd0492aa5875d89e5b4e2e00c51))\n- use new edit for \"Generate code\" cmd ([aac6f80](https://github.com/continuedev/continue/commit/aac6f805eb35107e7d8754413d955ccfeb964576))\n- use prompt file sys msg ([093225d](https://github.com/continuedev/continue/commit/093225d4fed4d5249ca506b9867f5b5305ca4003))\n- use proper alternating colors in KeyboardShortcuts.tsx ([f1bfcec](https://github.com/continuedev/continue/commit/f1bfcec8257c2b3f99fa99a643b0464239600490))\n- vitest ([3bc4bb7](https://github.com/continuedev/continue/commit/3bc4bb710cfb2d9ef340a61f7c0ca27cb2540a28))\n- vitest ([ceeb87e](https://github.com/continuedev/continue/commit/ceeb87e81b8848517dfc604025f11bf46e1955b0))\n- **vscode:** handle null values for edits and page in getSidebarContent ([#618](https://github.com/continuedev/continue/issues/618)) ([f8a02ad](https://github.com/continuedev/continue/commit/f8a02add1775805246410ee3df396a4028ff2ec9))\n- **vscode:** improve forceAutocomplete command reliability ([cc50768](https://github.com/continuedev/continue/commit/cc507687eb36be5b7fd59eac433a9a908c1ee07e))\n- **vscode:** keybinding full screen toggle shortcut ([#625](https://github.com/continuedev/continue/issues/625)) ([2921504](https://github.com/continuedev/continue/commit/292150476c1cfcace2f6434d11c3a2dab1e5ccdb))\n- wait for permissions before initializing messenger ([7d16b8a](https://github.com/continuedev/continue/commit/7d16b8a870ebf5348713fb116c109a646fe7c79e))\n- wait for sidebar before other commands ([1566adc](https://github.com/continuedev/continue/commit/1566adcaeb3bc5258a563b208a511cdfb3938d7a))\n- watcher ([f41c819](https://github.com/continuedev/continue/commit/f41c8194fc11da9c76de0531cccd73df78a4b1c2))\n- watcher ([be153f8](https://github.com/continuedev/continue/commit/be153f8ab966e210d6249ff179c4ec1580507d88))\n- whitespace in new line completions ([4db2637](https://github.com/continuedev/continue/commit/4db2637d5b65d427e959b7fd607a65ce4140c355))\n- workflow ([e5fb4f1](https://github.com/continuedev/continue/commit/e5fb4f17ad9d3b6e72d6c7e0920357e607c0e776))\n- wrap editor buttons when out of space ([#1727](https://github.com/continuedev/continue/issues/1727)) ([b38ec0e](https://github.com/continuedev/continue/commit/b38ec0e6153e9077d1e39dfd7a453746da31c75b))\n- wrong shortcut for JB ([b358be2](https://github.com/continuedev/continue/commit/b358be2cfb798bfb6646f5015117199e2ad3a6e2))\n\n### Features\n\n- :adhesive_bandage: ca_bundle_path for maybeproxyopenai ([1018cd4](https://github.com/continuedev/continue/commit/1018cd47306f95dde35e1a0cc6b2a830444af389))\n- :adhesive_bandage: QueuedLLM for simultaneous reqs (LM Studio) ([e9d2891](https://github.com/continuedev/continue/commit/e9d289173ec28a1a90ae58b1834c476bb46834b8))\n- :art: custom prompt templates per model ([2e69e11](https://github.com/continuedev/continue/commit/2e69e117e198698f57bda06794cf030afbfe69e9))\n- :art: small changes to /codebase ui ([1b68904](https://github.com/continuedev/continue/commit/1b689046a7f9323c7bd56e14d403675db0b38d54))\n- :bricks: Enable terminals for additional vscode Remote Host Types ([1500281](https://github.com/continuedev/continue/commit/1500281bb547f7308aa7316f6783072e4f81fbc1))\n- :bug: kill old server if needed ([a34046b](https://github.com/continuedev/continue/commit/a34046bbbe817f81cd6d8b7ff9025413589571aa))\n- :children_crossing: display troubleshooting link when loading ([698dccf](https://github.com/continuedev/continue/commit/698dccf474619963de0312d36af6d01e3df8b47a))\n- :children_crossing: drag continue to right bar tip ([04b1fde](https://github.com/continuedev/continue/commit/04b1fde6ea77e7698870063c3f588da93d763544))\n- :children_crossing: keep file context up to data by listening for filesystem events ([#396](https://github.com/continuedev/continue/issues/396)) ([b6435e1](https://github.com/continuedev/continue/commit/b6435e1e479edb1e4f049098dc8522e944317f2a))\n- :children_crossing: more keyboard shortcuts ([bd202df](https://github.com/continuedev/continue/commit/bd202df41755c581844d0ab1773ba55968b15450))\n- :children_crossing: sort history by reverse date ([fd77a4b](https://github.com/continuedev/continue/commit/fd77a4bd6b255260d0f4cad11947b38f4d30030e))\n- :children_crossing: tip to debug ([ec74169](https://github.com/continuedev/continue/commit/ec741697c42745d29539be08bc3e01dcd86b1643))\n- :construction: create new sessions ([19060a3](https://github.com/continuedev/continue/commit/19060a30faf94454f4d69d01828a33985d07f109))\n- :construction: first work on URLContextProvider ([6467759](https://github.com/continuedev/continue/commit/6467759012a139e76dcf022a681355f7d310a30d))\n- :construction: react-router-dom work ([31e7c98](https://github.com/continuedev/continue/commit/31e7c9828f985eceb16b4c9c749cc5d4d9fd5beb))\n- :construction: Router and new history page ([f19345c](https://github.com/continuedev/continue/commit/f19345c652cfcf1bdf13d0a44a2f302e0cd1aa4c))\n- :construction: successfully loading past sessions ([c255279](https://github.com/continuedev/continue/commit/c25527926ad1d1f861dbed01df577e962e08d746))\n- :construction: work on EditableDiv - better ctx prov. UI ([e33d579](https://github.com/continuedev/continue/commit/e33d579a1d2b643842827925d032c3de92cf5217))\n- :egg: getting to know users form ([5dcdcd8](https://github.com/continuedev/continue/commit/5dcdcd81da2050825212e216bf5e7e69678d8c6e))\n- :fire: fix duplication in reference ([1e3c8ad](https://github.com/continuedev/continue/commit/1e3c8adabba561eeef124144f3a2ef36d26334b4))\n- :globe_with_meridians: alpaca chat template ([568771d](https://github.com/continuedev/continue/commit/568771d9b94280f1cb47aae863e8faf168eb052b))\n- :green_heart: ship with binary ([2751dde](https://github.com/continuedev/continue/commit/2751ddeb2dd8150a0d7de6c5b65e275e1aa0e155))\n- :lipstick: add context button (plus icon) ([2b35e5f](https://github.com/continuedev/continue/commit/2b35e5f5cff948ca7d4f207b23db68f0da248a95))\n- :lipstick: add textgenwebui as official option in Ui ([09ac7a7](https://github.com/continuedev/continue/commit/09ac7a7fc07d915ac6f3ef96c8e8d1894b7719b9))\n- :lipstick: better loading experience ([e19c918](https://github.com/continuedev/continue/commit/e19c918bb1c517a6a119ae8437c46e0724d2be9d))\n- :lipstick: fixed footer and change button color ([48b8f1f](https://github.com/continuedev/continue/commit/48b8f1f0ad89a2b4e35f49c360576dd5aa99a7c0))\n- :lipstick: gallery banner color ([0647a43](https://github.com/continuedev/continue/commit/0647a43a24c50ff0e52f23c49d979bddfcfbcd87))\n- :lipstick: handful of UI improvements ([ceafdf1](https://github.com/continuedev/continue/commit/ceafdf18c9d9f0f8769d4a9e45c8a407179161c5))\n- :lipstick: improvements to keyboard shortcuts ([7bf8e5b](https://github.com/continuedev/continue/commit/7bf8e5b56a518979bc1d2602b8eb4a2caf2b5fdb))\n- :lipstick: more ui improvements ([f9a84bd](https://github.com/continuedev/continue/commit/f9a84bd2d65b3142cbcfcdd8e1e9394c9d4b458e))\n- :lipstick: more ui improvements ([6e3ff01](https://github.com/continuedev/continue/commit/6e3ff0173e79f5374da9962c964559e0fb7165f5))\n- :lipstick: nested context provider dropdown ([8d423fd](https://github.com/continuedev/continue/commit/8d423fd8d1d5b136e8138a906e8594ab93ec1982))\n- :lipstick: nicer \"continue server loading\" UI ([8070ce1](https://github.com/continuedev/continue/commit/8070ce17c6d666436df38c684f5868ee7f689422))\n- :lipstick: query input indicator for ctx provs ([4362a51](https://github.com/continuedev/continue/commit/4362a51214a683bfe1efd424ddb226d4e636eeed))\n- :lipstick: setting to change font size ([4ea1760](https://github.com/continuedev/continue/commit/4ea176007d2228364d4d3fa4519898047cef988f))\n- :lipstick: small ui tweaks, detached child process ([0181d62](https://github.com/continuedev/continue/commit/0181d6236d8b74c80adb62648fd6571431cf3210))\n- :lipstick: sticky top bar in gui.tsx ([ef86d66](https://github.com/continuedev/continue/commit/ef86d661d54295c1abb9712557080f1838f96b33))\n- :lipstick: UI Improvements! ([ae058c6](https://github.com/continuedev/continue/commit/ae058c6bac7ea37108e2894e419a22dfb95fd3ff))\n- :lipstick: update icon and description ([92e7c9b](https://github.com/continuedev/continue/commit/92e7c9bb627a5559769e0eca23e02e106d2cfe96))\n- :lipstick: update marketplace icon for pre-release ([cc98ad8](https://github.com/continuedev/continue/commit/cc98ad86d561d26c97dfdb24607a1d70afbcd2a1))\n- :loud_sound: add context to dev data loggign ([8ac1518](https://github.com/continuedev/continue/commit/8ac15184aaa30d13bf168ff5123a12fb7a2dd39f))\n- :loud_sound: display any server errors to the GUI ([daabebc](https://github.com/continuedev/continue/commit/daabebcc5d6df885a508582c0ca13e659305d2ff))\n- :loud_sound: fallback unique id when vscode returns someValue.machineId ([c479442](https://github.com/continuedev/continue/commit/c47944260c5600e49d83568b3c4bafa3b7c2a37e))\n- :loud_sound: give users access to Continue server logs ([5d97349](https://github.com/continuedev/continue/commit/5d973490687c40922f2b7a2ddf2a3e19c207eb0f))\n- :loud_sound: light telemetry or context providers ([2959042](https://github.com/continuedev/continue/commit/2959042fa5a940aa4e8851b9d4db91f0f86092ff))\n- :loud_sound: telemetry for vscode vs. jetbrains ([257cef6](https://github.com/continuedev/continue/commit/257cef697c93d2f2f59936587834908bd69ae842))\n- :memo: embeddings experimental walkthrough ([e1ce1fe](https://github.com/continuedev/continue/commit/e1ce1fefee6a3f4c17ac568ba87cb7a4bcf65795))\n- :memo: note about where session data is stored ([8ada89b](https://github.com/continuedev/continue/commit/8ada89b0f66f9e746394ee64591359537fe0c7f0))\n- :money_with_wings: free trial usage indicator ([354a3f4](https://github.com/continuedev/continue/commit/354a3f493074b1fb63ff4f206a94c35f05673e99))\n- :mute: complete removal of telemetry when allow_anonymous_telemetry false ([ae7dffa](https://github.com/continuedev/continue/commit/ae7dffa211af209aea2ca13b37729e390047dd7c))\n- :necktie: allow timeout param for OpenAI LLM class ([404f7f8](https://github.com/continuedev/continue/commit/404f7f8089190d04c05957dc653baff44f342dc7))\n- :recycle: load preset_urls at load_index ([3dabc4b](https://github.com/continuedev/continue/commit/3dabc4bd6c72e2d12afb059040ca75f606e47d9d))\n- :rocket: headers param on LLM class ([e16b8ff](https://github.com/continuedev/continue/commit/e16b8ff5f2d9187f2b207addc1cd70d0cacbf9c8))\n- :sparkles: [@terminal](https://github.com/terminal) context provider ([40cfabb](https://github.com/continuedev/continue/commit/40cfabb8ce8afe20e51ca4bafddc6a0b4755bf2c))\n- :sparkles: /cmd slash command ([011877c](https://github.com/continuedev/continue/commit/011877c09e88ffcc715defc33e5c74c71ccc8aea))\n- :sparkles: /share slash command ([4e38043](https://github.com/continuedev/continue/commit/4e3804351b76cc763d904f572ad525b651d8bc00))\n- :sparkles: add edit templates to model packages ([1de976a](https://github.com/continuedev/continue/commit/1de976a6a11a0b945d59800b5a58f354808a49fc))\n- :sparkles: add max_tokens option to LLM class ([ff2a397](https://github.com/continuedev/continue/commit/ff2a3978a1e2c95a4e288b56411bf0c32b86757b))\n- :sparkles: add searchcontextprovider to default_config.py ([64f41fc](https://github.com/continuedev/continue/commit/64f41fc8a0a2616fe7074d0a49e7642fd462c95d))\n- :sparkles: add stop_tokens option to LLM ([a16ba7a](https://github.com/continuedev/continue/commit/a16ba7a0166dbf9062ee4616e3ccfbff377e9f4b))\n- :sparkles: Add support for Claude Sonnet 4 ([b24a76f](https://github.com/continuedev/continue/commit/b24a76fdabb5eccada838d8fe5ed5834c0120df1))\n- :sparkles: add urlcontextprovider back to default config ([570891e](https://github.com/continuedev/continue/commit/570891e0201769defeabec95a58c997f6d5f3889))\n- :sparkles: allow AzureOpenAI Service through GGML ([1343d12](https://github.com/continuedev/continue/commit/1343d1227cc86c860fb12695e64eaeae1384f72a))\n- :sparkles: allow changing the summary prompt ([5c8b28b](https://github.com/continuedev/continue/commit/5c8b28b7fddf5b214de61102c768ef44d4087870))\n- :sparkles: allow custom OpenAI base_url ([cb0c815](https://github.com/continuedev/continue/commit/cb0c815ad799050ecc0abdf3d15981e9832b9829))\n- :sparkles: alt+cmd+d to automatically debug terminal! ([d0483ba](https://github.com/continuedev/continue/commit/d0483ba15b4ad13399a3385ae351cf33cca3db7f))\n- :sparkles: auto-reload for config.py ([e652e90](https://github.com/continuedev/continue/commit/e652e90806b84eb409c496dd0903a4817243edc2))\n- :sparkles: autoscrolling ([e6d369f](https://github.com/continuedev/continue/commit/e6d369f4312f0c8d175251e149c62d08608cb18c))\n- :sparkles: back button on history page ([aafa5d5](https://github.com/continuedev/continue/commit/aafa5d5ec91a533f70d644e4d3fadd6f388c3e4b))\n- :sparkles: change proxy url for openai class ([32a9a47](https://github.com/continuedev/continue/commit/32a9a477d33acd5cdde08164ebeb355b27a656b5))\n- :sparkles: codelens in config.py ([58cd4db](https://github.com/continuedev/continue/commit/58cd4db2534aba9fed98925e68dc342efbc54fb0))\n- :sparkles: Continue Quick Fix ([9af39a6](https://github.com/continuedev/continue/commit/9af39a67829a6770b93ffdaa6ea70af3125c7daf))\n- :sparkles: Continue Quick Fix ([52cd93a](https://github.com/continuedev/continue/commit/52cd93ad73f7df6a5140b7d629e4f6e473dc0380))\n- :sparkles: delete context groups ([2d3d96e](https://github.com/continuedev/continue/commit/2d3d96e5b55a225eb97251850909eb7a0a7242ed))\n- :sparkles: diff context provider ([99db0da](https://github.com/continuedev/continue/commit/99db0da9d68c64d0b5adcab21e07c2db438c2404))\n- :sparkles: display model params for previous prompts ([b1b30a4](https://github.com/continuedev/continue/commit/b1b30a459cbd589a471e1528ebfa9aaeb0514968))\n- :sparkles: edit previous inputs ([c6a3d8a](https://github.com/continuedev/continue/commit/c6a3d8add014ddfe08c62b3ccb1b01dbc47495f5))\n- :sparkles: EmbeddingContextProvider ([c6a1255](https://github.com/continuedev/continue/commit/c6a12550ffca1ffe35630e7aa9af6913ddbe0675))\n- :sparkles: FileTreeContextProvider ([8bd76be](https://github.com/continuedev/continue/commit/8bd76be6c0925e0d5e5f6d239e9c6907df3cfd23))\n- :sparkles: filter history by workspace ([0757bd2](https://github.com/continuedev/continue/commit/0757bd2b556996b9c434ac43e3e4a3b042ef5802))\n- :sparkles: Give the terminal color and ansi escape rendering ([e83290b](https://github.com/continuedev/continue/commit/e83290bd3a921929052b0c0c91751800e7e9fd2c))\n- :sparkles: highlight code on cmd+shift+L ([a1fdf16](https://github.com/continuedev/continue/commit/a1fdf164b776c5ff4ddfa1c4cad66e41de4254c0))\n- :sparkles: huggingface inference api llm update ([bbf7973](https://github.com/continuedev/continue/commit/bbf7973ec091823c4197d59daaf151b748ee52fc))\n- :sparkles: huggingface tgi LLM class ([a0e2e2d](https://github.com/continuedev/continue/commit/a0e2e2d3d606d8bf465eac541a84aa57316ee271))\n- :sparkles: improved edit prompts for OS models ([1785e92](https://github.com/continuedev/continue/commit/1785e92f118b855f4a655d9b617d54b5c857a6ef))\n- :sparkles: improved model dropdown ([2f792f4](https://github.com/continuedev/continue/commit/2f792f46026a6bb3c3580f2521b01ecb8c68117c))\n- :sparkles: improvement to @ search rankings ([5590f63](https://github.com/continuedev/continue/commit/5590f63e42fda38d780bdc390361a65b65576498))\n- :sparkles: llama-2 support ([72d18fb](https://github.com/continuedev/continue/commit/72d18fb8aaac9d192a508cd54fdb296321972379))\n- :sparkles: LlamaCpp LLM subclass ([fc9e8e4](https://github.com/continuedev/continue/commit/fc9e8e4e325782409258dd483e36abf441051ee6))\n- :sparkles: LSP connection over websockets ([a6d21f9](https://github.com/continuedev/continue/commit/a6d21f979fce6135fd76923478f76000b1b343cf))\n- :sparkles: make follow-up edits ([73ae5d3](https://github.com/continuedev/continue/commit/73ae5d306c16d7c372e831d3ca41067a62c8481f))\n- :sparkles: Make the terminal command aware of its OS, platform and shell ([1a786f6](https://github.com/continuedev/continue/commit/1a786f605da47abdfc66b02f4e88044ca95df960))\n- :sparkles: manually running server option ([29940ea](https://github.com/continuedev/continue/commit/29940ea4223194cc32f6324534ad75db9e39305a))\n- :sparkles: more model options, ollama error handling ([e2a7d4a](https://github.com/continuedev/continue/commit/e2a7d4a3c7832f8788feccf4168c13ec87a31fb2))\n- :sparkles: ollama LLM class ([402883e](https://github.com/continuedev/continue/commit/402883e0661c24c418fb5aa93832c6f62dc97a63))\n- :sparkles: refactor via search Step ([9cd249e](https://github.com/continuedev/continue/commit/9cd249ee007911037639281c6d7590889ad7b467))\n- :sparkles: run continue immediately from pypi pkg ([70f6da9](https://github.com/continuedev/continue/commit/70f6da9b1ad190a967974fb477db669cbb5c928f))\n- :sparkles: saved context groups ([c98f860](https://github.com/continuedev/continue/commit/c98f860460767fe14f8fbf139150b1bd1ee2ff12))\n- :sparkles: Search context provider with ripgrep ([4b9dd4c](https://github.com/continuedev/continue/commit/4b9dd4cf8e853e17d92fb76fc726260d79e4bd7a))\n- :sparkles: select custom model to use with edit step ([c1e5039](https://github.com/continuedev/continue/commit/c1e5039731941eb6b6eea166edd433cd49d4e858))\n- :sparkles: select model from dropdown ([044b7ca](https://github.com/continuedev/continue/commit/044b7caa6b26a5d78ae52faa0ae675abc8c4e161))\n- :sparkles: select model param count from UI ([105afec](https://github.com/continuedev/continue/commit/105afeccca903072bc48772bdaf8f100f996c4a7))\n- :sparkles: set session timeout on GGML requests ([d04eec7](https://github.com/continuedev/continue/commit/d04eec7ee97319a6bcc48d289cd6eb3e0d9b8e19))\n- :sparkles: set static urls for contextprovider ([d2b1aed](https://github.com/continuedev/continue/commit/d2b1aedcedf950d792baee202efdab199b05e57e))\n- :sparkles: support browser-based IDEs with createMemoryRouter ([c9d96be](https://github.com/continuedev/continue/commit/c9d96be5615b9d193a1eeff9ab00da7ca0fe0b6b))\n- :sparkles: support for Together.ai models ([8456b24](https://github.com/continuedev/continue/commit/8456b24318b13ea5d5dabec2328dd854f8a492b4))\n- :sparkles: support stablecoder with replicate LLM ([d5e8688](https://github.com/continuedev/continue/commit/d5e86883f05fe3e99e1d6ff64241a48f935cc927))\n- :sparkles: testing improved prompting for stablecode ([6112f26](https://github.com/continuedev/continue/commit/6112f26888086ccd47ca6bcfefdbc5b82ea86879))\n- :sparkles: testing in ci, final test of ([cbd7656](https://github.com/continuedev/continue/commit/cbd7656bb4c9aebfe98c746111af52cf7192aa1b))\n- :sparkles: text-gen-webui, cleaning config and welcome ([36d517e](https://github.com/continuedev/continue/commit/36d517e37d87b1bd39d6027577714b60c32e81e9))\n- :sparkles: verify_ssl option for all LLMs ([e0522b9](https://github.com/continuedev/continue/commit/e0522b92cfa80491718de07928ce6a31850dab70))\n- :technologist: bit of customization for DefaultPolicy ([3966790](https://github.com/continuedev/continue/commit/396679009fef21e13c1a6095212d1bd68e7f2a86))\n- :technologist: button to view logs when loading ([b83eb52](https://github.com/continuedev/continue/commit/b83eb52c98d637ab3e3becf98aed9899821ea00d))\n- :technologist: github workflow to measure issue/PR stats ([f75c423](https://github.com/continuedev/continue/commit/f75c42332c44c0d2a1a7e7a7ea32c2ef346df609))\n- :white_check_mark: update test and add model telemetry ([3541d6a](https://github.com/continuedev/continue/commit/3541d6a770c919e1f2e55a1ae53c4fc3abe31aa7))\n- :wrench: update default config.py imports ([e2d0baf](https://github.com/continuedev/continue/commit/e2d0baf39348597bdd1015897152f4e3bee0744d))\n- :zap: queue messages before load, then send ([4c8b561](https://github.com/continuedev/continue/commit/4c8b56135b0a1862a4f1984e80aa1409f15e177d))\n- :zap: reduce default max_tokens ([8fff1a8](https://github.com/continuedev/continue/commit/8fff1a811c477874482b65d014e4c5565d4a8649))\n- `create_rule_block` tool ([2c8032c](https://github.com/continuedev/continue/commit/2c8032c3397f6ac8c26b8ce5e5b2fe89d079d3da))\n- `description` in markdown yaml ([e4b70db](https://github.com/continuedev/continue/commit/e4b70dbfd843345ae0689a55802462473f5641dc))\n- `globs` on rules and block docs ([24e22db](https://github.com/continuedev/continue/commit/24e22db9d416be54426709948564aee4aa08d54e))\n- `requestRule` tool ([af30fbe](https://github.com/continuedev/continue/commit/af30fbe360def278cd56858d88c5373c80a2dab6))\n- add \"Gathering context...\" indicator ([dd865ea](https://github.com/continuedev/continue/commit/dd865eadea014ccc8588a49e6d9c338d89734c9d))\n- add \"onboarding\" slash command ([#1961](https://github.com/continuedev/continue/issues/1961)) ([5819ffb](https://github.com/continuedev/continue/commit/5819ffb43901eec8ad6af85737c28896f20c4e6e))\n- add `globs` to create rule tool ([047e4e4](https://github.com/continuedev/continue/commit/047e4e418b7f252cb303f19992025acc4fb79d4f))\n- add `index.ts` to sdk ([ae31c54](https://github.com/continuedev/continue/commit/ae31c54011ac365e560cb02b9851a6806641503a))\n- add `no-negated-condition` eslint ([eb7d67d](https://github.com/continuedev/continue/commit/eb7d67dcf1a338c473a1bb7f787e4f35e62f8f73))\n- add `stream` in defaultCompletionOptions yaml ([f72e293](https://github.com/continuedev/continue/commit/f72e293e2f6251e1cca632dffa7ec14554016a73))\n- add `tsc:watch` cmd to vs code ([1d5e164](https://github.com/continuedev/continue/commit/1d5e164325e8fe8e66759baf2ac47ce766355faf))\n- add accept/reject all btns ([011e2c2](https://github.com/continuedev/continue/commit/011e2c264407ff72a0bc90980ec7d8250da4bbd6))\n- add agentinteraction dev data ([971c4de](https://github.com/continuedev/continue/commit/971c4de33e1a3f117eafe9d64bf175c0d2a849ef))\n- add animated ellipsis to lump ([e14163c](https://github.com/continuedev/continue/commit/e14163cda2ff1de2e90621f1de2341fd90b136d2))\n- add API key support for TEI reranker ([5dc25b5](https://github.com/continuedev/continue/commit/5dc25b5eb3b1f914d106a18bbb34f7d28e3039f3))\n- add azure provider config ([#1764](https://github.com/continuedev/continue/issues/1764)) ([c9635de](https://github.com/continuedev/continue/commit/c9635def237e0bb4e1d899057e6b651b6a6cd1b2))\n- add best experience onboarding ([8b30504](https://github.com/continuedev/continue/commit/8b305046eff820999e805c8b2cd6400e3572da1b))\n- add chat scrollbar visibility configuration support ([14eaf32](https://github.com/continuedev/continue/commit/14eaf3272cde744ab6390d7113c2cb507e0d6734))\n- add Claude 3.7 support to toolSupport.ts ([50ad91b](https://github.com/continuedev/continue/commit/50ad91bced864272c7f1bb92c13d7c910a0b6be2))\n- Add cloudflare as provider ([ad8743a](https://github.com/continuedev/continue/commit/ad8743a9b563ae6e09cc140a0b3f9f202715e3b5))\n- add code that comment is based on ([e3653f9](https://github.com/continuedev/continue/commit/e3653f98f08e52fa6a55d41ca147add1c8e5515a))\n- add combobox for edit file selection ([7c6295f](https://github.com/continuedev/continue/commit/7c6295feeb32a478a2d8867f0a2b863583f6a97e))\n- add deepseek models for novita ai ([a31c3eb](https://github.com/continuedev/continue/commit/a31c3ebc8334f931281cdb0a348150347c99d0e6))\n- Add docs/getDetails endpoint in JetBrains ([da2dc1e](https://github.com/continuedev/continue/commit/da2dc1ed7de3caa9ae1e8b02a09526f57dd80bb8))\n- add example integration test ([781a792](https://github.com/continuedev/continue/commit/781a792a5939958d05fa316261190c164871c563))\n- add exponential backoff for API requests ([12ffa95](https://github.com/continuedev/continue/commit/12ffa956571acd12b383ba73560d3c42a7a4415c))\n- add file search to quick edit ([#1714](https://github.com/continuedev/continue/issues/1714)) ([21d1b0c](https://github.com/continuedev/continue/commit/21d1b0c16dd9cd454d543e4b387f873e54d89aa5))\n- add fixed version of the nodejs ([cd35857](https://github.com/continuedev/continue/commit/cd35857daf5577f4677f6675344fa8488e624352))\n- add free trial card to onboarding ([#1600](https://github.com/continuedev/continue/issues/1600)) ([9bae5a2](https://github.com/continuedev/continue/commit/9bae5a254df25d25dc848b5c7bf69fc7189e6461))\n- add gif to tutorial card ([e03eb9d](https://github.com/continuedev/continue/commit/e03eb9d5801985d1968d0528dd76767bf3907cbf))\n- add gitlab context class ([efc1f3b](https://github.com/continuedev/continue/commit/efc1f3b9216d5bf503ecb58ff956f26a2bc4a130))\n- add icon for URL ctx item peek ([e901cfd](https://github.com/continuedev/continue/commit/e901cfdf4ae383a2337c2e20182f6eee6d3010f9))\n- add Jira context provider ([#860](https://github.com/continuedev/continue/issues/860)) ([8ba15b1](https://github.com/continuedev/continue/commit/8ba15b16665be871e037ada88d51b3403a8d094e))\n- add Llama 3.1 8B to cloudflare provider options ([#1811](https://github.com/continuedev/continue/issues/1811)) ([bccbff2](https://github.com/continuedev/continue/commit/bccbff273c176c54f0209c5927a19b8c6d9375f9))\n- add Moonshot AI model provider support ([b8a278d](https://github.com/continuedev/continue/commit/b8a278d216db9851aa2b0772e0313ff4bc12c7cc))\n- add novita info ([94d8d39](https://github.com/continuedev/continue/commit/94d8d39cfc8d21f2544633330370d99ef17e0fdf))\n- Add num_treads to ollama along with docs ([#863](https://github.com/continuedev/continue/issues/863)) ([d71721d](https://github.com/continuedev/continue/commit/d71721d71540cb084d67559e850440129faa67c0))\n- add openai wrapper for sdk ([ab9a248](https://github.com/continuedev/continue/commit/ab9a248572a46e00f0fa6441af861dd260c4cb65))\n- add OpenAI, xAI, Replicate and free-trial model options to config schema ([89a90f9](https://github.com/continuedev/continue/commit/89a90f99baaf493f184a09aa6083a606076f9771))\n- Add prompt caching support ([24f9960](https://github.com/continuedev/continue/commit/24f9960e27ad065fec611661e69726c9be589042))\n- add proxy support for ripgrep download ([d42c050](https://github.com/continuedev/continue/commit/d42c050e3645b4eb4635dd5a73ae12107adca701))\n- add Quick Actions CodeLens feature ([#1674](https://github.com/continuedev/continue/issues/1674)) ([fdf3408](https://github.com/continuedev/continue/commit/fdf3408e0c9c2df749b5775d3c906abfdf40e799)), closes [#1536](https://github.com/continuedev/continue/issues/1536) [#1456](https://github.com/continuedev/continue/issues/1456) [#1564](https://github.com/continuedev/continue/issues/1564) [#1576](https://github.com/continuedev/continue/issues/1576) [#1570](https://github.com/continuedev/continue/issues/1570) [#1582](https://github.com/continuedev/continue/issues/1582) [#1600](https://github.com/continuedev/continue/issues/1600) [#1618](https://github.com/continuedev/continue/issues/1618) [#1626](https://github.com/continuedev/continue/issues/1626) [#1637](https://github.com/continuedev/continue/issues/1637)\n- add Qwen2.5-Coder support ([387a76a](https://github.com/continuedev/continue/commit/387a76aa5bab873565de25a2d269f0b5b1a53f1e))\n- add redux state for card logic ([5f32924](https://github.com/continuedev/continue/commit/5f32924becd3e8b03d19ffb4a5e4a2dcbfb6dbc3))\n- add redux state for mfe ([6e5236a](https://github.com/continuedev/continue/commit/6e5236a58a896ab78faa1672e56237b4f4684877))\n- add render util function ([413530a](https://github.com/continuedev/continue/commit/413530a029e32ab8e77520c0e5265cd37a84b24c))\n- add rich quick pick for quick edit ([#1706](https://github.com/continuedev/continue/issues/1706)) ([f3b15eb](https://github.com/continuedev/continue/commit/f3b15eb1b14dcc3f4c28ebfb95150b0d6627cecb))\n- add scope selector ([b78fdc4](https://github.com/continuedev/continue/commit/b78fdc4147e1ccb388b900a65099a2c61ba6ca26))\n- Add signature column to code_snippets table ([a535e9e](https://github.com/continuedev/continue/commit/a535e9e15fb3041fec732574636221e0a83dccfa))\n- add slash command cmd ([d4e1609](https://github.com/continuedev/continue/commit/d4e1609d2a65aeeece6caa8620a1b45196f40c98))\n- add support for `baseSystemPrompt` ([f6957bb](https://github.com/continuedev/continue/commit/f6957bb856ba5915ad6351a518dc4b4a47536947))\n- Add support for Cloudflare AI Gateway ([#1425](https://github.com/continuedev/continue/issues/1425)) ([837ad1c](https://github.com/continuedev/continue/commit/837ad1c552aef8909b9a1dbe01204ed2571134d7))\n- add support for custom headers in SSE transport ([d392d77](https://github.com/continuedev/continue/commit/d392d7736d5d31804cd54b3c307bbfb6289e5358))\n- add support for Mistral models in toolSupport.ts ([928cf2c](https://github.com/continuedev/continue/commit/928cf2c73752a23fe3fc026bbf3957c5c80d5604))\n- add support for multiple MCP server types (stdio, sse, websocket) in the YAML configuration schema, ensuring backward compatibility with legacy configurations ([63a2ffe](https://github.com/continuedev/continue/commit/63a2ffe9696d90786f030c2af02cec2c463d914e))\n- add tests ([0819bec](https://github.com/continuedev/continue/commit/0819becfd98127a187cc84484dd59bb4348a1ec3))\n- add tutorial card ([#1716](https://github.com/continuedev/continue/issues/1716)) ([cb8b207](https://github.com/continuedev/continue/commit/cb8b207582dec6ce997c2c602c9998a5a64504db))\n- add unified diff instant apply ([3c36b4d](https://github.com/continuedev/continue/commit/3c36b4dc378ddd8c5c5c2cc234e9a605d776d928))\n- add Vertex AI support ([dfaa02c](https://github.com/continuedev/continue/commit/dfaa02c5051c44303e055675324ff6eaaac0e91b))\n- allow input to exit ([18c5019](https://github.com/continuedev/continue/commit/18c50195248f6ca6c64e1eeb3cb8f10e6b85ffb5))\n- allow JetBrains users to index docs ([#1797](https://github.com/continuedev/continue/issues/1797)) ([e0079a4](https://github.com/continuedev/continue/commit/e0079a43721f6002ff59e2eaa79dc8768a6f66f5))\n- allow users to skip local onboaridng ([4685db0](https://github.com/continuedev/continue/commit/4685db02b854ab0a598defe0fdc4a68f8de50cc2))\n- apply e2e tests ([12b3ed9](https://github.com/continuedev/continue/commit/12b3ed9291b4f46e920f8417f1e6585e45b7edd9))\n- apply if state is active, onEnter callback dont working ([fed1651](https://github.com/continuedev/continue/commit/fed165161d1610db282b52230a4d914f4d2ffa62))\n- apply waiting cursor to chat-input enter-button during prompting ([6d08361](https://github.com/continuedev/continue/commit/6d0836189cd1c18cf97b1e604adf4e5f201665b9))\n- assistant select ([3b539d5](https://github.com/continuedev/continue/commit/3b539d53c39a98c361e27db6dfb72a8e14ddae50))\n- assistant select refresh on right ([04f5e3e](https://github.com/continuedev/continue/commit/04f5e3e165fddf3101e6210b1111c0f261793054))\n- **autocomplete:** recently visited ranged service ([811507a](https://github.com/continuedev/continue/commit/811507a406ed543adb4a8f876c3c3f30d8a9be35))\n- better buttons for account dialog ([2a4c69f](https://github.com/continuedev/continue/commit/2a4c69ff87b64e21ad04edfed2aad7b048833b53))\n- better editor content handling in edit ([7c72c4a](https://github.com/continuedev/continue/commit/7c72c4af878e4b683b582e40f483551314e92873))\n- better error handling around ripgrep ([22f6fdf](https://github.com/continuedev/continue/commit/22f6fdfa1be6f84ab8548155465f863cb65a7e30))\n- better type names ([43ebb1a](https://github.com/continuedev/continue/commit/43ebb1ad0a95ac1dfc0ff1e198fe53e8c50f0671))\n- bookmark first 5 prompts by default ([0909ef9](https://github.com/continuedev/continue/commit/0909ef938b6d2928fa650c0e0613c1e106a0315d))\n- brand text fix ([a25afee](https://github.com/continuedev/continue/commit/a25afeeda10c86072f80c025c20c21b167ddaaf0))\n- bugfixes on redux schema updates ([4efd661](https://github.com/continuedev/continue/commit/4efd66137592916167f562ac45533dfffacd35cc))\n- build steps for config-yaml ([73b95f3](https://github.com/continuedev/continue/commit/73b95f314703c266d9e348f9c1ba3dbbb3f3d266))\n- cache org selection results ([13d5668](https://github.com/continuedev/continue/commit/13d566831db9e66ae270ff70e2b5c63304df6458))\n- cache org selection results ([6e81720](https://github.com/continuedev/continue/commit/6e81720186d4215d6a650835f72ed1c6cc20e28f))\n- capture base class and interfaces implemented ([df2bbc8](https://github.com/continuedev/continue/commit/df2bbc89da7f7f9ee406a8fe6d0c4fd244abd488))\n- capture python definitions ([2f84ef0](https://github.com/continuedev/continue/commit/2f84ef0ab9a87aa6300415d30db02e3460a07195))\n- change model desc ([8d3ecaf](https://github.com/continuedev/continue/commit/8d3ecafa3dbd9a3373ec7784eda1feac6b77b684))\n- check for `nvm` version in install script ([b1a93b7](https://github.com/continuedev/continue/commit/b1a93b7a2dd22ab82ef9acdec5f1096c33c44c1b))\n- cleanup apply manager ([4b9bac5](https://github.com/continuedev/continue/commit/4b9bac55ffae743d0221df65f0a14d0981f74aeb))\n- cleanup inline edit code ([90d895e](https://github.com/continuedev/continue/commit/90d895e2e22ee40033c524a109b5777b5c3bf6cd))\n- Client Certificate Options Support ([#1658](https://github.com/continuedev/continue/issues/1658)) ([136bf9e](https://github.com/continuedev/continue/commit/136bf9e0f0ce0193f64fe291922431808c32406e))\n- close files ([d1eca9d](https://github.com/continuedev/continue/commit/d1eca9deeb3c1550982f679a58265593af603453))\n- close tutorial listener ([d4d3102](https://github.com/continuedev/continue/commit/d4d3102b61c800576513cbb07c9d482fe973a0ef))\n- collapsed codeblock by default ([979c247](https://github.com/continuedev/continue/commit/979c2473c61747157e65e1dcd33fd73ccfe404ad))\n- config.json validation ([429e54e](https://github.com/continuedev/continue/commit/429e54ed347446747d206a97cb80c91baf5d407e))\n- configure docs through config.json ([#1864](https://github.com/continuedev/continue/issues/1864)) ([d7dbdff](https://github.com/continuedev/continue/commit/d7dbdfff485f3970b8595c5a2680a012443747a5))\n- consolidate StepContainer ([58d9eb7](https://github.com/continuedev/continue/commit/58d9eb7a7e8102940d59f26a65e01edb130681fd))\n- consolidate toast logic into \"showToast\" ([ba777ed](https://github.com/continuedev/continue/commit/ba777edf4761f8bb781fce11ce45311c0b089048))\n- continue sdk ([c7829bb](https://github.com/continuedev/continue/commit/c7829bbd514b962caad99a4c8c660e94f9e577b4))\n- convert issue templates to issue form templates ([#507](https://github.com/continuedev/continue/issues/507)) ([fa7f2cb](https://github.com/continuedev/continue/commit/fa7f2cbdeb6013ec4bb081cb85988817f54d070c))\n- **core:** add support for Llama models on Bedrock ([#1499](https://github.com/continuedev/continue/issues/1499)) ([53aab1e](https://github.com/continuedev/continue/commit/53aab1e6fd2a0f21fbca1c29b82d1f96d2f5e074))\n- crawl `.mdx` docs ([d9f0c4f](https://github.com/continuedev/continue/commit/d9f0c4f131010bd4dae111b5e4290f3460d49e3a))\n- create \"rebuild index\" dialog ([c978d7c](https://github.com/continuedev/continue/commit/c978d7c871be71cd399d02159be8c1b37a0d36ff))\n- create `ApplyManager` ([652f8c4](https://github.com/continuedev/continue/commit/652f8c4c7b0879f9ab3b2cc1a25c9530de467c94))\n- create docs cache ([3f8afbe](https://github.com/continuedev/continue/commit/3f8afbefeb36c389528d420ef5945c0014ef18c0))\n- create file button in toolbar ([268f55b](https://github.com/continuedev/continue/commit/268f55b001e880fec92ef87df6be2bdf8fc25649))\n- create markdown rules in notch ([c2571e2](https://github.com/continuedev/continue/commit/c2571e2a86e373199fe9660f95fa5b168c776035))\n- create org slice ([71f8508](https://github.com/continuedev/continue/commit/71f85084f22333337a1e817e0502044771d66de4))\n- create profiles slice ([9c2db6b](https://github.com/continuedev/continue/commit/9c2db6b9c7d7fd0a0164b3e2288a4135df759707))\n- create RecentFilesService ([d597e6f](https://github.com/continuedev/continue/commit/d597e6fe5ea632f2bdf762ad2555c056fd736ba8))\n- create stream complete reducer ([ce7d982](https://github.com/continuedev/continue/commit/ce7d98291e30658b387dd5d7de96e0f3577065ab))\n- create SVG tooltip ([0cf152e](https://github.com/continuedev/continue/commit/0cf152ede652d975fd774faa40494020c7514196))\n- dallin feedback pt 2 ([d158fab](https://github.com/continuedev/continue/commit/d158fab740cf803074106717560941cca8cb96c9))\n- dallin's feedback ([e884853](https://github.com/continuedev/continue/commit/e88485348c8b53e9f19d1f9fc0c8b33dd5266df7))\n- Dallin's feedback ([fc1ee33](https://github.com/continuedev/continue/commit/fc1ee33ed912543f596d32ae6eaacbbdbc3bcae4))\n- dallins feedback ([33b1d39](https://github.com/continuedev/continue/commit/33b1d3994654f311dcae385f6ef2ef78696d1d89))\n- dallins feedback ([25b5b8c](https://github.com/continuedev/continue/commit/25b5b8c50b05fe872d74b60ced4a6d8569686ed6))\n- dallins feedback ([6070a64](https://github.com/continuedev/continue/commit/6070a645dd9741b433ad43fd09f37b42ec614a3e))\n- disable chat-input enter-button during prompting ([e01fbe7](https://github.com/continuedev/continue/commit/e01fbe7f2cf6f53f5a611afe6df53dfc5f710e55))\n- disable chat-input-editor during prompting ([7368767](https://github.com/continuedev/continue/commit/73687670da1cc7792d2c4d9ccd7b9813746a4658))\n- display rules used ([53f0679](https://github.com/continuedev/continue/commit/53f06795d3910e1ec5aecdbc1fde706f68649614))\n- editor change listener ([28d2c7d](https://github.com/continuedev/continue/commit/28d2c7d7e0a00c23c33ad25b8bfc3651ddafa2c5))\n- **embeddings:** add gemini provider ([#1362](https://github.com/continuedev/continue/issues/1362)) ([5224572](https://github.com/continuedev/continue/commit/52245724089329c792eb79c8d512cea2f617c4a1))\n- enable comment filtering ([471ca9e](https://github.com/continuedev/continue/commit/471ca9eae5e1f82e04b2d6d600631bf24876ec06))\n- enable sourcemap ([2be2b73](https://github.com/continuedev/continue/commit/2be2b73f2a28834cbfef9100f479ecffc0ad13c8))\n- enable WAL (Write-Ahead Logging) for improved performance and stability ([#1885](https://github.com/continuedev/continue/issues/1885)) ([e93ce84](https://github.com/continuedev/continue/commit/e93ce84cd57eacfe820c1f22668ed069020ee46e))\n- enhance help center ([#1755](https://github.com/continuedev/continue/issues/1755)) ([f2a04ef](https://github.com/continuedev/continue/commit/f2a04ef3e9e49876042077f36da5f457630dcaf1))\n- Enhance MCP connection refresh logic ([1a0d411](https://github.com/continuedev/continue/commit/1a0d41128f30c7f5c087c7255d6ec3292c288664))\n- Ensure CancelAutocompleteAction updates on EDT ([277c20f](https://github.com/continuedev/continue/commit/277c20f3b08eaa1b0b293b06bbfce40404e5477d))\n- evaluate rule ([1a1b854](https://github.com/continuedev/continue/commit/1a1b854fda4630e136d0d18b4ea69e71a00ced1e))\n- explore btn ([14c9528](https://github.com/continuedev/continue/commit/14c952887399f12e739d6a0fafbdbb4d1355fe90))\n- explore dialog ui ([c6cfa93](https://github.com/continuedev/continue/commit/c6cfa9334690cf7f9ecdf99959537ce522728079))\n- explore dialog watcher ([741ad35](https://github.com/continuedev/continue/commit/741ad35195d6b841b459076b8dbca1057d5f6358))\n- explore dialog watcher ([192a47f](https://github.com/continuedev/continue/commit/192a47f998d639f307933f33a731832ff35305ac))\n- explore hub card ([5a777ce](https://github.com/continuedev/continue/commit/5a777ce464243b09d466d7a11a39ff9e45a74ec5))\n- extract paths ([2ffb4ae](https://github.com/continuedev/continue/commit/2ffb4aefeb01073c924beb1c203e60c586cf96bb))\n- fall back to Cheerio for headless crawling ([fd7bbce](https://github.com/continuedev/continue/commit/fd7bbce79b831a1068c5367bc4f018fe820a917a))\n- fallback to chat model from apply model ([46e8dd7](https://github.com/continuedev/continue/commit/46e8dd7c38858173f375ac24f6857ed3bea2b190))\n- fallback to chat model in IDE ([b601c22](https://github.com/continuedev/continue/commit/b601c22d01478d74a66bf162f3e12c4c4e1a93d7))\n- filttrex ([b20b1ab](https://github.com/continuedev/continue/commit/b20b1ab5ee44d704619327a3b087b1290c634dd7))\n- fix address ([d59b3b5](https://github.com/continuedev/continue/commit/d59b3b5297924b2b6343a990c09551be4af9c670))\n- fix broken links ([66345e8](https://github.com/continuedev/continue/commit/66345e83550a898cd36eae67a68fb020ce957677))\n- fix ctx providers w/ slash cmds ([e5608a9](https://github.com/continuedev/continue/commit/e5608a94e543cd8b66d68c222eacc82e8c021aff))\n- fix filepaht issues w/ apply ([6c848ba](https://github.com/continuedev/continue/commit/6c848ba7a7fc7ea32a963653b83b5099e5ed6fad))\n- fix models ([d401a5c](https://github.com/continuedev/continue/commit/d401a5c11fdf5b3267269ff3603f83318a9dd9ad))\n- fix type error + formatting ([bb5e8b6](https://github.com/continuedev/continue/commit/bb5e8b641dc535fd8d60de92aba04680f7b93824))\n- fix version and address ([ad0fdb9](https://github.com/continuedev/continue/commit/ad0fdb988b76bebadcbb26a3069b7f821207680f))\n- get paths ([9cb5170](https://github.com/continuedev/continue/commit/9cb5170b1a23552f917265dc330ff1e05838a3de))\n- get return_type and parameters from snippets ([bf426d5](https://github.com/continuedev/continue/commit/bf426d5e2147248c06ab51cab31ac5cd9e834b1d))\n- glob rules ([8c5ca40](https://github.com/continuedev/continue/commit/8c5ca401d4fbda64eacf23cc0585e2c9830db345))\n- go definitions ([bf65b1e](https://github.com/continuedev/continue/commit/bf65b1ebbdd3b0913d141e9f60a11414088bc78f))\n- **gui:** add ability to change the session title ([1b32222](https://github.com/continuedev/continue/commit/1b32222c10c6ae6f1fd6c93f6abd8cbeab16f108))\n- **gui:** Add Azure as a provider ([b7c0623](https://github.com/continuedev/continue/commit/b7c0623ac6c5067ccefd7c8486eeef2ff56d9667))\n- **gui:** add flex-grow to TdDiv in history page ([#619](https://github.com/continuedev/continue/issues/619)) ([7106f42](https://github.com/continuedev/continue/commit/7106f42cb4ba7424b3dda8ed2d0e562e2fd99447))\n- **gui:** improve editor highlights ([7bffa2d](https://github.com/continuedev/continue/commit/7bffa2dbeab4dde415c113bb15b71968bc51f448))\n- **gui:** more config for azure provider ([e11206b](https://github.com/continuedev/continue/commit/e11206b0fa70075d21d1cc8dd4426af856ee05c2))\n- handle deletions ([2c8cee9](https://github.com/continuedev/continue/commit/2c8cee9fd42fc8f18c9e6e37781e27276dae2786))\n- hide empty code lines in markdown preview ([#815](https://github.com/continuedev/continue/issues/815)) ([793d022](https://github.com/continuedev/continue/commit/793d022eab292219cda75bb3255ed3d676585977))\n- **history:** add sticky headers to history sections ([#621](https://github.com/continuedev/continue/issues/621)) ([3d398d0](https://github.com/continuedev/continue/commit/3d398d0b3781db8bbc51ad00f47e80b3124f5c8f))\n- hover brightness on tool call div ([dba2887](https://github.com/continuedev/continue/commit/dba288748d850c7fa8047efb75e954384410a497))\n- **httpContextProvider:** load AC on fetch client ([#1150](https://github.com/continuedev/continue/issues/1150)) ([638f192](https://github.com/continuedev/continue/commit/638f1922590d7bd46d6e4d46a4d25bf1b33b26fa))\n- if rules ([6224af6](https://github.com/continuedev/continue/commit/6224af63d36e6b96b068c6d4230ee2315bd94d43))\n- if rules ([d0e922d](https://github.com/continuedev/continue/commit/d0e922dad4947ac8af4e9d0c76921da56cdc2953))\n- if-rule ([211fcb7](https://github.com/continuedev/continue/commit/211fcb77407cd261f2bb26198667388ea6270a05))\n- if-rule ([9baa4e9](https://github.com/continuedev/continue/commit/9baa4e93be7f9c2fe057e453237eaa9d1aaa8ff0))\n- if-rules ([06b76d7](https://github.com/continuedev/continue/commit/06b76d752fb4f83b14053fffe2f85cb819e39ae6))\n- if-rules ([0a13a46](https://github.com/continuedev/continue/commit/0a13a4604d7607793bea66b9cea3e89353bf3af4))\n- if-rules ([ccdc1b9](https://github.com/continuedev/continue/commit/ccdc1b953e22ca4aa26e32b9e80bc31ea30f274c))\n- ignore node_modules in js and ts code definitions ([3c9071a](https://github.com/continuedev/continue/commit/3c9071a09b114a019a593a7f241975f419cc4e43))\n- impl /multifile-edit ([f88f28b](https://github.com/continuedev/continue/commit/f88f28b8551e95e4eb346fafa68d6223f9017d5a))\n- improve chat thread ui for better readability ([#1786](https://github.com/continuedev/continue/issues/1786)) ([8478af6](https://github.com/continuedev/continue/commit/8478af63c18b0c35d59d2326ca4e8687a42a624b))\n- improve chunking desc on large projects ([3844208](https://github.com/continuedev/continue/commit/3844208a3b9b3daaff085daad8ab933f27fcb5f3))\n- improve dropshadow on jb inline edit ([35b94e7](https://github.com/continuedev/continue/commit/35b94e710260782d0311ee22a2849203b8965e38))\n- improve dropshadow on jb inline edit ([75e2d2f](https://github.com/continuedev/continue/commit/75e2d2f39df30424b3831615ee3720a2844a60a5))\n- improve fatal error message ([1d77ad3](https://github.com/continuedev/continue/commit/1d77ad31949042c7702b59f83a82ca51dcce4900))\n- improve input and tooltip ux ([#1923](https://github.com/continuedev/continue/issues/1923)) ([2c13776](https://github.com/continuedev/continue/commit/2c13776c06cfe9c775777bf2fffdcf7dfa2169ee))\n- improve model retrieval logic in InlineEditAction ([52b1704](https://github.com/continuedev/continue/commit/52b1704f3c0fe991b32959d04c660fe55e899b2c))\n- improve prompt log formatting + detail ([2376daf](https://github.com/continuedev/continue/commit/2376dafce325c87d87090e7cf995beffcb98d3b4))\n- Improve ProtocolClient initialization and GUI loading ([0c829fd](https://github.com/continuedev/continue/commit/0c829fd99d0e898c1afdc90a0957c907dcb7f4e2))\n- improve settings tabs ([a5f974b](https://github.com/continuedev/continue/commit/a5f974bf1602dabe3939ddc0ac7b5a05ac8e716b))\n- improve StreamError dialog ([ecb63a0](https://github.com/continuedev/continue/commit/ecb63a0222c7ae5e051bea02d98a4a2de7c1e2e5))\n- improve styling on code to edit ([47fbe4f](https://github.com/continuedev/continue/commit/47fbe4f9b81e1ef31d60f1110d2bf240fa6e7d40))\n- include recently + open files in codebase search ([#1833](https://github.com/continuedev/continue/issues/1833)) ([3e0fae3](https://github.com/continuedev/continue/commit/3e0fae35a75bb8dc117bc3eb008eee122e0e12ae))\n- init profiles prefs ([d6ad3e2](https://github.com/continuedev/continue/commit/d6ad3e2071ca1ee3f0b5cd4d8e93852a84fd138a))\n- insert prompt into input on click ([a73123e](https://github.com/continuedev/continue/commit/a73123e35ce29f1776b5afb456c9c67dc9495099))\n- instant apply check for diff rejectection ([98cb185](https://github.com/continuedev/continue/commit/98cb185c926d49a6f5af00f8b53e207743da815a))\n- integrate Moonshot AI model provider and update UI translations ([7ffb95e](https://github.com/continuedev/continue/commit/7ffb95e9d0fc964b55b3f92149515fa79fc3aa0d))\n- items used text ([#1973](https://github.com/continuedev/continue/issues/1973)) ([28a2042](https://github.com/continuedev/continue/commit/28a2042b7f604889cd0c7c76ee3ec0791a00e3ba))\n- **jb:** add ide logs ([22d84ae](https://github.com/continuedev/continue/commit/22d84ae726c40d65449c39b2ece4101f32608870))\n- **jb:** add plugin actions ([629641a](https://github.com/continuedev/continue/commit/629641a4adf2a1e2fe1c9a69fd431718b34c4263))\n- **jb:** create per-IDE tutorial files ([e3b5cbf](https://github.com/continuedev/continue/commit/e3b5cbfda453c727c6f81d04aa8bed1b9efe8301))\n- **jb:** fix meta key bugs in tiptap ([7682c71](https://github.com/continuedev/continue/commit/7682c71575c7ffc03d625510187d792e9d971227))\n- **jb:** impl \"apply\" button ([802cb43](https://github.com/continuedev/continue/commit/802cb43cd6b6abf8613591eb38c445e6bd4daa7d))\n- **JB:** impl `showFile` ([24bb5eb](https://github.com/continuedev/continue/commit/24bb5eb0dab8467f606d249f8be16fcb9ff083c0))\n- **JB:** scroll to top of file on full file edit ([c36a1ae](https://github.com/continuedev/continue/commit/c36a1ae62851383978080bc5674e7ba4ca0e9cc1))\n- jetbrains tutorial explore hub ([054b095](https://github.com/continuedev/continue/commit/054b095dab51b3abc7a3d83587e0b5f67b8acdf6))\n- make [@codebase](https://github.com/codebase) a hardcoded ctx provider ([#1818](https://github.com/continuedev/continue/issues/1818)) ([7b86678](https://github.com/continuedev/continue/commit/7b866787d947106b77ac0cef92b75ba36e09e7fc))\n- make `assistant` optional on sdk ([48429ae](https://github.com/continuedev/continue/commit/48429ae04f957a98c24d841316c722b91841c329))\n- make deletion line highlight wider ([10a5ec1](https://github.com/continuedev/continue/commit/10a5ec18a9df189d58c60886be296f1107a1dfc2))\n- make disabled state a tooltip ([#1653](https://github.com/continuedev/continue/issues/1653)) ([6cf0102](https://github.com/continuedev/continue/commit/6cf0102875316a89752d17d4fe08e4b21fd2e603))\n- manual ripgrep downloads for JB ([587459c](https://github.com/continuedev/continue/commit/587459c007ba130a4fd6f5ffde246dd8625486a8))\n- markdown rules ([a21d350](https://github.com/continuedev/continue/commit/a21d350ea14a98cf16904ae678de64a83775da7c))\n- merge conflicts ([82a328c](https://github.com/continuedev/continue/commit/82a328ca15306e0b902075c0661cd1b2bb8ed9a8))\n- model name update ([045c6bd](https://github.com/continuedev/continue/commit/045c6bdfec45ffadb25b3db2306b78f34780e84b))\n- modify vitest command, add test coverage feature ([96c70d0](https://github.com/continuedev/continue/commit/96c70d07d283130176cbda68afdb02ddc6b20366))\n- more cleanup ([6c294e7](https://github.com/continuedev/continue/commit/6c294e76834561848305ac8fb6bcb707ea21b172))\n- more cleanup ([bf7cf66](https://github.com/continuedev/continue/commit/bf7cf661cc4357029492f067ca57b1a43401aaaa))\n- more dynamic imports for LanceDB ([90c6631](https://github.com/continuedev/continue/commit/90c663146edc965bbe0b87b737755742315a8fa1))\n- more moving around ([b0492dd](https://github.com/continuedev/continue/commit/b0492ddfee2e6050d833d4ad07a942dbdd830c55))\n- more styling updates ([c7b3ea8](https://github.com/continuedev/continue/commit/c7b3ea890ac2bade08424e7a23301cea319aea35))\n- more visible assistant refresh + submenus ([1fd2ce9](https://github.com/continuedev/continue/commit/1fd2ce90f22c0dd6ebb9ec5b2542f85475844436))\n- move .prompts into slash cmd ([1d1f705](https://github.com/continuedev/continue/commit/1d1f705f7dfb1d517b90e03b82856e625f3f323e))\n- move `StepContainerPreToolbar` ([979bcb9](https://github.com/continuedev/continue/commit/979bcb97ec88d91d0ed0bd14fd5b6b7dc4c670ae))\n- move apply accept/reject into lump ([071ecb9](https://github.com/continuedev/continue/commit/071ecb91eb9c627e96439b7a5be260ec446500b0))\n- move apply manager instantiation ([2413d24](https://github.com/continuedev/continue/commit/2413d241c6448f65c99d9286e72a01bc8acc429a))\n- move Edit into Chat page ([731b54e](https://github.com/continuedev/continue/commit/731b54eb30bd7184e42c397db658020b4a70d5f7))\n- move error indicator into lump ([1f1d4f4](https://github.com/continuedev/continue/commit/1f1d4f4a1c5b365258db219e2a0ae8eecf3656a8))\n- move free trial out of assistant ([2c58a33](https://github.com/continuedev/continue/commit/2c58a33a6a937ea7beb1aa017bce64ec4cfe32d0))\n- move rule parsing to `config-yaml` ([f2d7290](https://github.com/continuedev/continue/commit/f2d72902cb45c165ca73483dd5c0a90ac89bcc49))\n- move SymbolLink ([e3bbd6b](https://github.com/continuedev/continue/commit/e3bbd6b7cfa04183f7a60470a55e4a2618083a21))\n- move vLLM rerank response in the VLLM.ts and remove unused types ([c8d9d95](https://github.com/continuedev/continue/commit/c8d9d959f10b34cb0af857cfa178c1f9df30f5cf))\n- onboarding card upgrade tab ([41bc0e9](https://github.com/continuedev/continue/commit/41bc0e922b333ee404ddb17e5bdcf7c83b7a6ca0))\n- only autoscroll Continue console when focused on last element ([40390f5](https://github.com/continuedev/continue/commit/40390f583bea74766c3eff48cb8ceda555112780))\n- open lump on submit onboarding ([f77f662](https://github.com/continuedev/continue/commit/f77f662c6e7313db645656e003c9d0145646388e))\n- org select ([dc751a3](https://github.com/continuedev/continue/commit/dc751a3274948cb12b47fb7b57106a8349e26870))\n- php definitions ([8da703f](https://github.com/continuedev/continue/commit/8da703f74525213dc212efa6b763d4be06dee38b))\n- php definitions ([eadc96e](https://github.com/continuedev/continue/commit/eadc96ebef6653a996dff32095fd87a86703058d))\n- poll in JB after upgrading ([89d4b32](https://github.com/continuedev/continue/commit/89d4b32bba3bede3ac9f001c6c013ec0bb5a9ca0))\n- postgres context provider first slice ([758a81c](https://github.com/continuedev/continue/commit/758a81c7373f398294ccb80d00a5c7d181f151d7))\n- pr_checks update ([1cc81d4](https://github.com/continuedev/continue/commit/1cc81d4b7e6ed6aedcc19753eea623acbfc8f8c1))\n- preserve edit when rejecting ([32bf80d](https://github.com/continuedev/continue/commit/32bf80d7d280fe815b547475b4a75fce92ba990d))\n- profiles slice ([b82d29c](https://github.com/continuedev/continue/commit/b82d29c2df154a632c89d32c6847f5bf43b6564b))\n- promote reject diff action ([16cbe94](https://github.com/continuedev/continue/commit/16cbe9499431d7eec64d0695821a223302d980cb))\n- prompt blocks ([9df325a](https://github.com/continuedev/continue/commit/9df325a71f3340c46820ac5335249095cda443ac))\n- Provide workspace path to HttpContextProvider ([4873f58](https://github.com/continuedev/continue/commit/4873f5805f89bc1f443211515dd7afb3fbc2cfff))\n- python context ([42d5f66](https://github.com/continuedev/continue/commit/42d5f660727dcf315d12193f70d23b283e7af10d))\n- recursively apply quick actions codelens ([#1925](https://github.com/continuedev/continue/issues/1925)) ([d5155da](https://github.com/continuedev/continue/commit/d5155dac697e5fa8b4241ebebe48bbfa66dc2c55))\n- redo action ([121be85](https://github.com/continuedev/continue/commit/121be85a69837a1d31dde31f7fca7098458ed38d))\n- refactor onboarding card ([b86bc6f](https://github.com/continuedev/continue/commit/b86bc6f727e292c2d3a8fb1511646c36e2a2531f))\n- reintroduce lazy apply for full files ([6504bd5](https://github.com/continuedev/continue/commit/6504bd55659823c8117d04ddb447f91d9932d086))\n- remove `models` property ([7f8882e](https://github.com/continuedev/continue/commit/7f8882eee136b90db9400175f9ee00fa1aa3ad93))\n- remove defaultTitle ([1bf359a](https://github.com/continuedev/continue/commit/1bf359a55fdb833569878167a1440e66c65d7249))\n- remove edit as a mode ([4f371a0](https://github.com/continuedev/continue/commit/4f371a0eb654067c626a41eb53718f565873975b))\n- remove tools from schema ([5bbda4f](https://github.com/continuedev/continue/commit/5bbda4f115a208ebd2df0c63ede75bfc5e131b54))\n- remove useFileExists ([2b34ee8](https://github.com/continuedev/continue/commit/2b34ee8c4156c544a3abd9616080f4d8e0b565a1))\n- rename to `Open Assistant configuration` ([9d8139a](https://github.com/continuedev/continue/commit/9d8139a6fae992eb1cb77248cc297d0a95683c12))\n- replace logo and deepseek model base info ([7749349](https://github.com/continuedev/continue/commit/7749349f8996f5da186ba1a8b2910443121ea2fc))\n- restructure for easier module publishing ([b51905d](https://github.com/continuedev/continue/commit/b51905da95307ed17f3fa7a466e94514bcd4113a))\n- retrieve AWS credentials from Env and from ECS/EC2 instance ([53426a1](https://github.com/continuedev/continue/commit/53426a12b858f341c9f70f8ee3513dbea39d648d))\n- reusable card ([d11376b](https://github.com/continuedev/continue/commit/d11376ba7f2b7ab1a85c75f361631fc2c5c26ab5))\n- rule colocation ([39dc367](https://github.com/continuedev/continue/commit/39dc367371c0df605f663c76fe490c4736f00153))\n- rule glob ([f95cdfa](https://github.com/continuedev/continue/commit/f95cdfaf83569159cd7aed02134be7dd5f4029cf))\n- rules ([2620064](https://github.com/continuedev/continue/commit/2620064c53ecf7de3d61b34043d2c0450f68b13c))\n- rules display ([40914a8](https://github.com/continuedev/continue/commit/40914a8bf3b4f2c1764873bc3172feab3bf23f3a))\n- rules policies ([8a2ffb3](https://github.com/continuedev/continue/commit/8a2ffb3a859d4fbec3299500a0d860c0f725573d))\n- rules preview ([d57d35d](https://github.com/continuedev/continue/commit/d57d35d5bdf6015593283a2aecbf88bca1c8c574))\n- run prettier ([ecdb4f5](https://github.com/continuedev/continue/commit/ecdb4f530dee01e8a67e7fd964206428d379b4db))\n- **scaleway:** update supported models ([8b04ba6](https://github.com/continuedev/continue/commit/8b04ba6b233d72f1e61ea67fd63de51d03629282))\n- separate toolbar action buttons ([aba6e4e](https://github.com/continuedev/continue/commit/aba6e4e61c6c7b548126a79ffeb0608e1de1147b))\n- show disableIndexing in More ([5d95b62](https://github.com/continuedev/continue/commit/5d95b62e683c0ea50eb519f7cc2851736e8ee64b))\n- show num diffs in toolbar ([ad6c0d0](https://github.com/continuedev/continue/commit/ad6c0d0bb850308993e53c4117cc45664fd8970e))\n- simplify types ([b18fd4a](https://github.com/continuedev/continue/commit/b18fd4aee0c66e41113bce4de1fa526da1dc16c6))\n- simplify typings ([c6fa6d7](https://github.com/continuedev/continue/commit/c6fa6d78a714ba6bba44ec2ef9825080b0a5b69a))\n- single default quick pick to edit ([#1743](https://github.com/continuedev/continue/issues/1743)) ([ca7bde9](https://github.com/continuedev/continue/commit/ca7bde9b5e10d684ea44291c67eb294edc357240))\n- skip hub onboarding in free trial ([d995149](https://github.com/continuedev/continue/commit/d99514995588cc2c502df99757e22f5ac3e2c7a1))\n- skip onboarding subtext ([9e6ff4b](https://github.com/continuedev/continue/commit/9e6ff4b91447b03d1b9949499cf5135d7a369df9))\n- smaller headings, use assistant name ([85804ff](https://github.com/continuedev/continue/commit/85804ff9e54adf30f1a4b23b3003b39338a82065))\n- split diff ([b7defc8](https://github.com/continuedev/continue/commit/b7defc8d912b09d9323e05a82442bea17c6faef4))\n- split diffs ([3c537f5](https://github.com/continuedev/continue/commit/3c537f51c13ee9064a3e17f3aec62dc5283ae3e2))\n- split diffs ([39087c2](https://github.com/continuedev/continue/commit/39087c2207e0dd3764761e91b9a2abd3ca6db906))\n- support o3/o4 as agents ([8bb18fd](https://github.com/continuedev/continue/commit/8bb18fd2ee4a0fb81b2f24903e47fb1b0beade36))\n- supports agent 4 deepseek ([82d8e1e](https://github.com/continuedev/continue/commit/82d8e1e6795e7258880d414c4bb78c6f83b17883))\n- toolbar header for all codeblocks ([635f7cb](https://github.com/continuedev/continue/commit/635f7cb168f264446e40c6887347327b101ee413))\n- tutorial listener ([6e01c7d](https://github.com/continuedev/continue/commit/6e01c7d0ca0d139e471edf40018d35e1033638f3))\n- unskip tests ([a5619e2](https://github.com/continuedev/continue/commit/a5619e29c8a0230f578c092c028c7cc6f3066afb))\n- update azure uri for foundry users ([638969b](https://github.com/continuedev/continue/commit/638969b7c7594f36bfefd59a986032ba597f88fa))\n- update btn colors ([b7c7171](https://github.com/continuedev/continue/commit/b7c71719e55e728f944399789f6749e493d5d13e))\n- update docs and input labelling ([1847317](https://github.com/continuedev/continue/commit/18473174365c88e7e83fd8123cb6af9318e10462))\n- update e2e tests ([93cec27](https://github.com/continuedev/continue/commit/93cec27e28773778e3223424fb72026c9e3ead44))\n- update executable perms on linux/macos ([dcbd80e](https://github.com/continuedev/continue/commit/dcbd80ecf47adeb49b9484bb1e1b3c43ff80bff8))\n- update onboarding w/ embeddings model ([#1570](https://github.com/continuedev/continue/issues/1570)) ([ed56c8f](https://github.com/continuedev/continue/commit/ed56c8f7f325f19c9b4d1c7e3cb775f850beaea9))\n- update PreToolbar ([3933791](https://github.com/continuedev/continue/commit/39337910f125a71414cf2c39b96250fc512777be))\n- update redux store schemas ([438cba4](https://github.com/continuedev/continue/commit/438cba4501026ef1705987b23f419570d1175ea1))\n- update styling ([ef349ec](https://github.com/continuedev/continue/commit/ef349ecec90dac3d15eb80ae17cf24c155d1e683))\n- update sys prompt ([25c7144](https://github.com/continuedev/continue/commit/25c714494588433d1c43d6af99cce175b48e21cc))\n- update to work as normal context provider ([0ace169](https://github.com/continuedev/continue/commit/0ace1699356be8024a8ab49ccadc4b1b22c18e08))\n- update tutorial files w/ agent mode step ([8eaf078](https://github.com/continuedev/continue/commit/8eaf078638c0d2d9c14aa0cc8e465df66f01e4df))\n- update URL replace logic ([fe6c9a1](https://github.com/continuedev/continue/commit/fe6c9a1477ab814f3cc4d0973e851701621d2c54))\n- updated prompt docs ([204aa51](https://github.com/continuedev/continue/commit/204aa51a944906fe629e92095a3f7f86113b9f6c))\n- use @lancedb/vectordb-win32-arm64-msvc ([0277320](https://github.com/continuedev/continue/commit/027732073aa0fb5202b558f4c0d71c71c6b8acbb))\n- use `fetch` instead of `http` ([401d67b](https://github.com/continuedev/continue/commit/401d67bbd0dc11fd6eda2f7d9c17dfd4da0a43f5))\n- use `instant` property on diff manager ([2f12bbd](https://github.com/continuedev/continue/commit/2f12bbdf25b1ff189a88d37bfba4f3bfe7f2f3bc))\n- use bm25 for fts ([2e5b579](https://github.com/continuedev/continue/commit/2e5b5794e9b394405c127893100be21e008b45ad))\n- use clipboard content ([9dd6284](https://github.com/continuedev/continue/commit/9dd6284b8e4b1dc09b704bc00d5231af9569b6bc))\n- use correct deployment for azure ([8e24fcf](https://github.com/continuedev/continue/commit/8e24fcf4a0ee655dfbe83d95b82855041f0ba246))\n- use crawlee for docs service ([a51c520](https://github.com/continuedev/continue/commit/a51c520deefa49f9d67aaa2469bb261af00dab80))\n- use exponential backoff in llm chat ([#1115](https://github.com/continuedev/continue/issues/1115)) ([a87df40](https://github.com/continuedev/continue/commit/a87df40a2c875a1538aa112d7d35c03137990ee2))\n- use git diff, improve comment formatting ([1adbd9a](https://github.com/continuedev/continue/commit/1adbd9a456f6c4c2f56b78d6bd5e3b8737f60ec5))\n- use hub blocks for local onboarding ([1b45308](https://github.com/continuedev/continue/commit/1b453083b14e60afda14de69b7bfb2f7ce74194b))\n- use meyers diff after initial edit/apply ([f5e7f9c](https://github.com/continuedev/continue/commit/f5e7f9ce71d4c34e8ee72b0d78fe89c68d0b7170))\n- use s3 for global docs cache on all docs ([0156409](https://github.com/continuedev/continue/commit/0156409dac5a445b055ede921c1c07706759ff4e))\n- use theme color for shortcuts rows ([44fdd65](https://github.com/continuedev/continue/commit/44fdd65967b561527865a02cafce0de4cdc9fa70))\n- use thunj ([eae1dd2](https://github.com/continuedev/continue/commit/eae1dd20bb296099db94175c43bf7af445e97200))\n- v1 onboarding card ([0d47647](https://github.com/continuedev/continue/commit/0d476473bda2ef8bea78064c89c5bc1f6818fec1))\n- vitest ([3cd19f5](https://github.com/continuedev/continue/commit/3cd19f5fabdb6eab3be381ccef8de743b37fcf29))\n- **VSC:** give option to disable quick fix ([951784c](https://github.com/continuedev/continue/commit/951784c790664a80d87b54e33886a3582137ad50))\n- vscode config for json ([666fb18](https://github.com/continuedev/continue/commit/666fb186d6b4cee9dcc95e1adc6b42748300de4c))\n- withExponentialBackoff utility function ([ad8991e](https://github.com/continuedev/continue/commit/ad8991eac0f7f40f3877d56392592d8d83e173a6))\n- working `copy-client` ([09beb58](https://github.com/continuedev/continue/commit/09beb58260f32043c89cff39c6e8567f25099c9c))\n- write initial unit test for apply ([f4a59dc](https://github.com/continuedev/continue/commit/f4a59dc185c3ea7ceb29a4629ea0c7dc82f44dbb))\n\n### Performance Improvements\n\n- :green_heart: hardcode distro paths ([edf0f56](https://github.com/continuedev/continue/commit/edf0f5603730d01d7ca4ca055dd49da596648627))\n- :zap: don't show server loading immediately ([5047dfc](https://github.com/continuedev/continue/commit/5047dfcd2a2e47468c15ed05c69781cc615ef723))\n- **llm:** Optimize pruneLines functions in countTokens ([35b3189](https://github.com/continuedev/continue/commit/35b3189538da5891617a79ba9c0badb0bb7dc1bc))\n- **llm:** Optimize pruneLines functions in countTokens ([28cdd1c](https://github.com/continuedev/continue/commit/28cdd1cee25973aa9a8a5cfdcbcc436e0b7f6240))\n- **llm:** Optimize pruneLines functions in countTokens ([881f8b3](https://github.com/continuedev/continue/commit/881f8b3139794aca0a6699c3d34ebf8fba01b789))\n\n### Reverts\n\n- Revert \"Add citations to the log\" ([caf7288](https://github.com/continuedev/continue/commit/caf7288d36c1361b6cb279821d9ee377636f72f7))\n- Revert \"feat: add bookmark logic\" ([730ac56](https://github.com/continuedev/continue/commit/730ac56d5a15a6b2931330aeb849e2d260d2644a))\n- Revert \"update TabBar to use Redux for session management\" ([9f13b92](https://github.com/continuedev/continue/commit/9f13b92b3fbc4157124ccf120028312390eed70f))\n- :bookmark: update version ([d6ebc6d](https://github.com/continuedev/continue/commit/d6ebc6d969ccafc753ecf00e0309777b96b4ad11))\n- :bug: revert unecessary changes from yesterday ([3629ddd](https://github.com/continuedev/continue/commit/3629dddb1daea23fbd29b03705e742ec2a22d6ec))\n- :fire: disable fallback_context_item ([a572db4](https://github.com/continuedev/continue/commit/a572db40b6c9ce98b07e89d34f8652e19f91187e))\n\n# 1.0.0 (2025-06-22)\n\n### Bug Fixes\n\n- :adhesive_bandage: allow GGML to use api.openai.com ([db19f6b](https://github.com/continuedev/continue/commit/db19f6bc98285d8ea45b4db16f619dffbec7c3db))\n- :adhesive_bandage: skip indexing really large files ([b773fe6](https://github.com/continuedev/continue/commit/b773fe6d7a0b489a658139ea5fc958abd46a20b2))\n- :ambulance: catch error from meilisearch client.health ([00775f5](https://github.com/continuedev/continue/commit/00775f54e6c3fa8044a996ea1a7cf0f2205735dd))\n- :ambulance: class_name hotfix ([83b0417](https://github.com/continuedev/continue/commit/83b0417f6c8c579d0ea5a0f689eceb822fe7a04d))\n- :ambulance: fix import of run from **main** ([ebfe428](https://github.com/continuedev/continue/commit/ebfe428b7f70de66bc5692cca1db7cd10ef4b997))\n- :ambulance: hotfix and package.json seo experiment ([42024ef](https://github.com/continuedev/continue/commit/42024effba73673b4080c25806c21293b5daad3e))\n- :ambulance: load global ~/.continue/assistants ([4b55cfb](https://github.com/continuedev/continue/commit/4b55cfb2e8ad4803855020111d0cff6a38ad79d5))\n- :ambulance: logging to file causing problems with starting server ([8b95ef7](https://github.com/continuedev/continue/commit/8b95ef7de258de8498b328d9e6107a95f57f8d2c))\n- :ambulance: specify packagePath for vsix ([512ccfd](https://github.com/continuedev/continue/commit/512ccfda670abb6132e9cd720280a472e53e3326))\n- :arrow_up: upgrade openai python package ([19cbe2c](https://github.com/continuedev/continue/commit/19cbe2cebae8e2155b6b4375c6a96a3b25e87615))\n- :art: many small improvements ([28f5d7b](https://github.com/continuedev/continue/commit/28f5d7bedab05a8b061e4e7ee9055a5403786bbc))\n- :bookmark: update extension version ([05b9642](https://github.com/continuedev/continue/commit/05b96420bda2da0c725cacc8141d87449eaf9e9c))\n- :bookmark: update version ([afae160](https://github.com/continuedev/continue/commit/afae1600255714d0a4f18f892d3e7b5e1d921962))\n- :bookmark: update version ([9aee2cc](https://github.com/continuedev/continue/commit/9aee2cc44c461ce0e001185af85352e78522bab5))\n- :bookmark: update version to try again ([1905f31](https://github.com/continuedev/continue/commit/1905f319470c02ee414498b9101b6e64b4b15d65))\n- :bookmark: v3 -> v4 of upload-artifact ([d24862a](https://github.com/continuedev/continue/commit/d24862a6fd22e4eac1b2ca27ce7bf029f0d8fa4a))\n- :bug: a few minor fixes ([c918bb3](https://github.com/continuedev/continue/commit/c918bb3af5ec4a4a409eb0a3add27951b00c3c59))\n- :bug: a handful of bug fixes ([e1325c0](https://github.com/continuedev/continue/commit/e1325c0153becb95b454810d9461efd7d3624a6a))\n- :bug: a number of small fixes + disable summaries ([a975560](https://github.com/continuedev/continue/commit/a9755603c3a2c0b3afe809f77a63824c77c6419e))\n- :bug: access highlighted_code through context_manager ([1afb37b](https://github.com/continuedev/continue/commit/1afb37b5bb901d95c493039591b9243cd2cdd6f7))\n- :bug: add data file for ca_bundle ([b82d83f](https://github.com/continuedev/continue/commit/b82d83f79389897ed5f05eb9b5e8daf9cf64ee6f))\n- :bug: Add requestOptions to YAML config for mcp ([81c20c1](https://github.com/continuedev/continue/commit/81c20c11dbbce2eccefd00364c5b74b298b2f24f))\n- :bug: add server/exe to .vscodeignore insteading of manually removing ([e8ebff1](https://github.com/continuedev/continue/commit/e8ebff1e6b07dfaafff81ee7013bb019cbfe2075))\n- :bug: additional fixes to ssh /edit ([4428acd](https://github.com/continuedev/continue/commit/4428acdd6f372c3724a908fafb1c793e0eae4096))\n- :bug: allow end/home keys to work ([615d30e](https://github.com/continuedev/continue/commit/615d30e3dce92a9993b0e93b044faadf228529b1))\n- :bug: allow None for timeout ([ff3de11](https://github.com/continuedev/continue/commit/ff3de1184737f1124090d384b877a30550b60869))\n- :bug: another hotfix - don't destructure selectors ([534304a](https://github.com/continuedev/continue/commit/534304a2a4f9abfc221a961f279d1b43d14b6d33))\n- :bug: another windows fix in typegen.js ([f38c8fb](https://github.com/continuedev/continue/commit/f38c8fb8b33a705ed4eb4d2e0974060ebb88afd3))\n- :bug: async with Client (meilisearch) ([9a0cd64](https://github.com/continuedev/continue/commit/9a0cd644dcb5ff46817a6ea686a6de0fb764c960))\n- :bug: attempting to fix mkdir ([c1a8097](https://github.com/continuedev/continue/commit/c1a8097f0a7f3cddb0aebac26e6197ffef186972))\n- :bug: automigrate between short/long imports ([eecc2b5](https://github.com/continuedev/continue/commit/eecc2b57c5c5a144abfc0623102438e902c4aeba))\n- :bug: avoid removing disallowed file windows ([19a3266](https://github.com/continuedev/continue/commit/19a3266b6f14186bd0839fac8b2a04b5a29f32e7))\n- :bug: bug when highlighting code prior to context_manager creation ([74a52c8](https://github.com/continuedev/continue/commit/74a52c8399b3ccf2d2100b088b79e65c6ca6ad7e))\n- :bug: bug where old server doesn't get updated ([bb776a0](https://github.com/continuedev/continue/commit/bb776a03df3e6a39a1726b781ea33c2ccebd5343))\n- :bug: catch error when workspace uri isn't defined ([fc9eb30](https://github.com/continuedev/continue/commit/fc9eb3051fd5a7c9cad57b5d6cd93374bd8210fb))\n- :bug: change for/backwardslash decoding scheme ([a3a05fe](https://github.com/continuedev/continue/commit/a3a05fee312ad7c04d2abb0e186da55c7d061462))\n- :bug: chmod for linux as well as mac ([089def0](https://github.com/continuedev/continue/commit/089def08c58120f78df78c10027639802ad8f77d))\n- :bug: clear all other selector destrucuring ([145642f](https://github.com/continuedev/continue/commit/145642f1eaf01d5809dabd79e7f64f234124683e))\n- :bug: Codebase Indexing was not starting on load ([e391d58](https://github.com/continuedev/continue/commit/e391d583041c54edb3fa0836eb9186c61e6b063d))\n- :bug: compatibility with python 3.8 ([275ad6f](https://github.com/continuedev/continue/commit/275ad6f72dafdfacffd9c9b5cc4847135a30f425))\n- :bug: convert to correct path sep in wsl URIs ([1b2341a](https://github.com/continuedev/continue/commit/1b2341a0113fadf8c8d23097ef1041d3e3088e84))\n- :bug: correct path sep for ssh-remote files ([b9bd8c1](https://github.com/continuedev/continue/commit/b9bd8c1848eaf38d5d15694a1ecae67f14566214))\n- :bug: correction to ContinueConfig serialization model ([b8aba4b](https://github.com/continuedev/continue/commit/b8aba4bc96d3b064012a40d837d5191cae20037e))\n- :bug: correctly generate uris for remote ([ab31cb1](https://github.com/continuedev/continue/commit/ab31cb15fae74592f49c2ceadc8d7810228fa7e2))\n- :bug: ctrl+c for windows overriding copy ([c3925c0](https://github.com/continuedev/continue/commit/c3925c04d981d2abc1e21cf72d6e77d165420a73))\n- :bug: custom escaping instead of URI for diff paths ([da3970e](https://github.com/continuedev/continue/commit/da3970e00061b7a223d23f51bd53012666d324dc))\n- :bug: default to counting chars if tiktoken blocked ([7006dbb](https://github.com/continuedev/continue/commit/7006dbb3e38a837a2580a516791874f6815ac25f))\n- :bug: don't fail on disconnected websocket ([0876610](https://github.com/continuedev/continue/commit/08766100cdb3638b3300ae4b700f8ec2af6b9a8a))\n- :bug: don't log stdout to console ([ee4701d](https://github.com/continuedev/continue/commit/ee4701dc45cd540728302ca8a09e9b7ce842597f))\n- :bug: don't open continue automatically ([8b76f51](https://github.com/continuedev/continue/commit/8b76f518313c20f13dda605931c9929ef58a7a22))\n- :bug: don't override context length param in OpenAI ([b2a6d07](https://github.com/continuedev/continue/commit/b2a6d07ea99be1f9288ee21477edc0874e780cad))\n- :bug: ebusy and logging bug fixes ([3d61469](https://github.com/continuedev/continue/commit/3d614690cd825ac5580074ecdc22f660455204f1))\n- :bug: fix \"code\" keyerror prior to context_manager.start ([866b16c](https://github.com/continuedev/continue/commit/866b16c3a9c9d88a7b90aa8a43610fc4884ab123))\n- :bug: fix /edit in ssh, pinyin input in combobox ([cda1be4](https://github.com/continuedev/continue/commit/cda1be46625abd8f44962cceeded04c8c47d9f65))\n- :bug: fix >c_d.png file path ([a9bc4e2](https://github.com/continuedev/continue/commit/a9bc4e26263faef8598dd8aa2aec7949c75ab70c))\n- :bug: fix 2 model config bugs ([b144d21](https://github.com/continuedev/continue/commit/b144d21b48a94aa8c203469eb7667bd22fc4e243))\n- :bug: fix 404 from undefined gif ([b467371](https://github.com/continuedev/continue/commit/b4673712e1a6a5b435125004a9b51498207fb7b6))\n- :bug: fix automigration ([ec41f55](https://github.com/continuedev/continue/commit/ec41f553c24d5f4b5bc4e601c989b1936d67ae1a))\n- :bug: fix azure openai bug for 07 version ([a8e69a0](https://github.com/continuedev/continue/commit/a8e69a02e6897689a1727fb7542ed5684b1348e2))\n- :bug: fix broken docs link ([210a02e](https://github.com/continuedev/continue/commit/210a02ef02341a98b4ed18095b2d656a7b994bd9))\n- :bug: fix bugs when selecting code to edit ([fa34214](https://github.com/continuedev/continue/commit/fa34214012d14385d231a1ac4f16006aaf4331fb))\n- :bug: fix ci to only upload from linux x64, not alpine ([4e1a5b1](https://github.com/continuedev/continue/commit/4e1a5b1fb3f96edc95b0938265da980e98566d56))\n- :bug: fix cmd+m bug ([38e8272](https://github.com/continuedev/continue/commit/38e827243ceff3732cd0f260e7a3bd4941a96bc5))\n- :bug: fix command enter, stop streaming on reject ([8e15ec3](https://github.com/continuedev/continue/commit/8e15ec3c2c1490d4a7d6371f877368376fd64e8a))\n- :bug: fix command enter, stop streaming on reject ([19b3886](https://github.com/continuedev/continue/commit/19b38863c21656526e0729776682430e0fa277da))\n- :bug: fix config.py import paths ([97861bf](https://github.com/continuedev/continue/commit/97861bf4117bbc36f8f87797a9ca60e6336f82cc))\n- :bug: fix context length bug for /edit ([d103263](https://github.com/continuedev/continue/commit/d103263030ad52debe73bd131c71bbf17f545956))\n- :bug: fix dialog links ([4c84e69](https://github.com/continuedev/continue/commit/4c84e6945a7c2018622eceb54e7fb54de193b03a))\n- :bug: fix for --meilisearch-url flag ([e2798c5](https://github.com/continuedev/continue/commit/e2798c5bb62eeb2a3bc8f5baee18f9d64ee86563))\n- :bug: fix for Azure OpenAI model names ([bcec2a0](https://github.com/continuedev/continue/commit/bcec2a0870d0ef649961b6c91ec866b612680b9e))\n- :bug: fix for edit=None in highlightedCode update ([247d3e9](https://github.com/continuedev/continue/commit/247d3e9a41ff8d9fe2da6386bfb0d0eb063b071c))\n- :bug: fix for lmstudio defaults ([0012922](https://github.com/continuedev/continue/commit/00129229cd881d6b910a4b01db68e702cdd63a40))\n- :bug: fix for windows drive difference bug ([d69c6d4](https://github.com/continuedev/continue/commit/d69c6d4f3729374ab40fcebc861e67f2da100ad9))\n- :bug: fix ggml bug ([1a75475](https://github.com/continuedev/continue/commit/1a75475c681053494984664ef1179171fe2a5d83))\n- :bug: fix headers for openai.;y ([44fe0c9](https://github.com/continuedev/continue/commit/44fe0c94a55a753ff5d6c3da6b63db4a5c70d780))\n- :bug: fix height bug after cmd+shift+R ([a7cb092](https://github.com/continuedev/continue/commit/a7cb0929bd064f73a1e3e49ba8dd6b6b7de387f4))\n- :bug: fix history.timeline indexing bug ([01ed2c7](https://github.com/continuedev/continue/commit/01ed2c7eb2d3417b2c190eea105008372f49a7c6))\n- :bug: fix huggingface tgi ([5316180](https://github.com/continuedev/continue/commit/5316180394d48d9877cda0cb3d7c3c6de9995d12))\n- :bug: fix import in run.py ([4cf1f75](https://github.com/continuedev/continue/commit/4cf1f75518053f9df174d5ab90c426124f85ecfa))\n- :bug: fix inability to copy/paste when ipynb is open ([850c8ae](https://github.com/continuedev/continue/commit/850c8aea7f3d9c46ff8e98bde936b92282376dae))\n- :bug: fix incorrect imports in default config file ([374bdd0](https://github.com/continuedev/continue/commit/374bdd037792825bf984026da12d4100ffebcac2))\n- :bug: fix keyboard shortcut for debugging ([c021958](https://github.com/continuedev/continue/commit/c021958ae893a9683352ba99e5c6301e38331492))\n- :bug: fix meilisearch empty body content-type bug ([598e243](https://github.com/continuedev/continue/commit/598e243fd292dd8851865ab1c3915ca55f4992cc))\n- :bug: fix missing path import ([5bfe68e](https://github.com/continuedev/continue/commit/5bfe68ea7f7e90e3cb1c3101360cf959b336a857))\n- :bug: fix model changing bug ([fd4a4dc](https://github.com/continuedev/continue/commit/fd4a4dcf004bea86d982ffffb66b4e3cb38193a6))\n- :bug: fix overriding of system message ([8444e76](https://github.com/continuedev/continue/commit/8444e76b7232fbddb62d3626de13653ae332d168))\n- :bug: fix paths ([b893c95](https://github.com/continuedev/continue/commit/b893c956fe75a9e45f06129290d043737f5c1007))\n- :bug: fix reducers for user input queue ([1a36a3c](https://github.com/continuedev/continue/commit/1a36a3c02acaf6bf29d4153c113217517b832942))\n- :bug: fix replicate to work with models requiring prompt input ([84ec574](https://github.com/continuedev/continue/commit/84ec574e182ec441e95d13c3543a934e0a036228))\n- :bug: fix serialization bug for context_providers ([2799249](https://github.com/continuedev/continue/commit/27992499af977baeb9124d9ab35ffec6d36a298a))\n- :bug: fix set_system_message ([084fdac](https://github.com/continuedev/continue/commit/084fdac3992f58dcf11241e7e5c2d5efa784ce0d))\n- :bug: fix ssh /edit by checking for file through vscode fs ([417d45c](https://github.com/continuedev/continue/commit/417d45ccddc2f434d7467e4f17113783996653dd))\n- :bug: fix telemetry bug ([042bc5a](https://github.com/continuedev/continue/commit/042bc5ac76800ee66e603ef23b2bb857fafe053e))\n- :bug: Fix the generating animation ([2f402b4](https://github.com/continuedev/continue/commit/2f402b4a1227ade5a4ba70f770974627b586e930))\n- :bug: fix timeout type ([e1a0290](https://github.com/continuedev/continue/commit/e1a0290d5a699e30464f1e682cb11c6aa119bd59))\n- :bug: fix togetherAI model json parsing ([deb291c](https://github.com/continuedev/continue/commit/deb291c1b225425cba543dd3b4c5557089abfb59))\n- :bug: fix undefined.filter bug ([9b58278](https://github.com/continuedev/continue/commit/9b582781ab0aceaaf1cff7432fed92fa6c205aae))\n- :bug: fix usages of LLM.complete ([f057ee4](https://github.com/continuedev/continue/commit/f057ee4d619b834dc245065d13417a86b44dc61b))\n- :bug: fix when multiple cursor ranges are selected ([f9c145c](https://github.com/continuedev/continue/commit/f9c145c9667e0cd9adb7f9b645f7abf12f7cf2a2))\n- :bug: fix yaml syntax error ([11c7cec](https://github.com/continuedev/continue/commit/11c7cecc107ef9f2571926055dbd80495fe0f8b2))\n- :bug: fixes for a few context_providers ([41b3233](https://github.com/continuedev/continue/commit/41b3233693c34cd81c872a1e7279721b5f640d60))\n- :bug: fixes to templating messages ([c56e24d](https://github.com/continuedev/continue/commit/c56e24d2a5f2b40702e4b495fa3f28d554eaa3ab))\n- :bug: fixing bugs with ggml ([87409c3](https://github.com/continuedev/continue/commit/87409c31832ccb707abbf134843323c9eb6e1183))\n- :bug: fixing issues with creating markdown files ([5c1c2d6](https://github.com/continuedev/continue/commit/5c1c2d626ffed786d00c79aadef26fa5718ca43d))\n- :bug: fixing small UI details ([088b7b8](https://github.com/continuedev/continue/commit/088b7b803866817aaedce6b61834f1ce5de7a7c2))\n- :bug: force kill old server with taskkill on windows ([b1b7d13](https://github.com/continuedev/continue/commit/b1b7d13dbf5b9f6ada28a5ef22ea6857d3b0bcb6))\n- :bug: ftc fix ([f5f10ef](https://github.com/continuedev/continue/commit/f5f10efee3402e117c34b6f0de4bf2fd7d2819c1))\n- :bug: gpt-4-32k in CHAT_MODELS ([b0445cd](https://github.com/continuedev/continue/commit/b0445cd5fc4538c8a9c4f3e76be0f3d724c99818))\n- :bug: handle when vscode workspace not open ([73c6827](https://github.com/continuedev/continue/commit/73c6827d02ff62313184e3745fd94c7591c98b61))\n- :bug: hotfix for user_input_queue.map ([610c576](https://github.com/continuedev/continue/commit/610c576cc9df72716c5e65838f805b15431011ea))\n- :bug: install python-virtualenv on linux, fix git hash files error ([6f0e634](https://github.com/continuedev/continue/commit/6f0e6340bb22ee150ef4b7996750f4c63c0bc2a7))\n- :bug: install python-virtualenv on linux, fix git hash files error ([7fa98ff](https://github.com/continuedev/continue/commit/7fa98ffe843320ddc63794a497a2d44570e005c3))\n- :bug: kill server before trying to delete exe on windows ([286fb0e](https://github.com/continuedev/continue/commit/286fb0e20e48859f129ccf568d03248805bcbc61))\n- :bug: let context providers work without meilisearch ([0f86a69](https://github.com/continuedev/continue/commit/0f86a69e4a83458db2e20e404c26dac2e02355cf))\n- :bug: llamacpp fix indexing max_tokens ([90590ab](https://github.com/continuedev/continue/commit/90590ab4e06fbc3fa721f73a4a922136946a756f))\n- :bug: make sure server_version.txt exists ([17806d9](https://github.com/continuedev/continue/commit/17806d932502adbf974ccd93a670e57b78be9a08))\n- :bug: make typegen.js windows compatible ([dc06228](https://github.com/continuedev/continue/commit/dc0622848b648ba27e7110b9b900673bb668ab4c))\n- :bug: MAX_TOKENS_FOR_MODEL bug fix, more testing ([1c288f7](https://github.com/continuedev/continue/commit/1c288f7749747c6b1908ae16c977f80e5597d2ca))\n- :bug: meilisearch fixes ([0de6e19](https://github.com/continuedev/continue/commit/0de6e1985d0e97ede5e19e7752a6be7cd2a5818d))\n- :bug: more reliable download with request ([fdb036b](https://github.com/continuedev/continue/commit/fdb036bcecec891adaf99d73101c458fc4087406))\n- :bug: more reliable setup of meilisearch ([12a8ae1](https://github.com/continuedev/continue/commit/12a8ae1c47f111b9f36633c96b26e8642c5ff223))\n- :bug: now progress bar when api_key entered ([bf82c6f](https://github.com/continuedev/continue/commit/bf82c6fd16a6777f0a9bb68ce4879d7bab9019bb))\n- :bug: number of bug fixes ([b9bdf58](https://github.com/continuedev/continue/commit/b9bdf5894c1c68b60d1919ae07b0f5909b00dec2))\n- :bug: numerous small fixes ([0940d75](https://github.com/continuedev/continue/commit/0940d756dec3b98071ae5e5a12966e02420b3cd2))\n- :bug: patch for ocassional 0 choices from older azure versions ([5c09b80](https://github.com/continuedev/continue/commit/5c09b8077588a447d6eaac9b7f624571be3ddb1d))\n- :bug: permissions for pypi-deployment step ([b237850](https://github.com/continuedev/continue/commit/b237850c4b64435e26dfb5f12275a16a93e556a8))\n- :bug: post-merge fixes ([96379a7](https://github.com/continuedev/continue/commit/96379a7bf5b576a2338142b10932d98cbc865d59))\n- :bug: remove empty grammar from llama_cpp_args ([e5bbe3b](https://github.com/continuedev/continue/commit/e5bbe3bc4d59b6f35db1ce1b94be14244c11c766))\n- :bug: replace hardcoded path for config file ([4fe9ace](https://github.com/continuedev/continue/commit/4fe9ace518bcdcf79999ce9938ba01b218d355e4))\n- :bug: require socksio ([bba5e5e](https://github.com/continuedev/continue/commit/bba5e5e5b1da2dd924aa2632e38d4bb702bbbdd9))\n- :bug: separately load ctx provs, fix filetree ([d8e821e](https://github.com/continuedev/continue/commit/d8e821e422678fd4248b472c7f3e67a32ecfefb5))\n- :bug: set api_keys in config.py, fix spawn error handling ([6823307](https://github.com/continuedev/continue/commit/68233071dd0d97a353a66fe5627d69f97a389ca8))\n- :bug: set export display in same step as linux npm test ([c3d62c5](https://github.com/continuedev/continue/commit/c3d62c5ae203aaca32583f75a7e80dfd9f196e11))\n- :bug: small bug fix ([32d1149](https://github.com/continuedev/continue/commit/32d1149692c26eb966693f03db6d9cf496ba57a4))\n- :bug: small bug fixes ([bc75ff2](https://github.com/continuedev/continue/commit/bc75ff294a2b5ec5eef5f77aff72aaa0c7f4a3f2))\n- :bug: small fixes, update troubleshooting docs ([51fc07c](https://github.com/continuedev/continue/commit/51fc07cf6441d6330ce64e45e56e8f333ca309ed))\n- :bug: solve EBUSY by polling ([9417973](https://github.com/continuedev/continue/commit/941797359f6554ac16a2e478047aabd5cbc0404b))\n- :bug: ssh compatibility by reading from vscode.workspace.fs ([e5f5630](https://github.com/continuedev/continue/commit/e5f56308c5fd87695278682b2a36ca60df0db863))\n- :bug: start meilisearch in parallel to server ([e4c1bb4](https://github.com/continuedev/continue/commit/e4c1bb4bedbe426d090f4bb2b8819ad935c5b3fb))\n- :bug: stop streaming on rejection ([8d05fc2](https://github.com/continuedev/continue/commit/8d05fc2bb5c5df617800c1abcf43bb03c574482f))\n- :bug: stop streaming on rejection ([9fc831e](https://github.com/continuedev/continue/commit/9fc831e7587cce99c8a6f2e56905c25068c8cab6))\n- :bug: streaming url_decode for Ollama ([3690101](https://github.com/continuedev/continue/commit/3690101b790f91c749f208693aaffc00b9fa2a42))\n- :bug: templating fix for queued LLM ([5c6609a](https://github.com/continuedev/continue/commit/5c6609ab5fa3a69cd0e3e8e61df643fcce1ecb47))\n- :bug: temporarily disable lsp before fixing w/ vscode ([98f340b](https://github.com/continuedev/continue/commit/98f340bd97cba6f30cfe55d47419e3925b9dc679))\n- :bug: temporarily remove replicate altogether ([bd79c00](https://github.com/continuedev/continue/commit/bd79c00e7790b92cfd8b8c8f8211b6c3d36e33a2))\n- :bug: test and fix small issues with GGML ([72e8332](https://github.com/continuedev/continue/commit/72e83325a8eb5032c448a5e891c157987921ced2))\n- :bug: timeout on blocking processes ([345b773](https://github.com/continuedev/continue/commit/345b7734d8c887d699d5038416d2a1f8193a33e9))\n- :bug: traceback fixes, remove replicate from hiddenimports ([45d9bab](https://github.com/continuedev/continue/commit/45d9bab5cea745573be7112d7130089c596c88fa))\n- :bug: try/except around starting meilisearch ([c867cd4](https://github.com/continuedev/continue/commit/c867cd40342d44901cf5277ded25f5dc5aaa4326))\n- :bug: update search path for ripgrep on windows ([e428dc5](https://github.com/continuedev/continue/commit/e428dc53cedf54f394a7cddfe8a7ce7fbf469bb9))\n- :bug: update the tip message for keyboard shortcut ([3558450](https://github.com/continuedev/continue/commit/355845002e178a618e9a792dd57b0649c3da8845))\n- :bug: urldecode ollama responses, make edit faster ([19050f8](https://github.com/continuedev/continue/commit/19050f83228b3e7f08a6aacd5bdd1804a8315e4a))\n- :bug: use certifi to set ca_bundle_path for openai ([3849420](https://github.com/continuedev/continue/commit/3849420948e491d5f84ac485169165d887751fd3))\n- :bug: use posthog-node, not -js ([88a8166](https://github.com/continuedev/continue/commit/88a8166476d38889fd4f9323472cc34a5226e05c))\n- :bug: use powershell remove-item ([64552dc](https://github.com/continuedev/continue/commit/64552dc881509c46aa14253ff94aee9d86ade256))\n- :bug: use windows equivalent of rm -rf ([8d19866](https://github.com/continuedev/continue/commit/8d198663e116c7c77b7e59015bc6032736f71f6e))\n- :bug: verify_ssl and ssl_context mutual exclusivity ([59b7453](https://github.com/continuedev/continue/commit/59b7453afed06418d4c171b65370a6a82f5a9221))\n- :bug: version patch in the publish step ([1936f72](https://github.com/continuedev/continue/commit/1936f725d226bea2e13d5d88c1dd7a9a02ddd259))\n- :bug: windowsHide on process spawn ([c3d31f0](https://github.com/continuedev/continue/commit/c3d31f00bb589df1c83308b7d9d69ed51c31341a))\n- :bug: write out npm run package as package.js ([4636c95](https://github.com/continuedev/continue/commit/4636c9590154d6b5995948003da212eb25003750))\n- :bug: write to local diff files ([6140d05](https://github.com/continuedev/continue/commit/6140d05e7d415d3334032c300ed593bdd181f7f5))\n- :children_crossing: add slash commands to default config ([58e5dc4](https://github.com/continuedev/continue/commit/58e5dc4a5c4fcbed25170b61fbd88d479c5aebcf))\n- :children_crossing: clear the dropdown after text input cleared ([23167a5](https://github.com/continuedev/continue/commit/23167a51d959fed5e4be057ceb9fff50cf34c6c8))\n- :children_crossing: don't order meilisearch results by contnet ([ab7a90a](https://github.com/continuedev/continue/commit/ab7a90a0972188dcc7b8c28b1263c918776ca19d))\n- :children_crossing: use default model in default config.py ([1bc5777](https://github.com/continuedev/continue/commit/1bc5777ed168e47e2ef2ab1b33eecf6cbd170a61))\n- :construction_worker: copy_metadata for replicate in run.spec ([b0426d8](https://github.com/continuedev/continue/commit/b0426d82a4871e9081367ad4e977b22f42db5a89))\n- :construction: working on fixing lsp ([1f95bb2](https://github.com/continuedev/continue/commit/1f95bb287846fc0501193d642420b574d9900857))\n- :fire: remove version from package.json ([27c0a40](https://github.com/continuedev/continue/commit/27c0a403de28345cd03c39ad46c02f68ff57b3a1))\n- :goal_net: catch errors when loading to meilisearch index ([7894c8e](https://github.com/continuedev/continue/commit/7894c8ed1517394aa00f6e496a97d9e27d204f5f))\n- :goal_net: display errors in SimpleChatStep ([72784f6](https://github.com/continuedev/continue/commit/72784f6f1161f0c5b647889c26089a8247111dc9))\n- :green_heart: cd extension before packaging ([0a6d72c](https://github.com/continuedev/continue/commit/0a6d72c099316e6cffa123d3ffa915f3fe13e770))\n- :green_heart: cleanup file ([951552d](https://github.com/continuedev/continue/commit/951552dd0eede1f8f255aeaf5d34a13ff0c7bfb7))\n- :green_heart: don't exclude jedi from pyinstaller ([a1328cb](https://github.com/continuedev/continue/commit/a1328cb5431f99cfe16b246ee4201b19530404e2))\n- :green_heart: fix build scripts ([ab799b0](https://github.com/continuedev/continue/commit/ab799b0b0133e926ea06a1a12c092f42b9e053a1))\n- :green_heart: fix copy statement to include.exe for windows ([36c3dfd](https://github.com/continuedev/continue/commit/36c3dfd51d319b9b9ad392988d13ef7f443e0937))\n- :green_heart: fix preview.yaml ([a736d62](https://github.com/continuedev/continue/commit/a736d62f0e8b9b80ab9a949fd1739fb9a3be26e1))\n- :green_heart: increase testing timeout to allow for fkill ([08b1cfd](https://github.com/continuedev/continue/commit/08b1cfdd2f6f456df7344c16f5d229a0ccfb841b))\n- :green_heart: install rosetta ([5b9ef10](https://github.com/continuedev/continue/commit/5b9ef102973c608bc409a7b9ec244a4be1494e96))\n- :green_heart: one last test ([758520f](https://github.com/continuedev/continue/commit/758520fe6b59d3330dec80ac07d05282d36e0058))\n- :green_heart: only upload once per binary ([490838a](https://github.com/continuedev/continue/commit/490838a8ad920a52ada7e85675aefd965e978d77))\n- :green_heart: package patch ([34f32ed](https://github.com/continuedev/continue/commit/34f32ed5f71055ea11d4332f18e77ceba5849631))\n- :green_heart: package:pre-release ([d3f21da](https://github.com/continuedev/continue/commit/d3f21da803e36b78f968c2216d9f93f90ebabd6a))\n- :green_heart: publish as pre-release! ([831bf5e](https://github.com/continuedev/continue/commit/831bf5e7f7c48dd80f71f1256f5597bd47bf22de))\n- :green_heart: publishing to depend on ALL tests ([a131c17](https://github.com/continuedev/continue/commit/a131c17326591e67a68faf6f96371ad8fc332b71))\n- :green_heart: pull origin main in main.yaml after pypi update ([6a9a079](https://github.com/continuedev/continue/commit/6a9a079914d94419183182cd0a5cc4439f2101ad))\n- :green_heart: remove \"patch\" from vsce publish command ([d8327ec](https://github.com/continuedev/continue/commit/d8327ec6f82058479bd294bfcdccaf3c2b54de0a))\n- :green_heart: remove npm_config_arch ([8c51664](https://github.com/continuedev/continue/commit/8c5166471b0d83b924d8bee1e0ca51822cc1bbdc))\n- :green_heart: remove version from apckage.json ([5438ce9](https://github.com/continuedev/continue/commit/5438ce94406baa0f7d131ecacadefc72912dca0d))\n- :green_heart: set permissions on apple silicon binary ([715cfed](https://github.com/continuedev/continue/commit/715cfed18747b6bc2e6d7bd7a977d249cc9066d5))\n- :green_heart: testing ([14062c5](https://github.com/continuedev/continue/commit/14062c5c385bb7ba80096bf7daf6b6a5568b0b54))\n- :green_heart: testing for failure to package dist in vsix ([19acf3b](https://github.com/continuedev/continue/commit/19acf3bb36c1e44274297c806b89b589ca02f5ba))\n- :green_heart: update permissions and version ([c4ed41c](https://github.com/continuedev/continue/commit/c4ed41c861573f4c9bdff1a21ca3e056cfdd766e))\n- :green_heart: update pylsp hidden import ([1e8ea65](https://github.com/continuedev/continue/commit/1e8ea654f5ad1e06bff2660b54a50955098703ba))\n- :green_heart: update pypi version, don't push from main ([208eb65](https://github.com/continuedev/continue/commit/208eb65f67ccc62ce6d683fd9bed2fe9524b2136))\n- :green_heart: use curl to download binary ([199e4b3](https://github.com/continuedev/continue/commit/199e4b3b99642ba5b1558132aa10119be1eeb525))\n- :heavy_plus_sign: add bs4 to requirements.txt ([8a1e6fb](https://github.com/continuedev/continue/commit/8a1e6fb4adec6e5febb2a0d78eb0b2a01bfa028b))\n- :heavy_plus_sign: add ripgrepy dependency to requirements.txt ([9801b50](https://github.com/continuedev/continue/commit/9801b50192ca661972d5b2997028db3cd0725fb7))\n- :heavy_plus_sign: hidden import for replicate ([b75555f](https://github.com/continuedev/continue/commit/b75555f106be3c7612e7c31818ff674485096e4f))\n- :heavy_plus_sign: include replicate in requirements.rtxt ([01b3f1f](https://github.com/continuedev/continue/commit/01b3f1ff8f4dd89ae79f15626ef5a3af2bc558c4))\n- :lipstick: don't display entirety of large tracebacks ([a74eda5](https://github.com/continuedev/continue/commit/a74eda56cfcafb5c463a74df564ced6f882f8d3e))\n- :lipstick: fix layout bugs ([b655781](https://github.com/continuedev/continue/commit/b6557810d70a7f341761d5018fa2835cc3a50af1))\n- :lipstick: fix UI problems in vscode light themes ([5e8866d](https://github.com/continuedev/continue/commit/5e8866da83f8a97cb8492f26e175b948d0282262))\n- :lipstick: logo alignment, better config failure description, patch ([c51ad53](https://github.com/continuedev/continue/commit/c51ad538deff06af6c9e5498b23e3536e18bfc4c))\n- :lipstick: nicer autoscroll ([88699ff](https://github.com/continuedev/continue/commit/88699ff909b026511da392bf2c0a96be02abc6fd))\n- :lipstick: small UI improvements ([ec4fb4d](https://github.com/continuedev/continue/commit/ec4fb4d9235151901c1f7367932ecc17ab55d8e4))\n- :lipstick: ui tweaks to history + scrollbars ([6e8885f](https://github.com/continuedev/continue/commit/6e8885fc2f7feb06ef6ac87d2d7688f9f33d15de))\n- :lipstick: update font size for input, remove first tutorial step ([73ff267](https://github.com/continuedev/continue/commit/73ff2678ad984c9d9082ec078a38450d5daa1376))\n- :lipstick: update light gray hex code ([e0e0482](https://github.com/continuedev/continue/commit/e0e0482f2af2eadd3df72fbdb6974c07ba11c527))\n- :lock: opt out of meilisearch analytics ([8db5b39](https://github.com/continuedev/continue/commit/8db5b39170229ba93b83f526e7fd80056e461c6a))\n- :loud_sound: better logging for ggml completion endpoint ([0459b0c](https://github.com/continuedev/continue/commit/0459b0c919903852254ac2cd081307788884cd84))\n- :loud_sound: fix logs to be sent from uvicorn ([d3b4103](https://github.com/continuedev/continue/commit/d3b4103cd2f639fc072b8a3269d7730478c8bb1c))\n- :loud_sound: websocket logging and horizontal scrollbar ([7bb0fe3](https://github.com/continuedev/continue/commit/7bb0fe34bbc8affce0c675b88ffb79a6b9985860))\n- :memo: escape <QUESTION> in docs ([7314e79](https://github.com/continuedev/continue/commit/7314e79ac5bc34936a2c3de0fd01aadbfe640e72))\n- :memo: fix deployent readme ([7a38025](https://github.com/continuedev/continue/commit/7a3802523c2e5ae136c39849e2fbb0d3e7bba63e))\n- :memo: remove reference duplicates for ctx providers ([043d695](https://github.com/continuedev/continue/commit/043d695198caed305fa6651918c3bbb2de87db36))\n- :memo: small fix in troubleshooting.md ([275a03b](https://github.com/continuedev/continue/commit/275a03b7f1e32f57bd68e501074aa80e0dbed40f))\n- :memo: use backup server links in docs ([815627b](https://github.com/continuedev/continue/commit/815627b167e4bf06308b51c6756e33c36b17b631))\n- :pencil2: Fix typo that was causing automatic version bumping not to work for intellij ([3daf2c7](https://github.com/continuedev/continue/commit/3daf2c7b23caf838b862c2d2791ae8655b761d12))\n- :rocket: fallback s3 bucket ([aa98080](https://github.com/continuedev/continue/commit/aa98080cb16c75d2b7d6d9771b97e63120052c62))\n- :safety_vest: more safely convert windows path to posix ([4309f9d](https://github.com/continuedev/continue/commit/4309f9def89c25611273d99db01e7cc477ad935e))\n- :white_check_mark: allow longer for python server to start in test ([d8f5f10](https://github.com/continuedev/continue/commit/d8f5f102f6f91487be0281316e581858ec4ca260))\n- :white_check_mark: allow longer wait in test ([40ec1a3](https://github.com/continuedev/continue/commit/40ec1a31a7cd37da8b75bbabf1f0d160bb7bec5d))\n- :zap: register vscode commands prior to server loading ([f7a3659](https://github.com/continuedev/continue/commit/f7a3659381f839b890f2c53086f7fedecf23d9ab))\n- :zap: update count_tokens method ([8214203](https://github.com/continuedev/continue/commit/82142033f935d6236620d82e31a70ea8f2fb243e))\n- 'inferenceConfig.stopSequences' failed to satisfy constraint: Member must have length less than or equal to 4 [#2538](https://github.com/continuedev/continue/issues/2538) ([90e994d](https://github.com/continuedev/continue/commit/90e994db3c106f9b63bd043203111e5e101071ba))\n- (very) small typo breaking the prompt file examples link! ([bd6f9b9](https://github.com/continuedev/continue/commit/bd6f9b969647ee8cf6138dca63538063414553bb))\n- `REPLACE INTO` code_snippet table ([bdb967d](https://github.com/continuedev/continue/commit/bdb967d7b3c18fa2e0237f3cc0e0ea7415102715))\n- üêõ Codebase Indexing still not work ([955ab93](https://github.com/continuedev/continue/commit/955ab93efc3f488127d16673b89f3900f75c2007))\n- üêõ typo in core.py ([#429](https://github.com/continuedev/continue/issues/429)) ([705324e](https://github.com/continuedev/continue/commit/705324ed2ef588b2885c0b03107b9e30ae358dae))\n- a bunch of bugs, commit residuals such as npm install pg ([6d74e6b](https://github.com/continuedev/continue/commit/6d74e6b1fedf18b11ea12ba38164fcd146fdba4b))\n- actions ([653ca20](https://github.com/continuedev/continue/commit/653ca205dfbe2f7fec94891c99053d393d3efd0c))\n- add .mvn/ to list of default ignored folders ([ec91021](https://github.com/continuedev/continue/commit/ec91021adfcbd49becc3fdfbee6784981238a9f4))\n- add 'rich' module to requirements.txt ([#612](https://github.com/continuedev/continue/issues/612)) ([5d21bdf](https://github.com/continuedev/continue/commit/5d21bdf2930b30723f1fd80b05d8c1c2ad589bb2))\n- add checkmark icon to indicate selected model in dropdown ([98a3219](https://github.com/continuedev/continue/commit/98a321939ea1ba551025b16dc09f13e7a5e980ca))\n- add code range for quick actions/fixes ([#1687](https://github.com/continuedev/continue/issues/1687)) ([9f2e9bc](https://github.com/continuedev/continue/commit/9f2e9bc2dff474447d8502e386bb0cc804730bb9))\n- add context provider ([ae888d9](https://github.com/continuedev/continue/commit/ae888d9e814cf10931222070ed1e2ae5438325fe))\n- Add context/getSymbolsForFiles endpoint in JetBrains and handle symbol retrieval errors ([ba1fdf7](https://github.com/continuedev/continue/commit/ba1fdf7bac07afe04e62bd659a87dc89800a302b))\n- Add ContinuePluginDisposable to avoid memory leaks in Jetbrains ([a48a150](https://github.com/continuedev/continue/commit/a48a150b2024fca63cb50fb09a914a391c0cfce5))\n- add Delphi/Pascal syntax highlighting support ([cfcb06b](https://github.com/continuedev/continue/commit/cfcb06b1763b17e9e6d150963e090523c372d5d6))\n- Add directory checks and optimize token usage ([79736c3](https://github.com/continuedev/continue/commit/79736c3108313204184bb7d833ac21e23341354f))\n- add focus to InputToolbar on click ([#703](https://github.com/continuedev/continue/issues/703)) ([6b17de4](https://github.com/continuedev/continue/commit/6b17de49ed0e991221baee986f9dbb758d55f291))\n- add hover effect and restrict clickable area for the history and more back buttons ([9662b18](https://github.com/continuedev/continue/commit/9662b18a1ff36d8eaf743003f4df501245a31be2))\n- add missing eos_token for qwen2.5-coder ([6a7eea2](https://github.com/continuedev/continue/commit/6a7eea27ca5d1ade790ce08dc1cf4eb7bbdeb585))\n- add missing import ([400be9b](https://github.com/continuedev/continue/commit/400be9b47f0b8ab52b4e32ca77339497c63c8f96))\n- add mistral model options to config schema ([d56b48c](https://github.com/continuedev/continue/commit/d56b48cad2fcb0bb8e4a3e0dd3e32ab7f704d762))\n- add Msty logo ([397dcf4](https://github.com/continuedev/continue/commit/397dcf4ccc663560a49ff41e5892a5c118bce02e))\n- add new UI breakpoints ([5105645](https://github.com/continuedev/continue/commit/5105645dedec92abd3fc613777f60fd2a638a0ab))\n- add unit tests for commandExtractor and improve multiline comment handling ([0d940ec](https://github.com/continuedev/continue/commit/0d940ec6d1377b38945bd6217040cd025f89a80c))\n- add uuid to chat msgs for deletions ([f63fd5d](https://github.com/continuedev/continue/commit/f63fd5d83cefbbe01721515a8f28f435353bd804))\n- addTag ([53f886f](https://github.com/continuedev/continue/commit/53f886fd40c7bca29de06156f0e713a9700cc189))\n- allow downloading missing embedder on Ollama, in VS Code ([1dc58c6](https://github.com/continuedev/continue/commit/1dc58c63ab174f579e860d778ae85e73f6fb8c9d))\n- always show close icon ([4faed0b](https://github.com/continuedev/continue/commit/4faed0bc17a2b79e24fe5133efc40868bccaa6aa))\n- apply actions in toolbar ([cd810eb](https://github.com/continuedev/continue/commit/cd810eb93939bccc5bc0ff1e5c54572c7d27a97d))\n- apply insertion bug ([ceb9277](https://github.com/continuedev/continue/commit/ceb92773e9a6e4da27aa2de8f871c07f1e23d1a7))\n- apply notch filename trunaction ([033ade0](https://github.com/continuedev/continue/commit/033ade024ed685ef66dad53e675449a47e2dc872))\n- artifact name ([85e3fbc](https://github.com/continuedev/continue/commit/85e3fbc1c73872b1dfd8aee1f1dcef26b9f37761))\n- artifact upload ([1039e77](https://github.com/continuedev/continue/commit/1039e777c8a081ed9362f29e20c7c6133f3b07cd))\n- attempt to fix [#485](https://github.com/continuedev/continue/issues/485) ([#498](https://github.com/continuedev/continue/issues/498)) ([1188dd7](https://github.com/continuedev/continue/commit/1188dd7e5f26ed57d034c927ba032739963b9abc))\n- auth ([72636d9](https://github.com/continuedev/continue/commit/72636d96c8d8bb303e66f2344f5eeef26f078816))\n- autocomplete label after selecting a query type context provider ([#1562](https://github.com/continuedev/continue/issues/1562)) ([6407458](https://github.com/continuedev/continue/commit/6407458d7626d52558ea760b79a03129caba14ff))\n- autocomplete logging + input layout ([c542e5d](https://github.com/continuedev/continue/commit/c542e5d6153078f42e8eaf4361a4e5bdf24d9bc2))\n- Autocomplete not working when lookup is selected ([346f8fe](https://github.com/continuedev/continue/commit/346f8fe191a96eab5903b9a9f5a1e7e5ea1e3ee2))\n- avoid parsing md rules as yaml ([6c24132](https://github.com/continuedev/continue/commit/6c24132fab5b1c828aeb2e162c0f07ab6c830d0d))\n- binary ([8f24197](https://github.com/continuedev/continue/commit/8f2419740044a5df2ca70905c2b9ecd0fef3389e))\n- broken build ([d09711a](https://github.com/continuedev/continue/commit/d09711ab5895aca1c91552e7d9140b3aa0535d77))\n- broken help center quickpick ([a83526d](https://github.com/continuedev/continue/commit/a83526d6a49df6589d107695f99c03d02b68629c))\n- broken imports ([5fd8d2c](https://github.com/continuedev/continue/commit/5fd8d2cae5926418116031dbab8ce9465bc4c8f9))\n- broken imports ([f60009c](https://github.com/continuedev/continue/commit/f60009c2ea7c1c1a980152e5aa3cda17c3f6e64d))\n- broken JB build ([0e2587c](https://github.com/continuedev/continue/commit/0e2587cc71f497141ccd5fc807624eac80f8f183))\n- broken link ([6a675d8](https://github.com/continuedev/continue/commit/6a675d8edce1e430faa32b5142554bf61b843e0a))\n- broken schema config ([0529ae8](https://github.com/continuedev/continue/commit/0529ae8e1e4f61d83f2a4c43c9b9d23a84192ec9))\n- browse functionality ([ef5d753](https://github.com/continuedev/continue/commit/ef5d7535f0860978baeb337930f266b1201a7a5c))\n- build issues with jetbrains ([267961f](https://github.com/continuedev/continue/commit/267961f5b67f8f4529917fc43ebc1df88db92b51))\n- bump extension tester version ([006e1e4](https://github.com/continuedev/continue/commit/006e1e4b3bad2a8545a492cbeeb3238a1bf3345b))\n- bump node.js dependencies ([18fd7fb](https://github.com/continuedev/continue/commit/18fd7fbe58a03dfda382f755e261c5f16e88f38e))\n- cache ([368d264](https://github.com/continuedev/continue/commit/368d264d221793b7f2439678cef9b8b032135529))\n- caching ([1e7aa1a](https://github.com/continuedev/continue/commit/1e7aa1a879c80c0acddf09fbe0d887b7ec9bd31d))\n- change from llama3.1-8b to llama3.1:8b ([ea060a3](https://github.com/continuedev/continue/commit/ea060a37567af620cbdb8d665beb3f36cfeada6d))\n- chat tests ([7be3bb4](https://github.com/continuedev/continue/commit/7be3bb4161e7a17f7b0ae6e8d3ce365479e70dde))\n- ci ([1f47f54](https://github.com/continuedev/continue/commit/1f47f5436dde900b04cf1b307efa64f0fecebd59))\n- ci ([f6f2633](https://github.com/continuedev/continue/commit/f6f263375be129e438ab46a3ddbbba0722d3eb24))\n- ci ([fa9f86b](https://github.com/continuedev/continue/commit/fa9f86b6fd9ddb8cfab7dc52772c9f5a0623d654))\n- CI checks + fix for broken config-yaml types ([2032a8c](https://github.com/continuedev/continue/commit/2032a8c31ce9f30a5996a0fa6566d840a1eae21b))\n- ci e2e tests ([ade32d4](https://github.com/continuedev/continue/commit/ade32d428c5ef0fc8f5daefd52571b95387002f3))\n- ci tests ([27d20eb](https://github.com/continuedev/continue/commit/27d20eb08b25b142a54641048670b036e201ea1d))\n- circular state in redux selector ([5921bfa](https://github.com/continuedev/continue/commit/5921bfac67237dd4ea34dc1aad7856b220adf062))\n- cleanup model select ([1933a29](https://github.com/continuedev/continue/commit/1933a29ec4536d4887130703f1b0f94c3cd3db8d))\n- Clear all diff blocks before streaming and ensure dispose is run on EDT ([41357cd](https://github.com/continuedev/continue/commit/41357cd00c72b61cd54b80aa3963d047c0bb4184))\n- Clipboard is not available in chat window ([ef26b7c](https://github.com/continuedev/continue/commit/ef26b7c80eb0cc503017df8b9abd634f868a9b89))\n- close extension after tests ([575a707](https://github.com/continuedev/continue/commit/575a70721b39e41bfe8b860824d5eb606ce29ba9))\n- close sidebar when cmd+l pressed on focus ([93c9507](https://github.com/continuedev/continue/commit/93c95075425179d27421c7b54d92c9235d914c3c))\n- Cloudflare Workers AI message handling ([7eff255](https://github.com/continuedev/continue/commit/7eff25595c6aa063db37fb2d5394447c8472ba20))\n- cmd+shft+l closes sidebar if focused ([#1638](https://github.com/continuedev/continue/issues/1638)) ([92b5c4c](https://github.com/continuedev/continue/commit/92b5c4ccf64a88c461bb32abc9ab02329651e6be)), closes [#1536](https://github.com/continuedev/continue/issues/1536) [#1456](https://github.com/continuedev/continue/issues/1456) [#1564](https://github.com/continuedev/continue/issues/1564) [#1576](https://github.com/continuedev/continue/issues/1576) [#1570](https://github.com/continuedev/continue/issues/1570) [#1582](https://github.com/continuedev/continue/issues/1582) [#1600](https://github.com/continuedev/continue/issues/1600) [#1618](https://github.com/continuedev/continue/issues/1618) [#1626](https://github.com/continuedev/continue/issues/1626) [#1637](https://github.com/continuedev/continue/issues/1637)\n- code automatically expands after pressing enter ([42c02be](https://github.com/continuedev/continue/commit/42c02bed5fb61f925eb829cd8e7e39425225cc30))\n- codestral template ([d56a33e](https://github.com/continuedev/continue/commit/d56a33ea06f7c9724ffaef86d71bbb8d31840fc8))\n- colocation ([fb28272](https://github.com/continuedev/continue/commit/fb282727e92237f6b14e28e3538e04e13b837f1e))\n- colocation ([e1d227c](https://github.com/continuedev/continue/commit/e1d227cbc3ea56e607fce5c5ce585c2b901f6029))\n- comment out auto apply on MFE ([f61476a](https://github.com/continuedev/continue/commit/f61476a4936f364b4d0adc679da067d5f3ee8411))\n- completions ([a2f62da](https://github.com/continuedev/continue/commit/a2f62dad3b4df8a9a243964584eed148835a077b))\n- config error handling ([170676a](https://github.com/continuedev/continue/commit/170676a46149a4e507f827aa566f4b1d39344e2e))\n- config-types \"useSuffix\" to \"useFileSuffix\" ([fa3afd5](https://github.com/continuedev/continue/commit/fa3afd52e766c298a9bfa62c57e8a9b6e695b1d6))\n- **continue:** update context for slash commands ([cfac639](https://github.com/continuedev/continue/commit/cfac639e44ff5ba13960bbed6756047a90ed93f2))\n- convert `walkDir` to an async generator ([#1783](https://github.com/continuedev/continue/issues/1783)) ([6824497](https://github.com/continuedev/continue/commit/68244977d377e1706a20892abc059bf5cb70bc71))\n- copy ([5e122b7](https://github.com/continuedev/continue/commit/5e122b76b3cf5f0f511101bb1b2cea1349fa3a7a))\n- **core:** delete indexed docs if re-indexing ([acea620](https://github.com/continuedev/continue/commit/acea620043d2c48826266d4f1f73eaf42819f0c6))\n- **core:** remove eslint config and fix errors ([#1457](https://github.com/continuedev/continue/issues/1457)) ([3e1c06b](https://github.com/continuedev/continue/commit/3e1c06b41b7b90a0cad026c0ba0433fb1be6d277))\n- **core:** use `TextDecoderStream` for stream response ([#1498](https://github.com/continuedev/continue/issues/1498)) ([09d256a](https://github.com/continuedev/continue/commit/09d256ad562eda0919b9fa2853176319bf4eda36))\n- correct formatting ([cbb5c21](https://github.com/continuedev/continue/commit/cbb5c21f591e078df186fd86865830cb8971be03))\n- correct llama2TemplateMessages ([#855](https://github.com/continuedev/continue/issues/855)) ([f95a54d](https://github.com/continuedev/continue/commit/f95a54d9c29247680839129c54eae912ba5ca85d))\n- correct package.json update ([02968fb](https://github.com/continuedev/continue/commit/02968fb1551c0a26af75340641312e1f9f5a1102))\n- correct typo in stop sequence ([0dac76e](https://github.com/continuedev/continue/commit/0dac76edf5f35226cf6b3e99c83a7c02273cc05c))\n- corrected typo in the info Alert paragraph ([be55052](https://github.com/continuedev/continue/commit/be55052d230b50635c43727127ee776e67bd4c20))\n- cursor focus issue causing unwanted return to text area ([#1086](https://github.com/continuedev/continue/issues/1086)) ([c54cd7a](https://github.com/continuedev/continue/commit/c54cd7a26dd2e4e8743be86c6ac22dd1c8dca774)), closes [#1078](https://github.com/continuedev/continue/issues/1078)\n- cursor position ([6284eee](https://github.com/continuedev/continue/commit/6284eeebef984ac1afd3846b606a7745c5dfb49d))\n- cycling through chat messages ([b3ac143](https://github.com/continuedev/continue/commit/b3ac14332e8bd89237dfa41847701e3378487145))\n- deepseek doesn't support 'https://api.deepseek.com/completions' URL currently. When user selecting code in editor, and then use 'CMD +I' command to let model to modify codes, 400 error happens. ([170b99d](https://github.com/continuedev/continue/commit/170b99d3261b7d2ef32ba9e03a51ac37bb1fb787))\n- delete all code_snippets ([b9114bb](https://github.com/continuedev/continue/commit/b9114bbdab90de0dd167c88b57bd96b27ad03524))\n- delete button default style ([5423d24](https://github.com/continuedev/continue/commit/5423d24c35c614b0970e472aafa1f1048bf0d970))\n- delete old docs index on force re-index ([#1778](https://github.com/continuedev/continue/issues/1778)) ([0d632ea](https://github.com/continuedev/continue/commit/0d632eabbfd512c813e65212c3f5765bbc3fc8c7))\n- dependencies ([384fb56](https://github.com/continuedev/continue/commit/384fb562e5aac023301d592a186982a4b575fe98))\n- dependencies ([8efb3be](https://github.com/continuedev/continue/commit/8efb3be1b1f2253f9a7fd3035d8f0055de8f426e))\n- dependencies ([cc063fd](https://github.com/continuedev/continue/commit/cc063fdd124d66515bdcb9e2c64f575b40cc99d8))\n- dependencies ([fccae87](https://github.com/continuedev/continue/commit/fccae8732e1a4f447ff0ab650e9ae717c2022ffc))\n- Desktop.browse on linux ([7a11446](https://github.com/continuedev/continue/commit/7a11446bfc5d4cb9c05f64f1377518456d30f530))\n- disable completions in the commit box ([2b611df](https://github.com/continuedev/continue/commit/2b611df53ba716c2b550ab3b275c571eed9d2c65))\n- disable indexing for unsupported CPUs ([a005aa7](https://github.com/continuedev/continue/commit/a005aa78ac75ac47be3e647e9e23b45eab7dafaf))\n- disable lancedb cpu check ([b496777](https://github.com/continuedev/continue/commit/b49677711b4340f57ba671bacda301138c2909fb))\n- Do not break completion for unsupported FS providers ([4d1a33f](https://github.com/continuedev/continue/commit/4d1a33fd794a7415b0cc3b8de1d349fbba5a01cb))\n- do not filter context providers if not in editMode ([b2182b9](https://github.com/continuedev/continue/commit/b2182b9c8568a38d3af179909fb305e1459f5276))\n- do not write to the cache if caching is disabled ([b29ed9d](https://github.com/continuedev/continue/commit/b29ed9d34aa7ae343a2fd0c767f315e46e9aa110))\n- docs and install dependencies task for new directory structure ([#551](https://github.com/continuedev/continue/issues/551)) ([6ef8ae6](https://github.com/continuedev/continue/commit/6ef8ae6d5ab7efb1e928ff6474b6ac7a804a34cc))\n- **docs:** contributing guide ([5a06dfe](https://github.com/continuedev/continue/commit/5a06dfe1fb865bed0ca76ff23816da6396a0b27d))\n- **docs:** correct openai api key name ([9cc25c1](https://github.com/continuedev/continue/commit/9cc25c18c9fff5fc6ac4d56153667b0e8311920c))\n- **DocsCrawler:** Add response.ok check and improve link splitting ([d613b5a](https://github.com/continuedev/continue/commit/d613b5a7da5ad15c8b4b7dfe949812045f69635c))\n- don't override embeddings model name with UNSPECIFIED ([bc79388](https://github.com/continuedev/continue/commit/bc79388634ebcfcf2e38aa240441f9939e9de85a))\n- don't return empty string ([403eb38](https://github.com/continuedev/continue/commit/403eb38b9e3e38c277daed4ac6a2b6ac54353570))\n- dont toggle models when toggling assistants ([5bd8dd4](https://github.com/continuedev/continue/commit/5bd8dd4cb759bbabc35bdc00c66d88e4a175f7a8))\n- double ssh retries ([c9e2ebc](https://github.com/continuedev/continue/commit/c9e2ebc4589f7a06e804daee7f2ce6d7b2b66ce2))\n- dropdown with the height of the sticky div ([#605](https://github.com/continuedev/continue/issues/605)) ([a8d6ff0](https://github.com/continuedev/continue/commit/a8d6ff04e43a81a3c76373469ffebdfcfa8ac9f2))\n- editor focus handling on click events ([#716](https://github.com/continuedev/continue/issues/716)) ([f1660a5](https://github.com/continuedev/continue/commit/f1660a533146c420253df6e2dec0e0d4a2a9ebf4))\n- Eliminate Uncaught TypeError in dev tools ([d342825](https://github.com/continuedev/continue/commit/d3428256543bcc5ab269df93c949b56df92793f2))\n- Empty file inserts incorrect content ([ba9027b](https://github.com/continuedev/continue/commit/ba9027baea44ca51d299804636771d1a084d5fa7))\n- empty last message ([d3f7795](https://github.com/continuedev/continue/commit/d3f7795e6133fe7452c0626e6a54fb229bacbd7b))\n- enable OSR for mac after validations ([db77b50](https://github.com/continuedev/continue/commit/db77b5020d9b93cb4a7504e213d81940dbee9eaa))\n- end thinking for reasoning models when request is complete ([5109685](https://github.com/continuedev/continue/commit/51096858ab1ddc2bee534c1ea848be2b4bdcfb3f))\n- ensure code preview uses one more backtick than in the selected code as a fence ([#742](https://github.com/continuedev/continue/issues/742)) ([9ce7770](https://github.com/continuedev/continue/commit/9ce7770066cf870d2d1f36883f5e867157373eda))\n- ensure template variables are only processed for stdio MCP servers ([99b1ab2](https://github.com/continuedev/continue/commit/99b1ab2ebe51b4678722703922ecca023afa911c))\n- Ensure that the state is cleared after refresh, and files->uris when the file changes ([abd8907](https://github.com/continuedev/continue/commit/abd890738111e51bf12a700f9e5e8e66a7c76235))\n- Ensure valid line index for highlighter ([f86c95e](https://github.com/continuedev/continue/commit/f86c95ebdbd1851bcbf0e6f22d18a348a8a154ed))\n- error ([4c7f918](https://github.com/continuedev/continue/commit/4c7f9182665a259fc6b9cf10c93e696e97400a2c))\n- error printing bug leading to uncaught err ([3af4d6b](https://github.com/continuedev/continue/commit/3af4d6b1edeac113499cc9626cea75a5083b1030))\n- exclude open-codestral-mamba from supported models ([c6591f8](https://github.com/continuedev/continue/commit/c6591f836ea73e1244a0e965111f7c599669d9fa))\n- **extensions:** schema for db ctx provider ([#1534](https://github.com/continuedev/continue/issues/1534)) ([6fdad55](https://github.com/continuedev/continue/commit/6fdad553b6ac7f64c1bb23617dbe313e16174a56))\n- failing azure llm test ([546b52a](https://github.com/continuedev/continue/commit/546b52a2df928687d56924f593108f5bb9235712))\n- failing test ([ec2a72d](https://github.com/continuedev/continue/commit/ec2a72dfabbe7bd9c220b18b55145f431e19f3bf))\n- failing test ([002ecc9](https://github.com/continuedev/continue/commit/002ecc9b519472e0968e9f227894f3143c87f1ec))\n- failing test ([07b6b0c](https://github.com/continuedev/continue/commit/07b6b0cbefe86f2efa5488a33060ee9eb9f20e4d))\n- failing tests ([88ccb74](https://github.com/continuedev/continue/commit/88ccb74f7e69cf816d27f15a318a42c11f6d2aa5))\n- failing tests ([cb2e0a7](https://github.com/continuedev/continue/commit/cb2e0a7c0e9330d9b79a3e9878c851b9eba92ade))\n- failing tests ([7b52ff7](https://github.com/continuedev/continue/commit/7b52ff74d4009cb85c32a5e9482bd3624931d21e))\n- failing tests ([8d29d41](https://github.com/continuedev/continue/commit/8d29d4158d0fe21b0c0c4722e8d23715acf9ebdf))\n- failing tests ([2b828b0](https://github.com/continuedev/continue/commit/2b828b0e7b455bf6b0f9ae5a6f6a705ab4cefe6a))\n- field name fix ([0de1a2f](https://github.com/continuedev/continue/commit/0de1a2ff8937452815311c806a038a7f6d75fdf8))\n- filepath replacements in prompt files ([#1939](https://github.com/continuedev/continue/issues/1939)) ([c0923a0](https://github.com/continuedev/continue/commit/c0923a0dd7ca3a6e9dcbd9b02bfc34250269d2ef))\n- fix [#709](https://github.com/continuedev/continue/issues/709) ([#712](https://github.com/continuedev/continue/issues/712)) ([9ae07cd](https://github.com/continuedev/continue/commit/9ae07cd711d011faaf9a79952c0e213a6aa72e87))\n- fix 100% indexing progress ([7f214ad](https://github.com/continuedev/continue/commit/7f214adcbdca33ebb7270f721fd57eeeb0bac1e8))\n- fixed misallignment in tests caused by linter ([15e93dd](https://github.com/continuedev/continue/commit/15e93dd5d0dc3b5dcd00504833beacbddbfe44f3))\n- format code ([1d000be](https://github.com/continuedev/continue/commit/1d000bec682d1d723bf6748f34c3dd86b684d775))\n- formatting ([9efd0e1](https://github.com/continuedev/continue/commit/9efd0e128ef464d45379e84abf15e98ad1b24d40))\n- formatting ([2284291](https://github.com/continuedev/continue/commit/22842919489ad272edfa182e9f8af3a26d9813ae))\n- formatting ([4d70d3d](https://github.com/continuedev/continue/commit/4d70d3dcbd646b1aa1cb6e586439459dec2db462))\n- free trial config.json updates ([905f064](https://github.com/continuedev/continue/commit/905f064543e71864eaeb4e039e5d55fe2da845fd))\n- fullscreen gui retains context when hidden, fixed fullscreen focusing ([#1582](https://github.com/continuedev/continue/issues/1582)) ([679e26d](https://github.com/continuedev/continue/commit/679e26d4a8a81402d57a64d819f347d9f56d42e0))\n- gemini tool calls with MCP ([9ca4ad4](https://github.com/continuedev/continue/commit/9ca4ad4afe04fbfc3a6e0e6d447dd81ff22b4369))\n- getting diff ([9431131](https://github.com/continuedev/continue/commit/9431131f1cbbc5fe742c033a8baf970241c63378))\n- **google:** remove unnecessary parameter ([#394](https://github.com/continuedev/continue/issues/394)) ([938c1db](https://github.com/continuedev/continue/commit/938c1db5b37d5332ff5d188f4fa79f3bc6b7549a))\n- grab PATH for mcp connections ([60ec3ac](https://github.com/continuedev/continue/commit/60ec3ac452f0b25d176728dfe6e83e4f34ade745))\n- grammar ([58feb0b](https://github.com/continuedev/continue/commit/58feb0b8622e5f64737594ccbfd94aff2e9687c5))\n- **gui:** ctx rendering w/ renderInlineAs: \"\" ([#1541](https://github.com/continuedev/continue/issues/1541)) ([8a92f43](https://github.com/continuedev/continue/commit/8a92f4312693549a1590e99c70152b843d59b119))\n- **gui:** passing style props on StyledTooltip ([41e1eb5](https://github.com/continuedev/continue/commit/41e1eb582bec8676fc973c3a2b76259344fbb317))\n- **gui:** typo ([3d53809](https://github.com/continuedev/continue/commit/3d5380908292bb4373a122a802f2011dd0283f62))\n- handle apiType for azure w/ proxy ([036e196](https://github.com/continuedev/continue/commit/036e19618b86600e4e9c5ad6e6ee5a98e55e15f1))\n- handle closed webview on quick edit ([#1942](https://github.com/continuedev/continue/issues/1942)) ([fe05db5](https://github.com/continuedev/continue/commit/fe05db541dafc45c9870d52d9f15e812e1ea90ad))\n- handle deleted blocks ([c1157db](https://github.com/continuedev/continue/commit/c1157dbf02f6750d385b28006c90202e6b159d5a))\n- Handle empty addedLines in revertDiff ([f280319](https://github.com/continuedev/continue/commit/f28031902f41c4ae9f7cab01398216272950b7d0))\n- handle line removal diff ([#1744](https://github.com/continuedev/continue/issues/1744)) ([6126eca](https://github.com/continuedev/continue/commit/6126eca35ab981c7b8cd6a56769dfe7cb9e69349))\n- handle optional params in EditSlashCommand ([#745](https://github.com/continuedev/continue/issues/745)) ([0141229](https://github.com/continuedev/continue/commit/014122957ff6e087b03def436b95797e59c6f6cb))\n- Handle status update in DocsService ([ac3df81](https://github.com/continuedev/continue/commit/ac3df814e45343b07f63c99e8710f4a6b185074e))\n- handle when dir might be a file path ([e43d3bd](https://github.com/continuedev/continue/commit/e43d3bda6aa833bfe686b8c84dfa64d43fa08748))\n- hardcode `fontSize` for tool call status msg ([f78969b](https://github.com/continuedev/continue/commit/f78969b147a8aecb5c9e5162d080cdafb3f4adb7))\n- hmr issues with non-react component exports ([0e55c08](https://github.com/continuedev/continue/commit/0e55c08be3ef5dcaec5305f41ae871ae6903b6c7))\n- hotkey for new session ([#572](https://github.com/continuedev/continue/issues/572)) ([1d3acd9](https://github.com/continuedev/continue/commit/1d3acd9f194802d78c99cd407bcb72543aa0ff19))\n- **HttpContextProvider:** support all possible local servers via a library ([c3b2552](https://github.com/continuedev/continue/commit/c3b2552ed5e47eef8dc2a691d5f76f30b61ceafd))\n- if rules ([a6dd5a8](https://github.com/continuedev/continue/commit/a6dd5a83dedcae099f320a2da9362490eadde299))\n- ignore CSV files ([#1972](https://github.com/continuedev/continue/issues/1972)) ([1272f34](https://github.com/continuedev/continue/commit/1272f3402a1fc12e5eb0566ad4a4aadc3db7dc06))\n- Implement RepoMapContextProvider ([48c442d](https://github.com/continuedev/continue/commit/48c442d1c687b15d5612786e87a09b016714abd6))\n- import posthog as type, use inline import ([57a84dc](https://github.com/continuedev/continue/commit/57a84dcfe16e3cdebea54f7a0b0fe354f0d8ad2b))\n- Improve error handling and ensure coroutines continue correctly in case of failure ([46bd573](https://github.com/continuedev/continue/commit/46bd573a4296bb8ce8dc4c43b7293ff575655e53))\n- improve textarea ux ([#1901](https://github.com/continuedev/continue/issues/1901)) ([228bc30](https://github.com/continuedev/continue/commit/228bc30e895b87e06c170775974c49e2781d9ae0))\n- include access to the Documents folder ([03839c3](https://github.com/continuedev/continue/commit/03839c30813f3242d9a1fce91f39af110d8d85a7))\n- incorporate feedback ([98efbbd](https://github.com/continuedev/continue/commit/98efbbd80f075928a45334f8cc4103221c2656c9))\n- incorrect scroll logic on inline edit ([ed3af3f](https://github.com/continuedev/continue/commit/ed3af3f3fc49f6678d8a1f940d06aa62bec1f4b5))\n- intellij freezing ([1f602e5](https://github.com/continuedev/continue/commit/1f602e53c29eb28d98754252823ad8f40f839d1b))\n- isRuleActive ([2b97efe](https://github.com/continuedev/continue/commit/2b97efe5c6d045b461c76068c0e06df298d95902))\n- issue 3732 ([ce5d664](https://github.com/continuedev/continue/commit/ce5d664f828de8681ee7129dfec3c16cad667c46))\n- **JB+GUI:** arrow keys in GUI on linux ([9110904](https://github.com/continuedev/continue/commit/9110904e7d4f727339666ce235388397b8c4ff99))\n- **jb:** meta keybinding issues ([b4dad78](https://github.com/continuedev/continue/commit/b4dad78a727feed0ce18769a8d3f9cfd00a3bd57))\n- **jb:** remove markup from `edit` model title ([f6d334a](https://github.com/continuedev/continue/commit/f6d334a190bf1132c33a68f472aaeb2e5389399f))\n- **jb:** set `setOffScreenRendering` conditionally ([fd66ff7](https://github.com/continuedev/continue/commit/fd66ff7d464d3138a127e2035bca88f6d42c4402))\n- **Jira Context:** add api abstraction ([cc9b960](https://github.com/continuedev/continue/commit/cc9b96038bb0b88156729d1c39e6eec1794fe76a))\n- jira ctx provider ([9d24d34](https://github.com/continuedev/continue/commit/9d24d3402f3e9f9937bfdff70fa5c4fa120b2dbf))\n- keyboard shortcuts test on linux ([e45aa16](https://github.com/continuedev/continue/commit/e45aa164c64a52936c7dd1c0467548aadf821639))\n- layout alignment ([#1779](https://github.com/continuedev/continue/issues/1779)) ([6e8fc12](https://github.com/continuedev/continue/commit/6e8fc1247c44c344a48db9758437cb7b3028591c))\n- lint ([f575721](https://github.com/continuedev/continue/commit/f575721980a596cdb08be8a2e472ca26e6012325))\n- lint ([3ba1153](https://github.com/continuedev/continue/commit/3ba1153243753682ca19b26031c2c3c73394b5f2))\n- lint ([03cd1a3](https://github.com/continuedev/continue/commit/03cd1a338f03fc1baa87feb02bd6e56111fc0b13))\n- lint ([747d380](https://github.com/continuedev/continue/commit/747d3806a99abf7f55823c2fbc06c74448ba3c1c))\n- linting error ([67ab058](https://github.com/continuedev/continue/commit/67ab05821191d75a201e516b725a679671f94734))\n- linux key ([9618327](https://github.com/continuedev/continue/commit/9618327cf45687f18aacdc04d62da86ce2d1e520))\n- linux test env ([9f2f9d8](https://github.com/continuedev/continue/commit/9f2f9d895b2fe6678f49e394e4657f044e2aef2a))\n- listbox z-index issue ([1d2bfcd](https://github.com/continuedev/continue/commit/1d2bfcd4b2ad7c3afef97348c55a1c668610ea07))\n- listDir ([a41a4bd](https://github.com/continuedev/continue/commit/a41a4bd2df5807bd216a4b23eabe8656ca098cdb))\n- **llm/Ollama:** changed buffer handling like in streamChat() ([f1f8967](https://github.com/continuedev/continue/commit/f1f8967b6c4b422428fcad0575c0b24ae78bf34a))\n- load last session when completing edits ([1e33400](https://github.com/continuedev/continue/commit/1e334008c08eb8796510ba6aaac434073c44b3b8))\n- local build script ([#810](https://github.com/continuedev/continue/issues/810)) ([76fe78a](https://github.com/continuedev/continue/commit/76fe78a1a3c05b0d6dd5ed6d15575e20e797cfa6))\n- log llm completition in edit command ([3094180](https://github.com/continuedev/continue/commit/30941800b04c59b9a45375d6934a4fb79cac0141))\n- logs ([e3cf75e](https://github.com/continuedev/continue/commit/e3cf75e0779d3bc02a468ea6551e2499979c08f7))\n- mac build issues ([9946a5c](https://github.com/continuedev/continue/commit/9946a5c05f85d20e2d766719a861eb0f8a55019c))\n- major fixes to getTheme ([c6002ba](https://github.com/continuedev/continue/commit/c6002ba2ac905bd5439173e6430676bdc3594a87))\n- make `env.apiVersion` required for azure openai ([4355618](https://github.com/continuedev/continue/commit/4355618079808bdfc3240022e79a0a9dd8457b02))\n- matching context code ([64fb928](https://github.com/continuedev/continue/commit/64fb928b8ff9c0f7753a8b8d374abc179de1f4d5))\n- mcp context provider bug with issue 2467 ([98f07be](https://github.com/continuedev/continue/commit/98f07bef818cf5acc4de2828cbeaa0b4b499fc53))\n- MDX compilation issue ([aca82d2](https://github.com/continuedev/continue/commit/aca82d2ef36a5bd57119343790875e2acf7819b3))\n- merge bugs ([009e578](https://github.com/continuedev/continue/commit/009e578416982a90af05539420a7137ccd2d03c1))\n- messages ([0b07020](https://github.com/continuedev/continue/commit/0b07020d7dd9bf59ca1d804668a215174273625b))\n- minor issues ([#581](https://github.com/continuedev/continue/issues/581)) ([570f61d](https://github.com/continuedev/continue/commit/570f61de5a404559b4ff69eb5cd2fa216e70e872))\n- missing arguments ([452b242](https://github.com/continuedev/continue/commit/452b2422d18c05dd28dbfa6fd127f4c3e0b0223c))\n- missing cd ([0d88b8f](https://github.com/continuedev/continue/commit/0d88b8f379d161962a72626fe2af583cce0f3fe0))\n- missing Content-Type header in OllamaEmbeddingsProvider ([#1855](https://github.com/continuedev/continue/issues/1855)) ([841294d](https://github.com/continuedev/continue/commit/841294d15f3ad6ea4a9f7cf277fbcb905d9bd978))\n- missing context items ([bdc2ec8](https://github.com/continuedev/continue/commit/bdc2ec8374ac9e7d14fc440ea80fa438005735e8))\n- missing env ([1ebd3a3](https://github.com/continuedev/continue/commit/1ebd3a31bab5ed31374518022c5f1b7658c379f2))\n- missing package ([e284cec](https://github.com/continuedev/continue/commit/e284cecd6584b14bee43794d0d238d64c734a10a))\n- missing useLegacyCompletionsEndpoint in yaml schema ([a4b4395](https://github.com/continuedev/continue/commit/a4b4395649e9fa7888f8204ddcf56b88632b5b90))\n- model name ([b1ec201](https://github.com/continuedev/continue/commit/b1ec2012c649ff29e84d6682234328e9c2fdb792))\n- models and mode are not synced ([57441da](https://github.com/continuedev/continue/commit/57441dad6696eeb6d4bd6d769aec06e389367a53))\n- modify access modifier public to private ([c36f233](https://github.com/continuedev/continue/commit/c36f23322265a0d9c28eb401acf49d76294a1249))\n- move autocomplete logic off of EDT ([c46083d](https://github.com/continuedev/continue/commit/c46083d4115a79938bc6ae2b9421f36803b0a992))\n- naming ([ba39842](https://github.com/continuedev/continue/commit/ba3984290ddb875608845904d31c5a5d57b1bf03))\n- naming ([42a03ad](https://github.com/continuedev/continue/commit/42a03ad020634a7c6be719a70bf40144582682dc))\n- naming ([fb1a75d](https://github.com/continuedev/continue/commit/fb1a75dcf33fa38863bda4f5b88c1a567aaf0269))\n- navigation bug ([fab5347](https://github.com/continuedev/continue/commit/fab53472a931b49d7f0c19af22263df6b54bc292))\n- not sure how that button fragment got there ([880f1de](https://github.com/continuedev/continue/commit/880f1de80d2bc33635cc9ebed01ae84e2c17c640))\n- number of completions to generate is not supported ([db90b09](https://github.com/continuedev/continue/commit/db90b0954b36e856258eed2b61af8bf107f13510))\n- old docs ([b52910b](https://github.com/continuedev/continue/commit/b52910b43351e5df8c07cfb27beca8fba5463074))\n- ollama tool use ([82ac684](https://github.com/continuedev/continue/commit/82ac6848cabca21122995460328b8f4420ae3222))\n- onboarding telemetry ([445dba3](https://github.com/continuedev/continue/commit/445dba3bb60654ac9c3f8d245f896d490574c3a1))\n- only alert free trial if using free trial ([f0e73ef](https://github.com/continuedev/continue/commit/f0e73ef63a8fd8bb7ce313f9ed5439c7c68969bf))\n- only perform lancedb cpu check for linux ([e0ee7d1](https://github.com/continuedev/continue/commit/e0ee7d1e446b18c05ed3166b0aa3b199ec021f1a))\n- open new session from any page ([258bbc6](https://github.com/continuedev/continue/commit/258bbc6b5cbbd3b897e8045df247bb2a1fc21f94))\n- overly eager suffix matching ([556a8c5](https://github.com/continuedev/continue/commit/556a8c5ee3f72311a384c64ba14fde7122e68741))\n- path ([d01f243](https://github.com/continuedev/continue/commit/d01f24309b490799014dca0fd03775c39745d660))\n- path ([3ecc89e](https://github.com/continuedev/continue/commit/3ecc89eaf55b970e1a494f88fed0850c4b85c565))\n- pathing issue in jb ([c33e390](https://github.com/continuedev/continue/commit/c33e390427e459a5426ee7669733c2b407649e40))\n- pause between tests to avoid db lock ([4cb84c1](https://github.com/continuedev/continue/commit/4cb84c11b09dac6eb0791e35b95ff6f571130391))\n- placeholder ([8ded13a](https://github.com/continuedev/continue/commit/8ded13a8f8e54df4951b547d468782117fb11b8b))\n- policy precedence ([de6a0ae](https://github.com/continuedev/continue/commit/de6a0ae36c43753e768bf0164dfbe1b938da790d))\n- preserve indentation in code blocks ([#1554](https://github.com/continuedev/continue/issues/1554)) ([574a392](https://github.com/continuedev/continue/commit/574a3923a6f97d8995561da3654aae3aca9ae439))\n- preserve system messages as user messages ([d60a1c1](https://github.com/continuedev/continue/commit/d60a1c1b1f3174cd501eca69746009bdeb4c6f7f))\n- prettier ([4d278e4](https://github.com/continuedev/continue/commit/4d278e479ae02f0d02ccff8db2448d6b38b3d3e5))\n- prevent autoscrolling on new tool msgs ([678a75f](https://github.com/continuedev/continue/commit/678a75f8f7b97b0f75407c6e1b1dd78c415d22eb))\n- Prevent duplicate context provider addition ([2d6eacb](https://github.com/continuedev/continue/commit/2d6eacbeab8eb89ddc32311ade4efe3ccb272bca))\n- Prevent multiple resumes in continuation ([1e96bfb](https://github.com/continuedev/continue/commit/1e96bfb9683fa0a45437f16b419be066917351cb))\n- Promise arrays being returned. Added a new definitions-only context provider. fix: typescript tagging. ([#835](https://github.com/continuedev/continue/issues/835)) ([00eeb00](https://github.com/continuedev/continue/commit/00eeb0024178953172941bba293808be4ca5eed2))\n- prompt format ([0f6d280](https://github.com/continuedev/continue/commit/0f6d280234c459e1935b62e5244f4577d64fac53))\n- promptblock parsing ([eb6735e](https://github.com/continuedev/continue/commit/eb6735ee25da93e01a7b09223960b8975b41d898))\n- Properly ignore ignored folders when generating /onboard context ([f7bca6a](https://github.com/continuedev/continue/commit/f7bca6a61d56b6e17b100294e605d0e2bbe0239b))\n- protocols ([c8a24c0](https://github.com/continuedev/continue/commit/c8a24c0c9040a0da02bdd215b77b0320c160256f))\n- qwen completions missing whitespaces ([75dce9c](https://github.com/continuedev/continue/commit/75dce9cbd14cc374d1ce4921522f3423202be7b1))\n- **reg:** :ambulance: remove log except when in NODE_ENV === \"test\" ([f60fd1c](https://github.com/continuedev/continue/commit/f60fd1c6188567c67d3d8285fd6c2c5d6acc5bfa))\n- regex ([05a3eec](https://github.com/continuedev/continue/commit/05a3eec21ed1c26aa4d9825e6be5e209cdb9563c))\n- reject logic ([5944fca](https://github.com/continuedev/continue/commit/5944fca7374851869e469ed1cd3f13056de5c8b3))\n- remove `any` from promptTemplate passing ([7834d26](https://github.com/continuedev/continue/commit/7834d266330625915542913f2d44ac0d80875d7e))\n- remove debugger statement ([3e4c75b](https://github.com/continuedev/continue/commit/3e4c75bf461c36c2200ac2922670bb76a612b292))\n- remove deugger ([3e8e29e](https://github.com/continuedev/continue/commit/3e8e29e8f2ce18e138df64441e436d3a07ffa45e))\n- remove duplicated context comments ([0c217a4](https://github.com/continuedev/continue/commit/0c217a40082c2c244de4f4f8c955d96314f130da))\n- remove gifs from media ([77d321f](https://github.com/continuedev/continue/commit/77d321ff82dcd34b55d0a0253eb45672be774fe1))\n- remove invalid schema ([2355eb6](https://github.com/continuedev/continue/commit/2355eb62a5c3c5034b528ab17e8ad7a0b661bc7c))\n- remove mismatch between last selected profile and current profile ([d9eb118](https://github.com/continuedev/continue/commit/d9eb118c514fc2a4364f532eb6438588fc9f712c))\n- remove some backup files generated by pkg if present ([#1287](https://github.com/continuedev/continue/issues/1287)) ([9f160ad](https://github.com/continuedev/continue/commit/9f160ad11a1d36bfc92d2da2a8ea31721652b3e4))\n- remove stale code ([cd97833](https://github.com/continuedev/continue/commit/cd97833be88ecaf3c2860dc41ca57c9e2b18ad48))\n- remove testing logic ([5df8177](https://github.com/continuedev/continue/commit/5df81770df526c69dd71f2421a9052deb17accf6))\n- remove tooltips when switching files ([200d291](https://github.com/continuedev/continue/commit/200d29103ea97db06283f2d4b5e0eb9b6afb86df))\n- remove unnecessary baseUrl ([5f159fb](https://github.com/continuedev/continue/commit/5f159fbd81c4b8680cbad8b3875aa54e2079928e))\n- remove unused argument ([7dd2c22](https://github.com/continuedev/continue/commit/7dd2c22632d50726271655d5c5d027a8219e6862))\n- remove unused imports ([1b82d39](https://github.com/continuedev/continue/commit/1b82d39ac5b8e0cfb990a4968b3e8bc81af768ce))\n- rename edit folder ([ab1ad0a](https://github.com/continuedev/continue/commit/ab1ad0ae48eeceee87043cae9a7862a1c4f70da0))\n- replace null with empty string in getSidebarContent function ([#822](https://github.com/continuedev/continue/issues/822)) ([50d1188](https://github.com/continuedev/continue/commit/50d1188479be206d1a4ed5dcd258d4101ae4f8ba))\n- report indexing errors in webview ([6a67514](https://github.com/continuedev/continue/commit/6a67514fcf108bd0eb6989777abaed02ce2e61e7))\n- reset config errors ([cc199c2](https://github.com/continuedev/continue/commit/cc199c28634d006d38416533b06f26e001cea3fe))\n- Resolve proxy error when adding Azure OpenAI model ([17dbfe6](https://github.com/continuedev/continue/commit/17dbfe6bfd38f67b7cb535da00fa626cdaf4f690))\n- Resolve type error in env comparison ([45eacf4](https://github.com/continuedev/continue/commit/45eacf4ebbd99a97eef32eef01d780a1f8c57b7a))\n- Resolved errors related to incorrect URIs ([0a7da20](https://github.com/continuedev/continue/commit/0a7da2067e8d8567084d05f69038b7ca95464d1f))\n- restore all e2e tests ([d3a6eeb](https://github.com/continuedev/continue/commit/d3a6eebeb7b5d96133d1c13e533913dc5ec2f19c))\n- restore cache ([91ea00d](https://github.com/continuedev/continue/commit/91ea00d284227f3875daf17591b15409004372ac))\n- restore comments ([067a3a8](https://github.com/continuedev/continue/commit/067a3a843baf5a0e12bd5b6f21f37a504836286c))\n- restore tests ([553c751](https://github.com/continuedev/continue/commit/553c751562afb736fb914549ba3d2c75e5a4b305))\n- return ctx items from prompt file ([84df7f4](https://github.com/continuedev/continue/commit/84df7f45bf91c70da79901c0a44cc3b156655f6f))\n- revert file changes ([7a44a38](https://github.com/continuedev/continue/commit/7a44a38c860cfbd0eeab35d8fdb3cb198edad680))\n- Revert filepath changes ([3df34b7](https://github.com/continuedev/continue/commit/3df34b725e873fcd42058474d83ef634e21d5cad))\n- revert launch.json change ([b3abfb8](https://github.com/continuedev/continue/commit/b3abfb80d71c16a0e51cc980d4262dbd218d6463))\n- reverted the url of llama.cpp back to 'completion'. ([e8523a2](https://github.com/continuedev/continue/commit/e8523a2b07c7a5d1539138ee8e4a4836fea7a36b))\n- rules ([bcae5b3](https://github.com/continuedev/continue/commit/bcae5b3cf210f1a8ced3b2c913961ed16aaf2ea9))\n- rules ([9e57412](https://github.com/continuedev/continue/commit/9e57412fe161a9451aaaa651a9f9ed2d7b6b5517))\n- rules ([f29110f](https://github.com/continuedev/continue/commit/f29110f98b1c71f4832d6b7ff0041e342e2a0c4a))\n- rules ([b39cb83](https://github.com/continuedev/continue/commit/b39cb836ad6180226d39abe85f5637a6e2dac44d))\n- sanitize lance table names ([5d3b2cb](https://github.com/continuedev/continue/commit/5d3b2cb5070505578cc16e67d2309a40a403bc94))\n- scaffolding for future language syntax highlighting ([647656c](https://github.com/continuedev/continue/commit/647656cd3d007c016bea372bdfcb5d69f9d1d52a))\n- scroll issues w/ code blocks ([#1688](https://github.com/continuedev/continue/issues/1688)) ([ceb8da0](https://github.com/continuedev/continue/commit/ceb8da0b20db70707a3443ff6f705bba6922488d))\n- scroll to the bottom on history change ([b94ccc6](https://github.com/continuedev/continue/commit/b94ccc6db557f705ba4f810ed56c55ce21f383dd))\n- selected profile id writing to global context in org profile rectification ([47755d1](https://github.com/continuedev/continue/commit/47755d1cba3b8c7cb5c0b9a19bf8ca0a92eec850))\n- selector for `isSingleRangeEdit` ([badeb1c](https://github.com/continuedev/continue/commit/badeb1c88b7e1f8eb711a8a2cfc8f341ac22f67e))\n- send feature flags ([2e3fdb3](https://github.com/continuedev/continue/commit/2e3fdb34e2e2358232cc12fcf0a0c2d16a3ef0cc))\n- serve to localhost ([7eccf2d](https://github.com/continuedev/continue/commit/7eccf2ddb9308111686251474fb79fa03dfc87d6))\n- set DEFAULT_CONTEXT_LENGTH to 8192 ([f163768](https://github.com/continuedev/continue/commit/f1637685c651180d1fc969c2f11712fda69eb9ee))\n- settings persistance ([56f3803](https://github.com/continuedev/continue/commit/56f38038b35cedc2aa1bca1b09780b21c811bd83))\n- setup ([6a93eae](https://github.com/continuedev/continue/commit/6a93eaeb0faf685dff216ecca850f9b2ef3cf2c3))\n- show error is GH sign in failed in vscode ([e92e052](https://github.com/continuedev/continue/commit/e92e052b16fb91173aa4dc85d4e80fa569b8e60a))\n- sign in ([fa5d2c5](https://github.com/continuedev/continue/commit/fa5d2c5aa2cdcc580486f81fde3ae69e9d3069e5))\n- sign in btn + colors ([1b669bf](https://github.com/continuedev/continue/commit/1b669bf80e2d81e4aa19c4d3f2dca730ddd6d4f8))\n- Skip duplicate checks at document end or blank text ([decd04e](https://github.com/continuedev/continue/commit/decd04eddd4965b9f76c7a9a3abd276d8d2e1692))\n- snippets in fim prefix ([51e1c42](https://github.com/continuedev/continue/commit/51e1c4216baa38d73141aca1d1e681eabf044b4b))\n- some typos ([#478](https://github.com/continuedev/continue/issues/478)) ([775d051](https://github.com/continuedev/continue/commit/775d051b4bcdbe07fbd38fae4d3e36e79234eb56))\n- spread tool degs ([4ddd275](https://github.com/continuedev/continue/commit/4ddd2755996012677c15069648158bd4334fa3dd))\n- sqlite binary build on fresh clone ([#1433](https://github.com/continuedev/continue/issues/1433)) ([d124aa0](https://github.com/continuedev/continue/commit/d124aa0534303b58b55bcc2de4ec460b4f499178))\n- stream ([c497517](https://github.com/continuedev/continue/commit/c49751713248e27f060b6eee11179e13b14c5adf))\n- system message ([c2707c6](https://github.com/continuedev/continue/commit/c2707c6988a9b6368f183f0602de0cc2872726da))\n- test ([7266efb](https://github.com/continuedev/continue/commit/7266efb4e8490accd69ad5818bd9c35fe6f59e5c))\n- test env path ([71a151f](https://github.com/continuedev/continue/commit/71a151f399d514497ee917eca521863d07b2a270))\n- test timing ([878e211](https://github.com/continuedev/continue/commit/878e2116d6a97c31cad97432d785aaf3b6deb196))\n- tests ([35c674c](https://github.com/continuedev/continue/commit/35c674c724c21c6dc0b699a37f868e68c62f1259))\n- tests ([cafb2e9](https://github.com/continuedev/continue/commit/cafb2e93c7a54b4a35ecc8a209f53982b3cc26d4))\n- tests ([b786591](https://github.com/continuedev/continue/commit/b786591415d79119845e01376bf5dc97c0676b23))\n- tests ([ff543e8](https://github.com/continuedev/continue/commit/ff543e8b321e4e48a088ba296ebac4504b97356b))\n- tests ([81acb44](https://github.com/continuedev/continue/commit/81acb446019a773ce2df98467e4dcb8e5c2a428e))\n- tests ([deea8f1](https://github.com/continuedev/continue/commit/deea8f1f44403b6d7d7bb4236ff5acb8d38bca01))\n- The selected item in inline chat is always the first one ([97bc074](https://github.com/continuedev/continue/commit/97bc07453ba067f0438192d8dc6cc1cc9a86d962))\n- timeout ([b9f161f](https://github.com/continuedev/continue/commit/b9f161f6d8b4e7400dff25912881f41f472a9137))\n- timeouts ([0bf9169](https://github.com/continuedev/continue/commit/0bf9169a968a42dfc083696b3e8be5f1997ad7e5))\n- timeouts ([0c4d9eb](https://github.com/continuedev/continue/commit/0c4d9eb819b9f1a41cddd7d0b84dc0e21ee372e9))\n- timeouts ([4c90f3b](https://github.com/continuedev/continue/commit/4c90f3bdd32761b7f34a6f05334dffc085805964))\n- tip tap undo and redo ([88e34fb](https://github.com/continuedev/continue/commit/88e34fb69caae3750621f9f93253a7425535928f))\n- tool use call ([c73f72f](https://github.com/continuedev/continue/commit/c73f72fbb379dae08aa41ac07608d9dc2aa8bc83))\n- transpile dynamic imports to cjs ([#1975](https://github.com/continuedev/continue/issues/1975)) ([1e3e8eb](https://github.com/continuedev/continue/commit/1e3e8ebe408de1bbb5f9fe0dc61fefef3b85ca69))\n- truncate `tagToString` to max filename len ([afb6bd0](https://github.com/continuedev/continue/commit/afb6bd0fb53505e29e961f5a78f0afe36c3b3071))\n- ts ignore declaration file issue w/ dbinfoz ([#1945](https://github.com/continuedev/continue/issues/1945)) ([477ecd6](https://github.com/continuedev/continue/commit/477ecd63acc85be25ec992d4329f6460c42c6785))\n- tsconfig ([8549981](https://github.com/continuedev/continue/commit/8549981e200e7e62f4a5895babc1c87c45b59db0))\n- type errors ([f976e54](https://github.com/continuedev/continue/commit/f976e54dc3b2b8072aef3ecadf259ee1e5299956))\n- type errors ([f575a36](https://github.com/continuedev/continue/commit/f575a3683c499f04766ac68af6b8e214ed2c3514))\n- type errors ([cd82f4c](https://github.com/continuedev/continue/commit/cd82f4c61fdff90c8ed0e845a3b2ee8b0ce1be9d))\n- type errors ([ea6b234](https://github.com/continuedev/continue/commit/ea6b2344d0b2cde7def58de549e604142a40a353))\n- type errors ([f19b6b8](https://github.com/continuedev/continue/commit/f19b6b8abdb4dfb64c804efee851dee0bb00471c))\n- type errors ([5c49505](https://github.com/continuedev/continue/commit/5c49505f2071504481c17976670d883efebddb4a))\n- type errors ([cdc9600](https://github.com/continuedev/continue/commit/cdc9600e689161877ff7416f63baa9dcbfa6c18a))\n- type errors ([7ee4d8d](https://github.com/continuedev/continue/commit/7ee4d8db505924a4d0edd5ef3ac49f4bb9a706b6))\n- type errors ([70ae585](https://github.com/continuedev/continue/commit/70ae58594cd136e16a53fcfde88a227dbec1104f))\n- type errors ([34489b1](https://github.com/continuedev/continue/commit/34489b14de0fc1c5cfb300a624b3b8571372e532))\n- type errors / formatting ([9dba473](https://github.com/continuedev/continue/commit/9dba4730b958e05ffad6e0bab74effefd228cd6c))\n- types ([7b9c5cc](https://github.com/continuedev/continue/commit/7b9c5cc4f062e2882706919b1effe34a460f45b8))\n- types ([82cad59](https://github.com/continuedev/continue/commit/82cad596c85d0ee0b17ceccdf1f6cca5811b8f94))\n- types ([5faecca](https://github.com/continuedev/continue/commit/5faecca09f27a7490b4050b7191197c74fe38299))\n- types ([be27552](https://github.com/continuedev/continue/commit/be27552e3330b3a18ecb1065b6ab7210a4375826))\n- types ([4b120e5](https://github.com/continuedev/continue/commit/4b120e543e7c2cec0d33714cfebe88800604fbfd))\n- types ([44212e4](https://github.com/continuedev/continue/commit/44212e45828bb72c035bf63de4c13342935dcf0f))\n- types ([72b253d](https://github.com/continuedev/continue/commit/72b253d14faf2d19832b0307fb4fba1e9aed11dd))\n- types ([a88f5d7](https://github.com/continuedev/continue/commit/a88f5d771ced366926c9d6d821c697e52847a2f9))\n- types ([41a7916](https://github.com/continuedev/continue/commit/41a79166aadc2bbbe80145d12bfeb1336d94c7e2))\n- typo ([be874ad](https://github.com/continuedev/continue/commit/be874addb463f7db81c43b52f6e69f9de1c11fa3))\n- **typo:** customize overview doc ([d4dfb69](https://github.com/continuedev/continue/commit/d4dfb699af1f5bffc923a5d850f877ed45ad74a6))\n- undefined in title ([dcdfa59](https://github.com/continuedev/continue/commit/dcdfa59db43ae4e864e5cb1ca04b6804cca64d2f))\n- unnecessary dependency ([ade83fd](https://github.com/continuedev/continue/commit/ade83fd0c6b3f2d3bb8175b6bdbdc0a6e9735c5b))\n- unnecessary scroll delay ([183ae03](https://github.com/continuedev/continue/commit/183ae03ee37ba6629201211e9a63fbcb04ebc527))\n- unset system message ([4480ee3](https://github.com/continuedev/continue/commit/4480ee3ac358e60182304e0e052c27ae45815e2f))\n- update adf2md package ([ecbc123](https://github.com/continuedev/continue/commit/ecbc1234f7419f7921b31354594e5841363c54f5))\n- Update Amazon Bedrock documentation and categorization ([b156d8f](https://github.com/continuedev/continue/commit/b156d8fd4c103e520ad720853752a4f9033ef68f))\n- update CodebaseIndexer path ([#1870](https://github.com/continuedev/continue/issues/1870)) ([00d19e6](https://github.com/continuedev/continue/commit/00d19e623f8abb04fc4a022d354a909e2087aa3c))\n- update imports in CodeSnippetsIndex.ts ([434e4ee](https://github.com/continuedev/continue/commit/434e4ee44b513db15d502c6a9d52ceb17fa805c3))\n- update install script ([91f87f8](https://github.com/continuedev/continue/commit/91f87f8fd0ca328a91c9fd033c58fd5d2db02892))\n- update intellij getDiff to return string[] ([1672d9f](https://github.com/continuedev/continue/commit/1672d9f864e99436b8f309af3529f2fda341172d))\n- update snippets index to upsert cmds ([676f0d9](https://github.com/continuedev/continue/commit/676f0d9335df7bf7f42aae3434847cc0032c3c0a))\n- update spacing ([a9d6597](https://github.com/continuedev/continue/commit/a9d6597a3fa054d093cb63fa5859e24b8f8d571b))\n- update streamId matching ([8a1ce0f](https://github.com/continuedev/continue/commit/8a1ce0f1f06e92b7bd140d7eabd8393dc5993fea))\n- update streamResponse function to use streaming decoding ([#1436](https://github.com/continuedev/continue/issues/1436)) ([2d1155d](https://github.com/continuedev/continue/commit/2d1155d7d4441042480bf928398b7a15a58d92d2))\n- uri utils ([f1026c6](https://github.com/continuedev/continue/commit/f1026c6a2cfdd9424097d2103a5ffc2216e402d9))\n- use `cmd` for windows MCP connections ([b8ca4cb](https://github.com/continuedev/continue/commit/b8ca4cb6ee91670e87ae331f5e4bab24a42adbe6))\n- use bolt icon for shortcuts ([ff2116b](https://github.com/continuedev/continue/commit/ff2116bacc4537e09792ce9817b68e9cdfacb93e))\n- use dir hash in `tagToString` ([9c2a780](https://github.com/continuedev/continue/commit/9c2a780fa986f5f98c3231a12971d3f16c7a8e74))\n- use exponential backoff when checking ollama ([ebbc2fd](https://github.com/continuedev/continue/commit/ebbc2fd55df1fc43e1eacceba042fa4c0bc38e77))\n- use ide scheme auth redirect ([bae74c1](https://github.com/continuedev/continue/commit/bae74c1b324b51c2e9b14b548f92557c75025f51))\n- use index when sending chat input ([9398945](https://github.com/continuedev/continue/commit/93989459e3806f1ba6d647036dc9db064bf30e38))\n- use introduction layout ([2b71869](https://github.com/continuedev/continue/commit/2b7186973695703a48daacea46c076231d308ddf))\n- Use MCP server_name from JSON config as display name ([481d60a](https://github.com/continuedev/continue/commit/481d60a74d9b7cd0492aa5875d89e5b4e2e00c51))\n- use new edit for \"Generate code\" cmd ([aac6f80](https://github.com/continuedev/continue/commit/aac6f805eb35107e7d8754413d955ccfeb964576))\n- use prompt file sys msg ([093225d](https://github.com/continuedev/continue/commit/093225d4fed4d5249ca506b9867f5b5305ca4003))\n- use proper alternating colors in KeyboardShortcuts.tsx ([f1bfcec](https://github.com/continuedev/continue/commit/f1bfcec8257c2b3f99fa99a643b0464239600490))\n- vitest ([3bc4bb7](https://github.com/continuedev/continue/commit/3bc4bb710cfb2d9ef340a61f7c0ca27cb2540a28))\n- vitest ([ceeb87e](https://github.com/continuedev/continue/commit/ceeb87e81b8848517dfc604025f11bf46e1955b0))\n- **vscode:** handle null values for edits and page in getSidebarContent ([#618](https://github.com/continuedev/continue/issues/618)) ([f8a02ad](https://github.com/continuedev/continue/commit/f8a02add1775805246410ee3df396a4028ff2ec9))\n- **vscode:** improve forceAutocomplete command reliability ([cc50768](https://github.com/continuedev/continue/commit/cc507687eb36be5b7fd59eac433a9a908c1ee07e))\n- **vscode:** keybinding full screen toggle shortcut ([#625](https://github.com/continuedev/continue/issues/625)) ([2921504](https://github.com/continuedev/continue/commit/292150476c1cfcace2f6434d11c3a2dab1e5ccdb))\n- wait for permissions before initializing messenger ([7d16b8a](https://github.com/continuedev/continue/commit/7d16b8a870ebf5348713fb116c109a646fe7c79e))\n- wait for sidebar before other commands ([1566adc](https://github.com/continuedev/continue/commit/1566adcaeb3bc5258a563b208a511cdfb3938d7a))\n- watcher ([f41c819](https://github.com/continuedev/continue/commit/f41c8194fc11da9c76de0531cccd73df78a4b1c2))\n- watcher ([be153f8](https://github.com/continuedev/continue/commit/be153f8ab966e210d6249ff179c4ec1580507d88))\n- whitespace in new line completions ([4db2637](https://github.com/continuedev/continue/commit/4db2637d5b65d427e959b7fd607a65ce4140c355))\n- workflow ([e5fb4f1](https://github.com/continuedev/continue/commit/e5fb4f17ad9d3b6e72d6c7e0920357e607c0e776))\n- wrap editor buttons when out of space ([#1727](https://github.com/continuedev/continue/issues/1727)) ([b38ec0e](https://github.com/continuedev/continue/commit/b38ec0e6153e9077d1e39dfd7a453746da31c75b))\n- wrong shortcut for JB ([b358be2](https://github.com/continuedev/continue/commit/b358be2cfb798bfb6646f5015117199e2ad3a6e2))\n\n### Features\n\n- :adhesive_bandage: ca_bundle_path for maybeproxyopenai ([1018cd4](https://github.com/continuedev/continue/commit/1018cd47306f95dde35e1a0cc6b2a830444af389))\n- :adhesive_bandage: QueuedLLM for simultaneous reqs (LM Studio) ([e9d2891](https://github.com/continuedev/continue/commit/e9d289173ec28a1a90ae58b1834c476bb46834b8))\n- :art: custom prompt templates per model ([2e69e11](https://github.com/continuedev/continue/commit/2e69e117e198698f57bda06794cf030afbfe69e9))\n- :art: small changes to /codebase ui ([1b68904](https://github.com/continuedev/continue/commit/1b689046a7f9323c7bd56e14d403675db0b38d54))\n- :bricks: Enable terminals for additional vscode Remote Host Types ([1500281](https://github.com/continuedev/continue/commit/1500281bb547f7308aa7316f6783072e4f81fbc1))\n- :bug: kill old server if needed ([a34046b](https://github.com/continuedev/continue/commit/a34046bbbe817f81cd6d8b7ff9025413589571aa))\n- :children_crossing: display troubleshooting link when loading ([698dccf](https://github.com/continuedev/continue/commit/698dccf474619963de0312d36af6d01e3df8b47a))\n- :children_crossing: drag continue to right bar tip ([04b1fde](https://github.com/continuedev/continue/commit/04b1fde6ea77e7698870063c3f588da93d763544))\n- :children_crossing: keep file context up to data by listening for filesystem events ([#396](https://github.com/continuedev/continue/issues/396)) ([b6435e1](https://github.com/continuedev/continue/commit/b6435e1e479edb1e4f049098dc8522e944317f2a))\n- :children_crossing: more keyboard shortcuts ([bd202df](https://github.com/continuedev/continue/commit/bd202df41755c581844d0ab1773ba55968b15450))\n- :children_crossing: sort history by reverse date ([fd77a4b](https://github.com/continuedev/continue/commit/fd77a4bd6b255260d0f4cad11947b38f4d30030e))\n- :children_crossing: tip to debug ([ec74169](https://github.com/continuedev/continue/commit/ec741697c42745d29539be08bc3e01dcd86b1643))\n- :construction: create new sessions ([19060a3](https://github.com/continuedev/continue/commit/19060a30faf94454f4d69d01828a33985d07f109))\n- :construction: first work on URLContextProvider ([6467759](https://github.com/continuedev/continue/commit/6467759012a139e76dcf022a681355f7d310a30d))\n- :construction: react-router-dom work ([31e7c98](https://github.com/continuedev/continue/commit/31e7c9828f985eceb16b4c9c749cc5d4d9fd5beb))\n- :construction: Router and new history page ([f19345c](https://github.com/continuedev/continue/commit/f19345c652cfcf1bdf13d0a44a2f302e0cd1aa4c))\n- :construction: successfully loading past sessions ([c255279](https://github.com/continuedev/continue/commit/c25527926ad1d1f861dbed01df577e962e08d746))\n- :construction: work on EditableDiv - better ctx prov. UI ([e33d579](https://github.com/continuedev/continue/commit/e33d579a1d2b643842827925d032c3de92cf5217))\n- :egg: getting to know users form ([5dcdcd8](https://github.com/continuedev/continue/commit/5dcdcd81da2050825212e216bf5e7e69678d8c6e))\n- :fire: fix duplication in reference ([1e3c8ad](https://github.com/continuedev/continue/commit/1e3c8adabba561eeef124144f3a2ef36d26334b4))\n- :globe_with_meridians: alpaca chat template ([568771d](https://github.com/continuedev/continue/commit/568771d9b94280f1cb47aae863e8faf168eb052b))\n- :green_heart: ship with binary ([2751dde](https://github.com/continuedev/continue/commit/2751ddeb2dd8150a0d7de6c5b65e275e1aa0e155))\n- :lipstick: add context button (plus icon) ([2b35e5f](https://github.com/continuedev/continue/commit/2b35e5f5cff948ca7d4f207b23db68f0da248a95))\n- :lipstick: add textgenwebui as official option in Ui ([09ac7a7](https://github.com/continuedev/continue/commit/09ac7a7fc07d915ac6f3ef96c8e8d1894b7719b9))\n- :lipstick: better loading experience ([e19c918](https://github.com/continuedev/continue/commit/e19c918bb1c517a6a119ae8437c46e0724d2be9d))\n- :lipstick: fixed footer and change button color ([48b8f1f](https://github.com/continuedev/continue/commit/48b8f1f0ad89a2b4e35f49c360576dd5aa99a7c0))\n- :lipstick: gallery banner color ([0647a43](https://github.com/continuedev/continue/commit/0647a43a24c50ff0e52f23c49d979bddfcfbcd87))\n- :lipstick: handful of UI improvements ([ceafdf1](https://github.com/continuedev/continue/commit/ceafdf18c9d9f0f8769d4a9e45c8a407179161c5))\n- :lipstick: improvements to keyboard shortcuts ([7bf8e5b](https://github.com/continuedev/continue/commit/7bf8e5b56a518979bc1d2602b8eb4a2caf2b5fdb))\n- :lipstick: more ui improvements ([f9a84bd](https://github.com/continuedev/continue/commit/f9a84bd2d65b3142cbcfcdd8e1e9394c9d4b458e))\n- :lipstick: more ui improvements ([6e3ff01](https://github.com/continuedev/continue/commit/6e3ff0173e79f5374da9962c964559e0fb7165f5))\n- :lipstick: nested context provider dropdown ([8d423fd](https://github.com/continuedev/continue/commit/8d423fd8d1d5b136e8138a906e8594ab93ec1982))\n- :lipstick: nicer \"continue server loading\" UI ([8070ce1](https://github.com/continuedev/continue/commit/8070ce17c6d666436df38c684f5868ee7f689422))\n- :lipstick: query input indicator for ctx provs ([4362a51](https://github.com/continuedev/continue/commit/4362a51214a683bfe1efd424ddb226d4e636eeed))\n- :lipstick: setting to change font size ([4ea1760](https://github.com/continuedev/continue/commit/4ea176007d2228364d4d3fa4519898047cef988f))\n- :lipstick: small ui tweaks, detached child process ([0181d62](https://github.com/continuedev/continue/commit/0181d6236d8b74c80adb62648fd6571431cf3210))\n- :lipstick: sticky top bar in gui.tsx ([ef86d66](https://github.com/continuedev/continue/commit/ef86d661d54295c1abb9712557080f1838f96b33))\n- :lipstick: UI Improvements! ([ae058c6](https://github.com/continuedev/continue/commit/ae058c6bac7ea37108e2894e419a22dfb95fd3ff))\n- :lipstick: update icon and description ([92e7c9b](https://github.com/continuedev/continue/commit/92e7c9bb627a5559769e0eca23e02e106d2cfe96))\n- :lipstick: update marketplace icon for pre-release ([cc98ad8](https://github.com/continuedev/continue/commit/cc98ad86d561d26c97dfdb24607a1d70afbcd2a1))\n- :loud_sound: add context to dev data loggign ([8ac1518](https://github.com/continuedev/continue/commit/8ac15184aaa30d13bf168ff5123a12fb7a2dd39f))\n- :loud_sound: display any server errors to the GUI ([daabebc](https://github.com/continuedev/continue/commit/daabebcc5d6df885a508582c0ca13e659305d2ff))\n- :loud_sound: fallback unique id when vscode returns someValue.machineId ([c479442](https://github.com/continuedev/continue/commit/c47944260c5600e49d83568b3c4bafa3b7c2a37e))\n- :loud_sound: give users access to Continue server logs ([5d97349](https://github.com/continuedev/continue/commit/5d973490687c40922f2b7a2ddf2a3e19c207eb0f))\n- :loud_sound: light telemetry or context providers ([2959042](https://github.com/continuedev/continue/commit/2959042fa5a940aa4e8851b9d4db91f0f86092ff))\n- :loud_sound: telemetry for vscode vs. jetbrains ([257cef6](https://github.com/continuedev/continue/commit/257cef697c93d2f2f59936587834908bd69ae842))\n- :memo: embeddings experimental walkthrough ([e1ce1fe](https://github.com/continuedev/continue/commit/e1ce1fefee6a3f4c17ac568ba87cb7a4bcf65795))\n- :memo: note about where session data is stored ([8ada89b](https://github.com/continuedev/continue/commit/8ada89b0f66f9e746394ee64591359537fe0c7f0))\n- :money_with_wings: free trial usage indicator ([354a3f4](https://github.com/continuedev/continue/commit/354a3f493074b1fb63ff4f206a94c35f05673e99))\n- :mute: complete removal of telemetry when allow_anonymous_telemetry false ([ae7dffa](https://github.com/continuedev/continue/commit/ae7dffa211af209aea2ca13b37729e390047dd7c))\n- :necktie: allow timeout param for OpenAI LLM class ([404f7f8](https://github.com/continuedev/continue/commit/404f7f8089190d04c05957dc653baff44f342dc7))\n- :recycle: load preset_urls at load_index ([3dabc4b](https://github.com/continuedev/continue/commit/3dabc4bd6c72e2d12afb059040ca75f606e47d9d))\n- :rocket: headers param on LLM class ([e16b8ff](https://github.com/continuedev/continue/commit/e16b8ff5f2d9187f2b207addc1cd70d0cacbf9c8))\n- :sparkles: [@terminal](https://github.com/terminal) context provider ([40cfabb](https://github.com/continuedev/continue/commit/40cfabb8ce8afe20e51ca4bafddc6a0b4755bf2c))\n- :sparkles: /cmd slash command ([011877c](https://github.com/continuedev/continue/commit/011877c09e88ffcc715defc33e5c74c71ccc8aea))\n- :sparkles: /share slash command ([4e38043](https://github.com/continuedev/continue/commit/4e3804351b76cc763d904f572ad525b651d8bc00))\n- :sparkles: add edit templates to model packages ([1de976a](https://github.com/continuedev/continue/commit/1de976a6a11a0b945d59800b5a58f354808a49fc))\n- :sparkles: add max_tokens option to LLM class ([ff2a397](https://github.com/continuedev/continue/commit/ff2a3978a1e2c95a4e288b56411bf0c32b86757b))\n- :sparkles: add searchcontextprovider to default_config.py ([64f41fc](https://github.com/continuedev/continue/commit/64f41fc8a0a2616fe7074d0a49e7642fd462c95d))\n- :sparkles: add stop_tokens option to LLM ([a16ba7a](https://github.com/continuedev/continue/commit/a16ba7a0166dbf9062ee4616e3ccfbff377e9f4b))\n- :sparkles: Add support for Claude Sonnet 4 ([b24a76f](https://github.com/continuedev/continue/commit/b24a76fdabb5eccada838d8fe5ed5834c0120df1))\n- :sparkles: add urlcontextprovider back to default config ([570891e](https://github.com/continuedev/continue/commit/570891e0201769defeabec95a58c997f6d5f3889))\n- :sparkles: allow AzureOpenAI Service through GGML ([1343d12](https://github.com/continuedev/continue/commit/1343d1227cc86c860fb12695e64eaeae1384f72a))\n- :sparkles: allow changing the summary prompt ([5c8b28b](https://github.com/continuedev/continue/commit/5c8b28b7fddf5b214de61102c768ef44d4087870))\n- :sparkles: allow custom OpenAI base_url ([cb0c815](https://github.com/continuedev/continue/commit/cb0c815ad799050ecc0abdf3d15981e9832b9829))\n- :sparkles: alt+cmd+d to automatically debug terminal! ([d0483ba](https://github.com/continuedev/continue/commit/d0483ba15b4ad13399a3385ae351cf33cca3db7f))\n- :sparkles: auto-reload for config.py ([e652e90](https://github.com/continuedev/continue/commit/e652e90806b84eb409c496dd0903a4817243edc2))\n- :sparkles: autoscrolling ([e6d369f](https://github.com/continuedev/continue/commit/e6d369f4312f0c8d175251e149c62d08608cb18c))\n- :sparkles: back button on history page ([aafa5d5](https://github.com/continuedev/continue/commit/aafa5d5ec91a533f70d644e4d3fadd6f388c3e4b))\n- :sparkles: change proxy url for openai class ([32a9a47](https://github.com/continuedev/continue/commit/32a9a477d33acd5cdde08164ebeb355b27a656b5))\n- :sparkles: codelens in config.py ([58cd4db](https://github.com/continuedev/continue/commit/58cd4db2534aba9fed98925e68dc342efbc54fb0))\n- :sparkles: Continue Quick Fix ([9af39a6](https://github.com/continuedev/continue/commit/9af39a67829a6770b93ffdaa6ea70af3125c7daf))\n- :sparkles: Continue Quick Fix ([52cd93a](https://github.com/continuedev/continue/commit/52cd93ad73f7df6a5140b7d629e4f6e473dc0380))\n- :sparkles: delete context groups ([2d3d96e](https://github.com/continuedev/continue/commit/2d3d96e5b55a225eb97251850909eb7a0a7242ed))\n- :sparkles: diff context provider ([99db0da](https://github.com/continuedev/continue/commit/99db0da9d68c64d0b5adcab21e07c2db438c2404))\n- :sparkles: display model params for previous prompts ([b1b30a4](https://github.com/continuedev/continue/commit/b1b30a459cbd589a471e1528ebfa9aaeb0514968))\n- :sparkles: edit previous inputs ([c6a3d8a](https://github.com/continuedev/continue/commit/c6a3d8add014ddfe08c62b3ccb1b01dbc47495f5))\n- :sparkles: EmbeddingContextProvider ([c6a1255](https://github.com/continuedev/continue/commit/c6a12550ffca1ffe35630e7aa9af6913ddbe0675))\n- :sparkles: FileTreeContextProvider ([8bd76be](https://github.com/continuedev/continue/commit/8bd76be6c0925e0d5e5f6d239e9c6907df3cfd23))\n- :sparkles: filter history by workspace ([0757bd2](https://github.com/continuedev/continue/commit/0757bd2b556996b9c434ac43e3e4a3b042ef5802))\n- :sparkles: Give the terminal color and ansi escape rendering ([e83290b](https://github.com/continuedev/continue/commit/e83290bd3a921929052b0c0c91751800e7e9fd2c))\n- :sparkles: highlight code on cmd+shift+L ([a1fdf16](https://github.com/continuedev/continue/commit/a1fdf164b776c5ff4ddfa1c4cad66e41de4254c0))\n- :sparkles: huggingface inference api llm update ([bbf7973](https://github.com/continuedev/continue/commit/bbf7973ec091823c4197d59daaf151b748ee52fc))\n- :sparkles: huggingface tgi LLM class ([a0e2e2d](https://github.com/continuedev/continue/commit/a0e2e2d3d606d8bf465eac541a84aa57316ee271))\n- :sparkles: improved edit prompts for OS models ([1785e92](https://github.com/continuedev/continue/commit/1785e92f118b855f4a655d9b617d54b5c857a6ef))\n- :sparkles: improved model dropdown ([2f792f4](https://github.com/continuedev/continue/commit/2f792f46026a6bb3c3580f2521b01ecb8c68117c))\n- :sparkles: improvement to @ search rankings ([5590f63](https://github.com/continuedev/continue/commit/5590f63e42fda38d780bdc390361a65b65576498))\n- :sparkles: llama-2 support ([72d18fb](https://github.com/continuedev/continue/commit/72d18fb8aaac9d192a508cd54fdb296321972379))\n- :sparkles: LlamaCpp LLM subclass ([fc9e8e4](https://github.com/continuedev/continue/commit/fc9e8e4e325782409258dd483e36abf441051ee6))\n- :sparkles: LSP connection over websockets ([a6d21f9](https://github.com/continuedev/continue/commit/a6d21f979fce6135fd76923478f76000b1b343cf))\n- :sparkles: make follow-up edits ([73ae5d3](https://github.com/continuedev/continue/commit/73ae5d306c16d7c372e831d3ca41067a62c8481f))\n- :sparkles: Make the terminal command aware of its OS, platform and shell ([1a786f6](https://github.com/continuedev/continue/commit/1a786f605da47abdfc66b02f4e88044ca95df960))\n- :sparkles: manually running server option ([29940ea](https://github.com/continuedev/continue/commit/29940ea4223194cc32f6324534ad75db9e39305a))\n- :sparkles: more model options, ollama error handling ([e2a7d4a](https://github.com/continuedev/continue/commit/e2a7d4a3c7832f8788feccf4168c13ec87a31fb2))\n- :sparkles: ollama LLM class ([402883e](https://github.com/continuedev/continue/commit/402883e0661c24c418fb5aa93832c6f62dc97a63))\n- :sparkles: refactor via search Step ([9cd249e](https://github.com/continuedev/continue/commit/9cd249ee007911037639281c6d7590889ad7b467))\n- :sparkles: run continue immediately from pypi pkg ([70f6da9](https://github.com/continuedev/continue/commit/70f6da9b1ad190a967974fb477db669cbb5c928f))\n- :sparkles: saved context groups ([c98f860](https://github.com/continuedev/continue/commit/c98f860460767fe14f8fbf139150b1bd1ee2ff12))\n- :sparkles: Search context provider with ripgrep ([4b9dd4c](https://github.com/continuedev/continue/commit/4b9dd4cf8e853e17d92fb76fc726260d79e4bd7a))\n- :sparkles: select custom model to use with edit step ([c1e5039](https://github.com/continuedev/continue/commit/c1e5039731941eb6b6eea166edd433cd49d4e858))\n- :sparkles: select model from dropdown ([044b7ca](https://github.com/continuedev/continue/commit/044b7caa6b26a5d78ae52faa0ae675abc8c4e161))\n- :sparkles: select model param count from UI ([105afec](https://github.com/continuedev/continue/commit/105afeccca903072bc48772bdaf8f100f996c4a7))\n- :sparkles: set session timeout on GGML requests ([d04eec7](https://github.com/continuedev/continue/commit/d04eec7ee97319a6bcc48d289cd6eb3e0d9b8e19))\n- :sparkles: set static urls for contextprovider ([d2b1aed](https://github.com/continuedev/continue/commit/d2b1aedcedf950d792baee202efdab199b05e57e))\n- :sparkles: support browser-based IDEs with createMemoryRouter ([c9d96be](https://github.com/continuedev/continue/commit/c9d96be5615b9d193a1eeff9ab00da7ca0fe0b6b))\n- :sparkles: support for Together.ai models ([8456b24](https://github.com/continuedev/continue/commit/8456b24318b13ea5d5dabec2328dd854f8a492b4))\n- :sparkles: support stablecoder with replicate LLM ([d5e8688](https://github.com/continuedev/continue/commit/d5e86883f05fe3e99e1d6ff64241a48f935cc927))\n- :sparkles: testing improved prompting for stablecode ([6112f26](https://github.com/continuedev/continue/commit/6112f26888086ccd47ca6bcfefdbc5b82ea86879))\n- :sparkles: testing in ci, final test of ([cbd7656](https://github.com/continuedev/continue/commit/cbd7656bb4c9aebfe98c746111af52cf7192aa1b))\n- :sparkles: text-gen-webui, cleaning config and welcome ([36d517e](https://github.com/continuedev/continue/commit/36d517e37d87b1bd39d6027577714b60c32e81e9))\n- :sparkles: verify_ssl option for all LLMs ([e0522b9](https://github.com/continuedev/continue/commit/e0522b92cfa80491718de07928ce6a31850dab70))\n- :technologist: bit of customization for DefaultPolicy ([3966790](https://github.com/continuedev/continue/commit/396679009fef21e13c1a6095212d1bd68e7f2a86))\n- :technologist: button to view logs when loading ([b83eb52](https://github.com/continuedev/continue/commit/b83eb52c98d637ab3e3becf98aed9899821ea00d))\n- :technologist: github workflow to measure issue/PR stats ([f75c423](https://github.com/continuedev/continue/commit/f75c42332c44c0d2a1a7e7a7ea32c2ef346df609))\n- :white_check_mark: update test and add model telemetry ([3541d6a](https://github.com/continuedev/continue/commit/3541d6a770c919e1f2e55a1ae53c4fc3abe31aa7))\n- :wrench: update default config.py imports ([e2d0baf](https://github.com/continuedev/continue/commit/e2d0baf39348597bdd1015897152f4e3bee0744d))\n- :zap: queue messages before load, then send ([4c8b561](https://github.com/continuedev/continue/commit/4c8b56135b0a1862a4f1984e80aa1409f15e177d))\n- :zap: reduce default max_tokens ([8fff1a8](https://github.com/continuedev/continue/commit/8fff1a811c477874482b65d014e4c5565d4a8649))\n- `create_rule_block` tool ([2c8032c](https://github.com/continuedev/continue/commit/2c8032c3397f6ac8c26b8ce5e5b2fe89d079d3da))\n- `description` in markdown yaml ([e4b70db](https://github.com/continuedev/continue/commit/e4b70dbfd843345ae0689a55802462473f5641dc))\n- `globs` on rules and block docs ([24e22db](https://github.com/continuedev/continue/commit/24e22db9d416be54426709948564aee4aa08d54e))\n- `requestRule` tool ([af30fbe](https://github.com/continuedev/continue/commit/af30fbe360def278cd56858d88c5373c80a2dab6))\n- add \"Gathering context...\" indicator ([dd865ea](https://github.com/continuedev/continue/commit/dd865eadea014ccc8588a49e6d9c338d89734c9d))\n- add \"onboarding\" slash command ([#1961](https://github.com/continuedev/continue/issues/1961)) ([5819ffb](https://github.com/continuedev/continue/commit/5819ffb43901eec8ad6af85737c28896f20c4e6e))\n- add `globs` to create rule tool ([047e4e4](https://github.com/continuedev/continue/commit/047e4e418b7f252cb303f19992025acc4fb79d4f))\n- add `index.ts` to sdk ([ae31c54](https://github.com/continuedev/continue/commit/ae31c54011ac365e560cb02b9851a6806641503a))\n- add `no-negated-condition` eslint ([eb7d67d](https://github.com/continuedev/continue/commit/eb7d67dcf1a338c473a1bb7f787e4f35e62f8f73))\n- add `stream` in defaultCompletionOptions yaml ([f72e293](https://github.com/continuedev/continue/commit/f72e293e2f6251e1cca632dffa7ec14554016a73))\n- add `tsc:watch` cmd to vs code ([1d5e164](https://github.com/continuedev/continue/commit/1d5e164325e8fe8e66759baf2ac47ce766355faf))\n- add accept/reject all btns ([011e2c2](https://github.com/continuedev/continue/commit/011e2c264407ff72a0bc90980ec7d8250da4bbd6))\n- add agentinteraction dev data ([971c4de](https://github.com/continuedev/continue/commit/971c4de33e1a3f117eafe9d64bf175c0d2a849ef))\n- add animated ellipsis to lump ([e14163c](https://github.com/continuedev/continue/commit/e14163cda2ff1de2e90621f1de2341fd90b136d2))\n- add API key support for TEI reranker ([5dc25b5](https://github.com/continuedev/continue/commit/5dc25b5eb3b1f914d106a18bbb34f7d28e3039f3))\n- add azure provider config ([#1764](https://github.com/continuedev/continue/issues/1764)) ([c9635de](https://github.com/continuedev/continue/commit/c9635def237e0bb4e1d899057e6b651b6a6cd1b2))\n- add best experience onboarding ([8b30504](https://github.com/continuedev/continue/commit/8b305046eff820999e805c8b2cd6400e3572da1b))\n- add chat scrollbar visibility configuration support ([14eaf32](https://github.com/continuedev/continue/commit/14eaf3272cde744ab6390d7113c2cb507e0d6734))\n- add Claude 3.7 support to toolSupport.ts ([50ad91b](https://github.com/continuedev/continue/commit/50ad91bced864272c7f1bb92c13d7c910a0b6be2))\n- Add cloudflare as provider ([ad8743a](https://github.com/continuedev/continue/commit/ad8743a9b563ae6e09cc140a0b3f9f202715e3b5))\n- add code that comment is based on ([e3653f9](https://github.com/continuedev/continue/commit/e3653f98f08e52fa6a55d41ca147add1c8e5515a))\n- add combobox for edit file selection ([7c6295f](https://github.com/continuedev/continue/commit/7c6295feeb32a478a2d8867f0a2b863583f6a97e))\n- add deepseek models for novita ai ([a31c3eb](https://github.com/continuedev/continue/commit/a31c3ebc8334f931281cdb0a348150347c99d0e6))\n- Add docs/getDetails endpoint in JetBrains ([da2dc1e](https://github.com/continuedev/continue/commit/da2dc1ed7de3caa9ae1e8b02a09526f57dd80bb8))\n- add example integration test ([781a792](https://github.com/continuedev/continue/commit/781a792a5939958d05fa316261190c164871c563))\n- add exponential backoff for API requests ([12ffa95](https://github.com/continuedev/continue/commit/12ffa956571acd12b383ba73560d3c42a7a4415c))\n- add file search to quick edit ([#1714](https://github.com/continuedev/continue/issues/1714)) ([21d1b0c](https://github.com/continuedev/continue/commit/21d1b0c16dd9cd454d543e4b387f873e54d89aa5))\n- add fixed version of the nodejs ([cd35857](https://github.com/continuedev/continue/commit/cd35857daf5577f4677f6675344fa8488e624352))\n- add free trial card to onboarding ([#1600](https://github.com/continuedev/continue/issues/1600)) ([9bae5a2](https://github.com/continuedev/continue/commit/9bae5a254df25d25dc848b5c7bf69fc7189e6461))\n- add gif to tutorial card ([e03eb9d](https://github.com/continuedev/continue/commit/e03eb9d5801985d1968d0528dd76767bf3907cbf))\n- add gitlab context class ([efc1f3b](https://github.com/continuedev/continue/commit/efc1f3b9216d5bf503ecb58ff956f26a2bc4a130))\n- add icon for URL ctx item peek ([e901cfd](https://github.com/continuedev/continue/commit/e901cfdf4ae383a2337c2e20182f6eee6d3010f9))\n- add Jira context provider ([#860](https://github.com/continuedev/continue/issues/860)) ([8ba15b1](https://github.com/continuedev/continue/commit/8ba15b16665be871e037ada88d51b3403a8d094e))\n- add Llama 3.1 8B to cloudflare provider options ([#1811](https://github.com/continuedev/continue/issues/1811)) ([bccbff2](https://github.com/continuedev/continue/commit/bccbff273c176c54f0209c5927a19b8c6d9375f9))\n- add Moonshot AI model provider support ([b8a278d](https://github.com/continuedev/continue/commit/b8a278d216db9851aa2b0772e0313ff4bc12c7cc))\n- add novita info ([94d8d39](https://github.com/continuedev/continue/commit/94d8d39cfc8d21f2544633330370d99ef17e0fdf))\n- Add num_treads to ollama along with docs ([#863](https://github.com/continuedev/continue/issues/863)) ([d71721d](https://github.com/continuedev/continue/commit/d71721d71540cb084d67559e850440129faa67c0))\n- add openai wrapper for sdk ([ab9a248](https://github.com/continuedev/continue/commit/ab9a248572a46e00f0fa6441af861dd260c4cb65))\n- add OpenAI, xAI, Replicate and free-trial model options to config schema ([89a90f9](https://github.com/continuedev/continue/commit/89a90f99baaf493f184a09aa6083a606076f9771))\n- Add prompt caching support ([24f9960](https://github.com/continuedev/continue/commit/24f9960e27ad065fec611661e69726c9be589042))\n- add proxy support for ripgrep download ([d42c050](https://github.com/continuedev/continue/commit/d42c050e3645b4eb4635dd5a73ae12107adca701))\n- add Quick Actions CodeLens feature ([#1674](https://github.com/continuedev/continue/issues/1674)) ([fdf3408](https://github.com/continuedev/continue/commit/fdf3408e0c9c2df749b5775d3c906abfdf40e799)), closes [#1536](https://github.com/continuedev/continue/issues/1536) [#1456](https://github.com/continuedev/continue/issues/1456) [#1564](https://github.com/continuedev/continue/issues/1564) [#1576](https://github.com/continuedev/continue/issues/1576) [#1570](https://github.com/continuedev/continue/issues/1570) [#1582](https://github.com/continuedev/continue/issues/1582) [#1600](https://github.com/continuedev/continue/issues/1600) [#1618](https://github.com/continuedev/continue/issues/1618) [#1626](https://github.com/continuedev/continue/issues/1626) [#1637](https://github.com/continuedev/continue/issues/1637)\n- add Qwen2.5-Coder support ([387a76a](https://github.com/continuedev/continue/commit/387a76aa5bab873565de25a2d269f0b5b1a53f1e))\n- add redux state for card logic ([5f32924](https://github.com/continuedev/continue/commit/5f32924becd3e8b03d19ffb4a5e4a2dcbfb6dbc3))\n- add redux state for mfe ([6e5236a](https://github.com/continuedev/continue/commit/6e5236a58a896ab78faa1672e56237b4f4684877))\n- add render util function ([413530a](https://github.com/continuedev/continue/commit/413530a029e32ab8e77520c0e5265cd37a84b24c))\n- add rich quick pick for quick edit ([#1706](https://github.com/continuedev/continue/issues/1706)) ([f3b15eb](https://github.com/continuedev/continue/commit/f3b15eb1b14dcc3f4c28ebfb95150b0d6627cecb))\n- add scope selector ([b78fdc4](https://github.com/continuedev/continue/commit/b78fdc4147e1ccb388b900a65099a2c61ba6ca26))\n- Add signature column to code_snippets table ([a535e9e](https://github.com/continuedev/continue/commit/a535e9e15fb3041fec732574636221e0a83dccfa))\n- add slash command cmd ([d4e1609](https://github.com/continuedev/continue/commit/d4e1609d2a65aeeece6caa8620a1b45196f40c98))\n- add support for `baseSystemPrompt` ([f6957bb](https://github.com/continuedev/continue/commit/f6957bb856ba5915ad6351a518dc4b4a47536947))\n- Add support for Cloudflare AI Gateway ([#1425](https://github.com/continuedev/continue/issues/1425)) ([837ad1c](https://github.com/continuedev/continue/commit/837ad1c552aef8909b9a1dbe01204ed2571134d7))\n- add support for custom headers in SSE transport ([d392d77](https://github.com/continuedev/continue/commit/d392d7736d5d31804cd54b3c307bbfb6289e5358))\n- add support for Mistral models in toolSupport.ts ([928cf2c](https://github.com/continuedev/continue/commit/928cf2c73752a23fe3fc026bbf3957c5c80d5604))\n- add support for multiple MCP server types (stdio, sse, websocket) in the YAML configuration schema, ensuring backward compatibility with legacy configurations ([63a2ffe](https://github.com/continuedev/continue/commit/63a2ffe9696d90786f030c2af02cec2c463d914e))\n- add tests ([0819bec](https://github.com/continuedev/continue/commit/0819becfd98127a187cc84484dd59bb4348a1ec3))\n- add tutorial card ([#1716](https://github.com/continuedev/continue/issues/1716)) ([cb8b207](https://github.com/continuedev/continue/commit/cb8b207582dec6ce997c2c602c9998a5a64504db))\n- add unified diff instant apply ([3c36b4d](https://github.com/continuedev/continue/commit/3c36b4dc378ddd8c5c5c2cc234e9a605d776d928))\n- add Vertex AI support ([dfaa02c](https://github.com/continuedev/continue/commit/dfaa02c5051c44303e055675324ff6eaaac0e91b))\n- allow input to exit ([18c5019](https://github.com/continuedev/continue/commit/18c50195248f6ca6c64e1eeb3cb8f10e6b85ffb5))\n- allow JetBrains users to index docs ([#1797](https://github.com/continuedev/continue/issues/1797)) ([e0079a4](https://github.com/continuedev/continue/commit/e0079a43721f6002ff59e2eaa79dc8768a6f66f5))\n- allow users to skip local onboaridng ([4685db0](https://github.com/continuedev/continue/commit/4685db02b854ab0a598defe0fdc4a68f8de50cc2))\n- apply e2e tests ([12b3ed9](https://github.com/continuedev/continue/commit/12b3ed9291b4f46e920f8417f1e6585e45b7edd9))\n- apply if state is active, onEnter callback dont working ([fed1651](https://github.com/continuedev/continue/commit/fed165161d1610db282b52230a4d914f4d2ffa62))\n- apply waiting cursor to chat-input enter-button during prompting ([6d08361](https://github.com/continuedev/continue/commit/6d0836189cd1c18cf97b1e604adf4e5f201665b9))\n- assistant select ([3b539d5](https://github.com/continuedev/continue/commit/3b539d53c39a98c361e27db6dfb72a8e14ddae50))\n- assistant select refresh on right ([04f5e3e](https://github.com/continuedev/continue/commit/04f5e3e165fddf3101e6210b1111c0f261793054))\n- **autocomplete:** recently visited ranged service ([811507a](https://github.com/continuedev/continue/commit/811507a406ed543adb4a8f876c3c3f30d8a9be35))\n- better buttons for account dialog ([2a4c69f](https://github.com/continuedev/continue/commit/2a4c69ff87b64e21ad04edfed2aad7b048833b53))\n- better editor content handling in edit ([7c72c4a](https://github.com/continuedev/continue/commit/7c72c4af878e4b683b582e40f483551314e92873))\n- better error handling around ripgrep ([22f6fdf](https://github.com/continuedev/continue/commit/22f6fdfa1be6f84ab8548155465f863cb65a7e30))\n- better type names ([43ebb1a](https://github.com/continuedev/continue/commit/43ebb1ad0a95ac1dfc0ff1e198fe53e8c50f0671))\n- bookmark first 5 prompts by default ([0909ef9](https://github.com/continuedev/continue/commit/0909ef938b6d2928fa650c0e0613c1e106a0315d))\n- brand text fix ([a25afee](https://github.com/continuedev/continue/commit/a25afeeda10c86072f80c025c20c21b167ddaaf0))\n- bugfixes on redux schema updates ([4efd661](https://github.com/continuedev/continue/commit/4efd66137592916167f562ac45533dfffacd35cc))\n- build steps for config-yaml ([73b95f3](https://github.com/continuedev/continue/commit/73b95f314703c266d9e348f9c1ba3dbbb3f3d266))\n- cache org selection results ([13d5668](https://github.com/continuedev/continue/commit/13d566831db9e66ae270ff70e2b5c63304df6458))\n- cache org selection results ([6e81720](https://github.com/continuedev/continue/commit/6e81720186d4215d6a650835f72ed1c6cc20e28f))\n- capture base class and interfaces implemented ([df2bbc8](https://github.com/continuedev/continue/commit/df2bbc89da7f7f9ee406a8fe6d0c4fd244abd488))\n- capture python definitions ([2f84ef0](https://github.com/continuedev/continue/commit/2f84ef0ab9a87aa6300415d30db02e3460a07195))\n- change model desc ([8d3ecaf](https://github.com/continuedev/continue/commit/8d3ecafa3dbd9a3373ec7784eda1feac6b77b684))\n- check for `nvm` version in install script ([b1a93b7](https://github.com/continuedev/continue/commit/b1a93b7a2dd22ab82ef9acdec5f1096c33c44c1b))\n- cleanup apply manager ([4b9bac5](https://github.com/continuedev/continue/commit/4b9bac55ffae743d0221df65f0a14d0981f74aeb))\n- cleanup inline edit code ([90d895e](https://github.com/continuedev/continue/commit/90d895e2e22ee40033c524a109b5777b5c3bf6cd))\n- Client Certificate Options Support ([#1658](https://github.com/continuedev/continue/issues/1658)) ([136bf9e](https://github.com/continuedev/continue/commit/136bf9e0f0ce0193f64fe291922431808c32406e))\n- close files ([d1eca9d](https://github.com/continuedev/continue/commit/d1eca9deeb3c1550982f679a58265593af603453))\n- close tutorial listener ([d4d3102](https://github.com/continuedev/continue/commit/d4d3102b61c800576513cbb07c9d482fe973a0ef))\n- collapsed codeblock by default ([979c247](https://github.com/continuedev/continue/commit/979c2473c61747157e65e1dcd33fd73ccfe404ad))\n- config.json validation ([429e54e](https://github.com/continuedev/continue/commit/429e54ed347446747d206a97cb80c91baf5d407e))\n- configure docs through config.json ([#1864](https://github.com/continuedev/continue/issues/1864)) ([d7dbdff](https://github.com/continuedev/continue/commit/d7dbdfff485f3970b8595c5a2680a012443747a5))\n- consolidate StepContainer ([58d9eb7](https://github.com/continuedev/continue/commit/58d9eb7a7e8102940d59f26a65e01edb130681fd))\n- consolidate toast logic into \"showToast\" ([ba777ed](https://github.com/continuedev/continue/commit/ba777edf4761f8bb781fce11ce45311c0b089048))\n- continue sdk ([c7829bb](https://github.com/continuedev/continue/commit/c7829bbd514b962caad99a4c8c660e94f9e577b4))\n- convert issue templates to issue form templates ([#507](https://github.com/continuedev/continue/issues/507)) ([fa7f2cb](https://github.com/continuedev/continue/commit/fa7f2cbdeb6013ec4bb081cb85988817f54d070c))\n- **core:** add support for Llama models on Bedrock ([#1499](https://github.com/continuedev/continue/issues/1499)) ([53aab1e](https://github.com/continuedev/continue/commit/53aab1e6fd2a0f21fbca1c29b82d1f96d2f5e074))\n- crawl `.mdx` docs ([d9f0c4f](https://github.com/continuedev/continue/commit/d9f0c4f131010bd4dae111b5e4290f3460d49e3a))\n- create \"rebuild index\" dialog ([c978d7c](https://github.com/continuedev/continue/commit/c978d7c871be71cd399d02159be8c1b37a0d36ff))\n- create `ApplyManager` ([652f8c4](https://github.com/continuedev/continue/commit/652f8c4c7b0879f9ab3b2cc1a25c9530de467c94))\n- create docs cache ([3f8afbe](https://github.com/continuedev/continue/commit/3f8afbefeb36c389528d420ef5945c0014ef18c0))\n- create file button in toolbar ([268f55b](https://github.com/continuedev/continue/commit/268f55b001e880fec92ef87df6be2bdf8fc25649))\n- create markdown rules in notch ([c2571e2](https://github.com/continuedev/continue/commit/c2571e2a86e373199fe9660f95fa5b168c776035))\n- create org slice ([71f8508](https://github.com/continuedev/continue/commit/71f85084f22333337a1e817e0502044771d66de4))\n- create profiles slice ([9c2db6b](https://github.com/continuedev/continue/commit/9c2db6b9c7d7fd0a0164b3e2288a4135df759707))\n- create RecentFilesService ([d597e6f](https://github.com/continuedev/continue/commit/d597e6fe5ea632f2bdf762ad2555c056fd736ba8))\n- create stream complete reducer ([ce7d982](https://github.com/continuedev/continue/commit/ce7d98291e30658b387dd5d7de96e0f3577065ab))\n- create SVG tooltip ([0cf152e](https://github.com/continuedev/continue/commit/0cf152ede652d975fd774faa40494020c7514196))\n- dallin feedback pt 2 ([d158fab](https://github.com/continuedev/continue/commit/d158fab740cf803074106717560941cca8cb96c9))\n- dallin's feedback ([e884853](https://github.com/continuedev/continue/commit/e88485348c8b53e9f19d1f9fc0c8b33dd5266df7))\n- Dallin's feedback ([fc1ee33](https://github.com/continuedev/continue/commit/fc1ee33ed912543f596d32ae6eaacbbdbc3bcae4))\n- dallins feedback ([33b1d39](https://github.com/continuedev/continue/commit/33b1d3994654f311dcae385f6ef2ef78696d1d89))\n- dallins feedback ([25b5b8c](https://github.com/continuedev/continue/commit/25b5b8c50b05fe872d74b60ced4a6d8569686ed6))\n- dallins feedback ([6070a64](https://github.com/continuedev/continue/commit/6070a645dd9741b433ad43fd09f37b42ec614a3e))\n- disable chat-input enter-button during prompting ([e01fbe7](https://github.com/continuedev/continue/commit/e01fbe7f2cf6f53f5a611afe6df53dfc5f710e55))\n- disable chat-input-editor during prompting ([7368767](https://github.com/continuedev/continue/commit/73687670da1cc7792d2c4d9ccd7b9813746a4658))\n- display rules used ([53f0679](https://github.com/continuedev/continue/commit/53f06795d3910e1ec5aecdbc1fde706f68649614))\n- editor change listener ([28d2c7d](https://github.com/continuedev/continue/commit/28d2c7d7e0a00c23c33ad25b8bfc3651ddafa2c5))\n- **embeddings:** add gemini provider ([#1362](https://github.com/continuedev/continue/issues/1362)) ([5224572](https://github.com/continuedev/continue/commit/52245724089329c792eb79c8d512cea2f617c4a1))\n- enable comment filtering ([471ca9e](https://github.com/continuedev/continue/commit/471ca9eae5e1f82e04b2d6d600631bf24876ec06))\n- enable sourcemap ([2be2b73](https://github.com/continuedev/continue/commit/2be2b73f2a28834cbfef9100f479ecffc0ad13c8))\n- enable WAL (Write-Ahead Logging) for improved performance and stability ([#1885](https://github.com/continuedev/continue/issues/1885)) ([e93ce84](https://github.com/continuedev/continue/commit/e93ce84cd57eacfe820c1f22668ed069020ee46e))\n- enhance help center ([#1755](https://github.com/continuedev/continue/issues/1755)) ([f2a04ef](https://github.com/continuedev/continue/commit/f2a04ef3e9e49876042077f36da5f457630dcaf1))\n- Enhance MCP connection refresh logic ([1a0d411](https://github.com/continuedev/continue/commit/1a0d41128f30c7f5c087c7255d6ec3292c288664))\n- Ensure CancelAutocompleteAction updates on EDT ([277c20f](https://github.com/continuedev/continue/commit/277c20f3b08eaa1b0b293b06bbfce40404e5477d))\n- evaluate rule ([1a1b854](https://github.com/continuedev/continue/commit/1a1b854fda4630e136d0d18b4ea69e71a00ced1e))\n- explore btn ([14c9528](https://github.com/continuedev/continue/commit/14c952887399f12e739d6a0fafbdbb4d1355fe90))\n- explore dialog ui ([c6cfa93](https://github.com/continuedev/continue/commit/c6cfa9334690cf7f9ecdf99959537ce522728079))\n- explore dialog watcher ([741ad35](https://github.com/continuedev/continue/commit/741ad35195d6b841b459076b8dbca1057d5f6358))\n- explore dialog watcher ([192a47f](https://github.com/continuedev/continue/commit/192a47f998d639f307933f33a731832ff35305ac))\n- explore hub card ([5a777ce](https://github.com/continuedev/continue/commit/5a777ce464243b09d466d7a11a39ff9e45a74ec5))\n- extract paths ([2ffb4ae](https://github.com/continuedev/continue/commit/2ffb4aefeb01073c924beb1c203e60c586cf96bb))\n- fall back to Cheerio for headless crawling ([fd7bbce](https://github.com/continuedev/continue/commit/fd7bbce79b831a1068c5367bc4f018fe820a917a))\n- fallback to chat model from apply model ([46e8dd7](https://github.com/continuedev/continue/commit/46e8dd7c38858173f375ac24f6857ed3bea2b190))\n- fallback to chat model in IDE ([b601c22](https://github.com/continuedev/continue/commit/b601c22d01478d74a66bf162f3e12c4c4e1a93d7))\n- filttrex ([b20b1ab](https://github.com/continuedev/continue/commit/b20b1ab5ee44d704619327a3b087b1290c634dd7))\n- fix address ([d59b3b5](https://github.com/continuedev/continue/commit/d59b3b5297924b2b6343a990c09551be4af9c670))\n- fix broken links ([66345e8](https://github.com/continuedev/continue/commit/66345e83550a898cd36eae67a68fb020ce957677))\n- fix ctx providers w/ slash cmds ([e5608a9](https://github.com/continuedev/continue/commit/e5608a94e543cd8b66d68c222eacc82e8c021aff))\n- fix filepaht issues w/ apply ([6c848ba](https://github.com/continuedev/continue/commit/6c848ba7a7fc7ea32a963653b83b5099e5ed6fad))\n- fix models ([d401a5c](https://github.com/continuedev/continue/commit/d401a5c11fdf5b3267269ff3603f83318a9dd9ad))\n- fix type error + formatting ([bb5e8b6](https://github.com/continuedev/continue/commit/bb5e8b641dc535fd8d60de92aba04680f7b93824))\n- fix version and address ([ad0fdb9](https://github.com/continuedev/continue/commit/ad0fdb988b76bebadcbb26a3069b7f821207680f))\n- get paths ([9cb5170](https://github.com/continuedev/continue/commit/9cb5170b1a23552f917265dc330ff1e05838a3de))\n- get return_type and parameters from snippets ([bf426d5](https://github.com/continuedev/continue/commit/bf426d5e2147248c06ab51cab31ac5cd9e834b1d))\n- glob rules ([8c5ca40](https://github.com/continuedev/continue/commit/8c5ca401d4fbda64eacf23cc0585e2c9830db345))\n- go definitions ([bf65b1e](https://github.com/continuedev/continue/commit/bf65b1ebbdd3b0913d141e9f60a11414088bc78f))\n- **gui:** add ability to change the session title ([1b32222](https://github.com/continuedev/continue/commit/1b32222c10c6ae6f1fd6c93f6abd8cbeab16f108))\n- **gui:** Add Azure as a provider ([b7c0623](https://github.com/continuedev/continue/commit/b7c0623ac6c5067ccefd7c8486eeef2ff56d9667))\n- **gui:** add flex-grow to TdDiv in history page ([#619](https://github.com/continuedev/continue/issues/619)) ([7106f42](https://github.com/continuedev/continue/commit/7106f42cb4ba7424b3dda8ed2d0e562e2fd99447))\n- **gui:** improve editor highlights ([7bffa2d](https://github.com/continuedev/continue/commit/7bffa2dbeab4dde415c113bb15b71968bc51f448))\n- **gui:** more config for azure provider ([e11206b](https://github.com/continuedev/continue/commit/e11206b0fa70075d21d1cc8dd4426af856ee05c2))\n- handle deletions ([2c8cee9](https://github.com/continuedev/continue/commit/2c8cee9fd42fc8f18c9e6e37781e27276dae2786))\n- hide empty code lines in markdown preview ([#815](https://github.com/continuedev/continue/issues/815)) ([793d022](https://github.com/continuedev/continue/commit/793d022eab292219cda75bb3255ed3d676585977))\n- **history:** add sticky headers to history sections ([#621](https://github.com/continuedev/continue/issues/621)) ([3d398d0](https://github.com/continuedev/continue/commit/3d398d0b3781db8bbc51ad00f47e80b3124f5c8f))\n- hover brightness on tool call div ([dba2887](https://github.com/continuedev/continue/commit/dba288748d850c7fa8047efb75e954384410a497))\n- **httpContextProvider:** load AC on fetch client ([#1150](https://github.com/continuedev/continue/issues/1150)) ([638f192](https://github.com/continuedev/continue/commit/638f1922590d7bd46d6e4d46a4d25bf1b33b26fa))\n- if rules ([6224af6](https://github.com/continuedev/continue/commit/6224af63d36e6b96b068c6d4230ee2315bd94d43))\n- if rules ([d0e922d](https://github.com/continuedev/continue/commit/d0e922dad4947ac8af4e9d0c76921da56cdc2953))\n- if-rule ([211fcb7](https://github.com/continuedev/continue/commit/211fcb77407cd261f2bb26198667388ea6270a05))\n- if-rule ([9baa4e9](https://github.com/continuedev/continue/commit/9baa4e93be7f9c2fe057e453237eaa9d1aaa8ff0))\n- if-rules ([06b76d7](https://github.com/continuedev/continue/commit/06b76d752fb4f83b14053fffe2f85cb819e39ae6))\n- if-rules ([0a13a46](https://github.com/continuedev/continue/commit/0a13a4604d7607793bea66b9cea3e89353bf3af4))\n- if-rules ([ccdc1b9](https://github.com/continuedev/continue/commit/ccdc1b953e22ca4aa26e32b9e80bc31ea30f274c))\n- ignore node_modules in js and ts code definitions ([3c9071a](https://github.com/continuedev/continue/commit/3c9071a09b114a019a593a7f241975f419cc4e43))\n- impl /multifile-edit ([f88f28b](https://github.com/continuedev/continue/commit/f88f28b8551e95e4eb346fafa68d6223f9017d5a))\n- improve chat thread ui for better readability ([#1786](https://github.com/continuedev/continue/issues/1786)) ([8478af6](https://github.com/continuedev/continue/commit/8478af63c18b0c35d59d2326ca4e8687a42a624b))\n- improve chunking desc on large projects ([3844208](https://github.com/continuedev/continue/commit/3844208a3b9b3daaff085daad8ab933f27fcb5f3))\n- improve dropshadow on jb inline edit ([35b94e7](https://github.com/continuedev/continue/commit/35b94e710260782d0311ee22a2849203b8965e38))\n- improve dropshadow on jb inline edit ([75e2d2f](https://github.com/continuedev/continue/commit/75e2d2f39df30424b3831615ee3720a2844a60a5))\n- improve fatal error message ([1d77ad3](https://github.com/continuedev/continue/commit/1d77ad31949042c7702b59f83a82ca51dcce4900))\n- improve input and tooltip ux ([#1923](https://github.com/continuedev/continue/issues/1923)) ([2c13776](https://github.com/continuedev/continue/commit/2c13776c06cfe9c775777bf2fffdcf7dfa2169ee))\n- improve model retrieval logic in InlineEditAction ([52b1704](https://github.com/continuedev/continue/commit/52b1704f3c0fe991b32959d04c660fe55e899b2c))\n- improve prompt log formatting + detail ([2376daf](https://github.com/continuedev/continue/commit/2376dafce325c87d87090e7cf995beffcb98d3b4))\n- Improve ProtocolClient initialization and GUI loading ([0c829fd](https://github.com/continuedev/continue/commit/0c829fd99d0e898c1afdc90a0957c907dcb7f4e2))\n- improve settings tabs ([a5f974b](https://github.com/continuedev/continue/commit/a5f974bf1602dabe3939ddc0ac7b5a05ac8e716b))\n- improve StreamError dialog ([ecb63a0](https://github.com/continuedev/continue/commit/ecb63a0222c7ae5e051bea02d98a4a2de7c1e2e5))\n- improve styling on code to edit ([47fbe4f](https://github.com/continuedev/continue/commit/47fbe4f9b81e1ef31d60f1110d2bf240fa6e7d40))\n- include recently + open files in codebase search ([#1833](https://github.com/continuedev/continue/issues/1833)) ([3e0fae3](https://github.com/continuedev/continue/commit/3e0fae35a75bb8dc117bc3eb008eee122e0e12ae))\n- init profiles prefs ([d6ad3e2](https://github.com/continuedev/continue/commit/d6ad3e2071ca1ee3f0b5cd4d8e93852a84fd138a))\n- insert prompt into input on click ([a73123e](https://github.com/continuedev/continue/commit/a73123e35ce29f1776b5afb456c9c67dc9495099))\n- instant apply check for diff rejectection ([98cb185](https://github.com/continuedev/continue/commit/98cb185c926d49a6f5af00f8b53e207743da815a))\n- integrate Moonshot AI model provider and update UI translations ([7ffb95e](https://github.com/continuedev/continue/commit/7ffb95e9d0fc964b55b3f92149515fa79fc3aa0d))\n- items used text ([#1973](https://github.com/continuedev/continue/issues/1973)) ([28a2042](https://github.com/continuedev/continue/commit/28a2042b7f604889cd0c7c76ee3ec0791a00e3ba))\n- **jb:** add ide logs ([22d84ae](https://github.com/continuedev/continue/commit/22d84ae726c40d65449c39b2ece4101f32608870))\n- **jb:** add plugin actions ([629641a](https://github.com/continuedev/continue/commit/629641a4adf2a1e2fe1c9a69fd431718b34c4263))\n- **jb:** create per-IDE tutorial files ([e3b5cbf](https://github.com/continuedev/continue/commit/e3b5cbfda453c727c6f81d04aa8bed1b9efe8301))\n- **jb:** fix meta key bugs in tiptap ([7682c71](https://github.com/continuedev/continue/commit/7682c71575c7ffc03d625510187d792e9d971227))\n- **jb:** impl \"apply\" button ([802cb43](https://github.com/continuedev/continue/commit/802cb43cd6b6abf8613591eb38c445e6bd4daa7d))\n- **JB:** impl `showFile` ([24bb5eb](https://github.com/continuedev/continue/commit/24bb5eb0dab8467f606d249f8be16fcb9ff083c0))\n- **JB:** scroll to top of file on full file edit ([c36a1ae](https://github.com/continuedev/continue/commit/c36a1ae62851383978080bc5674e7ba4ca0e9cc1))\n- jetbrains tutorial explore hub ([054b095](https://github.com/continuedev/continue/commit/054b095dab51b3abc7a3d83587e0b5f67b8acdf6))\n- make [@codebase](https://github.com/codebase) a hardcoded ctx provider ([#1818](https://github.com/continuedev/continue/issues/1818)) ([7b86678](https://github.com/continuedev/continue/commit/7b866787d947106b77ac0cef92b75ba36e09e7fc))\n- make `assistant` optional on sdk ([48429ae](https://github.com/continuedev/continue/commit/48429ae04f957a98c24d841316c722b91841c329))\n- make deletion line highlight wider ([10a5ec1](https://github.com/continuedev/continue/commit/10a5ec18a9df189d58c60886be296f1107a1dfc2))\n- make disabled state a tooltip ([#1653](https://github.com/continuedev/continue/issues/1653)) ([6cf0102](https://github.com/continuedev/continue/commit/6cf0102875316a89752d17d4fe08e4b21fd2e603))\n- manual ripgrep downloads for JB ([587459c](https://github.com/continuedev/continue/commit/587459c007ba130a4fd6f5ffde246dd8625486a8))\n- markdown rules ([a21d350](https://github.com/continuedev/continue/commit/a21d350ea14a98cf16904ae678de64a83775da7c))\n- merge conflicts ([82a328c](https://github.com/continuedev/continue/commit/82a328ca15306e0b902075c0661cd1b2bb8ed9a8))\n- model name update ([045c6bd](https://github.com/continuedev/continue/commit/045c6bdfec45ffadb25b3db2306b78f34780e84b))\n- modify vitest command, add test coverage feature ([96c70d0](https://github.com/continuedev/continue/commit/96c70d07d283130176cbda68afdb02ddc6b20366))\n- more cleanup ([6c294e7](https://github.com/continuedev/continue/commit/6c294e76834561848305ac8fb6bcb707ea21b172))\n- more cleanup ([bf7cf66](https://github.com/continuedev/continue/commit/bf7cf661cc4357029492f067ca57b1a43401aaaa))\n- more dynamic imports for LanceDB ([90c6631](https://github.com/continuedev/continue/commit/90c663146edc965bbe0b87b737755742315a8fa1))\n- more moving around ([b0492dd](https://github.com/continuedev/continue/commit/b0492ddfee2e6050d833d4ad07a942dbdd830c55))\n- more styling updates ([c7b3ea8](https://github.com/continuedev/continue/commit/c7b3ea890ac2bade08424e7a23301cea319aea35))\n- more visible assistant refresh + submenus ([1fd2ce9](https://github.com/continuedev/continue/commit/1fd2ce90f22c0dd6ebb9ec5b2542f85475844436))\n- move .prompts into slash cmd ([1d1f705](https://github.com/continuedev/continue/commit/1d1f705f7dfb1d517b90e03b82856e625f3f323e))\n- move `StepContainerPreToolbar` ([979bcb9](https://github.com/continuedev/continue/commit/979bcb97ec88d91d0ed0bd14fd5b6b7dc4c670ae))\n- move apply accept/reject into lump ([071ecb9](https://github.com/continuedev/continue/commit/071ecb91eb9c627e96439b7a5be260ec446500b0))\n- move apply manager instantiation ([2413d24](https://github.com/continuedev/continue/commit/2413d241c6448f65c99d9286e72a01bc8acc429a))\n- move Edit into Chat page ([731b54e](https://github.com/continuedev/continue/commit/731b54eb30bd7184e42c397db658020b4a70d5f7))\n- move error indicator into lump ([1f1d4f4](https://github.com/continuedev/continue/commit/1f1d4f4a1c5b365258db219e2a0ae8eecf3656a8))\n- move free trial out of assistant ([2c58a33](https://github.com/continuedev/continue/commit/2c58a33a6a937ea7beb1aa017bce64ec4cfe32d0))\n- move rule parsing to `config-yaml` ([f2d7290](https://github.com/continuedev/continue/commit/f2d72902cb45c165ca73483dd5c0a90ac89bcc49))\n- move SymbolLink ([e3bbd6b](https://github.com/continuedev/continue/commit/e3bbd6b7cfa04183f7a60470a55e4a2618083a21))\n- move vLLM rerank response in the VLLM.ts and remove unused types ([c8d9d95](https://github.com/continuedev/continue/commit/c8d9d959f10b34cb0af857cfa178c1f9df30f5cf))\n- onboarding card upgrade tab ([41bc0e9](https://github.com/continuedev/continue/commit/41bc0e922b333ee404ddb17e5bdcf7c83b7a6ca0))\n- only autoscroll Continue console when focused on last element ([40390f5](https://github.com/continuedev/continue/commit/40390f583bea74766c3eff48cb8ceda555112780))\n- open lump on submit onboarding ([f77f662](https://github.com/continuedev/continue/commit/f77f662c6e7313db645656e003c9d0145646388e))\n- org select ([dc751a3](https://github.com/continuedev/continue/commit/dc751a3274948cb12b47fb7b57106a8349e26870))\n- php definitions ([8da703f](https://github.com/continuedev/continue/commit/8da703f74525213dc212efa6b763d4be06dee38b))\n- php definitions ([eadc96e](https://github.com/continuedev/continue/commit/eadc96ebef6653a996dff32095fd87a86703058d))\n- poll in JB after upgrading ([89d4b32](https://github.com/continuedev/continue/commit/89d4b32bba3bede3ac9f001c6c013ec0bb5a9ca0))\n- postgres context provider first slice ([758a81c](https://github.com/continuedev/continue/commit/758a81c7373f398294ccb80d00a5c7d181f151d7))\n- pr_checks update ([1cc81d4](https://github.com/continuedev/continue/commit/1cc81d4b7e6ed6aedcc19753eea623acbfc8f8c1))\n- preserve edit when rejecting ([32bf80d](https://github.com/continuedev/continue/commit/32bf80d7d280fe815b547475b4a75fce92ba990d))\n- profiles slice ([b82d29c](https://github.com/continuedev/continue/commit/b82d29c2df154a632c89d32c6847f5bf43b6564b))\n- promote reject diff action ([16cbe94](https://github.com/continuedev/continue/commit/16cbe9499431d7eec64d0695821a223302d980cb))\n- prompt blocks ([9df325a](https://github.com/continuedev/continue/commit/9df325a71f3340c46820ac5335249095cda443ac))\n- Provide workspace path to HttpContextProvider ([4873f58](https://github.com/continuedev/continue/commit/4873f5805f89bc1f443211515dd7afb3fbc2cfff))\n- python context ([42d5f66](https://github.com/continuedev/continue/commit/42d5f660727dcf315d12193f70d23b283e7af10d))\n- recursively apply quick actions codelens ([#1925](https://github.com/continuedev/continue/issues/1925)) ([d5155da](https://github.com/continuedev/continue/commit/d5155dac697e5fa8b4241ebebe48bbfa66dc2c55))\n- redo action ([121be85](https://github.com/continuedev/continue/commit/121be85a69837a1d31dde31f7fca7098458ed38d))\n- refactor onboarding card ([b86bc6f](https://github.com/continuedev/continue/commit/b86bc6f727e292c2d3a8fb1511646c36e2a2531f))\n- reintroduce lazy apply for full files ([6504bd5](https://github.com/continuedev/continue/commit/6504bd55659823c8117d04ddb447f91d9932d086))\n- remove `models` property ([7f8882e](https://github.com/continuedev/continue/commit/7f8882eee136b90db9400175f9ee00fa1aa3ad93))\n- remove defaultTitle ([1bf359a](https://github.com/continuedev/continue/commit/1bf359a55fdb833569878167a1440e66c65d7249))\n- remove edit as a mode ([4f371a0](https://github.com/continuedev/continue/commit/4f371a0eb654067c626a41eb53718f565873975b))\n- remove tools from schema ([5bbda4f](https://github.com/continuedev/continue/commit/5bbda4f115a208ebd2df0c63ede75bfc5e131b54))\n- remove useFileExists ([2b34ee8](https://github.com/continuedev/continue/commit/2b34ee8c4156c544a3abd9616080f4d8e0b565a1))\n- rename to `Open Assistant configuration` ([9d8139a](https://github.com/continuedev/continue/commit/9d8139a6fae992eb1cb77248cc297d0a95683c12))\n- replace logo and deepseek model base info ([7749349](https://github.com/continuedev/continue/commit/7749349f8996f5da186ba1a8b2910443121ea2fc))\n- restructure for easier module publishing ([b51905d](https://github.com/continuedev/continue/commit/b51905da95307ed17f3fa7a466e94514bcd4113a))\n- retrieve AWS credentials from Env and from ECS/EC2 instance ([53426a1](https://github.com/continuedev/continue/commit/53426a12b858f341c9f70f8ee3513dbea39d648d))\n- reusable card ([d11376b](https://github.com/continuedev/continue/commit/d11376ba7f2b7ab1a85c75f361631fc2c5c26ab5))\n- rule colocation ([39dc367](https://github.com/continuedev/continue/commit/39dc367371c0df605f663c76fe490c4736f00153))\n- rule glob ([f95cdfa](https://github.com/continuedev/continue/commit/f95cdfaf83569159cd7aed02134be7dd5f4029cf))\n- rules ([2620064](https://github.com/continuedev/continue/commit/2620064c53ecf7de3d61b34043d2c0450f68b13c))\n- rules display ([40914a8](https://github.com/continuedev/continue/commit/40914a8bf3b4f2c1764873bc3172feab3bf23f3a))\n- rules policies ([8a2ffb3](https://github.com/continuedev/continue/commit/8a2ffb3a859d4fbec3299500a0d860c0f725573d))\n- rules preview ([d57d35d](https://github.com/continuedev/continue/commit/d57d35d5bdf6015593283a2aecbf88bca1c8c574))\n- run prettier ([ecdb4f5](https://github.com/continuedev/continue/commit/ecdb4f530dee01e8a67e7fd964206428d379b4db))\n- **scaleway:** update supported models ([8b04ba6](https://github.com/continuedev/continue/commit/8b04ba6b233d72f1e61ea67fd63de51d03629282))\n- separate toolbar action buttons ([aba6e4e](https://github.com/continuedev/continue/commit/aba6e4e61c6c7b548126a79ffeb0608e1de1147b))\n- show disableIndexing in More ([5d95b62](https://github.com/continuedev/continue/commit/5d95b62e683c0ea50eb519f7cc2851736e8ee64b))\n- show num diffs in toolbar ([ad6c0d0](https://github.com/continuedev/continue/commit/ad6c0d0bb850308993e53c4117cc45664fd8970e))\n- simplify types ([b18fd4a](https://github.com/continuedev/continue/commit/b18fd4aee0c66e41113bce4de1fa526da1dc16c6))\n- simplify typings ([c6fa6d7](https://github.com/continuedev/continue/commit/c6fa6d78a714ba6bba44ec2ef9825080b0a5b69a))\n- single default quick pick to edit ([#1743](https://github.com/continuedev/continue/issues/1743)) ([ca7bde9](https://github.com/continuedev/continue/commit/ca7bde9b5e10d684ea44291c67eb294edc357240))\n- skip hub onboarding in free trial ([d995149](https://github.com/continuedev/continue/commit/d99514995588cc2c502df99757e22f5ac3e2c7a1))\n- skip onboarding subtext ([9e6ff4b](https://github.com/continuedev/continue/commit/9e6ff4b91447b03d1b9949499cf5135d7a369df9))\n- smaller headings, use assistant name ([85804ff](https://github.com/continuedev/continue/commit/85804ff9e54adf30f1a4b23b3003b39338a82065))\n- split diff ([b7defc8](https://github.com/continuedev/continue/commit/b7defc8d912b09d9323e05a82442bea17c6faef4))\n- split diffs ([3c537f5](https://github.com/continuedev/continue/commit/3c537f51c13ee9064a3e17f3aec62dc5283ae3e2))\n- split diffs ([39087c2](https://github.com/continuedev/continue/commit/39087c2207e0dd3764761e91b9a2abd3ca6db906))\n- support o3/o4 as agents ([8bb18fd](https://github.com/continuedev/continue/commit/8bb18fd2ee4a0fb81b2f24903e47fb1b0beade36))\n- supports agent 4 deepseek ([82d8e1e](https://github.com/continuedev/continue/commit/82d8e1e6795e7258880d414c4bb78c6f83b17883))\n- toolbar header for all codeblocks ([635f7cb](https://github.com/continuedev/continue/commit/635f7cb168f264446e40c6887347327b101ee413))\n- tutorial listener ([6e01c7d](https://github.com/continuedev/continue/commit/6e01c7d0ca0d139e471edf40018d35e1033638f3))\n- unskip tests ([a5619e2](https://github.com/continuedev/continue/commit/a5619e29c8a0230f578c092c028c7cc6f3066afb))\n- update azure uri for foundry users ([638969b](https://github.com/continuedev/continue/commit/638969b7c7594f36bfefd59a986032ba597f88fa))\n- update btn colors ([b7c7171](https://github.com/continuedev/continue/commit/b7c71719e55e728f944399789f6749e493d5d13e))\n- update docs and input labelling ([1847317](https://github.com/continuedev/continue/commit/18473174365c88e7e83fd8123cb6af9318e10462))\n- update e2e tests ([93cec27](https://github.com/continuedev/continue/commit/93cec27e28773778e3223424fb72026c9e3ead44))\n- update executable perms on linux/macos ([dcbd80e](https://github.com/continuedev/continue/commit/dcbd80ecf47adeb49b9484bb1e1b3c43ff80bff8))\n- update onboarding w/ embeddings model ([#1570](https://github.com/continuedev/continue/issues/1570)) ([ed56c8f](https://github.com/continuedev/continue/commit/ed56c8f7f325f19c9b4d1c7e3cb775f850beaea9))\n- update PreToolbar ([3933791](https://github.com/continuedev/continue/commit/39337910f125a71414cf2c39b96250fc512777be))\n- update redux store schemas ([438cba4](https://github.com/continuedev/continue/commit/438cba4501026ef1705987b23f419570d1175ea1))\n- update styling ([ef349ec](https://github.com/continuedev/continue/commit/ef349ecec90dac3d15eb80ae17cf24c155d1e683))\n- update sys prompt ([25c7144](https://github.com/continuedev/continue/commit/25c714494588433d1c43d6af99cce175b48e21cc))\n- update to work as normal context provider ([0ace169](https://github.com/continuedev/continue/commit/0ace1699356be8024a8ab49ccadc4b1b22c18e08))\n- update tutorial files w/ agent mode step ([8eaf078](https://github.com/continuedev/continue/commit/8eaf078638c0d2d9c14aa0cc8e465df66f01e4df))\n- update URL replace logic ([fe6c9a1](https://github.com/continuedev/continue/commit/fe6c9a1477ab814f3cc4d0973e851701621d2c54))\n- updated prompt docs ([204aa51](https://github.com/continuedev/continue/commit/204aa51a944906fe629e92095a3f7f86113b9f6c))\n- use @lancedb/vectordb-win32-arm64-msvc ([0277320](https://github.com/continuedev/continue/commit/027732073aa0fb5202b558f4c0d71c71c6b8acbb))\n- use `fetch` instead of `http` ([401d67b](https://github.com/continuedev/continue/commit/401d67bbd0dc11fd6eda2f7d9c17dfd4da0a43f5))\n- use `instant` property on diff manager ([2f12bbd](https://github.com/continuedev/continue/commit/2f12bbdf25b1ff189a88d37bfba4f3bfe7f2f3bc))\n- use bm25 for fts ([2e5b579](https://github.com/continuedev/continue/commit/2e5b5794e9b394405c127893100be21e008b45ad))\n- use clipboard content ([9dd6284](https://github.com/continuedev/continue/commit/9dd6284b8e4b1dc09b704bc00d5231af9569b6bc))\n- use correct deployment for azure ([8e24fcf](https://github.com/continuedev/continue/commit/8e24fcf4a0ee655dfbe83d95b82855041f0ba246))\n- use crawlee for docs service ([a51c520](https://github.com/continuedev/continue/commit/a51c520deefa49f9d67aaa2469bb261af00dab80))\n- use exponential backoff in llm chat ([#1115](https://github.com/continuedev/continue/issues/1115)) ([a87df40](https://github.com/continuedev/continue/commit/a87df40a2c875a1538aa112d7d35c03137990ee2))\n- use git diff, improve comment formatting ([1adbd9a](https://github.com/continuedev/continue/commit/1adbd9a456f6c4c2f56b78d6bd5e3b8737f60ec5))\n- use hub blocks for local onboarding ([1b45308](https://github.com/continuedev/continue/commit/1b453083b14e60afda14de69b7bfb2f7ce74194b))\n- use meyers diff after initial edit/apply ([f5e7f9c](https://github.com/continuedev/continue/commit/f5e7f9ce71d4c34e8ee72b0d78fe89c68d0b7170))\n- use s3 for global docs cache on all docs ([0156409](https://github.com/continuedev/continue/commit/0156409dac5a445b055ede921c1c07706759ff4e))\n- use theme color for shortcuts rows ([44fdd65](https://github.com/continuedev/continue/commit/44fdd65967b561527865a02cafce0de4cdc9fa70))\n- use thunj ([eae1dd2](https://github.com/continuedev/continue/commit/eae1dd20bb296099db94175c43bf7af445e97200))\n- v1 onboarding card ([0d47647](https://github.com/continuedev/continue/commit/0d476473bda2ef8bea78064c89c5bc1f6818fec1))\n- vitest ([3cd19f5](https://github.com/continuedev/continue/commit/3cd19f5fabdb6eab3be381ccef8de743b37fcf29))\n- **VSC:** give option to disable quick fix ([951784c](https://github.com/continuedev/continue/commit/951784c790664a80d87b54e33886a3582137ad50))\n- vscode config for json ([666fb18](https://github.com/continuedev/continue/commit/666fb186d6b4cee9dcc95e1adc6b42748300de4c))\n- withExponentialBackoff utility function ([ad8991e](https://github.com/continuedev/continue/commit/ad8991eac0f7f40f3877d56392592d8d83e173a6))\n- working `copy-client` ([09beb58](https://github.com/continuedev/continue/commit/09beb58260f32043c89cff39c6e8567f25099c9c))\n- write initial unit test for apply ([f4a59dc](https://github.com/continuedev/continue/commit/f4a59dc185c3ea7ceb29a4629ea0c7dc82f44dbb))\n\n### Performance Improvements\n\n- :green_heart: hardcode distro paths ([edf0f56](https://github.com/continuedev/continue/commit/edf0f5603730d01d7ca4ca055dd49da596648627))\n- :zap: don't show server loading immediately ([5047dfc](https://github.com/continuedev/continue/commit/5047dfcd2a2e47468c15ed05c69781cc615ef723))\n- **llm:** Optimize pruneLines functions in countTokens ([35b3189](https://github.com/continuedev/continue/commit/35b3189538da5891617a79ba9c0badb0bb7dc1bc))\n- **llm:** Optimize pruneLines functions in countTokens ([28cdd1c](https://github.com/continuedev/continue/commit/28cdd1cee25973aa9a8a5cfdcbcc436e0b7f6240))\n- **llm:** Optimize pruneLines functions in countTokens ([881f8b3](https://github.com/continuedev/continue/commit/881f8b3139794aca0a6699c3d34ebf8fba01b789))\n\n### Reverts\n\n- Revert \"Add citations to the log\" ([caf7288](https://github.com/continuedev/continue/commit/caf7288d36c1361b6cb279821d9ee377636f72f7))\n- Revert \"feat: add bookmark logic\" ([730ac56](https://github.com/continuedev/continue/commit/730ac56d5a15a6b2931330aeb849e2d260d2644a))\n- Revert \"update TabBar to use Redux for session management\" ([9f13b92](https://github.com/continuedev/continue/commit/9f13b92b3fbc4157124ccf120028312390eed70f))\n- :bookmark: update version ([d6ebc6d](https://github.com/continuedev/continue/commit/d6ebc6d969ccafc753ecf00e0309777b96b4ad11))\n- :bug: revert unecessary changes from yesterday ([3629ddd](https://github.com/continuedev/continue/commit/3629dddb1daea23fbd29b03705e742ec2a22d6ec))\n- :fire: disable fallback_context_item ([a572db4](https://github.com/continuedev/continue/commit/a572db40b6c9ce98b07e89d34f8652e19f91187e))\n"}
{"source":"github","repo":"continue","path":"packages/config-yaml/src/README.md","content":"# config.yaml specification\n\nThis specification is a work in progress and subject to change.\n\n## Loading a config.yaml file\n\nconfig.yaml is loaded in the following steps\n\n## Unrolling\n\nA \"source\" config.yaml is \"unrolled\" so that its packages all get merged into a single config.yaml. This is done by recursively loading all packages and merging them into the config.yaml.\n\nThis happens on the server, unless using local mode.\n\n## Client rendering\n\nThe unrolled config.yaml is then rendered on the client. This is done by replacing all user secret template variables with their values and replacing all other secrets with secret locations.\n\n## Publishing\n\nMake sure you are logged into the npm registry with `npm login`.\n\nThen, bump the version in `package.json` and then run:\n\n```bash\nnpm run build\nnpm publish --access public\n```\n"}
{"source":"github","repo":"continue","path":"packages/config-yaml/src/schemas/review.rule.md","content":"No new additions to the ConfigYaml schema should be made for features that are experimental. This is because anything added to config.yaml can't easily be taken away without causing backward-compatibility issues for users that chose to use the setting.\n\nYou can tell whether a feature is experimental based off of whether anything was added to the \"Experimental Settings\" section of `UserSettingsForm.tsx`.\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/README.md","content":"# @continuedev/sdk\n\n> **‚ö†Ô∏è EXPERIMENTAL: This package is in early development and subject to frequent breaking changes without notice.**\n\nThis SDK provides a drop-in replacement for OpenAI libraries to easily integrate with Continue assistants.\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/README.md","content":"# openapi-client\n\nAPI for Continue IDE to fetch assistants and other related information.\nThese endpoints are primarily used by the Continue IDE extensions for VS Code and JetBrains.\n\nThis Python package is automatically generated by the [OpenAPI Generator](https://openapi-generator.tech) project:\n\n- API version: 1.0.0\n- Package version: 1.0.0\n- Generator version: 7.12.0\n- Build package: org.openapitools.codegen.languages.PythonClientCodegen\n  For more information, please visit [https://continue.dev](https://continue.dev)\n\n## Requirements.\n\nPython 3.8+\n\n## Installation & Usage\n\n### pip install\n\nIf the python package is hosted on a repository, you can install directly using:\n\n```sh\npip install git+https://github.com/GIT_USER_ID/GIT_REPO_ID.git\n```\n\n(you may need to run `pip` with root permission: `sudo pip install git+https://github.com/GIT_USER_ID/GIT_REPO_ID.git`)\n\nThen import the package:\n\n```python\nimport openapi_client\n```\n\n### Setuptools\n\nInstall via [Setuptools](http://pypi.python.org/pypi/setuptools).\n\n```sh\npython setup.py install --user\n```\n\n(or `sudo python setup.py install` to install the package for all users)\n\nThen import the package:\n\n```python\nimport openapi_client\n```\n\n### Tests\n\nExecute `pytest` to run the tests.\n\n## Getting Started\n\nPlease follow the [installation procedure](#installation--usage) and then run the following:\n\n```python\n\nimport openapi_client\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    owner_slug = 'owner_slug_example' # str | Slug of the user or organization that owns the assistant\n    package_slug = 'package_slug_example' # str | Slug of the assistant package\n    always_use_proxy = 'always_use_proxy_example' # str | Whether to always use the Continue-managed proxy for model requests (optional)\n    organization_id = 'organization_id_example' # str | ID of the organization to scope assistants to. If not provided, personal assistants are returned. (optional)\n\n    try:\n        # Get a specific assistant by slug\n        api_response = api_instance.get_assistant(owner_slug, package_slug, always_use_proxy=always_use_proxy, organization_id=organization_id)\n        print(\"The response of DefaultApi->get_assistant:\\n\")\n        pprint(api_response)\n    except ApiException as e:\n        print(\"Exception when calling DefaultApi->get_assistant: %s\\n\" % e)\n\n```\n\n## Documentation for API Endpoints\n\nAll URIs are relative to *https://api.continue.dev*\n\n| Class        | Method                                                                                  | HTTP request                                         | Description                                       |\n| ------------ | --------------------------------------------------------------------------------------- | ---------------------------------------------------- | ------------------------------------------------- |\n| _DefaultApi_ | [**get_assistant**](docs/DefaultApi.md#get_assistant)                                   | **GET** /ide/get-assistant/{ownerSlug}/{packageSlug} | Get a specific assistant by slug                  |\n| _DefaultApi_ | [**get_free_trial_status**](docs/DefaultApi.md#get_free_trial_status)                   | **GET** /ide/free-trial-status                       | Get free trial status for user                    |\n| _DefaultApi_ | [**get_models_add_on_checkout_url**](docs/DefaultApi.md#get_models_add_on_checkout_url) | **GET** /ide/get-models-add-on-checkout-url          | Get Stripe checkout URL for models add-on         |\n| _DefaultApi_ | [**get_policy**](docs/DefaultApi.md#get_policy)                                         | **GET** /ide/policy                                  | Get organization policy                           |\n| _DefaultApi_ | [**list_assistant_full_slugs**](docs/DefaultApi.md#list_assistant_full_slugs)           | **GET** /ide/list-assistant-full-slugs               | List assistant full slugs (currently returns 429) |\n| _DefaultApi_ | [**list_assistants**](docs/DefaultApi.md#list_assistants)                               | **GET** /ide/list-assistants                         | List assistants for IDE                           |\n| _DefaultApi_ | [**list_organizations**](docs/DefaultApi.md#list_organizations)                         | **GET** /ide/list-organizations                      | List organizations for user                       |\n| _DefaultApi_ | [**sync_secrets**](docs/DefaultApi.md#sync_secrets)                                     | **POST** /ide/sync-secrets                           | Synchronize secrets for user                      |\n\n## Documentation For Models\n\n- [GetAssistant200Response](docs/GetAssistant200Response.md)\n- [GetAssistant403Response](docs/GetAssistant403Response.md)\n- [GetAssistant404Response](docs/GetAssistant404Response.md)\n- [GetFreeTrialStatus200Response](docs/GetFreeTrialStatus200Response.md)\n- [GetModelsAddOnCheckoutUrl200Response](docs/GetModelsAddOnCheckoutUrl200Response.md)\n- [GetModelsAddOnCheckoutUrl500Response](docs/GetModelsAddOnCheckoutUrl500Response.md)\n- [GetPolicy200Response](docs/GetPolicy200Response.md)\n- [ListAssistantFullSlugs429Response](docs/ListAssistantFullSlugs429Response.md)\n- [ListAssistants200ResponseInner](docs/ListAssistants200ResponseInner.md)\n- [ListAssistants200ResponseInnerConfigResult](docs/ListAssistants200ResponseInnerConfigResult.md)\n- [ListAssistants401Response](docs/ListAssistants401Response.md)\n- [ListAssistants404Response](docs/ListAssistants404Response.md)\n- [ListOrganizations200Response](docs/ListOrganizations200Response.md)\n- [ListOrganizations200ResponseOrganizationsInner](docs/ListOrganizations200ResponseOrganizationsInner.md)\n- [SyncSecretsRequest](docs/SyncSecretsRequest.md)\n\n<a id=\"documentation-for-authorization\"></a>\n\n## Documentation For Authorization\n\nAuthentication schemes defined for the API:\n<a id=\"apiKeyAuth\"></a>\n\n### apiKeyAuth\n\n- **Type**: Bearer authentication\n\n## Author\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetAssistant403Response.md","content":"# GetAssistant403Response\n\n## Properties\n\n| Name        | Type    | Description | Notes      |\n| ----------- | ------- | ----------- | ---------- |\n| **message** | **str** |             | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.get_assistant403_response import GetAssistant403Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetAssistant403Response from a JSON string\nget_assistant403_response_instance = GetAssistant403Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetAssistant403Response.to_json())\n\n# convert the object into a dict\nget_assistant403_response_dict = get_assistant403_response_instance.to_dict()\n# create an instance of GetAssistant403Response from a dict\nget_assistant403_response_from_dict = GetAssistant403Response.from_dict(get_assistant403_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/DefaultApi.md","content":"# openapi_client.DefaultApi\n\nAll URIs are relative to *https://api.continue.dev*\n\n| Method                                                                             | HTTP request                                         | Description                                       |\n| ---------------------------------------------------------------------------------- | ---------------------------------------------------- | ------------------------------------------------- |\n| [**get_assistant**](DefaultApi.md#get_assistant)                                   | **GET** /ide/get-assistant/{ownerSlug}/{packageSlug} | Get a specific agent by slug                  |\n| [**get_free_trial_status**](DefaultApi.md#get_free_trial_status)                   | **GET** /ide/free-trial-status                       | Get free trial status for user                    |\n| [**get_models_add_on_checkout_url**](DefaultApi.md#get_models_add_on_checkout_url) | **GET** /ide/get-models-add-on-checkout-url          | Get Stripe checkout URL for models add-on         |\n| [**get_policy**](DefaultApi.md#get_policy)                                         | **GET** /ide/policy                                  | Get organization policy                           |\n| [**list_assistant_full_slugs**](DefaultApi.md#list_assistant_full_slugs)           | **GET** /ide/list-assistant-full-slugs               | List agent full slugs (currently returns 429) |\n| [**list_assistants**](DefaultApi.md#list_assistants)                               | **GET** /ide/list-assistants                         | List agents for IDE                           |\n| [**list_organizations**](DefaultApi.md#list_organizations)                         | **GET** /ide/list-organizations                      | List organizations for user                       |\n| [**sync_secrets**](DefaultApi.md#sync_secrets)                                     | **POST** /ide/sync-secrets                           | Synchronize secrets for user                      |\n\n# **get_assistant**\n\n> GetAssistant200Response get_assistant(owner_slug, package_slug, always_use_proxy=always_use_proxy, organization_id=organization_id)\n\nGet a specific agent by slug\n\nReturns a single agent configuration by its owner and package slug.\nThis endpoint is useful when you need to retrieve or refresh a specific agent\nwithout fetching the entire list.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.get_assistant200_response import GetAssistant200Response\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    owner_slug = 'owner_slug_example' # str | Slug of the user or organization that owns the agent\n    package_slug = 'package_slug_example' # str | Slug of the agent package\n    always_use_proxy = 'always_use_proxy_example' # str | Whether to always use the Continue-managed proxy for model requests (optional)\n    organization_id = 'organization_id_example' # str | ID of the organization to scope agents to. If not provided, personal agents are returned. (optional)\n\n    try:\n        # Get a specific agent by slug\n        api_response = api_instance.get_assistant(owner_slug, package_slug, always_use_proxy=always_use_proxy, organization_id=organization_id)\n        print(\"The response of DefaultApi->get_assistant:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->get_assistant: %s\\n\" % e)\n```\n\n### Parameters\n\n| Name                 | Type    | Description                                                                                       | Notes      |\n| -------------------- | ------- | ------------------------------------------------------------------------------------------------- | ---------- |\n| **owner_slug**       | **str** | Slug of the user or organization that owns the agent                                          |\n| **package_slug**     | **str** | Slug of the agent package                                                                     |\n| **always_use_proxy** | **str** | Whether to always use the Continue-managed proxy for model requests                               | [optional] |\n| **organization_id**  | **str** | ID of the organization to scope agents to. If not provided, personal agents are returned. | [optional] |\n\n### Return type\n\n[**GetAssistant200Response**](GetAssistant200Response.md)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                                       | Response headers |\n| ----------- | ------------------------------------------------- | ---------------- |\n| **200**     | Successfully retrieved agent                  | -                |\n| **401**     | Unauthorized - Authentication failed              | -                |\n| **403**     | Forbidden - Assistant not allowed in organization | -                |\n| **404**     | User or agent not found                       | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **get_free_trial_status**\n\n> GetFreeTrialStatus200Response get_free_trial_status()\n\nGet free trial status for user\n\nReturns the current free trial status for the authenticated user, including\nusage counts and limits for chat and autocomplete features.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.get_free_trial_status200_response import GetFreeTrialStatus200Response\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n\n    try:\n        # Get free trial status for user\n        api_response = api_instance.get_free_trial_status()\n        print(\"The response of DefaultApi->get_free_trial_status:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->get_free_trial_status: %s\\n\" % e)\n```\n\n### Parameters\n\nThis endpoint does not need any parameter.\n\n### Return type\n\n[**GetFreeTrialStatus200Response**](GetFreeTrialStatus200Response.md)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                              | Response headers |\n| ----------- | ---------------------------------------- | ---------------- |\n| **200**     | Successfully retrieved free trial status | -                |\n| **404**     | User not found                           | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **get_models_add_on_checkout_url**\n\n> GetModelsAddOnCheckoutUrl200Response get_models_add_on_checkout_url(profile_id=profile_id, vscode_uri_scheme=vscode_uri_scheme)\n\nGet Stripe checkout URL for models add-on\n\nCreates a Stripe checkout session for the models add-on subscription\nand returns the checkout URL.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.get_models_add_on_checkout_url200_response import GetModelsAddOnCheckoutUrl200Response\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    profile_id = 'profile_id_example' # str | Profile ID to include in the callback URL (optional)\n    vscode_uri_scheme = 'vscode_uri_scheme_example' # str | VS Code URI scheme to include in the callback URL (optional)\n\n    try:\n        # Get Stripe checkout URL for models add-on\n        api_response = api_instance.get_models_add_on_checkout_url(profile_id=profile_id, vscode_uri_scheme=vscode_uri_scheme)\n        print(\"The response of DefaultApi->get_models_add_on_checkout_url:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->get_models_add_on_checkout_url: %s\\n\" % e)\n```\n\n### Parameters\n\n| Name                  | Type    | Description                                       | Notes      |\n| --------------------- | ------- | ------------------------------------------------- | ---------- |\n| **profile_id**        | **str** | Profile ID to include in the callback URL         | [optional] |\n| **vscode_uri_scheme** | **str** | VS Code URI scheme to include in the callback URL | [optional] |\n\n### Return type\n\n[**GetModelsAddOnCheckoutUrl200Response**](GetModelsAddOnCheckoutUrl200Response.md)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                           | Response headers |\n| ----------- | ------------------------------------- | ---------------- |\n| **200**     | Successfully created checkout session | -                |\n| **404**     | User not found                        | -                |\n| **500**     | Failed to create checkout session     | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **get_policy**\n\n> GetPolicy200Response get_policy()\n\nGet organization policy\n\nReturns the policy configuration for the first organization\nthat the user belongs to which has a policy configured.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.get_policy200_response import GetPolicy200Response\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n\n    try:\n        # Get organization policy\n        api_response = api_instance.get_policy()\n        print(\"The response of DefaultApi->get_policy:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->get_policy: %s\\n\" % e)\n```\n\n### Parameters\n\nThis endpoint does not need any parameter.\n\n### Return type\n\n[**GetPolicy200Response**](GetPolicy200Response.md)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                   | Response headers |\n| ----------- | ----------------------------- | ---------------- |\n| **200**     | Successfully retrieved policy | -                |\n| **404**     | User not found                | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **list_assistant_full_slugs**\n\n> list_assistant_full_slugs()\n\nList agent full slugs (currently returns 429)\n\nThis endpoint is temporarily disabled and returns a 429 status code\nto prevent constant refreshes of the full agent list until a\nfixed client version can be deployed.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n\n    try:\n        # List agent full slugs (currently returns 429)\n        api_instance.list_assistant_full_slugs()\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->list_assistant_full_slugs: %s\\n\" % e)\n```\n\n### Parameters\n\nThis endpoint does not need any parameter.\n\n### Return type\n\nvoid (empty response body)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                                       | Response headers |\n| ----------- | ------------------------------------------------- | ---------------- |\n| **429**     | Too many requests - endpoint temporarily disabled | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **list_assistants**\n\n> List[ListAssistants200ResponseInner] list_assistants(always_use_proxy=always_use_proxy, organization_id=organization_id)\n\nList agents for IDE\n\nReturns a complete list of agents available to the user, with their full configurations,\nicons, and other metadata needed by the IDE to display and use them.\n\nThis endpoint performs a full refresh of the list of agents, including unrolling\nconfigurations and resolving secrets.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.list_assistants200_response_inner import ListAssistants200ResponseInner\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    always_use_proxy = 'always_use_proxy_example' # str | Whether to always use the Continue-managed proxy for model requests (optional)\n    organization_id = 'organization_id_example' # str | ID of the organization to scope agents to. If not provided, personal agents are returned. (optional)\n\n    try:\n        # List agents for IDE\n        api_response = api_instance.list_assistants(always_use_proxy=always_use_proxy, organization_id=organization_id)\n        print(\"The response of DefaultApi->list_assistants:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->list_assistants: %s\\n\" % e)\n```\n\n### Parameters\n\n| Name                 | Type    | Description                                                                                       | Notes      |\n| -------------------- | ------- | ------------------------------------------------------------------------------------------------- | ---------- |\n| **always_use_proxy** | **str** | Whether to always use the Continue-managed proxy for model requests                               | [optional] |\n| **organization_id**  | **str** | ID of the organization to scope agents to. If not provided, personal agents are returned. | [optional] |\n\n### Return type\n\n[**List[ListAssistants200ResponseInner]**](ListAssistants200ResponseInner.md)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                          | Response headers |\n| ----------- | ------------------------------------ | ---------------- |\n| **200**     | Successfully retrieved agents    | -                |\n| **401**     | Unauthorized - Authentication failed | -                |\n| **404**     | User not found                       | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **list_organizations**\n\n> ListOrganizations200Response list_organizations()\n\nList organizations for user\n\nReturns a list of organizations that the authenticated user belongs to,\nincluding organization metadata and pre-signed icon URLs.\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.list_organizations200_response import ListOrganizations200Response\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n\n    try:\n        # List organizations for user\n        api_response = api_instance.list_organizations()\n        print(\"The response of DefaultApi->list_organizations:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->list_organizations: %s\\n\" % e)\n```\n\n### Parameters\n\nThis endpoint does not need any parameter.\n\n### Return type\n\n[**ListOrganizations200Response**](ListOrganizations200Response.md)\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: Not defined\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                          | Response headers |\n| ----------- | ------------------------------------ | ---------------- |\n| **200**     | Successfully retrieved organizations | -                |\n| **404**     | User not found                       | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n\n# **sync_secrets**\n\n> List[Optional[object]] sync_secrets(sync_secrets_request)\n\nSynchronize secrets for user\n\nResolves and synchronizes secrets for the authenticated user based on\nthe provided Fully Qualified Secret Names (FQSNs).\n\n### Example\n\n- Bearer Authentication (apiKeyAuth):\n\n```python\nimport openapi_client\nfrom openapi_client.models.sync_secrets_request import SyncSecretsRequest\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    sync_secrets_request = openapi_client.SyncSecretsRequest() # SyncSecretsRequest |\n\n    try:\n        # Synchronize secrets for user\n        api_response = api_instance.sync_secrets(sync_secrets_request)\n        print(\"The response of DefaultApi->sync_secrets:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->sync_secrets: %s\\n\" % e)\n```\n\n### Parameters\n\n| Name                     | Type                                            | Description | Notes |\n| ------------------------ | ----------------------------------------------- | ----------- | ----- |\n| **sync_secrets_request** | [**SyncSecretsRequest**](SyncSecretsRequest.md) |             |\n\n### Return type\n\n**List[Optional[object]]**\n\n### Authorization\n\n[apiKeyAuth](../README.md#apiKeyAuth)\n\n### HTTP request headers\n\n- **Content-Type**: application/json\n- **Accept**: application/json\n\n### HTTP response details\n\n| Status code | Description                   | Response headers |\n| ----------- | ----------------------------- | ---------------- |\n| **200**     | Successfully resolved secrets | -                |\n| **404**     | User not found                | -                |\n\n[[Back to top]](#) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to Model list]](../README.md#documentation-for-models) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetAssistant200Response.md","content":"# GetAssistant200Response\n\n## Properties\n\n| Name                  | Type                                                                                            | Description                                               | Notes      |\n| --------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------- |\n| **config_result**     | [**ListAssistants200ResponseInnerConfigResult**](ListAssistants200ResponseInnerConfigResult.md) |                                                           |\n| **owner_slug**        | **str**                                                                                         | Slug of the user or organization that owns the agent  |\n| **package_slug**      | **str**                                                                                         | Slug of the agent package                             |\n| **icon_url**          | **str**                                                                                         | Pre-signed URL for the agent&#39;s icon               | [optional] |\n| **on_prem_proxy_url** | **str**                                                                                         | URL of the on-premises proxy if the organization uses one | [optional] |\n| **use_on_prem_proxy** | **bool**                                                                                        | Whether the organization uses an on-premises proxy        | [optional] |\n| **raw_yaml**          | **str**                                                                                         | Raw YAML configuration of the agent                   | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.get_assistant200_response import GetAssistant200Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetAssistant200Response from a JSON string\nget_assistant200_response_instance = GetAssistant200Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetAssistant200Response.to_json())\n\n# convert the object into a dict\nget_assistant200_response_dict = get_assistant200_response_instance.to_dict()\n# create an instance of GetAssistant200Response from a dict\nget_assistant200_response_from_dict = GetAssistant200Response.from_dict(get_assistant200_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListAssistants404Response.md","content":"# ListAssistants404Response\n\n## Properties\n\n| Name        | Type    | Description | Notes      |\n| ----------- | ------- | ----------- | ---------- |\n| **message** | **str** |             | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.list_assistants404_response import ListAssistants404Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants404Response from a JSON string\nlist_assistants404_response_instance = ListAssistants404Response.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants404Response.to_json())\n\n# convert the object into a dict\nlist_assistants404_response_dict = list_assistants404_response_instance.to_dict()\n# create an instance of ListAssistants404Response from a dict\nlist_assistants404_response_from_dict = ListAssistants404Response.from_dict(list_assistants404_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListAssistantFullSlugs429Response.md","content":"# ListAssistantFullSlugs429Response\n\n## Properties\n\n| Name        | Type    | Description | Notes      |\n| ----------- | ------- | ----------- | ---------- |\n| **message** | **str** |             | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.list_assistant_full_slugs429_response import ListAssistantFullSlugs429Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistantFullSlugs429Response from a JSON string\nlist_assistant_full_slugs429_response_instance = ListAssistantFullSlugs429Response.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistantFullSlugs429Response.to_json())\n\n# convert the object into a dict\nlist_assistant_full_slugs429_response_dict = list_assistant_full_slugs429_response_instance.to_dict()\n# create an instance of ListAssistantFullSlugs429Response from a dict\nlist_assistant_full_slugs429_response_from_dict = ListAssistantFullSlugs429Response.from_dict(list_assistant_full_slugs429_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListAssistants200ResponseInnerConfigResult.md","content":"# ListAssistants200ResponseInnerConfigResult\n\n## Properties\n\n| Name                        | Type          | Description                                           | Notes      |\n| --------------------------- | ------------- | ----------------------------------------------------- | ---------- |\n| **config**                  | **object**    | The unrolled agent configuration                  |\n| **config_load_interrupted** | **bool**      | Whether the configuration loading was interrupted     |\n| **errors**                  | **List[str]** | Any errors that occurred during configuration loading | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.list_assistants200_response_inner_config_result import ListAssistants200ResponseInnerConfigResult\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants200ResponseInnerConfigResult from a JSON string\nlist_assistants200_response_inner_config_result_instance = ListAssistants200ResponseInnerConfigResult.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants200ResponseInnerConfigResult.to_json())\n\n# convert the object into a dict\nlist_assistants200_response_inner_config_result_dict = list_assistants200_response_inner_config_result_instance.to_dict()\n# create an instance of ListAssistants200ResponseInnerConfigResult from a dict\nlist_assistants200_response_inner_config_result_from_dict = ListAssistants200ResponseInnerConfigResult.from_dict(list_assistants200_response_inner_config_result_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/SyncSecretsRequest.md","content":"# SyncSecretsRequest\n\n## Properties\n\n| Name               | Type             | Description                                      | Notes      |\n| ------------------ | ---------------- | ------------------------------------------------ | ---------- |\n| **fqsns**          | **List[object]** | Array of Fully Qualified Secret Names to resolve |\n| **org_scope_id**   | **str**          | Organization ID to scope secret resolution to    | [optional] |\n| **org_scope_slug** | **str**          | Organization slug to scope secret resolution to  | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.sync_secrets_request import SyncSecretsRequest\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of SyncSecretsRequest from a JSON string\nsync_secrets_request_instance = SyncSecretsRequest.from_json(json)\n# print the JSON string representation of the object\nprint(SyncSecretsRequest.to_json())\n\n# convert the object into a dict\nsync_secrets_request_dict = sync_secrets_request_instance.to_dict()\n# create an instance of SyncSecretsRequest from a dict\nsync_secrets_request_from_dict = SyncSecretsRequest.from_dict(sync_secrets_request_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetFreeTrialStatus200Response.md","content":"# GetFreeTrialStatus200Response\n\n## Properties\n\n| Name                       | Type      | Description                                                   | Notes      |\n| -------------------------- | --------- | ------------------------------------------------------------- | ---------- |\n| **opted_in_to_free_trial** | **bool**  | Whether the user has opted into the free trial                |\n| **chat_count**             | **float** | Current number of chat messages used                          | [optional] |\n| **autocomplete_count**     | **float** | Current number of autocomplete requests used                  | [optional] |\n| **chat_limit**             | **float** | Maximum number of chat messages allowed in free trial         |\n| **autocomplete_limit**     | **float** | Maximum number of autocomplete requests allowed in free trial |\n\n## Example\n\n```python\nfrom openapi_client.models.get_free_trial_status200_response import GetFreeTrialStatus200Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetFreeTrialStatus200Response from a JSON string\nget_free_trial_status200_response_instance = GetFreeTrialStatus200Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetFreeTrialStatus200Response.to_json())\n\n# convert the object into a dict\nget_free_trial_status200_response_dict = get_free_trial_status200_response_instance.to_dict()\n# create an instance of GetFreeTrialStatus200Response from a dict\nget_free_trial_status200_response_from_dict = GetFreeTrialStatus200Response.from_dict(get_free_trial_status200_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetPolicy200Response.md","content":"# GetPolicy200Response\n\n## Properties\n\n| Name         | Type       | Description                                  | Notes      |\n| ------------ | ---------- | -------------------------------------------- | ---------- |\n| **policy**   | **object** | Organization policy configuration            | [optional] |\n| **org_slug** | **str**    | Slug of the organization that has the policy | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.get_policy200_response import GetPolicy200Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetPolicy200Response from a JSON string\nget_policy200_response_instance = GetPolicy200Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetPolicy200Response.to_json())\n\n# convert the object into a dict\nget_policy200_response_dict = get_policy200_response_instance.to_dict()\n# create an instance of GetPolicy200Response from a dict\nget_policy200_response_from_dict = GetPolicy200Response.from_dict(get_policy200_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetModelsAddOnCheckoutUrl200Response.md","content":"# GetModelsAddOnCheckoutUrl200Response\n\n## Properties\n\n| Name    | Type    | Description                 | Notes |\n| ------- | ------- | --------------------------- | ----- |\n| **url** | **str** | Stripe checkout session URL |\n\n## Example\n\n```python\nfrom openapi_client.models.get_models_add_on_checkout_url200_response import GetModelsAddOnCheckoutUrl200Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetModelsAddOnCheckoutUrl200Response from a JSON string\nget_models_add_on_checkout_url200_response_instance = GetModelsAddOnCheckoutUrl200Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetModelsAddOnCheckoutUrl200Response.to_json())\n\n# convert the object into a dict\nget_models_add_on_checkout_url200_response_dict = get_models_add_on_checkout_url200_response_instance.to_dict()\n# create an instance of GetModelsAddOnCheckoutUrl200Response from a dict\nget_models_add_on_checkout_url200_response_from_dict = GetModelsAddOnCheckoutUrl200Response.from_dict(get_models_add_on_checkout_url200_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetModelsAddOnCheckoutUrl500Response.md","content":"# GetModelsAddOnCheckoutUrl500Response\n\n## Properties\n\n| Name        | Type    | Description | Notes      |\n| ----------- | ------- | ----------- | ---------- |\n| **message** | **str** |             | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.get_models_add_on_checkout_url500_response import GetModelsAddOnCheckoutUrl500Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetModelsAddOnCheckoutUrl500Response from a JSON string\nget_models_add_on_checkout_url500_response_instance = GetModelsAddOnCheckoutUrl500Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetModelsAddOnCheckoutUrl500Response.to_json())\n\n# convert the object into a dict\nget_models_add_on_checkout_url500_response_dict = get_models_add_on_checkout_url500_response_instance.to_dict()\n# create an instance of GetModelsAddOnCheckoutUrl500Response from a dict\nget_models_add_on_checkout_url500_response_from_dict = GetModelsAddOnCheckoutUrl500Response.from_dict(get_models_add_on_checkout_url500_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListAssistants401Response.md","content":"# ListAssistants401Response\n\n## Properties\n\n| Name        | Type    | Description | Notes      |\n| ----------- | ------- | ----------- | ---------- |\n| **message** | **str** |             | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.list_assistants401_response import ListAssistants401Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants401Response from a JSON string\nlist_assistants401_response_instance = ListAssistants401Response.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants401Response.to_json())\n\n# convert the object into a dict\nlist_assistants401_response_dict = list_assistants401_response_instance.to_dict()\n# create an instance of ListAssistants401Response from a dict\nlist_assistants401_response_from_dict = ListAssistants401Response.from_dict(list_assistants401_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListAssistants200ResponseInner.md","content":"# ListAssistants200ResponseInner\n\n## Properties\n\n| Name                  | Type                                                                                            | Description                                               | Notes      |\n| --------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------- |\n| **config_result**     | [**ListAssistants200ResponseInnerConfigResult**](ListAssistants200ResponseInnerConfigResult.md) |                                                           |\n| **owner_slug**        | **str**                                                                                         | Slug of the user or organization that owns the agent  |\n| **package_slug**      | **str**                                                                                         | Slug of the agent package                             |\n| **icon_url**          | **str**                                                                                         | Pre-signed URL for the agent&#39;s icon               | [optional] |\n| **on_prem_proxy_url** | **str**                                                                                         | URL of the on-premises proxy if the organization uses one | [optional] |\n| **use_on_prem_proxy** | **bool**                                                                                        | Whether the organization uses an on-premises proxy        | [optional] |\n| **raw_yaml**          | **str**                                                                                         | Raw YAML configuration of the agent                   | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.list_assistants200_response_inner import ListAssistants200ResponseInner\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants200ResponseInner from a JSON string\nlist_assistants200_response_inner_instance = ListAssistants200ResponseInner.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants200ResponseInner.to_json())\n\n# convert the object into a dict\nlist_assistants200_response_inner_dict = list_assistants200_response_inner_instance.to_dict()\n# create an instance of ListAssistants200ResponseInner from a dict\nlist_assistants200_response_inner_from_dict = ListAssistants200ResponseInner.from_dict(list_assistants200_response_inner_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListOrganizations200ResponseOrganizationsInner.md","content":"# ListOrganizations200ResponseOrganizationsInner\n\n## Properties\n\n| Name         | Type    | Description                                    | Notes      |\n| ------------ | ------- | ---------------------------------------------- | ---------- |\n| **id**       | **str** | Organization ID                                |\n| **name**     | **str** | Organization name                              |\n| **icon_url** | **str** | Pre-signed URL for the organization&#39;s icon | [optional] |\n| **slug**     | **str** | Organization slug                              |\n\n## Example\n\n```python\nfrom openapi_client.models.list_organizations200_response_organizations_inner import ListOrganizations200ResponseOrganizationsInner\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListOrganizations200ResponseOrganizationsInner from a JSON string\nlist_organizations200_response_organizations_inner_instance = ListOrganizations200ResponseOrganizationsInner.from_json(json)\n# print the JSON string representation of the object\nprint(ListOrganizations200ResponseOrganizationsInner.to_json())\n\n# convert the object into a dict\nlist_organizations200_response_organizations_inner_dict = list_organizations200_response_organizations_inner_instance.to_dict()\n# create an instance of ListOrganizations200ResponseOrganizationsInner from a dict\nlist_organizations200_response_organizations_inner_from_dict = ListOrganizations200ResponseOrganizationsInner.from_dict(list_organizations200_response_organizations_inner_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/ListOrganizations200Response.md","content":"# ListOrganizations200Response\n\n## Properties\n\n| Name              | Type                                                                                                          | Description | Notes |\n| ----------------- | ------------------------------------------------------------------------------------------------------------- | ----------- | ----- |\n| **organizations** | [**List[ListOrganizations200ResponseOrganizationsInner]**](ListOrganizations200ResponseOrganizationsInner.md) |             |\n\n## Example\n\n```python\nfrom openapi_client.models.list_organizations200_response import ListOrganizations200Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListOrganizations200Response from a JSON string\nlist_organizations200_response_instance = ListOrganizations200Response.from_json(json)\n# print the JSON string representation of the object\nprint(ListOrganizations200Response.to_json())\n\n# convert the object into a dict\nlist_organizations200_response_dict = list_organizations200_response_instance.to_dict()\n# create an instance of ListOrganizations200Response from a dict\nlist_organizations200_response_from_dict = ListOrganizations200Response.from_dict(list_organizations200_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/python/api/docs/GetAssistant404Response.md","content":"# GetAssistant404Response\n\n## Properties\n\n| Name        | Type    | Description | Notes      |\n| ----------- | ------- | ----------- | ---------- |\n| **message** | **str** |             | [optional] |\n\n## Example\n\n```python\nfrom openapi_client.models.get_assistant404_response import GetAssistant404Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of GetAssistant404Response from a JSON string\nget_assistant404_response_instance = GetAssistant404Response.from_json(json)\n# print the JSON string representation of the object\nprint(GetAssistant404Response.to_json())\n\n# convert the object into a dict\nget_assistant404_response_dict = get_assistant404_response_instance.to_dict()\n# create an instance of GetAssistant404Response from a dict\nget_assistant404_response_from_dict = GetAssistant404Response.from_dict(get_assistant404_response_dict)\n```\n\n[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/README.md","content":"# @continuedev/sdk\n\n> **‚ö†Ô∏è EXPERIMENTAL: This package is in early development and subject to frequent breaking changes without notice.**\n\nThis SDK provides programmatic access to Continue's Hub APIs and functionality.\n\n## Overview\n\nThe Continue SDK is structured into separate language-specific packages, currently with implementations for TypeScript and Python, with more languages planned for the future:\n\n- **TypeScript SDK**: Located in the `/typescript` directory, providing a drop-in replacement for OpenAI's TypeScript client libraries\n- **Python SDK**: Located in the `/python` directory, providing a drop-in replacement for OpenAI's Python client libraries\n- **Additional languages**: More language implementations are planned to be added in the future\n\nEach package includes:\n\n1. OpenAPI-generated clients for the Continue Hub API\n2. A wrapper layer that exposes a `Continue.from()` method to easily initialize and use Continue assistants with an OpenAI-compatible interface\n\n## For End Users\n\nEnd users should install the published packages directly:\n\n- For TypeScript/JavaScript: `npm install @continuedev/sdk`\n- For Python: `pip install continuedev`\n- For other languages: Check back for new language support\n\nEach published package includes its own documentation.\n\n## For Developers\n\n### Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Generate API clients\nnpm run generate-client:ALL\n\n# Start Swagger UI for API exploration\nnpm run swagger-ui\n```\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/typescript/README.md","content":"# @continuedev/sdk\n\n> **‚ö†Ô∏è EXPERIMENTAL: This package is in early development and subject to frequent breaking changes without notice.**\n\nThis SDK provides a drop-in replacement for OpenAI libraries to easily integrate with Continue assistants.\n\n## Installation\n\n```bash\nnpm install @continuedev/sdk\n```\n\n## Usage\n\nThe SDK provides a `Continue.from()` method that initializes an assistant and returns a client you can use as a drop-in replacement for the OpenAI SDK:\n\n```typescript\nimport { Continue } from \"@continuedev/sdk\";\n\n// Initialize the Continue client with your API key and assistant\nconst { client, assistant } = await Continue.from({\n  apiKey: process.env.CONTINUE_API_KEY,\n  assistant: \"owner-slug/assistant-slug\", // The assistant identifier\n});\n\n// Use the client just like the OpenAI SDK\nconst response = await client.chat.completions.create({\n  model: assistant.getModel(\"claude-3-7-sonnet-latest\"), // Use the assistant's model\n  messages: [\n    { role: \"system\", content: assistant.systemMessage }, // Use the assistant's system message\n    { role: \"user\", content: \"Hello!\" },\n  ],\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\nYou can also use the SDK without specifying an assistant to just get the Continue API client:\n\n```typescript\nimport { Continue } from \"@continuedev/sdk\";\n\n// Initialize just the Continue API client\nconst { api } = await Continue.from({\n  apiKey: process.env.CONTINUE_API_KEY,\n});\n\n// Make calls to the Continue API\nconst assistants = await api.listAssistants({});\n```\n\n## API Reference\n\n### Continue.from(options)\n\nCreates a Continue instance with a pre-configured OpenAI client and assistant.\n\n#### Options\n\n- `apiKey` (string, required): Your Continue API key\n- `assistant` (string, optional): The assistant identifier in the format `owner-slug/assistant-slug`\n- `organizationId` (string, optional): Optional organization ID\n- `baseURL` (string, optional): Base URL for the Continue API (defaults to `https://api.continue.dev/`)\n\n#### Returns\n\nWhen `assistant` is provided, returns an object containing:\n\n- `api`: The Continue API client for direct API access\n- `client`: An OpenAI-compatible client configured to use the Continue API\n- `assistant`: The assistant configuration with utility methods\n\nWhen assistant is not provided, returns an object containing:\n\n- `api`: The Continue API client for direct API access\n"}
{"source":"github","repo":"continue","path":"packages/continue-sdk/typescript/api/README.md","content":"## @continuedev/hub-api@0.0.1\n\nThis generator creates TypeScript/JavaScript client that utilizes [Fetch API](https://fetch.spec.whatwg.org/). The generated Node module can be used in the following environments:\n\nEnvironment\n\n- Node.js\n- Webpack\n- Browserify\n\nLanguage level\n\n- ES5 - you must have a Promises/A+ library installed\n- ES6\n\nModule system\n\n- CommonJS\n- ES6 module system\n\nIt can be used in both TypeScript and JavaScript. In TypeScript, the definition will be automatically resolved via `package.json`. ([Reference](https://www.typescriptlang.org/docs/handbook/declaration-files/consumption.html))\n\n### Building\n\nTo build and compile the typescript sources to javascript use:\n\n```\nnpm install\nnpm run build\n```\n\n### Publishing\n\nFirst build the package then run `npm publish`\n\n### Consuming\n\nnavigate to the folder of your consuming project and run one of the following commands.\n\n_published:_\n\n```\nnpm install @continuedev/hub-api@0.0.1 --save\n```\n\n_unPublished (not recommended):_\n\n```\nnpm install PATH_TO_GENERATED_PACKAGE --save\n```\n"}
{"source":"github","repo":"continue","path":"gui/README.md","content":"# Continue React App\n\nThe Continue React app is a notebook-like interface to the Continue server. It allows the user to submit arbitrary text input, then communicates with the server to takes steps, which are displayed as a sequence of editable cells. The React app should sit beside an IDE, as in the VS Code extension.\n"}
{"source":"github","repo":"continue","path":"gui/rules.md","content":"- Whenever adding links in the `gui` that direct to `hub.continue.dev`, you should use an onClick handler that calls `ideMessenger.request(\"controlPlane/openUrl\", { path, orgSlug: undefined });` instead of directly linking to the URL with an `href`.\n"}
{"source":"github","repo":"continue","path":".github/pull_request_template.md","content":"## Description\n\n[ What changed? Feel free to be brief. ]\n\n## AI Code Review\n\n- **Team members only**: AI review runs automatically when PR is opened or marked ready for review\n- Team members can also trigger a review by commenting `@continue-review`\n\n## Checklist\n\n- [] I've read the [contributing guide](https://github.com/continuedev/continue/blob/main/CONTRIBUTING.md)\n- [] The relevant docs, if any, have been updated or created\n- [] The relevant tests, if any, have been updated or created\n\n## Screen recording or screenshot\n\n[ When applicable, please include a short screen recording or screenshot - this makes it much easier for us as contributors to review and understand your changes. See [this PR](https://github.com/continuedev/continue/pull/6455) as a good example. ]\n\n## Tests\n\n[ What tests were added or updated to ensure the changes work as expected? ]\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/tasks.mdx","content":"---\ntitle: \"Tasks\"\ndescription: \"A Task is a unit of work shared between you and an agent. You trigger the task, the agent executes it, then you review or approve the results.  Tasks can run **on demand**, **on triggers**, or **on schedules**.\"\n---\n\n<Info>\n\n  **What is a Task?**\n\n  A Task is a unit of work that combines:\n  - Your instructions\n  - An agent's capabilities (model, rules, tools, prompt)\n  - Your codebase (repository and branch)\n\n  Tasks can be triggered manually, by events, or on a schedule, and can be assigned to anyone on your team for review.\n\n</Info>\n\n## Creating and Running a Task\n\n<Steps>\n\n  <Step title=\"Choose an Agent\">\n\n    Select the **Agent** you want to use for this task.\n\n    <Tip>\n\n      Don't have an Agent yet? Create one in **[Mission Control ‚Üí Agents](https://hub.continue.dev/new?type=agent)** before setting up your first task.\n\n    </Tip>\n\n  </Step>\n\n  <Step title=\"Select Your Repository\">\n\n    Pick the **repository** and **branch** where the Agent will work.\n\n\n  </Step>\n\n  <Step title=\"Describe the Task\">\n\n    Add your user input ‚Äî clear instructions for what you want the agent to accomplish.\n\n    **Example prompts:**\n\n    ```text\n    Refactor unused React components and clean up imports\n    ```\n\n    ```text\n    Add error handling to all API endpoints in the /api directory\n    ```\n\n    ```text\n    Update README with installation instructions for the new CLI tool\n    ```\n\n  </Step>\n\n  <Step title=\"Run the Task\">\n\n    Click the Submit button.\n\n    The Agent will:\n    - Analyze your repository and branch\n    - Execute the task based on your input\n    - Generate changes based on your task request\n    - Create a detailed session log\n\n    <Info>\n\n      Task execution time varies based on complexity. You can monitor progress in your inbox.\n\n    </Info>\n\n  </Step>\n\n  <Step title=\"Review Results\">\n\n    Each Task run creates a Session with complete traceability.\n  \n    <AccordionGroup>\n      <Accordion title=\"Summary\">\n\n        A high-level explanation of:\n        - What the Agent did\n        - Why decisions were made\n        - Key changes implemented\n\n      </Accordion>\n\n      <Accordion title=\"Diff\">\n\n        The exact code changes:\n            - Files modified\n            - Lines added/removed\n            - Side-by-side comparison\n\n      </Accordion>\n\n      <Accordion title=\"Logs\">\n\n        Full execution trace:\n            - Tool calls made\n            - Agent reasoning\n            - Any errors or warnings\n\n      </Accordion>\n\n    </AccordionGroup>\n\n\n    <Tip>\n\n      Review the diff carefully before accepting changes. You can:\n      - Accept all changes\n      - Request modifications\n      - Reject and retry with an updated input\n\n    </Tip>\n\n  </Step>\n\n</Steps>\n\n## Next Steps\n\n\n  <Card title=\"Create a Workflow\" icon=\"workflow\" href=\"/mission-control/workflows\">\n\n    Automate tasks with event-based triggers\n\n  </Card>\n\n \n"}
{"source":"github","repo":"continue","path":"docs/mission-control/secrets/secret-types.mdx","content":"---\ntitle: \"Secret Types\"\ndescription: \"The Continue Mission Control comes with secrets management built-in. Secrets are values such as API keys or endpoints that can be shared across configurations and within organizations.\"\n---\n\n## User secrets\n\nUser secrets are defined by the user for themselves. This means that user secrets are available only to the user that created them. User secrets are assumed to be safe for the user to know, so they will be sent to the IDE extensions alongside `config.yaml`.\n\nThis allows API requests to be made directly from the IDE extensions. You can use user secrets with [Solo](/hub/governance/pricing#solo), [Teams](/hub/governance/pricing#teams), and [Enterprise](/hub/governance/pricing#enterprise). User secrets can be managed [here](https://hub.continue.dev/settings/secrets) in Mission Control.\n\n## Org secrets\n\nOrg secrets are defined by admins for their organization. Org secrets are available to anyone in the organization to use with configurations in that organization. Org secrets are assumed to not be shareable with the user (e.g. you are a team lead who wants to give team members access to models without passing out API keys).\n\nThis is why LLM requests are proxied through api.continue.dev / on-premise proxy and secrets are never sent to the IDE extensions. You can only use org secrets on [Teams](/hub/governance/pricing#teams) and [Enterprise](/hub/governance/pricing#enterprise). If you are an admin, you can manage secrets for your organization from the org settings page.\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/secrets/secret-resolution.mdx","content":"---\ntitle: \"Secret Resolution\"\ndescription: \"User or Org secrets should be used depending on how users want them to be shared within their organization and configs.\"\n---\n\nFor individual users and [Solo](/hub/governance/pricing#solo) organizations, secret resolution is performed in the following order:\n\n1. User [models add-on](/hub/governance/pricing#models-add-on) (if subscribed)\n2. [User secrets](/hub/secrets/secret-types#user-secrets) (if set)\n3. [Free trial](/hub/governance/pricing#free-trial) (if below limit)\n\nFor [Teams](/hub/governance/pricing#teams) and [Enterprise](/hub/governance/pricing#enterprise) organizations, secret resolution is performed in the following order:\n\n1. Org [models add-on](/hub/governance/pricing#models-add-on) (if subscribed)\n2. [Org secrets](/hub/secrets/secret-types#org-secrets) (if set)\n3. [User secrets](/hub/secrets/secret-types#user-secrets) (if set)\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/integrations/sentry.mdx","content":"---\ntitle: \"Sentry Integration\"\ndescription: \"Automatically triage and resolve errors with Continue Agents connected to Sentry\"\n---\n\n## Overview\n\nConnect Sentry to Continue Mission Control to enable agents to automatically detect, analyze, and resolve production errors. When Sentry is enabled, Continue can generate PRs to fix issues, analyze error patterns, and maintain application health.\n\n<Card title=\"What You Can Do with Sentry Integration\" icon=\"shield-exclamation\">\n\n  - Automatically generate PRs to resolve new errors\n  - Analyze error patterns and identify root causes\n  - Prioritize issues based on impact and frequency\n  - Create detailed bug reports with stack traces\n  - Monitor error trends across releases\n\n</Card>\n\n## Setup\n\n<Steps>\n\n  <Step title=\"Navigate to Integrations\">\n\n    Go to your [Integrations Settings](https://hub.continue.dev/integrations).\n  </Step>\n\n  <Step title=\"Connect Sentry\">\n\n    Click \"Connect\" and select Sentry. You'll need the following credentials:\n\n    - **Sentry Organization Slug**: Your organization name (e.g., \"my-company\")\n    - **Auth Token**: Internal integration token from Sentry\n    - **Client Secret**: For webhook signature verification\n\n  </Step>\n\n  <Step title=\"Configure Sentry Internal Integration\">\n\n    In your Sentry account:\n    1. Create a new [internal integration](https://docs.sentry.io/product/integrations/integration-platform/)\n    2. Set the webhook URL to `https://api.continue.dev/webhooks/sentry`\n    3. Select \"Read and Write\" access for \"Issue and Event\"\n    4. Select \"issue\" in the webhooks section and click Save Changes\n    5. Copy the Auth Token and Client Secret\n\n  </Step>\n\n  <Step title=\"Add Credentials to Mission Control\">\n\n    Paste all three values into the integration form and click \"Create Connection\"\n\n  </Step>\n\n</Steps>\n\n<Tip>\n\n  **How to get these credentials:**\n  1. Create a new **internal integration** in Sentry\n  2. Set the webhook URL to `https://api.continue.dev/webhooks/sentry`\n  3. Select \"Read and Write\" access for \"Issue and Event\"\n  4. Select \"issue\" in the webhooks section and click Save Changes\n  5. Find your organization slug in the Sentry URL (your-slug.sentry.io)\n  6. Copy the Auth Token and Client Secret from the integration\n\n</Tip>\n\n## Running Sentry Agents in Mission Control\n\nYou can run Sentry-connected agents in two ways:\n\n### 1. Manual Tasks\n\nTrigger agents on-demand for error analysis:\n\n1. Go to [Mission Control Agents](https://hub.continue.dev/agents)\n2. Select or create a Sentry-enabled agent\n3. Click \"Run Agent\" and provide your task description\n4. Monitor progress and review results in real-time\n\n**Example Tasks:**\n- \"Analyze the top 10 errors from the last 24 hours\"\n- \"Create a PR to fix the authentication timeout error\"\n- \"Generate a report on errors affecting mobile users\"\n\n### 2. Automated Workflows\n\nSet up agents to run automatically:\n\n- **Webhook-triggered**: Execute when new Sentry errors occur\n- **Scheduled**: Run daily or weekly error analysis\n- **Threshold-based**: Trigger when error rates exceed limits\n\n<Tip>\n\n  Start with manual tasks to refine your prompts, then convert successful workflows to automations for continuous error monitoring.\n\n</Tip>\n\n## Integration with GitHub\n\nCombine Sentry with GitHub integration for a complete workflow:\n\n<Steps>\n\n  <Step title=\"Connect Both Integrations\">\n\n    Enable both Sentry and GitHub integrations in Mission Control\n\n  </Step>\n\n  <Step title=\"Create a Unified Agent\">\n\n    Build an agent that:\n    - Receives Sentry error notifications\n    - Analyzes the error and finds the problematic code\n    - Creates a PR with a fix\n    - Adds the PR link to the Sentry issue\n\n  </Step>\n\n  <Step title=\"Set Up Automated Workflow\">\n\n    Configure the agent to run automatically on new Sentry issues\n\n  </Step>\n\n</Steps>\n\n## Monitoring Agent Activity\n\nTrack your agent's error resolution performance:\n\n1. **View in Mission Control**: See all agent runs and their outcomes\n2. **Check Sentry**: Verify that issues are being resolved\n3. **Review PRs**: Ensure quality of generated fixes\n4. **Monitor Metrics**: Track resolution time and success rate\n\n\n## Troubleshooting\n\n<Accordion title=\"Webhooks not triggering\">\n\n  **Problem**: Agent isn't running when new errors occur\n\n  **Solutions**:\n  - Verify webhook URL is correct in Sentry\n  - Check that \"issue\" is selected in webhook events\n  - Ensure Client Secret matches in both Sentry and Mission Control\n  - Review webhook delivery logs in Sentry\n\n</Accordion>\n\n<Accordion title=\"Agent can't access Sentry data\">\n\n  **Problem**: Agent returns errors when trying to fetch issue details\n\n  **Solutions**:\n  - Verify Auth Token has \"Read and Write\" access for \"Issue and Event\"\n  - Check that organization slug is correct\n  - Ensure token hasn't expired\n  - Confirm project permissions in Sentry\n\n</Accordion>\n\n<Accordion title=\"PRs not linking to Sentry issues\">\n\n  **Problem**: Generated PRs don't reference the Sentry issue\n\n  **Solutions**:\n  - Ensure both Sentry and GitHub integrations are connected\n  - Verify agent has permission to access both services\n  - Check that the agent prompt includes instructions to link issues\n  - Review agent logs for errors\n\n</Accordion>\n\n## Support & Resources\n\n<CardGroup cols={2}>\n\n  <Card title=\"Sentry MCP Cookbook\" icon=\"book-open\" href=\"/guides/sentry-mcp-error-monitoring\">\n\n    Comprehensive guide to using Sentry with Continue agents\n\n  </Card>\n\n  <Card title=\"GitHub Integration\" icon=\"github\" href=\"/mission-control/integrations/github\">\n\n    Combine Sentry with GitHub for complete automation\n\n  </Card>\n\n</CardGroup>\n\n\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/integrations/index.mdx","content":"---\ntitle: \"Overview\"\ndescription: \"Connect GitHub, Slack, Sentry, and Snyk to power richer Agents, Tasks, and Workflows.\"\nsidebarTitle: \"Overview\"\n---\n\n<Info>\nIntegrations let Continue connect to the tools you already use, so Agents can read code, open pull requests, send messages, react to real-world events, and more.\n</Info>\n\nMission Control currently supports four first-class integrations:\n\n<CardGroup cols={2}>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"/mission-control/integrations/github\">\n    Required for PR creation, repo access, diffs, and workflow triggers.\n  </Card>\n\n  <Card title=\"Slack\" icon=\"slack\" href=\"/mission-control/integrations/slack-agent\">\n    Mention @continue to kick off Agents directly from Slack.\n  </Card>\n\n  <Card title=\"Sentry\" icon=\"alert-triangle\" href=\"/mission-control/integrations/sentry\">\n    Trigger Agents automatically when new Sentry issues appear.\n  </Card>\n\n  <Card title=\"Snyk\" icon=\"shield-check\" href=\"/mission-control/integrations/snyk\">\n    Detect and fix security vulnerabilities automatically.\n  </Card>\n\n</CardGroup>\n\n\n## Next Steps\n\n  <Card title=\"Try out an Integration\" icon=\"laptop\" href=\"https://hub.continue.dev/integrations\">\n  </Card>"}
{"source":"github","repo":"continue","path":"docs/mission-control/integrations/slack-agent.mdx","content":"---\ntitle: \"Slack Agent\"\ndescription: \"Kick off background Agents from Slack by mentioning @Continue\"\n---\n\n<Warning>\n\n  The Continue Slack app is currently in beta development and undergoing Slack's approval process.\n\n</Warning>\n\n## Overview\n\nMention @Continue in any channel with a task description, and it will:\n\n- Create a new development agent\n- Clone and work on your Repository\n- Generate code changes and create pull requests\n- Keep you updated on progress\n\n<Card title=\"What It's Great For\" icon=\"slack\">\n\n  - Quick bug fixes and feature implementations\n  - Code reviews and refactoring\n  - Documentation updates\n  - Automated development tasks\n\n</Card>\n\n## Setup\n\nContinue's Slack Bot can be installed to a Slack workspace via Mission Control, from:[Integrations Settings](https://hub.continue.dev/integrations)\n- Org Integrations Settings - `https://hub.continue.dev/organizations/{your-org}/settings/integrations`\n\n<Warning>\n\n  Continue's Slack app is in Beta development. During installation, you will see a warning that it is not yet approved/official.\n\n</Warning>\n\n<Steps>\n\n  <Step title=\"Connect GitHub\">\n\n    Click \"Connect\" next to GitHub in the Integrations settings, and grant access to the repositories you want Continue to work with.\n\n  </Step>\n\n  <Step title=\"Authorize Slack\">\n\n    Click the \"Connect\" button next to Slack and authorize the app in your workspace.\n\n  </Step>\n\n  <Step title=\"Set Default Repository (Optional)\">\n\n    Use the repository selector within the Slack tile to select a default repository for Continue Agents to work in.\n\n  </Step>\n\n</Steps>\n\n## Usage\n\nTo kick off an agent, mention `@Continue` with a short task description.\n\n### Thread Context\n\nWhen you mention `@Continue` in a **Slack thread**, Continue automatically uses the entire conversation as context. This means your agent starts with full situational awareness.\n\nIf you start a **new message** (not a reply), Continue only sees that single message for context.  \nUse this for quick standalone requests.\n\n<Tip>\n\n  Provide clear, detailed prompts to help Continue understand your requirements:\n\n  ‚úÖ **Good**: `@Continue Fix the authentication timeout issue in /api/v1/auth/login. Users are getting logged out after 5 minutes instead of 30.`\n\n  ‚ùå **Poor**: `@Continue fix auth`\n\n</Tip>\n\n<Accordion title=\"Repository Selection\">\n\n  Continue determines which repository to work in with the following precedence:\n\n  1. **Specify in message**: Add `repo=repo-name` to your message:\n\n  ```\n  @Continue repo=my-app Fix the login validation bug\n  ```\n\n  <Tip>\n\n    Be sure the repository name matches a GitHub repository that the Continue GitHub App has access to.\n\n  </Tip>\n\n  2. **Default repo**: Uses the default repository if set in the integration settings\n\n  3. **Interactive selection**: If no repository is found, Continue will follow up with a dropdown to select a repository\n\n</Accordion>\n\n### Sending Follow-up Messages\n\nOnce an agent is created, you can send additional messages by mentioning `@Continue` in the same thread:\n\n```\n@Continue Also add error handling for edge cases\n```\n\nContinue will forward your message to the existing agent session instead of creating a new one. \n\n## Monitoring Agent Progress\n\nClick the agent link in Slack to view the agent's progress in [Mission Control](https://hub.continue.dev/agents).\n\nYou can also click the Slack icon in the agents page to return to the Slack message.\n\n## Disconnecting\n\nTo remove the Slack integration:\n\n1. Go to [hub.continue.dev/settings/integrations](https://hub.continue.dev/settings/integrations)\n2. Find your Slack integration\n3. Click \"Disconnect\"\n\n## Support\n\n<Warning>\n\n  The Slack integration is in active development. We'd love to hear your feedback:\n  - Report issues: [GitHub Issues](https://github.com/continuedev/continue/issues)\n  - Feature requests: [GitHub Discussions](https://github.com/continuedev/continue/discussions)\n\n</Warning>\n\n## Related Resources\n\n<CardGroup cols={2}>\n\n  <Card title=\"Agents Introduction\" icon=\"robot\" href=\"../../agents/intro\">\n\n    Learn about Continue agents\n\n  </Card>\n\n  <Card title=\"GitHub Integration\" icon=\"github\" href=\"./github\">\n\n    Add the GitHub Integration to create issues and PRs\n\n  </Card>\n\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/integrations/snyk.mdx","content":"---\ntitle: \"Snyk Integration\"\ndescription: \"Automatically detect and fix vulnerabilities with Continue Agents connected to Snyk\"\n---\n\n## Overview\n\nConnect Snyk to Continue Mission Control to enable agents to automatically detect, analyze, and resolve security vulnerabilities. When Snyk is enabled, Continue can generate PRs to fix vulnerabilities, analyze security patterns, and maintain application security posture.\n\n<Card title=\"What You Can Do with Snyk Integration\" icon=\"shield-check\">\n\n  - Automatically generate PRs to fix security vulnerabilities\n  - Analyze dependency risks and suggest updates\n  - Monitor security posture across projects\n  - Create detailed vulnerability reports with remediation steps\n  - Track vulnerability trends and compliance status\n\n</Card>\n\n## Setup\n\n<Steps>\n\n  <Step title=\"Navigate to Integrations\">\n\n    Go to your [Integrations Settings](https://hub.continue.dev/integrations).\n\n  </Step>\n\n  <Step title=\"Connect Snyk\">\n\n    Click \"Connect\" next to Snyk integration.\n\n  </Step>\n\n  <Step title=\"Authorize with Snyk\">\n\n    Click the \"Connect with Snyk\" button to authorize Continue Mission Control:\n    \n    - You'll be redirected to Snyk's OAuth authorization page\n    - Review and approve the requested permissions\n    - You'll be automatically redirected back to the [Snyk inbox view](https://hub.continue.dev/inbox?view=snyk)\n\n  </Step>\n\n  <Step title=\"Verify Connection\">\n\n    Once redirected back, you'll see:\n    - A confirmation that Snyk is connected\n    - Your Snyk projects available for monitoring\n    - Any active vulnerability alerts\n\n  </Step>\n\n</Steps>\n\n<Info>\n\n  **OAuth Authorization**: No manual token configuration needed! Simply click the authorization button and Continue will securely handle the authentication flow with Snyk.\n\n</Info>\n\n## Integration with GitHub\n\nCombine Snyk with GitHub integration for a complete security workflow:\n\n<Steps>\n\n  <Step title=\"Connect Both Integrations\">\n\n    Enable both Snyk and GitHub integrations in Mission Control\n\n  </Step>\n\n  <Step title=\"Create a Security Agent\">\n\n    Build an agent that:\n    - Receives Snyk vulnerability alerts\n    - Analyzes the security issue in your codebase\n    - Creates a PR with the security fix\n    - Adds security impact analysis to the PR\n\n  </Step>\n\n  <Step title=\"Set Up Automated Security Workflow\">\n\n    Configure the agent to run automatically on new critical vulnerabilities\n\n  </Step>\n\n</Steps>\n\n## Monitoring Agent Activity\n\nTrack your agent's security remediation performance:\n\n1. **View in Inbox**: Navigate to the [Snyk view](https://hub.continue.dev/inbox?view=snyk) to see all Snyk-related agent activity\n2. **Check Snyk Dashboard**: Verify that vulnerabilities are being addressed\n3. **Review PRs**: Ensure quality and security of generated fixes\n4. **Monitor Metrics**: Track mean time to remediation (MTTR) and fix rate\n\n## Troubleshooting\n\n<Accordion title=\"OAuth authorization fails\">\n\n  **Problem**: Can't complete the Snyk authorization flow\n\n  **Solutions**:\n  - Ensure you have appropriate permissions in your Snyk organization\n  - Check if your organization allows OAuth integrations\n  - Try logging out and back into Snyk before authorizing\n  - Clear browser cookies and try again\n\n</Accordion>\n\n<Accordion title=\"Projects not appearing\">\n\n  **Problem**: Snyk projects aren't visible after connection\n\n  **Solutions**:\n  - Verify you have access to projects in Snyk\n  - Check that projects are properly imported in Snyk\n  - Refresh the integration connection\n  - Ensure the OAuth scope includes project access\n\n</Accordion>\n\n<Accordion title=\"Agent can't access vulnerability data\">\n\n  **Problem**: Agent returns errors when trying to fetch Snyk data\n\n  **Solutions**:\n  - Verify the OAuth connection is still valid\n  - Re-authorize if the token has expired\n  - Check that the agent has the necessary Snyk context provider\n  - Review agent logs for specific error messages\n\n</Accordion>\n\n<Accordion title=\"Webhooks not triggering\">\n\n  **Problem**: Agent isn't running when new vulnerabilities are found\n\n  **Solutions**:\n  - Verify webhook configuration in Snyk integration settings\n  - Check that webhook URL points to Continue Mission Control\n  - Ensure webhook events include vulnerability detection\n  - Review webhook delivery logs in Snyk\n\n</Accordion>\n\n## Support & Resources\n\n<CardGroup cols={2}>\n\n  <Card title=\"Snyk MCP Cookbook\" icon=\"book-open\" href=\"/guides/snyk-mcp-continue-cookbook\">\n\n    Complete guide with security scanning recipes, CI/CD integration, and automated remediation workflows\n\n  </Card>\n\n  <Card title=\"GitHub Integration\" icon=\"github\" href=\"/mission-control/integrations/github\">\n\n    Combine Snyk with GitHub for complete security automation\n\n  </Card>\n\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/integrations/github.mdx","content":"---\ntitle: \"GitHub Integration\"\ndescription: \"Connect to GitHub to allow Continue Agents to read and create PRs on your repositories\"\n---\n\n## Overview\n\nConnect your GitHub account to Continue Mission Control to enable agents to interact with your repositories. Once connected, agents can read code, create pull requests, manage issues, and automate your development workflow.\n\n<Card title=\"What You Can Do with GitHub Integration\" icon=\"github\">\n\n  - Automatically create pull requests from agent tasks\n  - Read and analyze repository code and issues\n  - Manage project workflows and milestones\n  - Automate code reviews and quality checks\n  - Generate release notes and documentation\n\n</Card>\n\n\n## Setup\n\n<Steps>\n\n  <Step title=\"Navigate to Integrations\">\n\n    Go to your [Integrations Settings](https://hub.continue.dev/integrations).\n\n  </Step>\n\n  <Step title=\"Connect GitHub\">\n\n    Click \"Connect\" next to GitHub and authorize the Continue app in your GitHub account.\n\n  </Step>\n\n  <Step title=\"Select Repository Access\">\n\n    Choose whether to grant access to all repositories or select specific ones. You can always modify this later in your GitHub settings.\n\n  </Step>\n\n  <Step title=\"Confirm Permissions\">\n\n    Review and approve the requested permissions. Continue needs these to perform automated tasks on your behalf.\n\n  </Step>\n\n</Steps>\n\n## Use Cases\n\n### Automated Issue Resolution\n\nCreate agents that automatically fix bugs and open PRs:\n\n<Card title=\"Bug Fixer Agent\" icon=\"bug\">\n\n  **Task Example**: \"Review issues labeled 'bug' and create PRs to fix any that are straightforward\"\n\n  **What the Agent Does**:\n  - Scans your repository for issues with the \"bug\" label\n  - Analyzes the bug description and reproduces the issue\n  - Generates a fix and creates a pull request\n  - Adds a link back to the original issue\n\n  **Run in Mission Control**: Create as a scheduled automation or trigger manually\n\n</Card>\n\n### Pull Request Management\n\nAutomate code reviews and PR maintenance:\n\n<Card title=\"PR Review Agent\" icon=\"code-pull-request\">\n\n  **Task Example**: \"Review open PRs for code quality, security issues, and best practices\"\n\n  **What the Agent Does**:\n  - Analyzes code changes in open pull requests\n  - Identifies potential bugs, security vulnerabilities, or style issues\n  - Adds review comments with suggestions\n  - Approves PRs that meet quality standards\n\n  **Run in Mission Control**: Set up as an automation triggered on new PRs\n\n</Card>\n\n### Documentation Generation\n\nKeep your docs up-to-date automatically:\n\n<Card title=\"Docs Updater Agent\" icon=\"book\">\n\n  **Task Example**: \"Update README and API documentation based on recent code changes\"\n\n  **What the Agent Does**:\n  - Detects changes to public APIs or features\n  - Generates updated documentation\n  - Creates a PR with doc updates\n  - Links documentation to related code changes\n\n  **Run in Mission Control**: Schedule to run after releases or major merges\n\n</Card>\n\n### Project Management\n\nAutomate issue triage and project organization:\n\n<Card title=\"Issue Triage Agent\" icon=\"list-check\">\n\n  **Task Example**: \"Triage new issues by adding labels, assigning to appropriate team members, and linking related issues\"\n\n  **What the Agent Does**:\n  - Analyzes new issue descriptions\n  - Applies relevant labels (bug, feature, documentation, etc.)\n  - Suggests team members to assign based on expertise\n  - Identifies and links related issues or PRs\n\n  **Run in Mission Control**: Create as an automation triggered on new issues\n\n</Card>\n\n### Release Management\n\nAutomate your release process:\n\n<Card title=\"Release Notes Generator\" icon=\"rocket\">\n\n  **Task Example**: \"Generate release notes from merged PRs since last release and create a draft release\"\n\n  **What the Agent Does**:\n  - Collects all merged PRs since the last tag\n  - Categorizes changes (features, fixes, breaking changes)\n  - Generates formatted release notes\n  - Creates a draft GitHub release for review\n\n  **Run in Mission Control**: Trigger manually or on a schedule before releases\n\n</Card>\n\n### Code Quality Monitoring\n\nMaintain code standards across your codebase:\n\n<Card title=\"Code Quality Agent\" icon=\"shield-check\">\n\n  **Task Example**: \"Scan the codebase for deprecated API usage and create issues with migration guides\"\n\n  **What the Agent Does**:\n  - Identifies usage of deprecated functions or libraries\n  - Generates migration recommendations\n  - Creates individual issues for each deprecated usage\n  - Provides code examples for the upgrade path\n\n  **Run in Mission Control**: Schedule monthly or after dependency updates\n\n</Card>\n\n## Running GitHub Agents in Mission Control\n\nYou can run GitHub-connected agents in two ways:\n\n### 1. Manual Tasks\n\nTrigger agents on-demand for specific tasks:\n\n1. Go to [Mission Control Agents](https://hub.continue.dev/agents)\n2. Select or create a GitHub-enabled agent\n3. Click \"Run Agent\" and provide your task description\n4. Monitor progress and review results in real-time\n\n### 2. Automated Workflows\n\nSet up agents to run automatically:\n\n- **Scheduled**: Run daily, weekly, or on a custom schedule\n- **Triggered**: Execute when specific events occur (new issues, PR opened, etc.)\n- **Webhook**: Integrate with external services to trigger agents\n\n<Tip>\n\n  Start with manual tasks to test your agent's behavior, then convert successful workflows to automations once you're confident in the results.\n\n</Tip>\n\n\n## Support & Resources\n\n<CardGroup cols={2}>\n\n  <Card title=\"GitHub MCP Cookbook\" icon=\"book-open\" href=\"/guides/github-mcp-continue-cookbook\">\n\n    Comprehensive guide to using GitHub with Continue agents\n\n  </Card>\n\n  <Card title=\"GitHub PR Review Bot\" icon=\"robot\" href=\"/guides/github-pr-review-bot\">\n\n    Automate code reviews with Continue\n\n  </Card>\n\n</CardGroup>\n\n\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/sharing.mdx","content":"---\ntitle: \"Sharing\"\ndescription: \"Connect with the Continue community to discover, share, and collaborate on AI development tools.\"\n---\n\n<Info>\nYou can share any Agent or Component (Models, Rules, Prompts, MCP Servers) by publishing it as Public or Organization-visible.  \nSharing makes it easy for teammates‚Äîor the entire community‚Äîto reuse your best workflows.\n</Info>\n\n## Where to Find Your Shareable Items\n\nYour personal and organization components live in your Profile.\n\nTo access your profile (or your organization‚Äôs profile):\n\n<Steps>\n\n  <Step title=\"Open Mission Control\">\n  </Step>\n\n  <Step title=\"Open the Organization Switcher in the top left sidebar nav\">\n  </Step>\n\n  <Step title=\"Select the Profile Icon\">\n  </Step>\n\n</Steps>\n\n<Note>\nProfiles look the same for personal users and organizations. The only difference is visibility options: personal profiles can create *Public* or *Private* items. Organizations can also create *Organization-only* items.\n</Note>\n\n## Sharing Options\n\nEach Agent or Component can be published with one of three visibility levels:\n\n<CardGroup cols={3}>\n\n  <Card title=\"Personal (Private)\" icon=\"lock\">\n    Only you can see and use the item.  \n    Great for prototypes or experiments.\n  </Card>\n\n  <Card title=\"Organization\" icon=\"users\">\n    Visible to everyone in your organization.  \n    Ideal for internal workflows, standardized agents, and shared tooling.\n  </Card>\n\n  <Card title=\"Public\" icon=\"globe\">\n    Available to everyone in Continue Mission Control.  \n    Perfect for templates, community contributions, and reusable examples.\n  </Card>\n\n</CardGroup>\n\n## Community\n<CardGroup cols={2}>\n  <Card title=\"Join our Discord\" icon=\"people\" href=\"https://discord.gg/vapESyrFmJs\">\n    Join thousands of developers using Continue and share your experiences.\n    </Card>\n  <Card title=\"Share your feedback\" icon=\"globe\" href=\"https://github.com/continuedev/continue/discussions\">\n    Let us know what you think of Mission Control.\n  </Card>\n\n</CardGroup>\n\n\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/metrics.mdx","content":"---\ntitle: \"Metrics\"\ndescription: \"Track agent performance, workflow activity, and PR outcomes.\"\n---\n\n<Info>\nMetrics give you visibility into how your Agents and Workflows perform across your repositories.  \nUse this page to understand activity patterns, identify bottlenecks, and measure success over time.\n</Info>\n\n## What Metrics Show\n\n<AccordionGroup>\n\n  <Accordion title=\"Activity (Last 14 Days)\">\n    A day-by-day graph of total Agent runs across your repositories.\n    - See which Agents are running most often  \n    - Spot spikes, trends, or recurring failures  \n    - Monitor automated Workflows in production  \n  </Accordion>\n\n  <Accordion title=\"Agent Statistics\">\n    Per-Agent breakdown of:\n    - **Total runs**  \n    - **PR creation rate**  \n    - **PR status** (open, merged, closed, failed)  \n    - **Success vs. intervention rate**  \n  </Accordion>\n\n  <Accordion title=\"Workflow Impact (if enabled)\">\n    Track which Workflows create the most tasks or PRs, and how often they complete successfully.\n  </Accordion>\n\n</AccordionGroup>\n\n## Why Metrics Matter\n\n<CardGroup cols={2}>\n\n  <Card title=\"Improve Agent Reliability\" icon=\"line-chart\">\n    Identify which Agents need better rules, tools, or prompts.\n  </Card>\n\n  <Card title=\"Measure Automation Value\" icon=\"bar-chart\">\n    See how much work your automated Workflows are completing across your repos.\n  </Card>\n\n</CardGroup>"}
{"source":"github","repo":"continue","path":"docs/mission-control/workflows.mdx","content":"---\ntitle: \"Workflows\"\ndescription: \"Workflows trigger agents on a schedule or when events occur.\"\n---\n\n\n<Info>\n\n  What is a Workflow?\n\n  A Workflow automatically runs an Agent on a repository when:\n  - A schedule is reached (cron)\n  - An event occurs (webhook, e.g. GitHub PR)\n\n  Use Workflows for recurring or event-driven tasks like:\n  - Daily vulnerability scans\n  - Weekly changelog generation\n  - Reacting to new pull requests or Sentry alerts\n\n</Info>\n\n---\n\n## Creating a Workflow\n\n<Steps>\n\n  <Step title=\"Start from scratch or a template\">\n\n    In Mission Control ‚Üí Workflows, create a [new workflow](https://hub.continue.dev/workflows/new).\n<AccordionGroup>\n\n  <Accordion title=\"Start with a Blank Workflow\">\n    Create a workflow from scratch by clicking Create Workflow.  \n    You'll choose the repository, agent, trigger type, and schedule or webhook.\n  </Accordion>\n\n  <Accordion title=\"Use a Workflow Template\">\n    Select any of the ‚ÄúTry these Workflows‚Äù cards such as:\n    - _Update agents.md_\n    - _Draft Changelog Updates_\n    - _Solve Snyk Vulnerabilities_\n\n    Templates preload the recommended agent and schedule so you can customize and run them quickly.\n  </Accordion>\n\n</AccordionGroup>\n\n  </Step>\n\n  <Step title=\"Name the Workflow\">\n\n    Give your Workflow a clear, descriptive name.\n\n    Examples:\n    - `Morning status check`\n    - `Weekly changelog draft`\n    - `Daily Snyk vulnerability scan`\n\n  </Step>\n\n  <Step title=\"Select Repository & Agent\">\n\n    Choose:\n\n    - Repository ‚Äì the codebase the Agent will run against  \n    - Agent ‚Äì the configured Agent (model, rules, tools, prompt) that should perform the work\n\n    <Tip>\n      Need a new Agent? Create one in [Mission Control ‚Üí Agents](https://hub.continue.dev/new?type=agent), then return to this screen.\n    </Tip>\n\n  </Step>\n\n  <Step title=\"Choose Trigger Type\">\n\n    Pick how this Workflow should run:\n\n    - Cron ‚Äì run on a schedule (daily, weekly, hourly, custom)  \n    - Webhook ‚Äì run when an external system sends a request (e.g. GitHub PR event)\n\n  </Step>\n\n  <Step title=\"Configure the Trigger\">\n\n    <AccordionGroup>\n\n      <Accordion title=\"Cron Workflow\">\n\n        For scheduled Workflows:\n\n        1. Select Cron as the trigger type  \n        2. Choose a schedule:\n           - Common presets (e.g. weekdays at 9 AM UTC)\n           - Hourly, Daily, Weekly, Monthly\n           - Or define a Custom expression  \n        3. (Optional) Add additional instructions for this Workflow run in the text area.\n\n        Example use cases:\n        - Run a morning status check on your repo every weekday  \n        - Refactor React components once per week  \n        - Generate a weekly changelog from recent commits  \n\n      </Accordion>\n\n      <Accordion title=\"Webhook Workflow\">\n\n        For event-based Workflows:\n\n        1. Select Webhook as the trigger type  \n        2. (Optional) Set a Secret Header (e.g. `X-Webhook-Secret`)  \n        3. (Optional) Set a Secret Value (token your external system must send)  \n\n        You‚Äôll receive a webhook URL to call from:\n        - GitHub Actions\n        - GitHub webhooks\n        - CI/CD pipelines\n        - Any external system that can send HTTP requests\n\n        Example use cases:\n        - Run an Agent whenever a pull request is opened  \n        - Kick off a workflow when Snyk or Sentry detects an issue  \n\n      </Accordion>\n\n    </AccordionGroup>\n\n  </Step>\n\n  <Step title=\"Create the Workflow\">\n\n    Click Create Cron Workflow or Create Webhook Workflow, depending on your trigger type.\n\n    Your Workflow will now appear in the Workflows list and begin running based on its configured schedule or events.\n\n  </Step>\n\n</Steps>\n\n## Monitoring Workflow Runs\n\nWorkflow runs show up just like Tasks:\n\n- Each execution creates a Session  \n- Workflows appear in your Inbox  \n- You can open the detail view to see the summary, diff, and logs.\n\n<CardGroup>\n  <Card title=\"Create a Workflow Now\" icon=\"workflow\" href=\"https://hub.continue.dev/workflows\">\n  </Card>\n\n  <Card title=\"Add an Integration\" icon=\"workflow\" href=\"/mission-control/integrations\">\n    Connect GitHub, Slack, and Sentry for richer triggers and actions.\n  </Card>\n\n</CardGroup>"}
{"source":"github","repo":"continue","path":"docs/mission-control/configs/create-a-config.mdx","content":"---\ntitle: \"How to Create a Config\"\ndescription: \"Learn how to create custom AI coding configs in Continue Mission Control by remixing existing configs or building new ones from scratch with reusable blocks and YAML configuration.\"\nsidebarTitle: \"Create a Config\"\n---\n\n## How to Create a Config from Scratch\n\nTo create an config from scratch, select ‚ÄúNew config‚Äù in the top bar.\n\nChoose a name, slug, description, and icon for your config.\n\nThe easiest way to create an config is to click \"Create config\" with the default configuration and then add / remove blocks using the sidebar.\n\nAlternatively, you can edit the config YAML directly before clicking \"Create config\". Refer to examples of configs on [hub.continue.dev](https://hub.continue.dev) and visit the [YAML Reference](/reference#complete-yaml-config-example) docs for more details.\n\n\n\n## How to Remix a Config\n\nYou can also create an config by remixing an existing one. This is useful if you want to start with a pre-configured config and make modifications.\n\nBy clicking the ‚Äúremix‚Äù button, you‚Äôll be taken to the ‚ÄúCreate a remix‚Äù page.\n\nOnce here, you‚Äôll be able to\n\n1. add or remove components from the config.\n2. change the name, description, icon, etc.\n\nClicking ‚ÄúCreate config‚Äù will make this config available for use.\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/configs/edit-a-config.mdx","content":"---\ntitle: \"How to Edit a Config\"\ndescription: \"New versions of a config can be created and published using the sidebar.\"\n---\n\nFirst, select a config either from Mission Control search or one that you've installed.\n\nWhile editing a config, you can explore Mission Control and click \"+\" to add it to your config.\n\nFor tools or models that require secret values like API keys, you will see a small notification on its tile that will indicate if action is needed.\n\n\nTo delete part of your config, you can remove it from the yaml.\n\nIf a rule, prompt, or tool you want to use does not exist yet, you can [create it](/hub/introduction#creating-components).\n\nWhen you are done editing, click \"Publish\" to publish a new version of the config.\n\nReload your terminal or IDE to use the latest version of the config in your agents."}
{"source":"github","repo":"continue","path":"docs/mission-control/configs/intro.mdx","content":"---\ntitle: \"Introduction to Configs\"\ndescription: \"Custom configuration options include Models, MCP Servers, Rules, Prompts, etc.\"\nsidebarTitle: \"Configs\"\n---\n\n<Info>\n  **New UI Update: The Hub is now Mission Control.**\n  If you're returning after the redesign, you can now find and manage configs in your [Mission Control Settings](https://hub.continue.dev/settings/configs).\n</Info>\n\n## What Are Configs?\n\nConfigs are flexible containers that combine multiple components to create powerful AI coding experiences using the Continue CLI or the Extensions. A single config can include:\n\n<CardGroup cols={2}>\n\n  <Card title=\"Models\" icon=\"cube\">\n    \n    Large Language Models for chat, autocomplete, editing, and more\n    \n  </Card>\n  \n  <Card title=\"MCPs\" icon=\"wrench\">\n    \n    Model Context Protocol servers that provide tools and capabilities\n    \n  </Card>\n  \n  <Card title=\"Rules\" icon=\"shield\">\n    \n    Guidelines that shape AI behavior and responses\n    \n  </Card>\n  \n  <Card title=\"Prompts\" icon=\"message-square\">\n    \n    Reusable instructions for common coding tasks\n    \n  </Card>\n\n</CardGroup>\n\n## How Configs Work\n\nThink of configs as recipes for AI assistance. Each config defines:\n\n- **What models** to use for different tasks (chat, autocomplete, code editing)\n- **Which tools** are available through MCP servers\n- **How the AI behaves** through rules and guidelines  \n- **Pre-built prompts** for common workflows\n\nThis flexibility lets you create specialized setups for different contexts, like a Next.js config with React-specific rules and tools, or a data science config with Python analysis capabilities.\n\n## Config Permissions\n\nWhen creating configs, you can set visibility levels:\n\n- **Personal**: Only you can see and use the config\n- **Public**: Anyone can discover and use your config\n- **Organization**: Members of your organization can access the config\n\n## Working with Configs\n\nYou can interact with configs in three main ways:\n\n- **Create**: Build a new config from scratch or start with a template\n- **Edit**: Modify your existing configs by adding/removing components\n- **Remix**: Take someone else's config and customize it for your needs\n\n## Getting Started\n\n<Steps>\n\n  <Step title=\"Explore existing configs\">\n    \n    Browse the [Continue Mission Control](https://hub.continue.dev/hub?type=configs) to see what configs others have built\n    \n  </Step>\n  \n  <Step title=\"Add a config to your account\">\n    \n    Click \"Install Config\" (+) on any config that interests you\n    \n  </Step>\n  \n  <Step title=\"Customize as needed\">\n    \n    Add your API keys and customize components to match your workflow\n    \n  </Step>\n  \n  <Step title=\"Use in your IDE\">\n    \n    Select the config from the dropdown in your Continue extension, type `/config` in the CLI, or use `--config` with the CLI commands\n    \n  </Step>\n\n</Steps>\n\n## Config Format\n\nAll configs follow the [`config.yaml`](/reference) format, whether you're using Mission Control interface or editing files locally. This ensures consistency between Mission Control-managed and local configurations.\n\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/configs/use-a-config.mdx","content":"---\ntitle: \"How to Use a Custom Config\"\ndescription: \"Learn how to add and use a custom configuration in Continue, including setting required inputs and selecting it in your IDE extension.\"\n---\n\n## Steps to use a custom configuration in Continue\n\nOnce you've found the configuration you want to use on Continue Mission Control:\n\n1. Click ‚ÄúAdd Config‚Äù on its page\n2. Add any required inputs (e.g. secrets like API keys)\n\nAfter saving, open the Continue CLI or IDE extension and\n\n- Select the configuration from the **config dropdown** in the Continue extension\n- Type `/config` and select the configuration in the Continue CLI\n- Begin using it for chat mode, agent mode, or other configured capabilities.\n\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/governance/pricing.mdx","content":"---\ntitle: \"Pricing\"\ndescription: \"Continue Mission Control pricing plans for individuals, teams, and enterprises, including the Models Add-On that provides access to frontier AI models for a flat monthly fee\"\n---\n\n## Solo\n\n**Solo** is best suited for individuals and small teams with \"single-player\" problems.\n\nYou can read more about what **Solo** includes [here](https://hub.continue.dev/pricing).\n\n## Teams\n\n**Teams** is best suited for growing teams with \"multiplayer\" problems.\n\nYou can read more about what **Teams** includes [here](https://hub.continue.dev/pricing).\n\n## Enterprise\n\n**Enterprise** is best suited for large teams with enterprise-grade requirements.\n\nYou can read more about what **Enterprise** includes [here](https://hub.continue.dev/pricing).\n\n## Models Add-On\n\nThe **Models Add-On** allows you to use a variety of frontier models for a flat monthly fee. It‚Äôs designed to cover the usage of most developers.\n\nYou can read more about usage limits and what models are included [here](https://hub.continue.dev/pricing).\n\n### Free Trial\n\nTo try out Continue, we offer a free trial of the **Models Add-On** that allows you to use 50 Chat requests and 2,000 autocomplete requests.\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/governance/org-permissions.mdx","content":"---\ntitle: \"Organization Permissions\"\ndescription: \"Users can have the following roles within an organization:\"\n---\n\n1. Admins are users who can manage members, secrets, blocks, configs, etc.\n2. Members are users who can use configs, blocks, secrets, etc.\n\n**User permissions for each role depend on the pricing plan:**\n\n- [Solo](/hub/governance/pricing#solo)\n- [Teams](/hub/governance/pricing#teams)\n- [Enterprise](/hub/governance/pricing#enterprise)\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/governance/creating-an-org.mdx","content":"---\ntitle: \"Creating an Organization\"\ndescription: \"To Create an Organization, click the organization selector in the top right and select\"\n---\n\n![create an org selector](/images/hub/governance/images/org-selector-4ea627afc7c0765633780920a0fbe16e.png)\n\n1. Choose a name, which will be used as the display name for your organization throughout Mission Control\n2. Add a slug, which will be used for your org URL and as the prefix to all organization configuration slugs\n3. Select an icon for your organization using the image uploader\n4. Finally, add a Biography, which will be displayed on your org Home Page\n\n![Creating an org](/images/hub/governance/images/create-org-form-1c7655050d8f423b0ecdfbd1cc85e0af.png)\n\nYou will then be signed in to your org and taken to the org home page\n\n![Org home page](/images/hub/governance/images/org-home-page-4509e2ff8c4e1d6325c8f0afae3b16dd.png)\n"}
{"source":"github","repo":"continue","path":"docs/mission-control/index.mdx","content":"---\ntitle: \"Continue Mission Control\"\ndescription: \"Your command center for Continue. Create Tasks, set up Workflows, manage Agents, connect Integrations, and share with your team‚Äîall in one place.\"\nsidebarTitle: \"Overview\"\n---\n\n## What You Can Do in Mission Control\n\nMission Control is where developers and teams use Continue to power everyday coding workflows. From quick one-off Tasks to fully automated pipelines, everything starts here.\n\n<CardGroup cols={4}>\n  <Card title=\"Create & Run Tasks\" icon=\"list\" href=\"/mission-control/tasks\">\n    Trigger your AI Agents and review or approve the results. \n  </Card>\n   <Card title=\"Manage Your Agents\" icon=\"robot\" href=\"/agents/intro\">\n        Create, configure, and monitor your custom Agents across repositories.\n  </Card>\n <Card title=\"Set Up Workflows\" icon=\"clock\" href=\"/mission-control/workflows\">\n    Schedule recurring Tasks, connect webhooks to trigger Agents automatically, or add \n  </Card>\n    <Card title=\"Connect Integrations\" icon=\"plug\" href=\"/mission-control/integrations\">\n    Link GitHub, Slack, and Sentry to power your Tasks and Workflows.\n  </Card>\n</CardGroup>\n\n## Inbox: One Place to Triage, Assign, and Take Action\n\n<Info>\nThe Inbox is your team's central queue for all AI-powered Tasks, agent activity, and first-class integrations like Sentry and Snyk. It gives developers a shared place to review work, delegate responsibility, and launch follow-up Tasks‚Äîall without leaving Mission Control.\n</Info>\n\n<CardGroup cols={4}>\n  <Card title=\"Unified View\" icon=\"list\">\n    See every incoming Task, Sentry alert, Snyk vulnerability, and workflow output in one place.\n  </Card>\n\n  <Card title=\"Powerful Filtering\" icon=\"filter\">\n    Slice the Inbox by Agent, repository, integration, creator, status, or time to surface what matters now.\n  </Card>\n\n  <Card title=\"Clear Ownership\" icon=\"user-check\">\n    Assign Sessions to teammates so responsibilities are explicit and reviews move forward.\n  </Card>\n\n  <Card title=\"Take Action Fast\" icon=\"zap\">\n    Launch follow-up Tasks, investigate errors, or generate fixes directly from the Inbox‚Äîno context switching.\n  </Card>\n</CardGroup>\n\n### First-Class Integration Tabs\n\nSome signals deserve their own dedicated view. When you connect integrations:\n\n<AccordionGroup>\n<Accordion title=\"Sentry\">\n  See error events organized by project with occurrences, first/last seen timestamps, and one-click **Solve** actions that launch an Agent to investigate or generate a fix.\n</Accordion>\n\n<Accordion title=\"Snyk\">\n  Review high-severity vulnerabilities, check CVSS scores, and trigger Agents to patch or upgrade dependencies.\n</Accordion>\n</AccordionGroup>\n\n## Monitoring & Insights\n\n<Info>\nTrack performance, share configurations across your organization, and monitor agent activity in real time.\n</Info>\n\n<CardGroup cols={3}>\n\n  <Card title=\"Metrics\" icon=\"chart-column\" href=\"/mission-control/metrics\">\n    Measure success rates, PR activity, and intervention trends across Agents.\n  </Card>\n\n  <Card title=\"Organization Sharing\" icon=\"users\" href=\"/mission-control/governance/org-permissions\">\n    Share Agents and Workflows with teammates using built-in access controls.\n  </Card>\n\n  <Card title=\"Agent Monitoring\" icon=\"radar\" href=\"/agents/overview#monitoring\">\n    View active runs, logs, and outcomes for every Agent session.\n  </Card>\n\n</CardGroup>\n---\n\n\n## Components Behind Mission Control\n\nMission Control uses your configured components to power Agents, Tasks, and Workflows:\nAgents are built from reusable components that you can create, share, and customize:\n\n<CardGroup cols={2}>\n\n  <Card title=\"Models\" icon=\"cube\">\n\n    Large Language Models from various providers (OpenAI, Anthropic, etc.) configured for specific roles like chat, autocomplete, or editing\n\n  </Card>\n\n  <Card title=\"MCPs\" icon=\"wrench\">\n\n    Model Context Protocol servers that provide tools and capabilities like database access, web search, or custom functions\n\n  </Card>\n\n  <Card title=\"Rules\" icon=\"shield\">\n\n    Guidelines that shape AI behavior - coding standards, constraints, or specific instructions for your domain\n\n  </Card>\n\n  <Card title=\"Prompts\" icon=\"message-square\">\n\n    Reusable instructions for common tasks, optimized for specific workflows or coding patterns\n\n  </Card>\n\n</CardGroup>\n\n<Tip>\n\n  Components are the building blocks for agents that you can create new or remix from an existing component. When you create a component in mission control, it becomes available according to the permissions you set: Personal, Public, or Organization.\n\n</Tip>\n\n\n<Accordion title=\"Component Inputs\">\n\nSome components can receive values, including secrets, as inputs through templating. For values that the user needs to set, you can use template variables (e.g. `${{ inputs.API_KEY}}`). Then, the user can set `API_KEY: ${{ secrets.MY_API_KEY }}` in the `with` clause of their agent or config.\n\n<Info>\n**Choosing between `secrets.` and `inputs.`**\n\nWhen creating components for your Agent:\n- Use `${{ inputs.INPUT_NAME }}` in your definition when you want users to be able to customize which secret is used\n- Users will then map their own secrets using `${{ secrets.SECRET_NAME }}` in the `with` clause\n\nFor personal or single-use configurations, you can skip the inputs layer and reference `${{ secrets.SECRET_NAME }}` directly.\n</Info>\n</Accordion>\n\n---\n## Get Started\n<Frame>\n\n\n<CardGroup cols={3}>\n\n  <Card title=\"Create a Task\" icon=\"plus\" href=\"https://hub.continue.dev\">\n    Start with a prompt and run an Agent on demand.\n  </Card>\n\n  <Card title=\"Build a Workflow\" icon=\"clock\" href=\"https://hub.continue.dev/workflows\">\n    Automate your Tasks using cron schedules or webhooks.\n  </Card>\n\n  <Card title=\"Add Integrations\" icon=\"workflow\" href=\"https://hub.continue.dev/integrations\">\n    Connect GitHub, Slack, and Sentry to extend your Agents.\n  </Card>\n\n</CardGroup>\n</Frame>"}
{"source":"github","repo":"continue","path":"docs/faqs.mdx","content":"---\ntitle: \"FAQs\"\ndescription: \"Frequently asked questions about Continue\"\n---\n\n## Networking Issues\n\n### Configure Certificates\n\nIf you're seeing a `fetch failed` error and your network requires custom certificates, you will need to configure them in your config file. In each of the objects in the `\"models\"` array, add `requestOptions.caBundlePath` like this:\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml\n    models:\n      - name: My Model\n        ...\n        requestOptions:\n          caBundlePath: /path/to/cert.pem\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json\n    {\n      \"models\": [\n        {\n          \"name\": \"My Model\",\n          \"...\": \"...\",\n          \"requestOptions\": {\n            \"caBundlePath\": \"/path/to/cert.pem\"\n          }\n        }\n      ]\n    }\n    ```\n  </Tab>\n</Tabs>\n\nYou may also set `requestOptions.caBundlePath` to an array of paths to multiple certificates.\n\n**_Windows VS Code Users_**: Installing the [win-ca](https://marketplace.visualstudio.com/items?itemName=ukoloff.win-ca) extension should also correct this issue.\n\n### VS Code Proxy Settings\n\nIf you are using VS Code and require requests to be made through a proxy, you are likely already set up through VS Code's [Proxy Server Support](https://code.visualstudio.com/docs/setup/network#_proxy-server-support). To double-check that this is enabled, use `cmd/ctrl` + `,` to open settings and search for \"Proxy Support\". Unless it is set to \"off\", then VS Code is responsible for making the request to the proxy.\n\n### code-server\n\nContinue can be used in [code-server](https://coder.com/), but if you are running across an error in the logs that includes \"This is likely because the editor is not running in a secure context\", please see [their documentation on securely exposing code-server](https://coder.com/docs/code-server/latest/guide#expose-code-server).\n\n## Changes to configs not showing in VS Code\n\nIf you've made changes to a config (adding, modifying, or removing it) but the changes aren't appearing in the Continue extension in VS Code, try reloading the VS Code window:\n\n1. Open the command palette (`cmd/ctrl` + `shift` + `P`)\n2. Type \"Reload Window\"\n3. Select the reload option\n\nThis will restart VS Code and reload all extensions, which should make your config changes visible.\n\n## I installed Continue, but don't see the sidebar window\n\nBy default the Continue window is on the left side of VS Code, but it can be dragged to right side as well, which we recommend in our tutorial. In the situation where you have previously installed Continue and moved it to the right side, it may still be there. You can reveal Continue either by using cmd/ctrl+L or by clicking the button in the top right of VS Code to open the right sidebar.\n\n## I'm getting a 404 error from OpenAI\n\nIf you have entered a valid API key and model, but are still getting a 404 error from OpenAI, this may be because you need to add credits to your billing account. You can do so from the [billing console](https://platform.openai.com/settings/organization/billing/overview). If you just want to check that this is in fact the cause of the error, you can try adding $1 to your account and checking whether the error persists.\n\n## I'm getting a 404 error from OpenRouter\n\nIf you have entered a valid API key and model, but are still getting a 404 error from OpenRouter, this may be because models that do not support function calling will return an error to Continue when a request is sent. Example error: `HTTP 404 Not Found from https://openrouter.ai/api/v1/chat/completions`\n\n## Indexing issues\n\nIf you are having persistent errors with indexing, our recommendation is to rebuild your index from scratch. Note that for large codebases this may take some time.\n\nThis can be accomplished using the following command: `Continue: Rebuild codebase index`.\n\n## Agent mode is unavailable or tools aren't working\n\nIf Agent mode is grayed out or tools aren't functioning properly, this is likely due to model capability configuration issues.\n\n<Info>\n  Continue uses system message tools as a fallback for models without native tool support, so most models should work with Agent mode automatically.\n</Info>\n\n### Check if your model has tool support\n\n1. Not all models support native tool/function calling, but Continue will automatically use system message tools as a fallback\n2. Try adding `capabilities: [\"tool_use\"]` to your model config to force tool support\n3. Verify your provider supports function calling or that system message tools are working correctly\n\n### Tools Not Working\n\nIf tools aren't being called:\n\n1. Ensure `tool_use` is in your capabilities\n2. Check that your API endpoint actually supports function calling\n3. Some providers may use different function calling formats\n\n### Images Not Uploading\n\nIf you can't upload images:\n\n1. Add `image_input` to capabilities\n2. Ensure your model actually supports vision (e.g., gpt-4-vision, claude-3)\n3. Check that your provider passes through image data\n\n### Add capabilities\n\nIf Continue's autodetection isn't working correctly, you can manually add capabilities in your `config.yaml`:\n\n```yaml\nmodels:\n  - name: my-model\n    provider: openai\n    model: gpt-4\n    capabilities:\n      - tool_use\n      - image_input\n```\n\n### Verify with provider\n\nSome proxy services (like OpenRouter) or custom deployments may not preserve tool calling capabilities. Check your provider's documentation.\n\n### Verifying Current Capabilities\n\nTo see what capabilities Continue detected for your model:\n\n1. Check the mode selector tooltips - they indicate if tools are available\n2. Try uploading an image - if disabled, the model lacks `image_input`\n3. Check if Agent mode is available - requires `tool_use`\n\nSee the [Model Capabilities guide](/customize/deep-dives/model-capabilities) for complete configuration details.\n\n## Android Studio - \"Nothing to show\" in Chat\n\nThis can be fixed by selecting `Actions > Choose Boot runtime for the IDE` then selecting the latest version, and then restarting Android Studio. [See this thread](https://github.com/continuedev/continue/issues/596#issuecomment-1789327178) for details.\n\n## I received a \"Codebase indexing disabled - Your Linux system lacks required CPU features (AVX2, FMA)\" notification\n\nWe use LanceDB as our vector database for codebase search features. On x64 Linux systems, LanceDB requires specific CPU features (FMA and AVX2) which may not be available on older processors.\n\nMost Continue features will work normally, including autocomplete and chat. However, commands that rely on codebase indexing, such as `@codebase`, `@files`, and `@folder`, will be disabled.\n\nFor more details about this requirement, see the [LanceDB issue #2195](https://github.com/lancedb/lance/issues/2195).\n\n## Ollama Issues\n\nFor a comprehensive guide on setting up and troubleshooting Ollama, see the [Ollama Guide](/guides/ollama-guide).\n\n### Unable to connect to local Ollama instance\n\nIf you're getting \"Unable to connect to local Ollama instance\" errors:\n\n1. **Verify Ollama is running**: Check http://localhost:11434 in your browser - you should see \"Ollama is running\"\n2. **Start Ollama properly**: Use `ollama serve` (not just `ollama run model-name`)\n3. **Check your config**: Ensure your `config.yaml` has the correct setup:\n\n```yaml\nmodels:\n  - name: llama3\n    provider: ollama\n    model: llama3:latest\n```\n\n### Connection failed to remote Ollama (EHOSTUNREACH/ECONNREFUSED)\n\nWhen connecting to Ollama on another machine:\n\n1. **Configure Ollama to listen on all interfaces**:\n   - Set environment variable: `OLLAMA_HOST=0.0.0.0:11434`\n   - For systemd: Edit `/etc/systemd/system/ollama.service` and add under `[Service]`:\n     ```\n     Environment=\"OLLAMA_HOST=0.0.0.0:11434\"\n     Environment=\"OLLAMA_ORIGINS=*\"\n     ```\n   - Restart Ollama: `sudo systemctl restart ollama`\n\n2. **Update your Continue config**:\n```yaml\nmodels:\n  - name: llama3\n    provider: ollama\n    apiBase: http://192.168.1.136:11434  # Use your server's IP\n    model: llama3:latest\n```\n\n3. **Check firewall settings**: Ensure port 11434 is open on the server\n\n### Ollama not working in WSL\n\nFor WSL users having connection issues:\n\n#### Windows 11 22H2+ (Recommended)\nCreate or edit `%UserProfile%\\.wslconfig`:\n```ini\n[wsl2]\nnetworkingMode=mirrored\n```\nThen restart WSL: `wsl --shutdown`\n\n#### Older Windows/WSL versions\nIn PowerShell (as Administrator):\n```powershell\n# Add firewall rules\nNew-NetFireWallRule -DisplayName 'WSL Ollama' -Direction Inbound -LocalPort 11434 -Action Allow -Protocol TCP\nNew-NetFireWallRule -DisplayName 'WSL Ollama' -Direction Outbound -LocalPort 11434 -Action Allow -Protocol TCP\n\n# Get WSL IP (run 'ip addr' in WSL to find eth0 IP)\n# Then add port proxy (replace <WSL_IP> with your actual IP)\nnetsh interface portproxy add v4tov4 listenport=11434 listenaddress=0.0.0.0 connectport=11434 connectaddress=<WSL_IP>\n```\n\n### Docker container can't connect to host Ollama\n\nWhen running Continue or other tools in Docker that need to connect to Ollama on the host:\n\n**Windows/Mac**: Use `host.docker.internal`:\n```yaml\nmodels:\n  - name: llama3\n    provider: ollama\n    apiBase: http://host.docker.internal:11434\n    model: llama3:latest\n```\n\n**Linux**: Use the Docker bridge IP (usually `172.17.0.1`):\n```yaml\nmodels:\n  - name: llama3\n    provider: ollama\n    apiBase: http://172.17.0.1:11434\n    model: llama3:latest\n```\n\n**Docker run command**: Add host mapping:\n```bash\ndocker run -d --add-host=host.docker.internal:host-gateway ...\n```\n\n### Parse errors with remote Ollama\n\nIf you're getting parse errors with remote Ollama:\n\n1. **Verify the model is installed on the remote**:\n   ```bash\n   OLLAMA_HOST=192.168.1.136:11434 ollama list\n   ```\n   \n2. **Install missing models**:\n   ```bash\n   OLLAMA_HOST=192.168.1.136:11434 ollama pull llama3\n   ```\n\n3. **Check URL format**: Ensure you're using `http://` not `https://` for local network addresses\n\n## Local Config\n\n### Managing Local Secrets and Environment Variables\n\nFor running Continue completely offline without internet access, see the [Running Continue Without Internet guide](/guides/running-continue-without-internet).\n\nContinue supports multiple methods for managing secrets locally, searched in this order:\n\n1. **Workspace `.env` files**: Place a `.env` file in your workspace root directory\n2. **Workspace Continue folder**: Place a `.env` file in `<workspace-root>/.continue/.env`  \n3. **Global `.env` file**: Place a `.env` file in `~/.continue/.env` for user-wide secrets\n4. **Process environment variables**: Use standard system environment variables\n\n#### Creating `.env` files\n\nCreate a `.env` file in one of these locations:\n- **Per-workspace**: `<workspace-root>/.env` or `<workspace-root>/.continue/.env`\n- **Global**: `~/.continue/.env`\n\nExample `.env` file:\n```\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nCUSTOM_API_URL=https://api.example.com\n```\n\n#### Using secrets in config.yaml\n\nReference your local secrets using the `secrets` namespace:\n```yaml\nmodels:\n  - provider: openai\n    apiKey: ${{ secrets.OPENAI_API_KEY }}\n```\n\n#### Mission Control-managed secrets\n\nFor centralized team secret management, use `${{ inputs.SECRET_NAME }}` syntax in your config.yaml and manage them at [https://hub.continue.dev/settings/secrets](https://hub.continue.dev/settings/secrets):\n```yaml\nmodels:\n  - provider: openai\n    apiKey: ${{ inputs.OPENAI_API_KEY }}\n```\n\n#### Important notes\n\n- **Never commit `.env` files** to version control - add them to `.gitignore`\n- The `.env` file uses standard dotenv format (KEY=value, no quotes needed)\n- Secrets are loaded when Continue starts, so restart your IDE after changes\n- Local `.env` files take precedence over Hub secrets when both exist\n\n#### Troubleshooting secrets\n\nIf your API keys aren't being recognized:\n1. Check the `.env` file is in the correct location\n2. Ensure there are no quotes around values in the `.env` file\n3. Restart your IDE after adding/changing secrets\n4. Verify the variable name matches exactly (case-sensitive)\n5. Check that your `.env` file has proper line endings (LF, not CRLF on Windows)\n\n### Using Model Addons Locally\n\nYou can leverage model addons from the Continue Mission Control in your local configurations using the `uses:` syntax. This allows you to reference pre-configured model blocks without duplicating configuration.\n\n#### Requirements\n\n- You must be logged in to Continue\n- Internet connection is required (model addons are fetched from Mission Control)\n\n#### Usage\n\nIn your local `config.yaml`, reference model addons using the format `provider/model-name`:\n\n```yaml\nname: My Local Config\nversion: 0.0.1\nschema: v1\nmodels:\n  - uses: ollama/llama3.1-8b\n  - uses: anthropic/claude-3.5-sonnet\n  - uses: openai/gpt-4\n```\n\n#### With local configuration\n\nYou can combine hub model addons with local models:\n\n```yaml\nname: My Local Config\nversion: 0.0.1\nschema: v1\nmodels:\n  # Hub model addon\n  - uses: anthropic/claude-3.5-sonnet\n  \n  # Local model configuration\n  - name: Local Ollama\n    provider: ollama\n    model: codellama:latest\n    apiBase: http://localhost:11434\n```\n\n#### Override addon settings\n\nYou can override specific settings from the model addon:\n\n```yaml\nmodels:\n  - uses: ollama/llama3.1-8b\n    override:\n      apiBase: http://192.168.1.100:11434  # Use remote Ollama server\n      roles:\n        - chat\n        - autocomplete\n```\n\nThis feature allows you to maintain consistent model configurations across teams while still allowing local customization when needed.\n\n## How do I reset the state of the extension?\n\nContinue stores its data in the `~/.continue` directory (`%USERPROFILE%\\.continue` on Windows).\n\nIf you'd like to perform a clean reset of the extension, including removing all configuration files, indices, etc, you can remove this directory, uninstall, and then reinstall.\n\n## Still having trouble?\n\nYou can also join our Discord community [here](https://discord.gg/vapESyrFmJ) for additional support and GitHub Discussions. Alternatively, you can create a GitHub issue [here](https://github.com/continuedev/continue/issues/new?assignees=&labels=bug&projects=&template=bug-report-%F0%9F%90%9B.md&title=), providing details of your problem, and we'll be able to help you out more quickly."}
{"source":"github","repo":"continue","path":"docs/cli/quick-start.mdx","content":"---\ntitle: \"Continue CLI Quick Start\"\ndescription: \"Get hands-on experience with Continue CLI through practical examples and common use cases\"\nsidebarTitle: \"Quick Start\"\n---\n\nGet hands-on experience with Continue CLI through practical development workflows.\n\n<Tabs>\n  <Tab title=\"TUI Mode\">\n    TUI Mode (`cn` command) is for **large development tasks** that require\n    agentic workflows with human oversight. Perfect for complex refactors,\n    feature implementation, or one-off automation tasks that need monitoring and\n    iteration.\n  </Tab>\n  <Tab title=\"Headless Mode\">\n    Headless Mode (`-p` flag) is for **reliable, repeatable tasks** that no\n    longer need constant supervision. Perfect for CI/CD pipelines, git hooks,\n    and automated workflows you've already tested and refined.\n  </Tab>\n</Tabs>\n\n## TUI Mode: Large Development Tasks\n\nMake sure you have [Continue CLI installed](/cli/install) and are in a project directory.\n\n### Example: Implementing a New Feature\n\n```bash\n# Navigate to your project directory\ncd your-awesome-project\n\n# Start TUI Mode for complex development work\ncn\n```\n\n**Example workflow for adding authentication:**\n\n```\n> I need to add JWT authentication to this Express app. Let me start by showing you the current structure.\n\n> @src/app.js @package.json Here's my current setup. Can you implement JWT auth with middleware, login/register routes, and user model?\n\n> [Agent analyzes codebase and implements auth system]\n> [You review changes, test, and provide feedback]\n> [Agent iterates based on your input until the feature is complete]\n```\n\n### Example: Complex Refactoring\n\n```bash\ncn\n```\n\n**Refactoring a monolithic component:**\n\n```\n> @src/components/Dashboard.jsx This component is 800 lines and does too much. Help me break it into smaller, reusable components.\n\n> [Agent analyzes component structure]\n> [Proposes component breakdown strategy]\n> [You approve approach]\n> [Agent implements the refactor with proper props and state management]\n> [You test and request adjustments]\n```\n\n### When to Use TUI Mode\n\n‚úÖ **Large development tasks** that need oversight  \n‚úÖ **Complex refactors** requiring multiple steps  \n‚úÖ **New feature implementation** with unknowns  \n‚úÖ **One-off automation tasks** you haven't done before  \n‚úÖ **Debugging complex issues** that need exploration\n\n## Headless Mode: Automated Workflows\n\nOnce you've refined a workflow in TUI Mode, convert it to Continuous AI for automation.\n\n### Example: From REPL ‚Üí Continuous AI\n\n**Step 1: Develop in TUI Mode**\n\n```bash\ncn\n> @package.json @CHANGELOG.md Generate release notes for version 2.1.0\n> [Test and refine the prompt until it works perfectly]\n```\n\n**Step 2: Convert to Continuous AI**\n\n```bash\n# Now use the refined workflow in automation\ncn -p \"Generate release notes for the current version based on package.json and recent commits\"\n```\n\n### Common Continuous AI Workflows\n\n```bash\n# Git automation\ncn -p \"Generate a conventional commit message for staged changes\"\ncn -p \"Review the last 3 commits for potential issues\"\n\n# Code quality\ncn -p \"Fix all TypeScript errors in the src/ directory\"\ncn -p \"Update outdated dependencies and fix breaking changes\"\n\n# Documentation\ncn -p \"@README.md Update documentation based on recent changes\"\ncn -p \"Generate API documentation from JSDoc comments\"\n\n# CI/CD integration\ncn -p \"Analyze test failures and create GitHub issue with findings\"\ncn -p \"Update version numbers and create release branch\"\n```\n\n### When to Use Headless Mode\n\n‚úÖ **Proven workflows** you've tested in TUI Mode  \n‚úÖ **Repetitive tasks** that no longer need oversight  \n‚úÖ **CI/CD automation** in pipelines  \n‚úÖ **Git hooks** for automated checks  \n‚úÖ **Scheduled tasks** that run unattended\n\n<Info>\n  **Tool Permission Differences Between Modes**\n\n  Tools requiring user confirmation (\"ask\" permission) like `writeFile` and `runTerminalCommand` are:\n  - **Available in TUI mode** - AI can use them with your approval\n  - **Excluded in headless mode** - AI cannot see or call them\n\n  This prevents the AI from attempting operations that require user interaction when running in automated environments. Choose TUI mode if your workflow needs tools that modify files or run commands.\n</Info>\n\n### Available Slash Commands\n\nCommon slash commands available in CLI:\n\n- `/clear` - Clear the chat history\n- `/compact` - Summarize chat history into a compact form\n- `/config` - Switch configuration or organization\n- `/exit` - Exit the chat\n- `/fork` - Start a forked chat session from the current history\n- `/help` - Show help message\n- `/info` - Show session information\n- `/init` - Create an AGENTS.md file\n- `/login` - Authenticate with your account\n- `/logout` - Sign out of your current session\n- `/mcp` - Manage MCP server connections\n- `/model` - Switch between available chat models\n- `/resume` - Resume a previous chat session\n- `/whoami` - Check who you're currently logged in as\n\n<Info>Use `/help` to see all available commands.</Info>\n\n## The Development Workflow\n\n<Info>\n  **Recommended Approach**: Start complex tasks in TUI Mode to iterate and\n  refine your approach. Once you have a reliable workflow, convert it to\n  Headless Mode for automation.\n</Info>\n\n### Workflow Pattern\n\n1. **üî¨ Experiment in TUI Mode**\n\n   - Try complex development tasks with human oversight\n   - Iterate on prompts and approaches until they work reliably\n   - Test edge cases and refine the agent's behavior\n\n2. **‚ö° Automate with Continuous AI**\n   - Convert proven REPL workflows to single commands\n   - Deploy in CI/CD, git hooks, or scheduled tasks\n   - Run with confidence knowing the approach is tested\n\n## Next Steps\n\n<CardGroup cols={2}>\n  <Card title=\"Create Your First Workflow\" href=\"/guides/posthog-github-continuous-ai\">\n    Build an automated PostHog to GitHub issues workflow\n  </Card>\n\n  <Card title=\"Continuous AI Guide\" href=\"/guides/continuous-ai\">\n    Learn to build and deploy AI-powered development workflows\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/cli/guides.mdx","content":"---\ntitle: \"Guides\"\nurl: \"/guides/overview#mcp-integration-cookbooks\"\n---"}
{"source":"github","repo":"continue","path":"docs/cli/install.mdx","content":"---\ntitle: \"Install Continue CLI\"\ndescription: \"Get Continue CLI installed and configured for command-line AI coding assistance and automation\"\nsidebarTitle: \"Install\"\n---\n\n<iframe\n  width=\"100%\"\n  height=\"400\"\n  src=\"https://www.youtube.com/embed/vm9RcP9xM4o\"\n  title=\"Install Continue CLI\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\n<Info>\n  Make sure you have [Node.js 18 or higher\n  installed](https://nodejs.org/en/download/).\n</Info>\n\n## Installation\n\nInstall Continue CLI globally using npm:\n\n```bash\nnpm i -g @continuedev/cli\n```\n\n## Two Ways to Use Continue CLI\n\n<Info>\n  **Quick Overview**: Continue CLI works in two modes - TUI for interactive\n  conversations or headless for automated commands.\n</Info>\n\n<Tabs>\n<Tab title=\"TUI Mode\">\n**Interactive development sessions**\n\nStart a conversation with AI in your terminal:\n\n```bash\ncn\n> @src/app.js Generate unit tests for this component\n```\n\nPerfect for exploration, debugging, and iterative development.\n\n</Tab>\n\n<Tab title=\"Headless Mode\">\n**Automation and scripting**\n\nSingle commands that return results:\n\n```bash\ncn -p \"Generate a commit message for current changes\"\ncn -p \"Review the last 5 commits for issues\"\n```\n\nPerfect for CI/CD, git hooks, and automated workflows.\n\n</Tab>\n</Tabs>\n\n## Setup\n\n<Tabs>\n<Tab title=\"cn TUI Mode\">\n\nFor interactive development and exploration:\n\n<Steps>\n<Step title=\"Login to Continue CLI\">\n```bash\ncn login\n```\nThis will open your browser to authenticate with Continue Mission Control.\n</Step>\n\n<Step title=\"Test TUI Mode\">\nStart an interactive session:\n```bash\ncn\n```\n\nTry asking a question:\n\n```\n> Tell me about the CLI\n```\n\n</Step>\n</Steps>\n</Tab>\n<Tab title=\"cn Headless Mode\">\n\nFor automation workflows and scripting:\n\n<Steps>\n<Step title=\"Get API Key for Automation\">\nFor automation workflows, get an API key:\n1. Visit [Continue Mission Control API Keys](https://hub.continue.dev/settings/api-keys)\n2. Click **\"+ New API Key\"**\n3. Copy the API key immediately (you won't see it again!)\n4. Login using your Continue account\n</Step>\n\n<Step title=\"Add Secrets for Workflows\">\nStore secure credentials for CLI workflows:\n1. Visit [Continue Mission Control Secrets](https://hub.continue.dev/settings/secrets)\n2. Add your API keys and sensitive data\n3. Reference in configurations with `${{ secrets.SECRET_NAME }}`\n</Step>\n\n<Step title=\"Test Headless Mode\">\nTry headless mode for automation:\n```bash\ncn -p \"Generate a conventional commit message for the current git changes\"\n```\n</Step>\n</Steps>\n</Tab>\n</Tabs>\n\n## What's Next?\n\n<CardGroup cols={2}>\n<Card title=\"Quick Start Guide\" icon=\"rocket\" href=\"/cli/quick-start\">\n  Learn basic usage with practical examples\n</Card>\n\n<Card title=\"Create your first workflow\" icon=\"gear\" href=\"/guides/posthog-github-continuous-ai\">\n  Build automated workflows with Continue CLI\n</Card>\n</CardGroup>\n\n## Getting Help\n\nIf you encounter issues:\n\n- Ask for help in [our discussions](https://github.com/continuedev/continue/discussions)\n- Report bugs on [GitHub](https://github.com/continuedev/continue)\n"}
{"source":"github","repo":"continue","path":"docs/cli/overview.mdx","content":"---\ntitle: \"Continue CLI (cn) Overview\"\ndescription: \"Command-line interface for automated coding tasks, scripting, and headless development workflows with Continue's AI coding capabilities\"\nsidebarTitle: \"Overview\"\n---\n\n<Frame>\n  <img src=\"/images/cn-demo.gif\" />\n</Frame>\n\n**Continue enables developers to ship faster with Continuous AI.**\n\n<Tip>\n  Build features from descriptions. Debug and fix issues. Navigate any codebase.\n  Automate tedious tasks.\n</Tip>\n\n## Get started in 30 seconds\n\nPrerequisites:\n\n- [Node.js 18 or newer](https://nodejs.org/en/download/)\n- A [Continue Mission Control](https://hub.continue.dev) account (recommended) or local configuration\n\n```bash npm\n# Install Continue CLI\nnpm install -g @continuedev/cli\n\n# Navigate to your project\n\ncd your-awesome-project\n\n# Start coding with Continue\n\ncn\n\n# You'll be prompted to set up on first use\n\n```\n\nThat's it! You're ready to start automating with Continue CLI.\n\n[Continue with CLI Quickstart ‚Üí](/cli/quick-start)\n\n## Two Ways to Use Continue CLI\n\nContinue CLI offers two distinct modes designed for different workflows:\n\n### TUI Mode: Interactive Development\n\n**Perfect for exploration, debugging, and iterating on AI workflows**\n\n```bash\ncn\n> @src/components/UserProfile.js Review this component for security issues\n> Generate comprehensive unit tests\n> Suggest performance improvements\n```\n\n- **Interactive conversations** with your codebase\n- **Iterate and refine** prompts and approaches\n- **Explore and understand** complex codebases\n- **Perfect for experimentation** and learning\n\n### Headless Mode: Production Automation\n\n**Perfect for CI/CD, automation, and reliable workflows**\n\n```bash\ncn -p \"Generate a conventional commit message for staged changes\"\ncn -p \"Review pull request changes for security vulnerabilities\"\ncn -p \"Update documentation based on recent code changes\"\n```\n\n- **Single-command execution** for automation\n- **Reliable, repeatable results** for production use\n- **CI/CD and pipeline integration**\n- **Git hooks and automated workflows**\n\n<Info>\n  **Tool Permissions in TUI vs Headless Mode**\n\n  Continue CLI tools have three permission levels:\n  - **allow**: Executes automatically without confirmation\n  - **ask**: Prompts for user confirmation before execution (e.g., `writeFile`, `runTerminalCommand`)\n  - **exclude**: Tool is not available to the AI\n\n  **In headless mode**, tools with \"ask\" permission are automatically excluded to prevent the AI from seeing tools it cannot call. This ensures reliable automation without user intervention.\n\n  **In TUI mode**, tools with \"ask\" permission are available and will prompt for confirmation when the AI attempts to use them.\n\n  üí° **Tip**: If your workflow requires tools that need confirmation (like file writes or terminal commands), use TUI mode. For fully automated workflows with read-only operations, use headless mode.\n</Info>\n\n### Development Workflow: TUI ‚Üí Headless\n\n<Tip>\n  **Pro Tip**: Start in TUI mode to iterate on your AI agent. Once\n  you have a workflow that works reliably, deploy it as a Continuous AI\n  automation.\n</Tip>\n\n1. **Experiment in TUI mode** to perfect your agent\n2. **Test different approaches** interactively until you get consistent results\n3. **Convert successful workflows** to automated Continuous AI commands\n4. **Deploy in production** with confidence in your proven approach\n\n## Why developers love Continue CLI\n\n- **Works in your terminal**: Not another chat window. Not another IDE. Continue CLI meets you where you already work, with the tools you already love.\n- **Takes action**: Continue CLI can directly edit files, run commands, and create commits. Need more? Check out our [MCPs](/customize/deep-dives/mcp).\n- **Automate tasks**: Create issues from PostHog data, automatically assign labels to issues, and more. Do all this in a single command from your developer machines, or automatically in CI.\n- **Flexible development flow**: Start interactive, then automate proven workflows.\n\n## Key Capabilities\n\n### Context Engineering\n\n- Use `@` to reference files and provide context\n- Use `/` to run slash commands for specific tasks\n- Access the same context providers as IDE extensions\n\n### Tool Integration\n\n- File editing and creation\n- Terminal command execution\n- Codebase understanding and analysis\n- Git integration\n- Web search and documentation access\n\n### Model Flexibility\n\n- Switch between models with `/model` command\n- Use any model configured in your `config.yaml`\n- Access Continue Mission Control models and configurations\n\n## Continue Mission Control Integration\n\nContinue CLI integrates seamlessly with [Continue Mission Control](https://hub.continue.dev) for:\n\n### API Access\n\nGet an API key for automation workflows:\n\n1. Visit [Continue Mission Control API Keys](https://hub.continue.dev/settings/api-keys)\n2. Create a new API key\n3. Use with `cn login` or in your automation scripts\n\n### Secrets Management\n\nStore secure credentials for CLI workflows:\n\n1. Visit [Continue Mission Control Secrets](https://hub.continue.dev/settings/secrets)\n2. Add your API keys and sensitive data\n3. Reference in configurations with `${{ secrets.SECRET_NAME }}`\n\n### Configuration Sync\n\n- Cloud-managed configurations automatically sync\n- Share configurations across team members\n- Version control for your AI workflows\n\n## Common Use Cases\n\n### TUI Mode Examples\n\n**Interactive development and exploration:**\n\n<CodeGroup>\n```bash Codebase Exploration\n# Start interactive session\ncn\n> @src/components Find all unused React components\n> /explain How does authentication work in this codebase?\n> @auth/ What security patterns are used here?\n```\n\n```bash Iterative Debugging\n# Debug issues interactively\ncn\n> @tests/auth.test.js This test is failing, help me understand why\n> @src/auth/middleware.js Let's examine this middleware\n> /debug What could be causing the timeout error?\n```\n\n```bash Workflow Development\n# Develop and test automation workflows\ncn\n> @package.json @CHANGELOG.md Generate a release notes template\n> # Test and refine the approach\n> # Once working, convert to: cn -p \"Generate release notes\"\n```\n\n</CodeGroup>\n\n### Headless Mode Examples\n\n**Production automation and scripting:**\n\n<CodeGroup>\n```bash Git Automation\n# Generate commit messages\ncn -p \"Generate a conventional commit message for the current changes\"\n\n# Code review automation\n\ncn -p \"Review the current git changes for bugs and suggest improvements\"\n\n````\n\n```bash CI/CD Integration\n# In your pipeline scripts\ncn -p \"Analyze test failures and suggest fixes\"\n\n# Automated documentation updates\ncn -p \"@README.md Update this documentation based on recent changes\"\n````\n\n```bash Issue Management\n# Create GitHub issues from PostHog data\ncn -p \"@posthog-data.json Create GitHub issues for UX problems found in this session data\"\n\n# Automated security audits\ncn -p \"Scan the codebase for potential security vulnerabilities\"\n```\n\n</CodeGroup>\n\n## Next steps\n\n<CardGroup cols={3}>\n  <Card title=\"CLI Quickstart\" href=\"/cli/quick-start\">\n    Learn basic commands and common workflows\n  </Card>\n\n<Card title=\"Installation Guide\" href=\"/cli/install\">\n  Install Continue CLI and set up your environment\n</Card>\n\n  <Card title=\"CLI Workflows\" href=\"/guides/overview#continuous-ai\">\n    Task-specific tutorials and examples\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/reference.mdx","content":"---\ntitle: \"config.yaml Reference\"\ndescription: \"Comprehensive guide to the config.yaml format used by Continue.dev for building custom coding agents. Learn how to define models, context providers, rules, prompts, and more using YAML configuration.\"\n---\n\n## Introduction\n\nContinue Agents are defined using the `config.yaml` specification.\n\n**Agents** are composed of models, rules, and tools (MCP servers).\n\n<Columns cols={2}>\n<Card title=\"Configuring Models, Rules, and Tools\" icon=\"cube\" href=\"/guides/configuring-models-rules-tools\">\nLearn how to work with Continue's configuration system, including using hub models, rules, and tools, creating local configurations, and organizing your setup.\n</Card>\n\n  <Card title=\"Understanding Configs\" icon=\"robot\" href=\"/guides/understanding-configs\">\n  Learn how to build and configure configs, understand their capabilities, and customize them for your development workflow.\n  </Card>\n</Columns>\n\n## Properties\n\nBelow are details for each property that can be set in `config.yaml`.\n\n**All properties at all levels are optional unless explicitly marked as required.**\n\nThe top-level properties in the `config.yaml` configuration file are:\n\n- [`name`](#name) (**required**)\n- [`version`](#version) (**required**)\n- [`schema`](#schema) (**required**)\n- [`models`](#models)\n- [`context`](#context)\n- [`rules`](#rules)\n- [`prompts`](#prompts)\n- [`docs`](#docs)\n- [`mcpServers`](#mcpservers)\n- [`data`](#data)\n\n---\n\n### `name`\n\nThe `name` property specifies the name of your project or configuration.\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\n```\n\n---\n\n### `version`\n\nThe `version` property specifies the version of your project or configuration.\n\n### `schema`\n\nThe `schema` property specifies the schema version used for the `config.yaml`, e.g. `v1`\n\n---\n\n### `models`\n\nThe `models` section defines the language models used in your configuration. Models are used for functionalities such as chat, editing, and summarizing.\n\n**Properties:**\n\n- `name` (**required**): A unique name to identify the model within your configuration.\n\n- `provider` (**required**): The provider of the model (e.g., `openai`, `ollama`).\n\n- `model` (**required**): The specific model name (e.g., `gpt-4`, `starcoder`).\n\n- `apiBase`: Can be used to override the default API base that is specified per model\n\n- `roles`: An array specifying the roles this model can fulfill, such as `chat`, `autocomplete`, `embed`, `rerank`, `edit`, `apply`, `summarize`. The default value is `[chat, edit, apply, summarize]`. Note that the `summarize` role is not currently used.\n\n- `capabilities`: Array of strings denoting model capabilities, which will overwrite Continue's autodetection based on provider and model. See the [Model Capabilities guide](/customize/deep-dives/model-capabilities) for detailed information. Supported capabilities include:\n\n  - `tool_use`: Enables function/tool calling support (required for Agent mode)\n  - `image_input`: Enables image upload and processing support\n\n  Continue automatically detects these capabilities for most models, but you can override this when using custom deployments or if autodetection isn't working correctly.\n\n- `maxStopWords`: Maximum number of stop words allowed, to avoid API errors with extensive lists.\n\n- `promptTemplates`: Can be used to override the default prompt templates for different model roles. Valid values are [`chat`](), [`edit`](/customize/model-roles/edit#edit-prompt-templating), [`apply`](/customize/model-roles/apply#apply-prompt-templating) and [`autocomplete`](/customize/model-roles/autocomplete#autocomplete-prompt-templating). The `chat` property must be a valid template name, such as `llama3` or `anthropic`.\n\n- `chatOptions`: If the model includes role `chat`, these settings apply for Agent and Chat mode:\n\n  - `baseSystemMessage`: Can be used to override the default system prompt for **Chat** mode.\n  - `baseAgentSystemMessage`: Can be used to override the default system prompt for **Agent** mode.\n  - `basePlanSystemMessage`: Can be used to override the default system prompt for **Plan** mode.\n\n- `embedOptions`: If the model includes role `embed`, these settings apply for embeddings:\n\n  - `maxChunkSize`: Maximum tokens per document chunk. Minimum is 128 tokens.\n  - `maxBatchSize`: Maximum number of chunks per request. Minimum is 1 chunk.\n\n- `defaultCompletionOptions`: Default completion options for model settings.\n\n  - `contextLength`: Maximum context length of the model, typically in tokens.\n  - `maxTokens`: Maximum number of tokens to generate in a completion.\n  - `temperature`: Controls the randomness of the completion. Values range from `0.0` (deterministic) to `1.0` (random).\n  - `topP`: The cumulative probability for nucleus sampling.\n  - `topK`: Maximum number of tokens considered at each step.\n  - `stop`: An array of stop tokens that will terminate the completion.\n  - `reasoning`: Boolean to enable thinking/reasoning for Anthropic Claude 3.7+ and some Ollama models.\n  - `reasoningBudgetTokens`: Budget tokens for thinking/reasoning in Anthropic Claude 3.7+ models.\n\n- `requestOptions`: HTTP request options specific to the model.\n\n  - `timeout`: Timeout for each request to the language model.\n\n  - `verifySsl`: Whether to verify SSL certificates for requests.\n\n  - `caBundlePath`: Path to a custom CA bundle for HTTP requests.\n\n  - `proxy`: Proxy URL for HTTP requests.\n\n  - `headers`: Custom headers for HTTP requests.\n\n  - `extraBodyProperties`: Additional properties to merge with the HTTP request body.\n\n  - `noProxy`: List of hostnames that should bypass the specified proxy.\n\n  - `clientCertificate`: Client certificate for HTTP requests.\n    - `cert`: Path to the client certificate file.\n    - `key`: Path to the client certificate key file.\n    - `passphrase`: Optional passphrase for the client certificate key file.\n\n- `autocompleteOptions`: If the model includes role `autocomplete`, these settings apply for tab autocompletion:\n\n  - `disable`: If `true`, disables autocomplete for this model.\n  - `maxPromptTokens`: Maximum number of tokens for the autocomplete prompt.\n  - `debounceDelay`: Delay before triggering autocomplete in milliseconds.\n  - `modelTimeout`: Model timeout for autocomplete requests in milliseconds.\n  - `maxSuffixPercentage`: Maximum percentage of prompt allocated for suffix.\n  - `prefixPercentage`: Percentage of input allocated for prefix.\n  - `transform`: If `false`, disables trimming of multiline completions. Defaults to `true`. Useful for models that generate better multiline completions without transformations.\n  - `template`: Custom template for autocomplete using Mustache syntax. You can use the `{{{ prefix }}}`, `{{{ suffix }}}`, `{{{ filename }}}`, `{{{ reponame }}}`, and `{{{ language }}}` variables.\n  - `onlyMyCode`: Only includes code within the repository for context.\n  - `useCache`: If `true`, enables caching for completions.\n  - `useImports`: If `true`, includes imports in context.\n  - `useRecentlyEdited`: If `true`, includes recently edited files in context.\n  - `useRecentlyOpened`: If `true`, includes recently opened files in context.\n\n**Example:**\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\nmodels:\n  - name: GPT-4o\n    provider: openai\n    model: gpt-4o\n    roles:\n      - chat\n      - edit\n      - apply\n    defaultCompletionOptions:\n      temperature: 0.7\n      maxTokens: 1500\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    roles:\n      - autocomplete\n    autocompleteOptions:\n      debounceDelay: 250\n      maxPromptTokens: 1024\n      onlyMyCode: true\n  - name: My Model - OpenAI-Compatible\n    provider: openai\n    apiBase: http://my-endpoint/v1\n    model: my-custom-model\n    capabilities:\n      - tool_use\n      - image_input\n    roles:\n      - chat\n      - edit\n```\n\n---\n\n### `context`\n\nThe `context` section defines context providers, which supply additional information or context to the language models. Each context provider can be configured with specific parameters.\n\nMore information about usage/params for each context provider can be found [here](/customize/deep-dives/custom-providers)\n\n**Properties:**\n\n- `provider` (**required**): The identifier or name of the context provider (e.g., `code`, `docs`, `web`)\n- `name`: Optional name for the provider\n- `params`: Optional parameters to configure the context provider's behavior.\n\n**Example:**\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\ncontext:\n  - provider: file\n  - provider: code\n  - provider: diff\n  - provider: http\n    name: Context Server 1\n    params:\n      url: \"https://api.example.com/server1\"\n  - provider: terminal\n```\n\n---\n\n### `rules`\n\nRules are concatenated into the system message for all [Agent](/ide-extensions/agent/quick-start), [Chat](/ide-extensions/chat/quick-start), and [Edit](/ide-extensions/edit/quick-start) requests. \n\nConfiugration example:\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\nrules:\n  - uses: sanity/sanity-opinionated # rules file stored on Continue Mission Control\n  - uses: file://user/Desktop/rules.md # rules file stored on local computer\n```\n\nRules file example:\n\n```md title=\"rules.md\"\n---\nname: Pirate rule\n---\n\nTalk like a pirate\n```\n\nSee the [rules deep dive](/customize/deep-dives/rules) for more details.\n\n---\n\n### `prompts`\n\nPrompts can be invoked with a <kbd>/</kbd> command.\n\nConfiguration example:\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\nprompts:\n  - uses: supabase/create-functions # prompts file stored on Continue Mission Control \n  - uses: file://user/Desktop/prompts.md # prompts file stored on local computer\n```\n\nPrompts file example:\n\n```md title=\"prompts.md\"\n---\nname: Make pirate comments\ninvokable: true\n---\n\nRewrite all comments in the active file to talk like a pirate\n```\n\nSee the [prompts deep dive](/customize/deep-dives/prompts) for more details.\n\n---\n\n### `docs`\n\nList of documentation sites to index.\n\n**Properties:**\n\n- `name` (**required**): Name of the documentation site, displayed in dropdowns, etc.\n- `startUrl` (**required**): Start page for crawling - usually root or intro page for docs\n- `favicon`: URL for site favicon (default is `/favicon.ico` from `startUrl`).\n- `useLocalCrawling`: Skip the default crawler and only crawl using a local crawler.\n\n**Example:**\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\ndocs:\n  - name: Continue\n    startUrl: https://docs.continue.dev/intro\n    favicon: https://docs.continue.dev/favicon.ico\n```\n\n---\n\n### `mcpServers`\n\nThe [Model Context Protocol](https://modelcontextprotocol.io/introduction) is a standard proposed by Anthropic to unify prompts, context, and tool use. Continue supports any MCP server with the MCP context provider.\n\n**Properties:**\n\n- `name` (**required**): The name of the MCP server.\n- `command` (**required**): The command used to start the server.\n- `args`: An optional array of arguments for the command.\n- `env`: An optional map of environment variables for the server process.\n- `cwd`: An optional working directory to run the command in. Can be absolute or relative path.\n- `requestOptions`: Optional request options for `sse` and `streamable-http` servers. Same format as [model requestOptions](#models).\n- `connectionTimeout`: Optional timeout for _initial_ connection to MCP server\n\n**Example:**\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - ./test.db\n    cwd: /Users/NAME/project\n    env:\n      NODE_ENV: production\n```\n\n### `data`\n\nDestinations to which [development data](/customize/deep-dives/development-data) will be sent.\n\n**Properties:**\n\n- `name` (**required**): The display name of the data destination\n- `destination` (**required**): The destination/endpoint that will receive the data. Can be:\n  - an HTTP endpoint that will receive a POST request with a JSON blob\n  - a file URL to a directory in which events will be dumpted to `.jsonl` files\n- `schema` (**required**): the schema version of the JSON blobs to be sent. Options include `0.1.0` and `0.2.0`\n- `events`: an array of event names to include. Defaults to all events if not specified.\n- `level`: a pre-defined filter for event fields. Options include `all` and `noCode`; the latter excludes data like file contents, prompts, and completions. Defaults to `all`\n- `apiKey`: api key to be sent with request (Bearer header)\n- `requestOptions`: Options for event POST requests. Same format as [model requestOptions](#models).\n\n**Example:**\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\ndata:\n  - name: Local Data Bank\n    destination: file:///Users/dallin/Documents/code/continuedev/continue-extras/external-data\n    schema: 0.2.0\n    level: all\n  - name: My Private Company\n    destination: https://mycompany.com/ingest\n    schema: 0.2.0\n    level: noCode\n    events:\n      - autocomplete\n      - chatInteraction\n```\n\n---\n\n## Complete YAML Config Example\n\nPutting it all together, here's a complete example of a `config.yaml` configuration file:\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 1.0.0\nschema: v1\nmodels:\n  - uses: anthropic/claude-3.5-sonnet\n    with:\n      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n    override:\n      defaultCompletionOptions:\n        temperature: 0.8\n  - name: GPT-4\n    provider: openai\n    model: gpt-4\n    roles:\n      - chat\n      - edit\n    defaultCompletionOptions:\n      temperature: 0.5\n      maxTokens: 2000\n    requestOptions:\n      headers:\n        Authorization: Bearer YOUR_OPENAI_API_KEY\n  - name: Ollama Starcoder\n    provider: ollama\n    model: starcoder\n    roles:\n      - autocomplete\n    autocompleteOptions:\n      debounceDelay: 350\n      maxPromptTokens: 1024\n      onlyMyCode: true\n    defaultCompletionOptions:\n      temperature: 0.3\n      stop:\n        - \"\\n\"\nrules:\n  - Give concise responses\n  - Always assume TypeScript rather than JavaScript\nprompts:\n  - name: test\n    description: Unit test a function\n    prompt: |\n      Please write a complete suite of unit tests for this function. You should use the Jest testing framework.\n      The tests should cover all possible edge cases and should be as thorough as possible.\n      You should also include a description of each test case.\n  - uses: myprofile/my-favorite-prompt\ncontext:\n  - provider: diff\n  - provider: file\n  - provider: code\nmcpServers:\n  - name: DevServer\n    command: npm\n    args:\n      - run\n      - dev\n    env:\n      PORT: \"3000\"\ndata:\n  - name: My Private Company\n    destination: https://mycompany.com/ingest\n    schema: 0.2.0\n    level: noCode\n    events:\n      - autocomplete\n      - chatInteraction\n```\n\n## Using YAML anchors to avoid config duplication\n\nYou can also use node anchors to avoid duplication of properties. To do so, adding the YAML version header `%YAML 1.1` is needed, here's an example of a `config.yaml` configuration file using anchors:\n\n```yaml title=\"config.yaml\"\n%YAML 1.1\n---\nname: My Config\nversion: 1.0.0\nschema: v1\nmodel_defaults: &model_defaults\n  provider: openai\n  apiKey: my-api-key\n  apiBase: https://api.example.com/llm\nmodels:\n  - name: mistral\n    <<: *model_defaults\n    model: mistral-7b-instruct\n    roles:\n      - chat\n      - edit\n  - name: qwen2.5-coder-7b-instruct\n    <<: *model_defaults\n    model: qwen2.5-coder-7b-instruct\n    roles:\n      - chat\n      - edit\n  - name: qwen2.5-coder-7b\n    <<: *model_defaults\n    model: qwen2.5-coder-7b\n    useLegacyCompletionsEndpoint: false\n    roles:\n      - autocomplete\n    autocompleteOptions:\n      debounceDelay: 350\n      maxPromptTokens: 1024\n      onlyMyCode: true\n```\n\n---\n\n## `config.json` Deprecation\n\n`config.yaml `replaces `config.json`, which is deprecated. View the **[Migration Guide](/reference/yaml-migration)** for help transitioning from the old format.\n"}
{"source":"github","repo":"continue","path":"docs/telemetry.mdx","content":"---\ntitle: \"Telemetry\"\nicon: \"chart-line\"\ndescription: \"Learn about Continue's anonymous telemetry collection practices, what usage data is tracked, and how to opt out of data collection to maintain your privacy preferences\"\n---\n\n## Overview\n\nThe open-source Continue Extensions collect and report **anonymous** usage information to help us improve our product. This data enables us to understand user interactions and optimize the user experience effectively. You can opt out of telemetry collection at any time if you prefer not to share your usage information.\n\nWe utilize [Posthog](https://posthog.com/), an open-source platform for product analytics, to gather and store this data. For transparency, you can review the implementation code [here](https://github.com/continuedev/continue/blob/main/gui/src/hooks/CustomPostHogProvider.tsx) or read our [official privacy policy](https://continue.dev/privacy).\n\n## Tracking Policy\n\nAll data collected by the open-source Continue extensions is anonymized and stripped of personally identifiable information (PII) before being sent to PostHog. We are committed to maintaining the privacy and security of your data.\n\n## What We Track\n\nThe following usage information is collected and reported:\n\n- **Suggestion Interactions:** Whether you accept or reject suggestions (excluding the actual code or prompts involved).\n- **Model and Command Information:** The name of the model and command used.\n- **Token Metrics:** The number of tokens generated.\n- **System Information:** The name of your operating system (OS) and integrated development environment (IDE).\n- **Pageviews:** General pageview statistics.\n\n## How to Opt Out\n\n### IDE extensions\n\nYou can disable anonymous telemetry by toggling \"Allow Anonymous Telemetry\" off in the user settings.\n\n#### VS Code\n\nAlternatively in VS Code, you can disable telemetry through your VS Code settings by unchecking the \"Continue: Telemetry Enabled\" box (this will override the Settings Page settings). VS Code settings can be accessed with `File` > `Preferences` > `Settings` (or use the keyboard shortcut `ctrl` + `,` on Windows/Linux or `cmd` + `,` on macOS).\n\n### CLI\n\nFor `cn`, the Continue CLI, set the environment variable `CONTINUE_TELEMETRY_ENABLED=0` before running commands:\n\n```bash\nexport CONTINUE_TELEMETRY_ENABLED=0\ncn <your prompt>\n```\n\nOr run it inline:\n\n```bash\nCONTINUE_TELEMETRY_ENABLED=0 cn <your prompt>\n```"}
{"source":"github","repo":"continue","path":"docs/troubleshooting.mdx","content":"---\ntitle: \"Troubleshooting\"\ndescription: \"Comprehensive guide to resolving common issues with Continue, including logging, keyboard shortcuts, networking problems, model capabilities, and extension configuration troubleshooting\"\n---\n\n1. [Check the logs](#check-the-logs)\n2. [Try the latest pre-release](#download-the-latest-pre-release)\n3. [Download an older version](#download-an-older-version)\n4. [Resolve keyboard shortcut issues](#keyboard-shortcuts-not-resolving)\n5. [MCP Server connection issues](#mcp-server-connection-issues)\n6. [Check FAQs for common issues](/faqs)\n\n## Check the logs\n\nTo solve many problems, the first step is reading the logs to find the relevant error message. To do this, follow these steps:\n\n### VS Code\n\n#### Console logs\n\n<Info>\n  In order to view debug logs, which contain extra information, click the\n  dropdown at the top that says \"Default levels\" and select \"Verbose\".\n</Info>\n\n1. `cmd` + `shift` + `P` for MacOS or `ctrl`\n   - `shift` + `P` for Windows\n2. Search for and then select \"Developer: Toggle Developer Tools\"\n3. This will open the [Chrome DevTools window](https://developer.chrome.com/docs/devtools/)\n4. Select the `Console` tab\n5. Read the console logs\n\n#### Prompt Logs (Continue Console)\n\nTo view prompt logs/analytics, you can enable the Continue Console.\n\n1. Open VS Code settings (`cmd/ctrl` + `,`)\n2. Search for the setting \"Continue: Enable Console\" and enable it\n3. Reload the window\n4. Open the Continue Console by using the command palette (`cmd/ctrl` + `shift` + `P`) and searching for \"Continue: Focus on Continue Console View\"\n\n![Continue Console](/images/images/continue-console-d387a10c2918c117c6c253a3b5f18c22.png)\n\n### JetBrains\n\nOpen `~/.continue/logs/core.log` to view the logs for the Continue plugin. The most recent logs are found at the bottom of the file.\n\nSome JetBrains-related logs may also be found by clicking \"Help\" > \"Show Log in Explorer/Finder\".\n\n## Download the latest pre-release\n\n### VS Code\n\nWe are constantly making fixes and improvements to Continue, but the latest changes remain in a \"pre-release\" version for roughly a week so that we can test their stability. If you are experiencing issues, you can try the pre-release by going to the Continue extension page in VS Code and selecting \"Switch to Pre-Release\" as shown below.\n\n![Pre-Release](/images/images/prerelease-9bed93e846914165d30a3b227a680d9b.png)\n\n### JetBrains\n\nOn JetBrains, the \"pre-release\" happens through their Early Access Program (EAP) channel. To download the latest EAP version, enable the EAP channel:\n\n1. Open JetBrains settings (`cmd/ctrl` + `,`) and go to \"Plugins\"\n2. Click the gear icon at the top\n3. Select \"Manage Plugin Repositories...\"\n4. Add \"[https://plugins.jetbrains.com/plugins/eap/list](https://plugins.jetbrains.com/plugins/eap/list)\" to the list\n5. You'll now always be able to download the latest EAP version from the marketplace\n\n## Download an Older Version\n\nIf you've tried everything, reported an error, know that a previous version was working for you, and are waiting to hear back, you can try downloading an older version of the extension.\n\nFor VS Code, All versions are hosted on the Open VSX Registry [here](https://open-vsx.org/extension/Continue/continue). Once you've downloaded the extension, which will be a .vsix file, you can install it manually by following the instructions [here](https://code.visualstudio.com/docs/editor/extension-gallery#_install-from-a-vsix).\n\nYou can find older versions of the JetBrains extension on their [marketplace](https://plugins.jetbrains.com/plugin/22707-continue), which will walk you through installing from disk.\n\n## Keyboard shortcuts not resolving\n\nIf your keyboard shortcuts are not resolving, you may have other commands that are taking precedence over the Continue shortcuts. You can see if this is the case, and change your shortcut mappings, in the configuration of your IDE.\n\n- [VSCode keyboard shortcuts docs](https://code.visualstudio.com/docs/getstarted/keybindings)\n- [IntelliJ keyboard shortcut docs](https://www.jetbrains.com/help/idea/configuring-keyboard-and-mouse-shortcuts.html)\n\n## MCP Server connection issues\n\n### \"spawn ENAMETOOLONG\" error on macOS\n\nIf you're seeing an error like `Failed to connect to \"<MCP Server Name>\"` with `Error: spawn ENAMETOOLONG` when using MCP servers on macOS, this is due to the environment being too large when spawning the MCP process.\n\n**Workaround:** Use the full path to the command instead of relying on PATH resolution:\n\n```yaml\nmcpServers:\n  - name: Memory MCP server\n    command: /usr/local/bin/npx  # Use full path instead of just \"npx\"\n    args:\n      - -y\n      - \"@modelcontextprotocol/server-memory\"\n```\n\nTo find the full path to a command on your system:\n- For `npx`: run `which npx`\n- For `docker`: run `which docker`\n- For `uv`/`uvx`: run `which uv` or `which uvx`\n\nThis issue typically affects macOS users with large development environments and is being tracked in [#7870](https://github.com/continuedev/continue/issues/7870) and [#6699](https://github.com/continuedev/continue/issues/6699).\n\n\n## Still having trouble?\n\nYou can also join our Discord community [here](https://discord.gg/vapESyrFmJ) for additional support and discussions. Alternatively, you can create a GitHub issue [here](https://github.com/continuedev/continue/issues/new?assignees=&labels=bug&projects=&template=bug-report-%F0%9F%90%9B.md&title=), providing details of your problem, and we'll be able to help you out more quickly.\n"}
{"source":"github","repo":"continue","path":"docs/guides/configuring-models-rules-tools.mdx","content":"---\ntitle: \"Configuring Models, Rules, and Tools\"\ndescription: \"Learn how to work with Continue's configuration system. Understand how to use hub models, rules, and tools, create local configurations, and organize your setup for maximum reusability.\"\n---\n\n## What Are Models, Rules, and Tools?\n\nContinue configs are built from three main types of configuration:\n\n<Columns cols={3}>\n<Card title=\"Models\" icon=\"cube\">\nLanguage models that power different capabilities like chat, autocomplete, and agent mode\n</Card>\n\n<Card title=\"Rules\" icon=\"pencil\">\n  Guidelines and instructions that shape how the AI behaves and responds\n</Card>\n\n<Card title=\"Tools\" icon=\"server\">\nMCP tools that provide additional capabilities like database access, web search, or custom functions\n</Card>\n</Columns>\n\nThere are two places where you can define these configurations:\n\n<Columns cols={2}>\n<Card title=\"Local\" icon=\"computer\">\nCustom configurations you create and manage in your workspace or globally\n</Card>\n\n<Card title=\"Hub\" icon=\"cloud\">\nPre-built models, rules, and tools from the Continue community that you can import and use immediately\n</Card>\n</Columns>\n\n## Local\n\nLocal configurations let you create custom models, rules, and tools that automatically apply to multiple configs, reducing duplication and ensuring consistency across your setup.\n\n<Columns cols={2}>\n<Card title=\"Global\" icon=\"globe\">\nApplied to all configs across all workspaces. Ideal for personal preferences, universal coding standards, or tools you use everywhere.\n</Card>\n<Card title=\"Workspace\" icon=\"folder-open\">\nApplied automatically to all configs when working in a specific project.\n\nPerfect for project-specific setups like TypeScript rules for web apps or the Playwright MCP tool.\n\n</Card>\n\n</Columns>\n\n## Hub\n\nContinue hub uses a slug in the format of `owner/item-name` to resolve blocks.\n\nFor example, to use the [Claude 4 Sonnet model](https://hub.continue.dev/anthropic/claude-4-sonnet), you'd reference it as `anthropic/claude-4-sonnet`.\n\nImport from Mission Control using the `uses` syntax alongside your custom configurations:\n\n```yaml config.yaml highlight={6}\nname: Team Config\nversion: 1.0.0\nschema: v1\n\nmodels:\n  - uses: anthropic/claude-4-sonnet\n    with:\n      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }} # Use a hub secret\n```\n\n### Organization\n\nOrganize your local configurations using these directories:\n\n<Columns cols={3}>\n<Card title=\"Models\" icon=\"cube\">\n  `.continue/models`\n</Card>\n\n<Card title=\"Rules\" icon=\"pencil\">\n  `.continue/rules`\n</Card>\n\n<Card title=\"Tools\" icon=\"server\">\n  `.continue/mcpServers`\n</Card>\n</Columns>\n\n## Working with Secrets\n\nModels, and many MCP servers, require a secret for things like API keys.\n\nOn Mission Control, you can configure secrets when adding a model or MCP server.\n\nThis will use mustache notation to pass the secret, eg `${{ secrets.SECRET_NAME }}`\n\nWhen configuring a local model or MCP server, you can use the same mustache notation for secrets which read from:\n\n<Columns cols={2}>\n<Card title=\"Global\" icon=\"globe\">\n`.env` file in `~/.continue/.env`\n</Card>\n\n<Card title=\"Workspace\" icon=\"folder-open\">\n`.env` file at your project root\n</Card>\n</Columns>\n\n<Info>\n**When to use `secrets.` vs `inputs.`**\n\nFor most use cases, **use `${{ secrets.SECRET_NAME }}`** directly in your configuration. This is the recommended approach for both personal and organizational workflows.\n\nUse `${{ inputs.INPUT_NAME }}` only when you need flexibility to:\n- Allow users to customize which secret a block uses without editing the block itself\n- Change the secret name without modifying the block configuration\n- Create reusable blocks where different users may have differently-named secrets\n\nThis pattern is inspired by GitHub Actions, where inputs provide an abstraction layer between block definitions and user-specific values. For most scenarios, directly referencing `secrets.` keeps configuration simpler and more straightforward.\n</Info>\n\n## Overriding Properties\n\nYou can directly override properties using the `override` syntax:\n\n```yaml title=\"config.yaml\" highlight={10-13}\nname: myprofile/custom-config\nversion: 1.0.0\nschema: v1\n\nmodels:\n  - uses: myprofile/custom-model\n    with:\n      ANTHROPIC_API_KEY: ${{ secrets.MY_ANTHROPIC_API_KEY }}\n      TEMP: 0.9\n    override:\n      roles:\n        - chat\n```\n\n## Advanced\n\n### Inputs\n\nModels and MCP server authors can configure inputs that require the user to provide a secret value by defining a `${{ inputs.SECRET_NAME }}` value.\n\nFor example, here is how you could require that the user provide a value for the `apiKey` property on a model:\n\n```yaml title=\"config.yaml\" highlight={8}\nname: myprofile/custom-model\nversion: 1.0.0\nschema: v1\n\nmodels:\n  - name: My Favorite Model\n    # ... other model properties ...\n    apiKey: ${{ inputs.SECRET_NAME }}\n```\n\nUsers then map their secret to this input using a `${{ secrets.SECRET_NAME }}` value that maps to a property name which matches the required input, e.g. `SECRET_NAME`.\n\n```yaml title=\"config.yaml\" highlight={8}\nname: myprofile/custom-config\nversion: 1.0.0\nschema: v1\n\nmodels:\n  - uses: myprofile/custom-model\n    with:\n      SECRET_NAME: ${{ secrets.SECRET_NAME }}\n```\n\n## Next Steps\n\nNow that you understand how models, rules, and tools work, explore:\n\n- **[Config Reference](/reference)**: Detailed documentation of all available properties\n- **[Continue Mission Control](https://hub.continue.dev)**: Browse community models, rules, and tools\n- **[Custom Context Providers](/customize/deep-dives/custom-providers)**: Create advanced context integrations\n- **[Model Roles](/customize/model-roles/intro)**: Understanding how different models work together\n"}
{"source":"github","repo":"continue","path":"docs/guides/codebase-documentation-awareness.mdx","content":"---\ntitle: How to Make Agent mode Aware of Codebases and Documentation\ndescription: Learn how to give your Agent mode access to codebases and documentation for more context-aware assistance\nkeywords: [agent, codebase, documentation, MCP, context, RAG, tools]\nsidebarTitle: Codebase and Documentation Awareness\n---\n\nAgent mode works best when it understands the context of your project. This guide shows you how to give agent mode access to codebases and documentation, making it more helpful and accurate.\n\n## Make agent mode aware of your open codebase\n\nWhen agent mode understands your current codebase, it can provide more relevant suggestions and answers.\n\n### Let agent mode explore the codebase using tools\n\nAgent mode can use built-in tools to navigate and understand your code:\n\n1. **File exploration tools**: The agent can read files, search for patterns, and understand project structure\n2. **Code search**: Use search to find relevant code snippets\n3. **Git integration**: Access commit history and understand code evolution\n\n### Create rules to help the agent understand your codebase\n\nRules guide agent mode's behavior and understanding. Place markdown files in `.continue/rules` in your project to provide context:\n\n```markdown\n# Project Architecture\n\nThis is a React application with:\n\n- Components in `/src/components`\n- API routes in `/src/api`\n- State management using Redux in `/src/store`\n\n## Coding Standards\n\n- Use TypeScript for all new files\n- Follow the existing naming conventions\n- Write tests for all new features\n```\n\n<Tip>\n  Place rules files at different levels of your project hierarchy to scope when\n  they trigger\n</Tip>\n\nLearn more about [rules configuration](/customize/deep-dives/rules).\n\n## Make agent mode aware of other codebases\n\nSometimes you need agent mode to understand code beyond your current project.\n\n### Public codebases\n\nFor open-source projects and public repositories, you have several options:\n\n#### Rules with hyperlinks\n\nCreate rules that point to external codebases:\n\n```markdown\n# External Dependencies\n\nOur authentication system is based on:\n\n- [Auth.js documentation](https://authjs.dev/)\n- [Example implementation](https://github.com/nextauthjs/next-auth-example)\n\nWhen implementing auth features, reference these patterns.\n```\n\n#### GitHub and GitLab CLIs\n\nEnable `gh` or `glab` CLI access for agent mode to interact with repositories.\n\nAdd rules to guide CLI usage:\n\n```markdown\n# Repository Access\n\nYou can use the `gh` CLI to:\n\n- Search for issues: `gh issue list --repo owner/repo`\n- View pull requests: `gh pr list --repo owner/repo`\n- Clone repositories: `gh repo clone owner/repo`\n```\n\n#### DeepWiki MCP\n\n[DeepWiki MCP](https://hub.continue.dev/deepwiki/deepwiki-mcp) lets agent mode explore any public GitHub repository.\n\nOnce configured, agent mode can explore repositories like:\n\n- \"Explore the React repository structure\"\n- \"Find how authentication is implemented in NextAuth.js\"\n\n### Internal codebases\n\nFor private and internal repositories, you need additional setup:\n\n#### Custom MCP servers\n\nCreate an MCP server that has access to your internal repositories.\n\n#### Custom code RAG\n\nFor faster retrieval and lower costs with very large internal codebases, consider implementing a [custom code RAG](/guides/custom-code-rag) system. This is an advanced approach that requires more setup but can provide performance benefits at scale.\n\n## Make agent mode aware of relevant documentation\n\nDocumentation provides crucial context for agent mode to understand APIs, frameworks, and best practices.\n\n### Public documentation\n\n#### Rules with documentation links\n\nGuide agent mode to relevant documentation:\n\n```markdown\n# Documentation Resources\n\nFor framework-specific questions, refer to:\n\n- React: https://react.dev/reference/react\n- Next.js: https://nextjs.org/docs\n- Tailwind CSS: https://tailwindcss.com/docs\n\nAlways cite documentation when explaining concepts.\n```\n\n#### Context7 MCP\n\n[Context7 MCP](https://hub.continue.dev/upstash/context7-mcp) enables agent mode to search and retrieve information from public documentation:\n\nAgent mode can then answer questions like:\n\n- \"How do I use React hooks?\"\n- \"What's the syntax for Tailwind CSS animations?\"\n\n### Internal documentation\n\nFor private documentation and wikis:\n\n#### Rules with internal links\n\nCreate rules that reference internal resources:\n\n```markdown\n# Internal Documentation\n\nOur team documentation is available at:\n\n- API Documentation: https://internal.docs/api\n- Architecture Guide: https://internal.docs/architecture\n- Deployment Process: https://internal.docs/deployment\n\nAlways follow our internal standards when suggesting code.\n```\n\n#### Custom MCP servers for docs\n\nCreate an MCP server that accesses your internal documentation.\n\n## Migrating from deprecated context providers\n\nIf you were previously using the `@Codebase` or `@Docs` context providers, here's how to migrate to the new approach:\n\n### Migrating from @Codebase\n\nThe `@Codebase` context provider has been deprecated. Instead:\n\n1. **Use built-in tools**: Agent mode can now use file exploration and search tools to understand your codebase\n2. **Add rules**: Create `.continue/rules` files to provide context about your project structure\n3. **Use MCP servers**: For external codebases, use DeepWiki MCP or custom MCP servers\n\n### Migrating from @Docs\n\nThe `@Docs` context provider has been deprecated. Instead:\n\n1. **Use Context7 MCP**: For public documentation, Context7 MCP provides similar functionality\n2. **Add documentation links in rules**: Create rules that reference documentation URLs\n3. **Use custom MCP servers**: For internal documentation, create an MCP server with access to your docs\n\nThe new approach provides better integration with Continue's Agent mode features and more intelligent context selection.\n\n## Next steps\n\n- Learn more about [MCP servers](/reference/continue-mcp)\n- Explore [rules configuration](/customize/deep-dives/rules)\n  - Set up [other custom configurations](/guides/understanding-configs) with specific knowledge domains\n"}
{"source":"github","repo":"continue","path":"docs/guides/supabase-mcp-database-workflow.mdx","content":"---\ntitle: \"Supabase RLS Security Audit and Fixes with MCP\"\ndescription: \"Learn how to audit Row Level Security in your Supabase database, identify vulnerabilities, and automatically generate fixes using Continue CLI and Supabase MCP.\"\nsidebarTitle: \"Supabase RLS Security Audit\"\n---\n\n<Card title=\"What You'll Build\" icon=\"shield-check\">\n  A security audit workflow that uses Continue CLI with Supabase MCP to identify RLS vulnerabilities, generate secure policies, fix permission issues, and ensure your database follows security best practices.\n</Card>\n\n## What You'll Learn\n\nThis cookbook teaches you to:\n\n- Use [Supabase MCP](https://supabase.com/docs/guides/getting-started/mcp) to audit database security\n- Identify tables without Row Level Security (RLS) enabled\n- Find and fix overly permissive or missing RLS policies\n- Generate secure RLS migrations following best practices\n- Automate security audits with GitHub Actions\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- [Supabase account](https://supabase.com) with an active project\n- Node.js 18+ installed locally\n- [Continue CLI](https://docs.continue.dev/guides/cli) with **active credits** (required for API usage)\n- Basic understanding of SQL and database concepts\n\n<Steps>\n  <Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n\n  <Step title=\"Set up Continue CLI Account & API Key\">\n    1. Visit [Continue Organizations](https://hub.continue.dev/settings/organizations)\n    2. Sign up or log in to your Continue account\n    3. Navigate to your organization settings\n    4. Click **\"API Keys\"** and then **\"+ New API Key\"**\n    5. Copy the API key immediately (you won't see it again!)\n    6. Login to the CLI: `cn login`\n  </Step>\n</Steps>\n\n<Tip>\n  Continue CLI can analyze your database schema and generate complex SQL queries - you just need to describe what you want in plain language!\n</Tip>\n\n## Step 1: Set Up Your Credentials\n\nFirst, you'll need to set up access to your Supabase project.\n\n<Tabs>\n  <Tab title=\"Configure Supabase MCP\">\n\n    <Warning>\n      **Security First**: Follow [Supabase's security best practices](https://supabase.com/docs/guides/getting-started/mcp#security-risks) when using MCP:\n      - Never connect to production databases directly\n      - Use development or staging environments\n      - Enable read-only mode when possible\n      - Scope MCP access to specific projects\n    </Warning>\n\n    The Supabase MCP supports OAuth authentication for secure access:\n\n    **OAuth Configuration (Recommended)**\n\n    The Supabase MCP will prompt for OAuth authentication when first used. Simply follow the authorization flow.\n\n    **Remote MCP URL**\n    ```\n    https://mcp.supabase.com/mcp\n    ```\n\n    <Info>\n      The MCP server can be scoped to a specific project for better security. Configure this during setup.\n    </Info>\n\n  </Tab>\n  <Tab title=\"Supabase Project Setup\">\n    To use Supabase MCP effectively, ensure your project has:\n\n    1. **Development Environment** - Create a separate development project:\n       - Go to [Supabase Dashboard](https://supabase.com/dashboard)\n       - Create a new project for development/testing\n       - Copy the project URL and anon key\n\n    2. **Database Schema** - Set up some initial tables for testing:\n       ```sql\n       -- Example schema for testing\n       CREATE TABLE users (\n         id UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n         email TEXT UNIQUE NOT NULL,\n         created_at TIMESTAMPTZ DEFAULT NOW()\n       );\n\n       CREATE TABLE posts (\n         id UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n         user_id UUID REFERENCES users(id),\n         title TEXT NOT NULL,\n         content TEXT,\n         published BOOLEAN DEFAULT false,\n         created_at TIMESTAMPTZ DEFAULT NOW()\n       );\n       ```\n\n    3. **Row Level Security (RLS)** - Enable RLS for security:\n       ```sql\n       ALTER TABLE users ENABLE ROW LEVEL SECURITY;\n       ALTER TABLE posts ENABLE ROW LEVEL SECURITY;\n       ```\n\n    <Tip>\n      Use [Supabase Branching](https://supabase.com/docs/guides/deployment/branching) to create isolated development branches for safe testing.\n    </Tip>\n\n  </Tab>\n</Tabs>\n\n## Supabase Database Workflow Options\n\n<Card title=\"Fastest Path to Success\" icon=\"rocket\">\n  Skip the manual setup and use our pre-built Supabase Continuous AI agent that includes\n  optimized prompts, rules, and the Supabase MCP for intelligent database management.\n</Card>\n\n<Info>\n  **How Supabase MCP Works**:\n  - Connects to your Supabase project via OAuth\n  - Provides tools for database queries, schema inspection, and migrations\n  - Supports read-only mode for safer operations\n  - Can be scoped to specific projects for security\n</Info>\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    **Perfect for:** Immediate database analysis with AI-powered query optimization and schema insights\n\n    <Steps>\n      <Step title=\"Add the Pre-Built Agent\">\n        Visit the [Supabase Agent](https://hub.continue.dev/continuedev/supabase-agent) on Continue Mission Control and click **\"Install Agent\"** or run:\n        \n        ```bash\n        cn --agent continuedev/supabase-agent\n        ```\n\n        This agent includes:\n        - **Optimized prompts** for database analysis and query generation\n        - **Built-in rules** for SQL best practices and security\n        - **[Supabase MCP](https://supabase.com/docs/guides/getting-started/mcp)** for secure database access\n        - **Automatic authentication** via OAuth flow\n      </Step>\n\n      <Step title=\"Run Database Analysis\">\n        Navigate to your project directory and enter this prompt in the Continue CLI TUI:\n        \n        ```\n        Analyze my Supabase database schema and suggest performance optimizations\n        ```\n\n        That's it! The agent handles everything automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** Get consistent results with pre-tested prompts and built-in SQL optimization rules.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n    <Steps>\n      <Step title=\"Add Supabase MCP to Continue CLI\">\n        Configure the [Supabase MCP](https://supabase.com/docs/guides/getting-started/mcp) using OAuth:\n\n        The MCP server will automatically prompt for OAuth authentication when you first use it.\n      </Step>\n\n      <Step title=\"Verify Supabase Connection\">\n        Test your Supabase MCP connection with this prompt:\n        \n        ```\n        List all tables in my Supabase database\n        ```\n      </Step>\n\n      <Step title=\"Create Custom Database Analysis Prompts\">\n        Use this prompt template with Continue CLI to analyze your database:\n\n        ```\n        Analyze my Supabase database:\n        - List all tables with row counts\n        - Identify tables without indexes\n        - Find potential N+1 query patterns\n        - Suggest missing foreign key constraints\n        - Recommend index optimizations based on table structure\n        - Generate SQL migrations for suggested improvements\n        - Check for security issues (missing RLS policies)\n        ```\n      </Step>\n    </Steps>\n\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n  To use the pre-built agent, you need either:\n  - **Continue CLI Pro Plan** with the models add-on, OR\n  - **Your own API keys** added to Continue Mission Control secrets\n  \n  The agent will automatically detect and use your configuration. For Supabase MCP:\n  - **Supabase account** with at least one project\n  - **OAuth authentication** (handled automatically)\n  - Development or staging environment (not production)\n</Accordion>\n\n---\n\n<Warning>\n  **Security Best Practices**: \n  - **Never use MCP with production databases** - Always use development or staging environments\n  - **Enable read-only mode** when analyzing data to prevent accidental modifications\n  - **Scope to specific projects** to limit access\n  - **Use branch databases** for testing schema changes\n</Warning>\n\n## Step 2: Analyze Your Database with AI\n\nUse Continue CLI to perform intelligent database analysis. Enter these prompts in the Continue CLI TUI:\n\n<Tabs>\n  <Tab title=\"Schema Analysis\">\n    **Prompt:**\n    ```\n    Analyze my Supabase database schema and provide:\n    - Complete table structure with data types\n    - Relationships between tables\n    - Missing indexes that could improve performance\n    - Unused or redundant columns\n    - Suggestions for normalization improvements\n    ```\n  </Tab>\n\n  <Tab title=\"RLS Security Audit\">\n    **Prompt:**\n    ```\n    Perform a comprehensive RLS (Row Level Security) audit on my Supabase database:\n    \n    For each table:\n    - Check if RLS is enabled\n    - List all existing RLS policies\n    - Identify tables without any RLS policies (security risk)\n    - Find overly permissive policies (e.g., allowing all operations)\n    - Suggest missing policies based on common patterns\n    - Generate SQL to fix identified security issues\n    \n    Prioritize findings by risk level:\n    1. Critical: Tables with sensitive data but no RLS\n    2. High: Overly permissive policies\n    3. Medium: Missing common policies (e.g., users can only see own data)\n    4. Low: Policy optimization opportunities\n    ```\n  </Tab>\n\n  <Tab title=\"Fix RLS Policies\">\n    **Prompt:**\n    ```\n    Based on the RLS audit, generate SQL migrations to fix all security issues:\n    \n    1. Enable RLS on all tables that don't have it:\n       - Include ALTER TABLE statements\n       - Add comment explaining why RLS is needed\n    \n    2. Create secure default policies for common patterns:\n       - Users can only read/update their own records\n       - Admins have full access (with role checking)\n       - Public read-only access where appropriate\n       - Proper INSERT policies for new records\n    \n    3. Fix overly permissive policies:\n       - Replace 'true' conditions with proper checks\n       - Add user_id or role-based restrictions\n       - Include USING clauses for read operations\n       - Include WITH CHECK clauses for write operations\n    \n    4. For each policy, include:\n       - Clear naming convention (e.g., 'users_select_own')\n       - Comments explaining the security model\n       - Rollback statements\n    \n    Generate the complete migration file with all fixes.\n    ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Available Supabase MCP Capabilities**:\n  - **Database Queries**: Execute SQL queries and analyze results\n  - **Schema Inspection**: View table structures, relationships, and constraints\n  - **Performance Analysis**: Identify slow queries and optimization opportunities\n  - **Security Review**: Check RLS policies and access controls\n  - **Migration Generation**: Create SQL migrations for schema changes\n</Info>\n\n## Step 3: Generate Database Migrations\n\nCreate and apply database migrations based on AI recommendations. Enter this prompt in the Continue CLI TUI:\n\n**Example: Complete RLS Security Fix**\n\n```\nI need to secure my Supabase database. Please:\n\n1. First, audit all tables for RLS security issues\n2. Generate a complete migration to fix all issues found\n3. Create a security report I can share with my team\n\nHere's what I need fixed:\n- Enable RLS on all tables\n- Create policies so users can only access their own data\n- Ensure admins (role = 'admin') have full access\n- Add policies for service accounts (role = 'service')\n- Include audit logging for sensitive operations\n\nFor the users table specifically:\n- Users can read their own profile\n- Users can update their own profile (except role field)\n- Only admins can view all users\n- Only admins can delete users\n- New users can insert their own record during signup\n\nGenerate the complete migration with:\n- All SQL statements\n- Clear policy names and comments\n- Rollback statements\n- A summary of what each policy does\n```\n\n**Expected Output:**\nThe AI will generate a complete SQL migration file that:\n- Enables RLS on all tables\n- Creates secure, well-documented policies\n- Follows Supabase security best practices\n- Includes rollback capabilities\n- Provides a security summary for your team\n\n<Tip>\n  **Best Practice**: Always review AI-generated migrations before applying them. Test in a development branch first using Supabase's branching feature.\n</Tip>\n\n## Step 4: Set Up Automated Database Monitoring\n\nAutomate database health checks with Continue CLI and GitHub Actions:\n\n```yaml\nname: Database Health Monitor\n\non:\n  schedule:\n    # Run daily at 2 AM UTC\n    - cron: \"0 2 * * *\"\n  workflow_dispatch: # Allow manual triggers\n\njobs:\n  monitor-database:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: \"22\"\n\n      - name: Install Continue CLI\n        run: |\n          npm install -g @continuedev/cli\n          echo \"‚úÖ Continue CLI installed\"\n\n      - name: Analyze Database Security\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n        run: |\n          echo \"üîç Performing security audit...\"\n          \n          # Use Continue CLI to audit RLS and generate fixes\n          cn -p \"Using Supabase MCP, perform a comprehensive RLS security audit:\n                 1. Check all tables for RLS enablement\n                 2. Identify tables with missing or weak RLS policies\n                 3. Find overly permissive policies (e.g., 'true' conditions)\n                 4. Check for common security anti-patterns:\n                    - Missing user_id checks\n                    - No role-based access control\n                    - Unrestricted DELETE operations\n                    - Missing WITH CHECK clauses\n                 5. Generate fixes for all security issues found\n                 6. Create a security report with:\n                    - Critical vulnerabilities (tables without RLS)\n                    - High-risk policies that need immediate fixes\n                    - SQL migrations to fix all issues\n                    - Best practice recommendations\n                    - Compliance checklist (GDPR, SOC2, etc.)\n                 \n                 If critical security issues are found:\n                 - Generate the complete fix migration\n                 - Create a GitHub issue with severity labels\n                 - Tag security team members for review\"\n\n      - name: Save Health Report\n        run: |\n          echo \"## üìä Database Health Report\" >> $GITHUB_STEP_SUMMARY\n          echo \"Generated at $(date)\" >> $GITHUB_STEP_SUMMARY\n          echo \"\" >> $GITHUB_STEP_SUMMARY\n          echo \"Check the 'Analyze Database Security' step for the full report\" >> $GITHUB_STEP_SUMMARY\n\n      - name: Upload Health Report\n        uses: actions/upload-artifact@v4\n        with:\n          name: database-health-report\n          path: |\n            *.md\n            *.sql\n```\n\n<Warning>\n  **Required GitHub Secrets**:\n  - `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n\n  Add this at: **Repository Settings ‚Üí Secrets and variables ‚Üí Actions**\n</Warning>\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered database management system** that:\n\n- **Analyzes schema design** - Identifies optimization opportunities and best practices\n- **Optimizes queries** - Suggests indexes and query rewrites for better performance\n- **Generates migrations** - Creates SQL code for schema improvements\n- **Monitors health** - Runs automated checks for performance and security issues\n- **Provides insights** - Delivers actionable recommendations based on data patterns\n\n<Card title=\"Continuous AI Database Management\" icon=\"shield-check\">\n  Your system now operates at **[Level 2 Continuous AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** - AI handles routine database analysis and optimization with human oversight for migration approval.\n</Card>\n\n## Advanced Database Prompts\n\nEnhance your workflow with these advanced Continue CLI prompts:\n\n<CardGroup cols={2}>\n  <Card title=\"Real-time Performance\" icon=\"gauge\">\n    Monitor query performance in real-time and get alerts for slow queries exceeding threshold times\n  </Card>\n  <Card title=\"Data Quality Checks\" icon=\"check-circle\">\n    Automatically validate data integrity, find duplicates, and ensure consistency across related tables\n  </Card>\n  <Card title=\"Access Pattern Analysis\" icon=\"chart-line\">\n    Analyze API logs to understand data access patterns and optimize indexes accordingly\n  </Card>\n  <Card title=\"Cost Optimization\" icon=\"dollar-sign\">\n    Review database usage and suggest ways to reduce costs while maintaining performance\n  </Card>\n</CardGroup>\n\n## Security Best Practices\n\n<Warning>\n  **Database Security Guidelines**:\n  - Always use development/staging environments with MCP\n  - Enable read-only mode for analysis tasks\n  - Never store production credentials in code\n  - Review all AI-generated SQL before execution\n  - Use RLS policies to enforce access control\n  - Regularly audit database permissions\n</Warning>\n\n## Troubleshooting\n\n### Supabase MCP Connection Issues\n\nIf you encounter connection issues:\n\n1. Verify OAuth authentication is complete\n2. Check your Supabase project is accessible\n3. Ensure you're not connecting to a production database\n4. Verify the MCP URL is correct: `https://mcp.supabase.com/mcp`\n\n### Common Database Analysis Issues\n\n| Issue | Solution |\n|:------|:---------|\n| No tables found | Verify your database has tables created |\n| Permission denied | Check OAuth scopes and project permissions |\n| Slow query analysis | Ensure your database has query logs enabled |\n| Migration failures | Test migrations in a branch database first |\n\n## Next Steps\n\n- Set up [Supabase Edge Functions](https://supabase.com/docs/guides/functions) for automated workflows\n- Configure [Supabase Realtime](https://supabase.com/docs/guides/realtime) for live data monitoring\n- Implement [Database Webhooks](https://supabase.com/docs/guides/database/webhooks) for event-driven automation\n- Join the [Continue Discord](https://discord.gg/continue) for support\n\n## Resources\n\n- [Supabase MCP Documentation](https://supabase.com/docs/guides/getting-started/mcp)\n- [Supabase Database Guide](https://supabase.com/docs/guides/database)\n- [Supabase Security Best Practices](https://supabase.com/docs/guides/database/security)\n- [Model Context Protocol Docs](https://modelcontextprotocol.io/introduction)\n- [Continue CLI Guide](https://docs.continue.dev/guides/cli)\n- [Continuous AI Best Practices](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)"}
{"source":"github","repo":"continue","path":"docs/guides/cli.mdx","content":"---\ntitle: \"How to Use Continue CLI (cn)\"\nsidebarTitle: \"Continue CLI (cn)\"\ndescription: \"Learn how to use Continue's command-line interface for context engineering, automated coding tasks, and headless development workflows with customizable models, rules, and tools\"\n---\n\n`cn` is an open-source, modular coding agent for the command line.\n\nIt provides a battle-tested agent loop so you can simply plug in your model, rules, and tools.\n\n![cn](/images/cn-demo.gif)\n\n## Quick start\n\n<Info>\n  Make sure you have [Node.js 18 or higher\n  installed](https://nodejs.org/en/download/).\n</Info>\n\n```bash\n# Install\nnpm i -g @continuedev/cli\n\n# Interactive mode\ncn\n\n# Headless mode\ncn -p \"Generate a conventional commit name for the current git changes\"\n```\n\n## How to Use Continue CLI - Basic Usage\n\nOut of the box, `cn` comes with tools that let it understand your codebase, edit files, run terminal commands, and more (if you approve). You can ask `cn` to:\n\n- Fix failing tests\n- Find something in the codebase\n- Execute a refactor\n- Write a new feature\n- And a lot more\n\nUse '@' to give it file context, or '/' to run slash commands.\n\nIf you want to resume a previous conversation, run `cn --resume`.\n\n## How to Use Headless Mode (`-p` flag)\n\nIn headless mode, `cn` will only output its final response, making it perfect for Unix Philosophy-style scripting and automation. For example, you could pipe your git diff into `cn` to generate a commit message, and write this to a file:\n\n```bash\necho \"$(git diff) Generate a conventional commit name for the current git changes\" | cn -p > commit-message.txt\n```\n\n## How to Configure Continue CLI\n\n`cn` uses [`config.yaml`](/reference), the exact same configuration file as Continue. This means that you can log in to [Continue Mission Control](/hub/introduction) or use your existing local configuration.\n\nTo switch between configurations, you can use the `/config` slash command in `cn`, or you can start it with the `--config` flag (e.g. `cn --config continuedev/default-cli-config` or `cn --config ~/.continue/config.yaml`).\n\n### How to Add Custom Models\n\nLearn how to add custom models [here](/customize/overview). Then, you can use the `/model` slash command to switch between them in `cn`.\n\n### How to Configure Rules\n\n`cn` supports [rules](/customize/deep-dives/rules) in the same way as the Continue IDE extensions. You can also use the `--rule` flag to manually include a rule from Mission Control. For example, `cn --rule nate/spanish` will tell `cn` to use [this rule](https://hub.continue.dev/nate/spanish) to always speak in Spanish.\n\n### How to Configure Tools\n\n`cn` supports MCP tools, which can be configured in the [same way](/customize/deep-dives/mcp) as with the Continue IDE extensions.\n\n#### How to Set Tool Permissions\n\n`cn` includes a tool permission system to make sure you approve of the agent's actions. It will begin with minimal permissions but as you approve tool calls, it will add policies to `~/.continue/permissions.yaml` to remember your preferences.\n\nIf you want to explicitly allow or deny tools for a single session, you can use the command line flags `--allow`, `--ask`, and `--exclude`. For example:\n\n```bash\n# Always allow the Write tool\ncn --allow Write()\n\n# Always ask before running curl\ncn --ask Bash(curl*)\n\n# Never use the Fetch tool\ncn --exclude Fetch\n```\n\n## API Key Authentication\n\nFor automation in CI or other headless environments, you can use an API key to authenticate with Continue. First, obtain your personal API key [here](https://hub.continue.dev/settings/api-keys). Then, set it as the `CONTINUE_API_KEY` environment variable. You can now use `cn -p` (headless mode) without needing to log in.\n\nIf you wish to run an automation on behalf of your organization you can obtain an organization-scoped API key by going to [your organization's settings](https://hub.continue.dev/settings/organizations) -> API Keys.\n\n## Troubleshooting\n\nRun `cn` with the `--verbose` flag to see more detailed logs. These will be output to `~/.continue/logs/cn.log`.\n\nIf you have feedback on the beta, please [share in our Discord](https://discord.com/invite/EfJEfdFnDQ) or [leave feedback in the GitHub discussion](https://github.com/continuedev/continue/discussions/7307).\n"}
{"source":"github","repo":"continue","path":"docs/guides/posthog-github-continuous-ai.mdx","content":"---\ntitle: \"Building a Continuous AI Workflow with PostHog and GitHub\"\ndescription: \"Build an automated system that continuously monitors PostHog analytics, analyzes user behavior with AI, and creates GitHub issues automatically using PostHog MCP.\"\nsidebarTitle: \"PostHog Analytics with Continue CLI\"\n---\n\n<Card title=\"What You'll Build\" icon=\"robot\">\n  A fully automated workflow that uses Continue CLI with the PostHog MCP to fetch analytics data, analyze user experience issues with AI, and automatically create GitHub\n  issues with the GitHub CLI.\n</Card>\n\n## What You'll Learn\n\nThis cookbook teaches you to:\n\n- Use [PostHog MCP](https://posthog.com/docs/model-context-protocol) to query [analytics](https://posthog.com/docs/web-analytics), [errors](https://posthog.com/docs/error-tracking), and [feature flags](https://posthog.com/docs/feature-flags)\n- Analyze user behavior patterns with AI\n- Automatically create GitHub issues using GitHub CLI\n- Set up continuous monitoring with GitHub Actions\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- GitHub repository where you want to create issues\n- [PostHog account](https://posthog.com) with [session recordings enabled](https://posthog.com/docs/session-replay/installation) and data collecting\n- Node.js 18+ installed locally\n- [Continue CLI](https://docs.continue.dev/guides/cli) with **active credits** (required for API usage)\n- [GitHub CLI](https://cli.github.com/) installed (`gh` command)\n\n<Steps>\n  <Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n  \n  <Step title=\"Set up Continue CLI Account & API Key\">\n        1. Visit [Continue Organizations](https://hub.continue.dev/settings/organizations)\n        2. Sign up or log in to your Continue account\n        3. Navigate to your organization settings\n        4. Click **\"API Keys\"** and then **\"+ New API Key\"** \n        5. Copy the API key immediately (you won't see it again!)\n        6. Login to the CLI: `cn login`\n  </Step>\n\n  <Step title=\"Add Required Secrets to Continue CLI\">\n    Continue CLI will securely store your API keys as secrets that can be referenced in prompts.\n  </Step>\n</Steps>\n\n<Tip>\n  Continue CLI handles the complex API interactions - you just need to provide\n  the right prompts!\n</Tip>\n\n## Step 1: Set Up Your Credentials\n\nFirst, you'll need to gather your PostHog and GitHub API credentials and add them as secrets in Continue CLI.\n\n<Tabs>\n  <Tab title=\"PostHog API Credentials\">\n    You'll need a **Personal API Key** (not a Project API key) to access session recordings:\n\n    1. Go to [Personal API Keys](https://app.posthog.com/settings/user-api-keys) in PostHog\n    2. Click **+ Create a personal API Key**\n    3. Name it \"Continue CLI Session Analysis\"\n    4. Select these scopes:\n       - `session_recording:read` - **Required** for accessing session data\n       - `feature_flag:read` - **Required** for feature flag auditing\n       - `insight:read`\n       - `query:read`\n       - `session_recording_playlist:read`\n    5. Copy the key immediately (you won't see it again!)\n    6. Note your **Project ID** from your PostHog project settings\n    7. Note your PostHog host URL (e.g., `https://us.posthog.com` or your custom domain)\n    8. You'll also need your POSTHOG_AUTH_HEADER value, which is simply `Bearer YOUR_API_KEY`\n\n      <Info>\n  **Continue Secrets**: The `POSTHOG_AUTH_HEADER` secret should be stored in\n  Continue's secure secrets storage. This keeps your API key safe and the MCP\n  automatically connects to your default PostHog project.\n</Info>\n\n  </Tab>\n  <Tab title=\"Set Up GitHub CLI Authentication\">\n   GitHub CLI handles authentication automatically - no manual PAT needed:\n\n    1. Install GitHub CLI if not already installed\n    2. Run `gh auth login` and follow the prompts\n    3. Choose authentication method (browser or token)\n    4. Grant necessary permissions when prompted (`issues:write` is **required** for creating issues)\n\n  </Tab>\n<Tab title= \"Configure API Access in Continue CLI\">\n \n<Tip>\nSee https://docs.continue.dev/hub/secrets/secret-types#secret-types for adding secrets\n</Tip>\n\nYou only need to configure the PostHog MCP credential - it automatically handles project selection. To add environment variables to your Continue Mission Control account:\n\n1.  Go to the [Continue Mission Control](https://hub.continue.dev)\n2.  Sign in to your account\n3.  Navigate to your user settings\n4.  Look for the \"Secrets\" section\n5.  Add your Personal API Key from PostHog (phx_...) as POSTHOG_AUTH_HEADER secret with format: Bearer YOUR_API_KEY\n\n</Tab>\n</Tabs>\n\n## PostHog GitHub Continuous AI Workflow Options\n\n<Card title=\"üöÄ Fastest Path to Success\" icon=\"zap\">\n  Skip the manual setup and use our pre-built PostHog GitHub agent that includes\n  optimized prompts, rules, and the PostHog MCP for more consistent results.\n</Card>\n\n<Info>\n  **How PostHog MCP Works**: \n  - Your API key is tied to your PostHog account and\n  organization \n  - It automatically uses your default project (no project ID\n  needed) \n  - If you have multiple projects, use `mcp__posthog__switch-project` to\n  change \n  - The MCP connects via `https://mcp.posthog.com/sse` using your account context.\n\n</Info>\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    **Perfect for:** Immediate results with optimized prompts and built-in debugging\n\n    <Steps>\n      <Step title=\"Add the Pre-Built Agent\">\n        Visit the [PostHog Continuous AI Agent](https://hub.continue.dev/continuedev/posthog-continuous-ai-agent) on Continue Mission Control and click **\"Install Agent\"** or run:\n        \n        ```bash\n        cn --agent continuedev/posthog-continuous-ai-agent\n        ```\n\n        This agent includes:\n        - **Optimized prompts** for PostHog analysis and GitHub issue creation\n        - **Built-in rules** for consistent formatting and error handling\n        - **PostHog MCP** for more reliable API interactions\n      </Step>\n\n      <Step title=\"Run the Analysis\">\n        From your project directory, run:\n        ```bash\n        cn \"Give me my PostHog Session data and create GitHub issues based on the problems.\"\n        ```\n\n        That's it! The agent handles everything automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** Results are more consistent and debugging is easier thanks to the PostHog MCP integration and pre-tested prompts.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n  <Steps>\n    <Step title=\"Add PostHog MCP to Continue CLI\">\n      First, install the [PostHog MCP](https://hub.continue.dev/posthog/http-mcp) to the Continue CLI by\n      1. Adding it to a Hub Config by clicking \"Use MCP Server\" and selecting a Config\n      2. Adding it to your [Local Config](/reference#mcpservers)\n    </Step>\n    <Step title=\"Add PostHog + GitHub Analysis Rules\">\n      Visit [PostHog GitHub Continuous AI Rules](https://hub.continue.dev/bekah-hawrot-weigel/posthog-github-continuous-ai-rules) and click \"Use Rule\" to add it to your Config.\n      \n      You could also\n      1. Pass `--rule bekah-hawrot-weigel/posthog-github-continuous-ai-rules` to `cn` OR\n      2. Copy the rules to your local `.continue/rules` folder. See the [Rules Guide](/customize/deep-dives/rules#how-to-create-and-manage-rules-in-continue).\n    </Step>\n    <Step title=\"Create Your Custom Prompts\">\n      Use this prompt with Continue CLI to analyze PostHog data and create GitHub issues:\n\n      ```bash\n\n       # In cn TUI mode:\n      \"Create GitHub issues from the PostHog analysis using gh CLI:\n      - For each issue, run: gh issue create --title 'üîç UX Issue: [title]' --body '[details]'\n      - Add labels: --label 'bug,user-experience,automated'\n      - Set priority labels (high/medium/low)\n      - Include session data and technical details in the body\n      Execute the commands and confirm each issue was created with URL.\"\n      ```\n    </Step>\n</Steps>\n\n  </Tab>\n</Tabs>\n<Info>\n\n  **Why GitHub CLI over GitHub MCP**: While GitHub MCP is available, it can be\n  token-expensive to run. The `gh` CLI is more efficient, requires no API tokens\n  (authenticated via `gh auth login`), and provides a cleaner command-line\n  experience. GitHub MCP remains an option if you prefer full MCP integration.\n</Info>\n\n<Accordion title=\"Agent Requirements\">\n    To use the pre-built agent, you need either: \n      - **Continue CLI Pro Plan** with\n      the models add-on, OR \n      - **Your own API keys** added to Continue Mission Control secrets\n      The agent will automatically detect and use your\n      configuration.\n\n</Accordion>\n\n---\n\n<Warning>\n**Repository Labels Required**: Make sure your GitHub repository has these labels:\n- `bug`, `enhancement`, `technical-debt`\n- `high-priority`, `medium-priority`, `low-priority`\n- `user-experience`, `automated`, `feature-flag`, `cleanup`\n\nCreate missing labels in your repo at: **Settings ‚Üí Labels ‚Üí New label**\n\n</Warning>\n\n<Info>\n    **What Continue CLI Does:** \n    - Parses your analysis results automatically \n    - Makes authenticated GitHub API calls using your stored token \n    - Creates properly formatted issues with appropriate labels \n    - Checks for duplicate issues to avoid spam \n    - Provides confirmation with issue URLs\n\n</Info>\n\n\n## What You've Built\n\nAfter completing this guide, you have a complete **Continuous AI system** that:\n\n- **Monitors user experience** - Automatically fetches and analyzes PostHog session data\n\n- **Identifies problems intelligently** - Uses AI to spot patterns and technical issues\n\n- **Creates actionable tasks** - Generates GitHub issues with specific recommendations\n\n- **Runs autonomously** - Operates daily without manual intervention using GitHub Actions\n\n- **Scales with your team** - Handles growing amounts of session data automatically\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your system now operates at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine analysis tasks with human oversight through GitHub issue\n  review and prioritization.\n</Card>\n\n## Security Best Practices\n\n<Warning>\n\n    **Protect Your API Keys:**\n    - Store all credentials as GitHub Secrets, never in\n    code\n    - Use Continue CLI's secure secret storage\n    - Limit token scopes to minimum required permissions\n    - Rotate API keys regularly (every 90 days recommended)\n    - Monitor token usage for unusual activity\n\n</Warning>\n\n## Example Use Cases\n\nHere are practical examples of what you can build with PostHog MCP and Continue CLI:\n\n### Session Recording Analysis (Current Implementation)\n\nThe main workflow above focuses on analyzing session recordings to identify UX issues and create GitHub issues automatically.\n\n### Feature Flag Audit and Cleanup\n\n<Card title=\"üèÅ Feature Flag Management\" icon=\"flag\">\n  Automatically audit your feature flags to identify unused, outdated, or problematic flags that need attention.\n</Card>\n\n**What this workflow does:**\n- Fetches all feature flags from your PostHog project\n- Analyzes flag usage, rollout status, and configuration\n- Identifies flags that may be candidates for removal or updates\n- Creates GitHub issues for flag cleanup tasks\n\n**Example Continue CLI prompts:**\n\n```bash\n# Get all feature flags and analyze them\ncn \"Use PostHog MCP to fetch all feature flags with mcp__posthog__feature-flag-get-all. Then analyze each flag to identify: 1) Flags that are 100% rolled out and could be removed, 2) Flags that haven't been updated in 90+ days, 3) Flags with complex targeting that might need simplification, 4) Experimental flags that should be cleaned up.\"\n\n# Create cleanup issues for identified flags\ncn \"For each problematic feature flag identified, create a GitHub issue using gh CLI:\n- Title: 'üèÅ Feature Flag Cleanup: [flag_name]'\n- Include flag details: rollout percentage, last modified date, targeting rules\n- Add labels: 'technical-debt', 'feature-flag', 'cleanup'\n- Set priority based on risk level (high for 100% rollouts, medium for stale flags)\n- Include specific recommendations for each flag\"\n\n# Audit flag performance impact\ncn \"Cross-reference feature flags with PostHog performance metrics to identify flags that may be impacting user experience or site performance. Create performance-focused GitHub issues for flags showing negative impact.\"\n```\n\n**Required PostHog MCP Tools:**\n- `feature-flag-get-all` - Retrieve all feature flags\n- `feature-flag-get-definition` - Get detailed flag configuration\n- `query-run` - Run analytics queries to check flag usage\n- `insights-get-all` - Get insights related to flag performance\n\n**Sample Output:**\nThis workflow creates GitHub issues like:\n- \"üèÅ Feature Flag Cleanup: dark-mode-toggle\" (100% rollout, safe to remove)\n- \"üèÅ Feature Flag Review: experimental-checkout\" (unused for 120 days)\n- \"üèÅ Feature Flag Simplify: complex-user-targeting\" (overly complex rules)\n\n### Advanced Prompts\n\nConsider enhancing your workflow with these advanced Continue CLI prompts:\n\n<CardGroup cols={2}>\n  <Card title=\"Performance Analysis\" icon=\"chart-line\">\n    \"Analyze [PostHog performance\n    metrics](https://posthog.com/docs/web-analytics) alongside session\n    recordings to identify slow page loads affecting user experience\"\n  </Card>\n  <Card title=\"Error Correlation\" icon=\"bug\">\n    \"Cross-reference JavaScript console errors with user actions to identify the\n    root cause of UX issues\"\n  </Card>\n  <Card title=\"Feature Flag Performance Impact\" icon=\"flag\">\n    \"Use PostHog MCP to correlate feature flag rollouts with performance metrics and user behavior changes to identify flags causing issues\"\n  </Card>\n  <Card title=\"Custom Slack Alerts\" icon=\"slack\">\n    \"Create Slack alerts when critical UX issues are detected in PostHog\n    sessions or when feature flags need attention\"\n  </Card>\n</CardGroup>\n\n## Next Steps\n\n- Consider [GitHub MCP](https://hub.continue.dev/github/github-mcp) as an alternative (note: can be token-expensive)\n- Configure [Slack MCP](https://hub.continue.dev/slack/slack-mcp) for alerts\n- Set up [PostHog performance monitoring](https://posthog.com/docs/web-analytics)\n- Join the [Continue Discord](https://discord.gg/continue) for support\n\n## Resources\n\n- [PostHog API Documentation](https://posthog.com/docs/api)\n- [PostHog MCP Documentation](https://posthog.com/docs/model-context-protocol)\n- [PostHog Session Replay](https://posthog.com/docs/session-replay)\n- [PostHog Feature Flags](https://posthog.com/docs/feature-flags)\n- [PostHog Error Tracking](https://posthog.com/docs/error-tracking)\n- [GitHub CLI Documentation](https://cli.github.com/)\n- [GitHub MCP on Continue Mission Control](https://hub.continue.dev/github/github-mcp) (alternative option)\n- [Continue CLI Guide](https://docs.continue.dev/guides/cli)\n- [Continuous AI Best Practices](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)\n"}
{"source":"github","repo":"continue","path":"docs/guides/understanding-configs.mdx","content":"---\ntitle: \"How to Understand Hub vs Local Configuration\"\ndescription: \"Learn how to choose between cloud-managed Hub and local configuration for AI development assistance in Continue, including setup, management, and best practices for each approach\"\n---\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/ljym650IW3Y\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\nEvery developer has unique needs when it comes to AI assistance. Some prefer the convenience of cloud-managed configurations, while others need the control and privacy of local setups. Continue offers both paths, and this guide will help you choose the right one for your workflow.\n\n## What Are the Two Paths to AI Assistance?\n\nContinue provides two distinct ways to configure:\n\nThink of Continue's configuration options like choosing between a managed service and self-hosting. Both get you to the same destination‚Äîpowerful AI assistance in your IDE‚Äîbut the journey and control level differ significantly.\n\n### How to Access Your Configuration\n\nBefore we dive into the specifics, let's understand how to access your configuration:\n\n1. Open the Continue Chat sidebar by pressing <kbd>cmd/ctrl</kbd> + <kbd>L</kbd> (VS Code) or <kbd>cmd/ctrl</kbd> + <kbd>J</kbd> (JetBrains)\n2. Click the Config selector above the main chat input\n3. Hover over a config and click:\n   - `new window` icon for Hub configs\n   - `gear` icon for Local configs\n\n![configure](/images/configure-continue.png)\n\n## What Are Hub Configurations: The Managed Experience\n\nHub Configurations represent the \"it just works\" philosophy. When you [sign in to Continue Mission Control](https://auth.continue.dev/), you gain access to a curated ecosystem of established configurations that sync seamlessly across all your development environments.\n\n### Why Should You Choose Hub Configs?\n\n**The Power of Simplicity**\n\n- **Instant Setup**: Browse the [configuration marketplace](https://hub.continue.dev) and add any config to your account with a single click\n- **Web-Based Management**: Configure models, add secrets, and customize settings through an intuitive web interface‚Äîno JSON editing required\n- **Automatic Synchronization**: Make a change on Mission Control, and it reflects immediately across all your IDE instances\n- **Team Collaboration**: Share custom configurations with your team, ensuring everyone uses the same optimized configurations\n\n![nextjs config](/images/nextjs-assistant.png)\n\n### How to Get Started with Hub Configs\n\nThe journey from zero to AI-powered coding takes just four steps:\n\n1. **Select Your Config**: Click the config selector in your IDE's Continue panel\n2. **Explore or Create**: Browse community configurations or craft your own specialized setup\n3. **Secure Your Keys**: Add API keys as [User Secrets](https://hub.continue.dev/settings/secrets) in Mission Control‚Äîthey're encrypted and never exposed\n4. **Sync and Code**: Click \"Reload config\" to pull your latest settings\n\nPro tip: Hub configurations are perfect for teams. Create a custom config with your team's coding standards, preferred models, and context sources, then share it with a simple link.\n\n### How to Manage Hub Configs\n\nAll Hub config management happens through [Mission Control](https://hub.continue.dev). For detailed customization, see our guide on [Editing a Config](/hub/configs/edit-a-config).\n\n## What Are Local Configs: The Power User's Choice\n\nLocal configuration puts you in the driver's seat. Using a `config.yaml` file, you have complete control over every aspect of your Continue experience with all configuration stored directly on your machine.\n\n### Why Should You Choose Local Configs?\n\n**Complete Control and Privacy**\n\n- **Your Data, Your Rules**: All configuration stays on your machine‚Äîperfect for air-gapped environments or strict data policies\n- **Version Control Integration**: Check your `config.yaml` into git alongside your code, ensuring reproducible environments\n- **Offline Capability**: Once configured, no internet connection needed (assuming you're using local models)\n- **Unlimited Customization**: Access every configuration option, experimental feature, and advanced setting\n\n### How to Set Up Local Configs\n\nLocal configuration lives in a single YAML file in your home directory:\n\n**File Locations:**\n\n- macOS/Linux: `~/.continue/config.yaml`\n- Windows: `%USERPROFILE%\\.continue\\config.yaml`\n\n**Quick Access Method:**\n\n1. Open the configs dropdown in your IDE\n2. Click the gear icon next to \"Local Config\"\n3. The `config.yaml` file opens in your editor\n\n![local-config-open-steps](/images/local-config-open-steps.png)\n\n### The Local Config Experience\n\nWhen you edit your `config.yaml`, Continue provides intelligent autocomplete for all available options. Save the file, and Continue automatically reloads your configuration‚Äîno restart required.\n\nThe first time you use Continue, it generates a `config.yaml` with sensible defaults. From there, you can customize everything from model selection to context providers, slash commands, and more.\n\nFor the complete configuration reference, see our [config.yaml documentation](/reference).\n\n## How to Make the Right Choice\n\nThe decision between Hub and Local configs often comes down to your specific needs and constraints. Here's a framework to help you decide:\n\n### Choose Hub Configs When You:\n\n**Value Convenience Over Control**\n\n- Want to start coding with AI assistance in under 60 seconds\n- Prefer visual interfaces over editing configuration files\n- Need to switch between multiple machines frequently\n- Work in a team that needs standardized AI assistance\n\n**Need Advanced Collaboration**\n\n- Want to share custom configs with teammates\n- Need centralized API key management\n- Require quick updates across your entire organization\n\n**Are Getting Started**\n\n- New to AI-powered development\n- Want to experiment with different models and configurations\n- Prefer guided setup experiences\n\n### Choose Local Configs When You:\n\n**Require Maximum Control**\n\n- Need to version control your exact configuration\n- Want to customize every aspect of the AI behavior\n- Require reproducible development environments\n\n**Have Privacy Requirements**\n\n- Work with sensitive code that requires air-gapped environments\n- Need to ensure all configuration data stays local\n- Have strict compliance requirements about data storage\n\n**Are a Power User**\n\n- Comfortable editing YAML/JSON files\n- Want access to experimental features\n- Need to integrate with local tools and scripts\n\n## How to Use the Hybrid Approach\n\nHere's a secret: you don't have to choose just one. Many developers use both approaches:\n\n- **Hub Configs** for general development and experimentation\n- **Local Configs** for production work or client projects with specific requirements\n\nYou can switch between them seamlessly using the configs selector in your IDE.\n\n## Common Patterns and Best Practices\n\n### For Hub Config Users\n\n1. **Start with Community Configs**: Before creating your own, explore what others have built\n2. **Use Secrets Properly**: Never hardcode API keys‚Äîalways use the User Secrets feature\n3. **Create Specialized Configs**: Make different configs for different contexts (frontend, backend, DevOps)\n4. **Share Liberally**: If you create something useful, share it with the community\n\n### For Local Config Users\n\n1. **Version Control Your Config**: Treat your `config.yaml` like code‚Äîcommit it, review changes, and maintain history\n2. **Use Environment Variables**: For sensitive data, reference environment variables instead of hardcoding values\n3. **Document Your Setup**: Add comments to your config explaining non-obvious choices\n4. **Keep a Backup**: Before major changes, save a working copy of your configuration\n\n## Troubleshooting and Tips\n\n### Hub Config Issues\n\n**Changes Not Reflecting?**\n\n- Click \"Reload config\" in your IDE\n- Check your internet connection\n- Ensure you're signed in to the correct account\n\n**Config Not Available?**\n\n- Verify it's added to your account on Mission Control\n- Check if it requires specific API keys\n\n### Local Config Issues\n\n**Config Not Loading?**\n\n- Verify file location matches your OS\n- Check YAML syntax (Continue will show errors)\n- Ensure file permissions allow reading\n\n**Autocomplete Not Working?**\n\n- Update to the latest Continue version\n- Check that you're editing the correct file\n\n## Next Steps\n\nNow that you understand both configuration approaches, you're ready to dive deeper:\n\n  - **For Hub Users**: [Create A Config](/hub/configs/create-a-config)\n- **For Local Users**: [Explore the Config Reference](/reference)\n- **For Everyone**: [Discover Available Models](/customize/model-providers/overview)\n\nRemember, the best configuration is the one that helps you code more effectively. Start simple, experiment freely, and gradually refine your setup as you discover what works best for your workflow.\n\nHappy coding with Continue! üöÄ\n"}
{"source":"github","repo":"continue","path":"docs/guides/github-pr-review-bot.mdx","content":"---\ntitle: \"Code Review Bot with Continue and GitHub Actions\"\ndescription: \"Set up automated, context-aware pull request reviews using Continue CLI in GitHub Actions - privacy-first with custom rules\"\nsidebarTitle: \"Pull Request Review Bot\"\n---\n\n<Card title=\"What You'll Build\" icon=\"code-pull-request\">\n  An automated pull request review system that:\n  - Reviews code automatically when pull requests open or update\n  - Applies your team's custom rules and standards\n  - Runs in your GitHub Actions runner (code is sent directly to your configured LLM)\n  - Posts actionable feedback as pull request comments\n  - Responds to interactive review requests\n</Card>\n\n## Why This Approach?\n\n<CardGroup cols={3}>\n  <Card title=\"Privacy-First\" icon=\"shield\">\n    All logs and processing happen in your runner: Continue CLI runs in GitHub Actions ‚Üí code to your LLM provider (OpenAI, Anthropic, etc.). No hosted Continue service reads your code.\n\n  </Card>\n\n  <Card title=\"Customizable\" icon=\"sliders\">\n    Define team-specific rules in `.continue/rules/` that automatically apply to every pull request.\n\n  </Card>\n\n  <Card title=\"Context Awareness\" icon=\"robot\">\n    Leverage Continue's AI agent for intelligent, context-aware reviews with full control over your configuration.\n\n  </Card>\n</CardGroup>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- A GitHub repository with pull requests\n- Continue account with **Hub access**\n  - Read: [Understanding Configs](/guides/understanding-configs)\n- A Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n- Continue assistant configured for code reviews (or use our recommended default)\n\n<Info>\n  **Want to customize the review bot?**\n  \n  You can remix the default review bot configuration at [hub.continue.dev/continuedev/review-bot](https://hub.continue.dev/continuedev/review-bot) to create your own personalized version with custom prompts, rules, and behaviors.\n</Info>\n\n## Quick Setup (10 Minutes)\n\n<Steps>\n\n<Step title=\"Configure Repository Secrets and Variables\">\n\n    Navigate to your repository settings: **Settings ‚Üí Secrets and variables ‚Üí Actions**\n\n    **Required Secrets:**\n    - `CONTINUE_API_KEY` - Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n\n    **Optional (for better permissions):**\n    - **Variables** tab: `APP_ID` - GitHub App ID (for enhanced API rate limits)\n    - **Secrets** tab: `APP_PRIVATE_KEY` - GitHub App private key\n\n    <Accordion title=\"Setting up a GitHub App (Optional but Recommended)\">\n\n      For better rate limits and permissions, create a GitHub App:\n\n      1. Go to Settings ‚Üí Developer settings ‚Üí GitHub Apps ‚Üí New GitHub App\n      2. Set permissions:\n          - **Contents**: Read\n          - **Pull Requests**: Write\n          - **Issues**: Write\n      3. Generate a private key\n      4. Install the app on your repository\n      5. Add `APP_ID` as a repository **variable** (Variables tab)\n      6. Add `APP_PRIVATE_KEY` as a repository **secret** (Secrets tab)\n\n      Without a GitHub App, the action will use the default `GITHUB_TOKEN`.\n\n    </Accordion>\n\n  </Step>\n  <Step title=\"Add Workflow File\">\n\n    Create a GitHub Actions workflow file at `.github/workflows/code-review.yml` with the provided configuration.\n\n```yaml\nname: Continue Code Review\n\non:\n  pull_request:\n    types: [opened, synchronize, ready_for_review]\n  issue_comment:\n    types: [created]\n\npermissions:\n  contents: read\n  pull-requests: write\n  issues: write\n\njobs:\n  review:\n    runs-on: ubuntu-latest\n    # Only run on PRs or when @review-bot is mentioned\n    if: |\n      github.event_name == 'pull_request' ||\n      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@review-bot'))\n    \n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Full history for better context\n\n      # Optional: Use GitHub App token for better rate limits\n      - name: Generate App Token\n        id: app-token\n        if: vars.APP_ID != ''\n        uses: actions/create-github-app-token@v1\n        with:\n          app-id: ${{ vars.APP_ID }}\n          private-key: ${{ secrets.APP_PRIVATE_KEY }}\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - name: Install Continue CLI\n        run: npm i -g @continuedev/cli\n\n      - name: Get Pull Request Details\n        id: pr\n        env:\n          GH_TOKEN: ${{ steps.app-token.outputs.token || github.token }}\n        run: |\n          # Get pull request number and details\n          if [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n            PR_NUMBER=${{ github.event.pull_request.number }}\n          else\n            PR_NUMBER=$(jq -r .issue.number \"$GITHUB_EVENT_PATH\")\n          fi\n          \n          echo \"pr_number=$PR_NUMBER\" >> $GITHUB_OUTPUT\n          \n          # Get pull request diff\n          gh pr diff $PR_NUMBER > pr.diff\n          \n          # Get changed files\n          gh pr view $PR_NUMBER --json files -q '.files[].path' > changed_files.txt\n\n      - name: Run Continue Review\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n          GH_TOKEN: ${{ steps.app-token.outputs.token || github.token }}\n        run: |\n          # Check if custom rules exist\n          if [ -d \".continue/rules\" ]; then\n            echo \"üìã Found custom rules in .continue/rules/\"\n            RULES_CONTEXT=\"Apply the custom rules found in .continue/rules/ directory.\"\n          else\n            echo \"‚ÑπÔ∏è  No custom rules found. Using general best practices.\"\n            RULES_CONTEXT=\"Review for general best practices, security issues, and code quality.\"\n          fi\n          \n          # Build review prompt\n          PROMPT=\"Review this pull request with the following context:\n\n          ## Changed Files\n          $(cat changed_files.txt)\n\n          ## Diff\n          \\`\\`\\`diff\n          $(cat pr.diff)\n          \\`\\`\\`\n\n          ## Instructions\n          $RULES_CONTEXT\n\n          Provide:\n          1. A brief summary of changes\n          2. Key findings (potential issues, security concerns, suggestions)\n          3. Positive observations (good practices, improvements)\n          4. Specific actionable recommendations\n\n          Format as markdown suitable for a GitHub pull request comment.\"\n\n          # Run Continue CLI in headless mode\n          cn --config continuedev/review-bot \\\n             -p \"$PROMPT\" \\\n             --auto > review_output.md\n\n      - name: Post Review Comment\n        env:\n          GH_TOKEN: ${{ steps.app-token.outputs.token || github.token }}\n        run: |\n          # Add header\n          cat > review_comment.md <<'EOF'\n          ## ü§ñ AI Code Review\n          \n          EOF\n          \n          # Add review content\n          cat review_output.md >> review_comment.md\n          \n          # Add footer\n          cat >> review_comment.md <<'EOF'\n          \n          ---\n          *Powered by [Continue](https://continue.dev) ‚Ä¢ Need a focused review? Comment `@review-bot check for [specific concern]`*\n          EOF\n          \n          # Check for existing review comment\n          EXISTING_COMMENT=$(gh pr view ${{ steps.pr.outputs.pr_number }} \\\n            --json comments -q '.comments[] | select(.body | contains(\"ü§ñ AI Code Review\")) | .id' | head -1)\n          \n          if [ -n \"$EXISTING_COMMENT\" ]; then\n            echo \"Updating existing comment...\"\n            gh api --method PATCH \\\n              \"repos/${{ github.repository }}/issues/comments/$EXISTING_COMMENT\" \\\n              -f body=\"$(cat review_comment.md)\"\n          else\n            echo \"Creating new comment...\"\n            gh pr comment ${{ steps.pr.outputs.pr_number }} \\\n              --body-file review_comment.md\n          fi\n```\n</Step >\n\n<Step title=\"Create Custom Rules (Optional)\" >\n\nDefine your team's standards in `.continue/rules/`:\n\n<Tabs>\n  <Tab title=\"Security Rule\">\n    \n Create `.continue/rules/security.md`:\n\n```markdown\n---\nglobs: \"**/*.{ts,tsx,js,jsx,py}\"\ndescription: \"Security Review Standards\"\nalwaysApply: true\n---\n\n# Security Checklist\n\n- No hardcoded credentials, API keys, or secrets\n- All user inputs are validated and sanitized\n- SQL queries use parameterization (no string concatenation)\n- Authentication and authorization checks are in place\n- Sensitive data is properly encrypted\n- Error messages don't leak sensitive information\n```\n  </Tab>\n  <Tab title=\"TypeScript Standards\">\n    Create `.continue/rules/typescript.md`:\n\n```markdown\n  ---\n  globs: \"**/*.{ts,tsx}\"\n  description: \"TypeScript Best Practices\"\n  ---\n\n  # TypeScript Standards\n\n  - Use strict type checking (avoid `any` types)\n  - Prefer interfaces for object shapes\n  - Use proper error handling with typed errors\n  - Document complex types with JSDoc comments\n  - Export types for public APIs\n  - Use const assertions where appropriate\n```\n</Tab >\n<Tab title=\"Testing Standards\">\nCreate `.continue/rules/testing.md`:\n\n```markdown\n---\nglobs: \"**/*.{test,spec}.{ts,tsx,js,jsx}\"\ndescription: \"Testing Requirements\"\n---\n\n# Testing Guidelines\n\n- Write tests for new features and bug fixes\n- Test both happy paths and error conditions\n- Use descriptive test names that explain intent\n- Keep tests focused and isolated (no shared state)\n- Mock external dependencies\n- Aim for meaningful assertions, not coverage metrics\n```\n</Tab>\n<Tab title=\"Python Standards\">\nCreate `.continue/rules/python.md`:\n\n```markdown\n---\nglobs: \"**/*.py\"\ndescription: \"Python Code Standards\"\n---\n\n# Python Best Practices\n\n- Follow PEP 8 style guidelines\n- Use type hints for function signatures\n- Write docstrings for public functions/classes\n- Use context managers for resource management\n- Prefer list comprehensions for simple transformations\n- Handle exceptions explicitly, avoid bare except\n```\n</Tab>\n</Tabs>\n</Step>\n</Steps>\n## How It Works\n\nThe workflow follows these steps:\n\n1. **Pull Request Created/Updated** - A pull request is opened or synchronized\n2. **Workflow Triggered** - GitHub Actions workflow starts automatically\n3. **Load Custom Rules** - Reads your team's rules from `.continue/rules/`\n4. **Get Pull Request Diff** - Fetches the diff and list of changed files\n5. **Continue CLI Analyzes Code** - AI agent reviews the code with your rules\n6. **Post or Update Review Comment** - Creates or updates a single PR comment with feedback\n\n## Interactive Commands\n\nComment on any pull request to trigger focused reviews:\n\n```\n@review-bot check for security issues\n@review-bot review the TypeScript types\n@review-bot explain the architecture changes\n@review-bot focus on error handling\n```\n\nThe workflow will respond with a targeted review based on your request.\n\n## Advanced Configuration\n<Tabs>\n  <Tab title=\"Use Your Own Continue Config\">\n    \n  By default, the workflow uses the `continuedev/review-bot` config optimized for code reviews. Replace `continuedev/review-bot` with your own config:\n\n  ```yaml\n  - name: Run Continue Review\n    env:\n      CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n      CONTINUE_ORG: your-org-name      # Add your org\n      CONTINUE_CONFIG: username/config-name  # Add your config\n    run: |\n      cn --config $CONTINUE_ORG/$CONTINUE_CONFIG \\\n        -p \"$PROMPT\" \\\n        --auto > review_output.md\n  ```\n\n  Store `CONTINUE_ORG` and `CONTINUE_CONFIG` as repository variables for easy updates.\n</Tab>\n<Tab title=\"Filter by File Types\">\n\n  Only review specific file types in pull requests:\n\n  ```yaml\n  on:\n    pull_request:\n      types: [opened, synchronize, ready_for_review]\n      paths:\n        - '**.ts'\n        - '**.tsx'\n        - '**.py'\n        - '**.go'\n  ```\n</Tab>\n\n<Tab title=\"Review Size Limits\">\n\n  Skip large pull requests that would be expensive to review:\n\n  ```yaml\n  - name: Check PR Size\n    id: size-check\n    env:\n      GH_TOKEN: ${{ steps.app-token.outputs.token || github.token }}\n    run: |\n      ADDITIONS=$(gh pr view ${{ steps.pr.outputs.pr_number }} --json additions -q .additions)\n      DELETIONS=$(gh pr view ${{ steps.pr.outputs.pr_number }} --json deletions -q .deletions)\n      TOTAL=$((ADDITIONS + DELETIONS))\n      \n      if [ $TOTAL -gt 1000 ]; then\n        echo \"skip=true\" >> $GITHUB_OUTPUT\n        echo \"‚ö†Ô∏è  Pull request too large for automatic review ($TOTAL lines changed)\"\n      else\n        echo \"skip=false\" >> $GITHUB_OUTPUT\n      fi\n\n  - name: Run Continue Review\n    if: steps.size-check.outputs.skip != 'true'\n    # ... rest of review step\n  ```\n\n</Tab>\n</Tabs>\n## Troubleshooting\n<AccordionGroup>\n<Accordion title=\"Continue CLI not found\">\n- The workflow installs the CLI automatically, but ensure Node.js 20+ is available\n- Check the \"Install Continue CLI\" step logs for errors\n</Accordion>\n<Accordion title=\"Authentication failed\">\n- Verify your `CONTINUE_API_KEY` is valid\n- Check that GitHub token has required permissions\n</Accordion>\n<Accordion title=\"No review generated\">\n- Verify your Continue config is accessible\n- Check Continue CLI logs in the workflow run\n- Try running locally: `cn -p \"Test prompt\" --auto`\n</Accordion>\n<Accordion title=\"Review comment not posted\">\n- Ensure `pull-requests: write` permission is set\n- Verify GitHub token has scope to comment on pull requests\n- Check if the repository requires signed commits\n</Accordion>\n</AccordionGroup>\n\n\n## Example Output\n\nHere's what a typical review comment looks like:\n\n> ## ü§ñ AI Code Review\n>\n> ### Summary\n> This pull request introduces a new user authentication system with JWT tokens and password hashing. The implementation follows security best practices with a few minor suggestions.\n>\n> ### Key Findings\n>\n> **Security** ‚úÖ\n> - Password hashing properly implemented with bcrypt\n> - JWT tokens include appropriate expiry\n> - Input validation present for all endpoints\n>\n> **Code Quality** üí°\n> - Consider adding rate limiting to login endpoint\n> - The `secretKey` should be loaded from environment variables, not hardcoded\n> - Add unit tests for token expiration edge cases\n>\n> ### Positive Observations\n> - Good separation of concerns with middleware pattern\n> - Clear error messages for authentication failures\n> - Proper async/await usage throughout\n>\n> ### Recommendations\n> 1. Move `secretKey` to environment variables (see `.continue/rules/security.md`)\n> 2. Add rate limiting middleware to prevent brute force attacks\n> 3. Consider adding integration tests for the auth flow\n> 4. Document the JWT payload structure\n>\n> ---\n> *Powered by [Continue](https://continue.dev) ‚Ä¢ Need a focused review? Comment `@review-bot check for security`*\n\n## What You've Built\n\nAfter completing this setup, you have an **AI-powered code review system** that:\n\n- ‚úÖ **Runs automatically** - Reviews every pull request without manual intervention\n- ‚úÖ **Privacy-first** - CLI runs in your GitHub Actions runner, code sent directly to your configured LLM\n- ‚úÖ **Customizable** - Team-specific rules apply automatically\n- ‚úÖ **Interactive** - Responds to focused review requests\n- ‚úÖ **Continuous** - Updates reviews as pull requests change\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your pull request workflow now operates at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine code review with human oversight through review and\n  approval.\n</Card>\n\n## Next Steps\n\n1. **Test it out** - Create a test pull request and watch the review appear\n2. **Refine rules** - Add more custom rules specific to your codebase\n3. **Customize prompts** - Adjust the review prompt to match your team's style\n4. **Add metrics** - Track review effectiveness over time\n5. **Create team config** - Set up a shared Continue config for consistent reviews\n\n## Inspiration & Resources\n\n<CardGroup cols={2}>\n  <Card title=\"CodeBunny\" icon=\"github\" href=\"https://github.com/bdougie/codebunny\">\n    Original inspiration - Privacy-first AI code reviews\n  </Card>\n  <Card title=\"Continue CLI Guide\" icon=\"terminal\" href=\"/guides/cli\">\n    Learn more about Continue CLI capabilities\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"globe\" href=\"https://hub.continue.dev\">\n    Browse shared configs and create your own\n  </Card>\n  <Card title=\"Rules Documentation\" icon=\"book\" href=\"/customize/deep-dives/rules\">\n    Deep dive into custom rules\n  </Card>\n</CardGroup>\n\n## Community Examples\n\nShare your pull request review bot configuration:\n- Tweet your setup with `#ContinueDev`\n- Share improvements as [GitHub discussions](https://github.com/continuedev/continue/discussions)\n- Contribute example rules to the docs\n"}
{"source":"github","repo":"continue","path":"docs/guides/continue-docs-mcp-cookbook.mdx","content":"---\ntitle: \"Contributing to Continue with Model Context Protocol (MCP)\"\ndescription: \"Use the Continue Docs MCP to write cookbooks, guides, and documentation with AI-powered workflows.\"\nsidebarTitle: \"Continue Docs MCP Cookbook\"\n---\n\n<Card title=\"What You'll Build\" icon=\"book-open-cover\">\n  Master using the Continue Docs MCP to contribute documentation, create cookbooks, and maintain consistency across Continue's docs - all through natural language prompts.\n</Card>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Continue account with **Hub access**\n- Read: [Contributing to Continue Documentation](/CONTRIBUTING) for setup instructions\n- Forked the [continuedev/continue](https://github.com/continuedev/continue) repository\n- Node.js 20+ and Continue CLI installed\n\n<Warning>\n  This cookbook assumes you've read the setup in the [CONTRIBUTING guide](/CONTRIBUTING). If you haven't, start there first.\n</Warning>\n\n## Continue Docs MCP Setup\n\n<Card title=\"Fastest Path to Success\" icon=\"rocket\">\n  Use the pre-built [Docs Assistant - Mintlify agent](https://hub.continue.dev/continuedev/docs-mintlify) that includes the Continue Docs MCP and is ready to use immediately.\n</Card>\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    ```bash\n    # From your Continue docs directory\n    cn --config continuedev/docs-mintlify\n    ```\n\n    This agent includes:\n    - **Continue Docs MCP** for searching Continue documentation\n    - **Mintlify formatting rules** for proper component usage\n    - **Documentation-focused prompts** for common tasks\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Custom Agent\">\n    If you want to customize, create your own agent and add:\n    1. [Continue Docs MCP](https://hub.continue.dev/continuedev/continue-docs-mcp)\n    2. [Mintlify Technical Writing Rule](https://hub.continue.dev/mintlify/technical-writing-rule)\n\n    See the [CONTRIBUTING guide](/CONTRIBUTING#option-2-create-your-own-custom-agent) for details.\n  </Tab>\n</Tabs>\n\n---\n\n## What is the Continue Docs MCP?\n\n<Card title=\"Continue Docs MCP\" icon=\"circle-info\">\n  A Model Context Protocol server built with [Mintlify's MCP generation](https://www.mintlify.com/blog/generate-mcp-servers-for-your-docs) that enables semantic search across Continue documentation. [Learn more ‚Üí](/reference/continue-mcp)\n</Card>\n\nThe MCP helps you:\n- **Find examples** from existing documentation\n- **Maintain consistency** with established patterns\n- **Source accurate information** about Continue features\n- **Write better documentation** faster\n\n---\n\n## Documentation Workflows with Prompts\n\n### üÜï Creating a New Cookbook\n\nCookbooks show how to use Continue CLI with specific tools or services. Here's how to create one using the Continue Docs MCP:\n\n<Steps>\n  <Step title=\"Research Existing Cookbooks\">\n    ```bash\n    cn --config continuedev/docs-mintlify\n    ```\n\n    **Prompt:**\n    ```\n    \"Show me the structure of existing MCP cookbooks in the Continue docs.\n    I want to create a cookbook for GitHub MCP.\"\n    ```\n\n    The agent will find examples like the dlt, Snyk, and Sanity cookbooks.\n  </Step>\n\n  <Step title=\"Source Official Documentation\">\n    **Prompt:**\n    ```\n    \"Using the Continue Docs MCP, find information about how GitHub MCP works.\n    Then search the web for the official GitHub MCP documentation and combine\n    both sources to create a cookbook following the same structure as the\n    dlt cookbook.\"\n    ```\n\n    The agent will:\n    - Search Continue docs for MCP patterns\n    - Fetch GitHub MCP official documentation\n    - Combine both sources\n    - Generate a cookbook with consistent formatting\n  </Step>\n\n  <Step title=\"Add to Navigation\">\n    **Prompt:**\n    ```\n    \"Add my new github-mcp-continue-cookbook.mdx to the docs.json navigation\n    under the Cookbooks section.\"\n    ```\n  </Step>\n\n  <Step title=\"Preview and Iterate\">\n    ```bash\n    npm run dev\n    # Visit http://localhost:3000\n    ```\n\n    **Refine with prompts:**\n    ```\n    \"Add more examples to the troubleshooting section\"\n    \"Include a CI/CD workflow example using GitHub Actions\"\n    \"Add authentication setup using environment variables\"\n    ```\n  </Step>\n</Steps>\n\n<Info>\n  **Pro Tip:** The agent uses the Continue Docs MCP to maintain consistency with existing cookbooks automatically.\n</Info>\n\n---\n\n### ‚úèÔ∏è Updating Existing Documentation\n\n<Steps>\n  <Step title=\"Find the Documentation\">\n    **Prompt:**\n    ```\n    \"Where is the MCP tools documentation located? Show me the file path.\"\n    ```\n  </Step>\n\n  <Step title=\"Make Targeted Updates\">\n    **Prompt:**\n    ```\n    \"Update the MCP tools documentation at docs/customization/mcp-tools.mdx\n    to include information about the new Slack MCP server. Use the Continue\n    Docs MCP to find examples of how other MCP servers are documented, then\n    add a similar section for Slack.\"\n    ```\n  </Step>\n</Steps>\n\n---\n\n### üìù Adding New Guides\n\n<Steps>\n  <Step title=\"Research Similar Content\">\n    **Prompt:**\n    ```\n    \"I want to create a guide for setting up Continue with Amazon Bedrock.\n    Search the Continue docs for similar model provider setup guides and\n    show me the common structure they follow.\"\n    ```\n  </Step>\n\n  <Step title=\"Create the Guide\">\n    **Prompt:**\n    ```\n    \"Create a new guide at docs/guides/amazon-bedrock-setup.mdx following\n    the structure you found. Include:\n    - Prerequisites\n    - Step-by-step setup with code examples\n    - Configuration options\n    - Troubleshooting common issues\n\n    Use the Continue Docs MCP to find accurate information about Continue's\n    model provider configuration.\"\n    ```\n  </Step>\n\n  <Step title=\"Add Navigation Entry\">\n    **Prompt:**\n    ```\n    \"Add this guide to docs.json under the Model Providers section\"\n    ```\n  </Step>\n</Steps>\n\n---\n\n## Real-World Cookbook Examples\n\n### Example 1: PostgreSQL MCP Cookbook\n\n**Prompt:**\n```\n\"Create a cookbook for using Continue with PostgreSQL MCP. Follow the same\nstructure as the dlt cookbook, but customize it for database operations.\n\nInclude these sections:\n1. Quick start with pre-built agent\n2. Manual setup with MCP configuration\n3. Common database tasks (schema exploration, query writing, migrations)\n4. Troubleshooting database connection issues\n\nUse the Continue Docs MCP to find MCP configuration patterns and search the\nweb for PostgreSQL MCP documentation.\"\n```\n\n### Example 2: Sentry Error Tracking Cookbook\n\n**Prompt:**\n```\n\"Create a cookbook showing how to use Continue to analyze Sentry errors.\n\nThe cookbook should demonstrate:\n1. Setting up Sentry MCP integration\n2. Querying recent errors\n3. Analyzing error patterns with AI\n4. Generating fixes for common errors\n5. CI/CD integration for automated error analysis\n\nResearch the Snyk cookbook structure since it's also about error detection,\nand adapt it for Sentry.\"\n```\n\n### Example 3: OpenAPI Documentation Cookbook\n\n**Prompt:**\n```\n\"I want to create a cookbook for using Continue to work with OpenAPI specs.\n\nShow how to:\n1. Load OpenAPI specs into Continue's context\n2. Generate API client code from specs\n3. Create tests based on API endpoints\n4. Update documentation when APIs change\n\nUse the Continue Docs MCP to find how context providers work, then combine\nthat with OpenAPI MCP information.\"\n```\n\n---\n\n## Advanced Prompt Patterns\n\n<CardGroup cols={2}>\n  <Card title=\"Multi-Source Research\" icon=\"magnifying-glass\">\n    ```bash\n    \"Use the Continue Docs MCP to understand how agents\n    work in Continue, then search the web for Anthropic's\n    Computer Use MCP. Create a cookbook showing how to\n    combine Continue agents with Computer Use for\n    automated testing.\"\n    ```\n  </Card>\n\n  <Card title=\"Cross-Reference Documentation\" icon=\"link\">\n    ```bash\n    \"Find all mentions of MCP servers across Continue\n    documentation. Then create a comprehensive reference\n    page that links to all MCP-related guides and\n    configurations.\"\n    ```\n  </Card>\n\n  <Card title=\"Consistency Checking\" icon=\"check-double\">\n    ```bash\n    \"Review all cookbook files in docs/guides/ and check\n    if they follow the same structure. List any\n    inconsistencies and suggest updates to make them\n    uniform.\"\n    ```\n  </Card>\n\n  <Card title=\"Example Extraction\" icon=\"code\">\n    ```bash\n    \"Extract all code examples showing MCP server\n    configuration from the Continue docs. Format them\n    as a single reference page with explanations for\n    each pattern.\"\n    ```\n  </Card>\n</CardGroup>\n\n---\n\n## Testing Your Documentation\n\n### Local Preview\n\n```bash\ncd docs\nnpm run dev\n# Visit http://localhost:3000\n```\n\n### Validation Prompts\n\n```bash\n# Check formatting\n\"Review this documentation for Mintlify formatting issues and fix any problems\"\n\n# Verify links\n\"Check all links in this file and ensure they point to existing documentation\"\n\n# Test code examples\n\"Verify that all code examples in this cookbook are syntactically correct\"\n```\n\n---\n\n## Submitting Your Contribution\n\n**Prompt:**\n```bash\n\"Create a pull request with my documentation changes and use the repo's\nPR template to write the description\"\n```\n\nThe agent will:\n- Create a feature branch\n- Stage and commit your changes\n- Push to your fork\n- Generate a PR description following the template\n- Open the PR for review\n\n<Info>\n  See the full [Contributing Guide](/CONTRIBUTING#submitting-your-contribution) for manual PR submission details.\n</Info>\n\n---\n\n## Automated Documentation Checks with GitHub Actions\n\nAdd automated documentation checks to your PR workflow using the Continue Docs MCP agent:\n\n```yaml title=\".github/workflows/docs-check.yml\"\nname: Documentation Check\n\non:\n  pull_request:\n    types: [opened, synchronize]\n    paths:\n      - 'core/**'\n      - 'extensions/**'\n      - 'packages/**'\n      - 'gui/**'\n\njobs:\n  check-docs:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - name: Install Continue CLI\n        run: npm i -g @continuedev/cli\n\n      - name: Analyze Changes for Documentation Needs\n        id: analyze\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n        run: |\n          echo \"üîç Analyzing code changes for documentation needs...\"\n\n          # Get changed files\n          git diff origin/${{ github.base_ref }}..HEAD --name-only > changed_files.txt\n\n          # Create detailed diff\n          git diff origin/${{ github.base_ref }}..HEAD > changes.diff\n\n          # Use Continue agent to check if docs need updates\n          PROMPT=\"Review these code changes and determine if documentation updates are needed.\n\n          Changed files: $(cat changed_files.txt | tr '\\n' ' ')\n\n          Consider:\n          - New features or APIs\n          - Breaking changes\n          - New configuration options\n          - Modified CLI commands\n          - New MCP integrations\n\n          If docs updates are needed, specify which files in docs/ should be updated.\n          If no updates needed, respond with 'NO_DOCS_NEEDED'.\n          Be specific and concise.\"\n\n          cn --config continuedev/docs-mintlify -p \"$PROMPT\" --auto > analysis.md || {\n            echo \"‚ö†Ô∏è Analysis failed\"\n            echo \"needs_docs=false\" >> $GITHUB_OUTPUT\n            exit 0\n          }\n\n          if grep -q \"NO_DOCS_NEEDED\" analysis.md; then\n            echo \"needs_docs=false\" >> $GITHUB_OUTPUT\n          else\n            echo \"needs_docs=true\" >> $GITHUB_OUTPUT\n          fi\n\n      - name: Post Documentation Recommendation\n        if: steps.analyze.outputs.needs_docs == 'true'\n        env:\n          GH_TOKEN: ${{ github.token }}\n        run: |\n          echo \"üìù Creating PR comment with documentation recommendations...\"\n\n          cat > pr-comment.md <<'EOF'\n## üìö Documentation Update Recommended\n\nBased on the code changes in this PR, documentation updates may be needed:\n\nEOF\n\n          cat analysis.md >> pr-comment.md\n\n          cat >> pr-comment.md <<'EOF'\n\n---\n\n### Next Steps\n\n1. Review the suggested documentation areas above\n2. Update relevant files in the `/docs` folder\n3. Consider updating:\n   - API references for interface changes\n   - Guides for workflow changes\n   - Configuration docs for new settings\n   - MCP cookbooks for new integrations\n\n### Resources\n- [Contributing to Docs](/CONTRIBUTING)\n- [Continue Docs MCP Cookbook](/guides/continue-docs-mcp-cookbook)\n\n*This analysis was generated using the Continue Docs MCP agent.*\nEOF\n\n          gh pr comment ${{ github.event.pull_request.number }} --body-file pr-comment.md\n\n      - name: Add Label\n        if: steps.analyze.outputs.needs_docs == 'true'\n        env:\n          GH_TOKEN: ${{ github.token }}\n        run: |\n          gh pr edit ${{ github.event.pull_request.number }} --add-label \"docs-needed\"\n```\n\n<Info>\n  This workflow uses the Continue Docs MCP agent to analyze code changes and automatically comment on PRs when documentation updates are recommended.\n</Info>\n\n---\n\n## Documentation MCP for Other Projects\n\nWant to create documentation MCPs for your own projects? Mintlify makes it easy:\n\n<Steps>\n  <Step title=\"Set Up Mintlify Docs\">\n    ```bash\n    npm i -g mintlify\n    mintlify init\n    ```\n  </Step>\n\n  <Step title=\"MCP Auto-Generated\">\n    Mintlify automatically generates an MCP server for your documentation, enabling semantic search.\n\n    [Learn more about Mintlify MCP generation ‚Üí](https://www.mintlify.com/blog/generate-mcp-servers-for-your-docs)\n  </Step>\n\n  <Step title=\"Publish to Continue Mission Control\">\n    Share your docs MCP on [Continue Mission Control](https://hub.continue.dev/new?type=mcp) so others can use it with Continue agents.\n  </Step>\n</Steps>\n\n<Info>\n  The [Continue Docs MCP](/reference/continue-mcp) itself was built this way!\n</Info>\n\n---\n\n## Key Prompts Cheat Sheet\n\n| Task | Prompt |\n|:-----|:-------|\n| **Find structure** | \"Show me the structure of existing cookbooks in Continue docs\" |\n| **Create cookbook** | \"Create a cookbook for [tool] MCP following the dlt cookbook structure\" |\n| **Update docs** | \"Update [file] to include information about [feature], using Continue Docs MCP to find examples\" |\n| **Add navigation** | \"Add [file] to docs.json under the [section] section\" |\n| **Check consistency** | \"Review this file for consistency with other Continue documentation\" |\n| **Fix formatting** | \"Review for Mintlify formatting issues and fix any problems\" |\n| **Extract examples** | \"Find all examples of [topic] in Continue docs\" |\n| **Research feature** | \"Use Continue Docs MCP to explain how [feature] works in Continue\" |\n\n---\n\n## Resources\n\n### Continue Documentation\n- [Contributing Guide](/CONTRIBUTING) - Setup and submission process\n- [Continue Docs MCP Reference](/reference/continue-mcp) - MCP server details\n  - [Understanding Configs](/guides/understanding-configs) - How configs work\n\n### Mintlify\n- [MCP Generation Guide](https://www.mintlify.com/blog/generate-mcp-servers-for-your-docs)\n- [Mintlify Documentation](https://mintlify.com/docs)\n\n### Community\n- [Continue Discord](https://discord.gg/continue) - #documentation channel\n- [GitHub Discussions](https://github.com/continuedev/continue/discussions)\n\n---\n\n## Next Steps\n\n<CardGroup cols={2}>\n  <Card title=\"Start Contributing\" icon=\"code\" href=\"/CONTRIBUTING\">\n    Set up your environment\n  </Card>\n\n  <Card title=\"Use Docs Agent\" icon=\"robot\" href=\"https://hub.continue.dev/continuedev/docs-mintlify\">\n    Install pre-built agent\n  </Card>\n\n  <Card title=\"Browse Cookbooks\" icon=\"book-open\" href=\"/guides/overview\">\n    See cookbook examples\n  </Card>\n\n  <Card title=\"Join Discord\" icon=\"discord\" href=\"https://discord.gg/continue\">\n    Get help from community\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/guides/klavis-mcp-continue-cookbook.mdx","content":"---\ntitle: \"Slack and Gmail Automation with Klavis AI Strata MCP and Continue\"\ndescription: \"Use Continue and Klavis AI's Strata MCP to automate communication workflows across Slack and Gmail with natural language prompts.\"\nsidebarTitle: \"Klavis AI with Continue\"\n---\n\n<Card title=\"What You'll Build\" icon=\"wand-magic-sparkles\">\n  A communication workflow assistant that uses Continue CLI with Klavis AI's Strata MCP to manage and automate multi-platform workflows including Slack and Gmail‚Äîall through simple natural language prompts.\n</Card>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- [Continue CLI](https://docs.continue.dev/guides/cli) with **active credits** (required for API usage)\n- A Klavis [AI account](https://www.klavis.ai/) with access to your Strata URL\n- OAuth authentication completed in Klavis dashboard for Slack and Gmail (see [Klavis integrations guide](https://www.klavis.ai/integrations))\n\n<Warning>\n  To use agents in headless mode, you need a [Continue API key](https://hub.continue.dev/settings/api-keys).\n  Klavis AI Strata requires proper OAuth authentication for each service integration.\n</Warning>\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n  ```bash\n  npm i -g @continuedev/cli\n  ```\n  </Step>\n\n<Step title=\"Get Klavis AI's Strata URL\">\n  1. Visit [klavis.ai](https://www.klavis.ai/) and create an account\n  2. Navigate to your dashboard and generate [Strata URL](https://www.klavis.ai/home/mcp-servers)\n  3. Copy the Strata URL for configuration\n  </Step>\n</Steps>\n\n## Klavis AI Strata MCP Setup\n\n<Tabs>\n  <Tab title=\"Manual Setup\">\n    <Steps>\n      <Step title=\"Create an Agent on Continue Hub\">\n        Go to the [Continue Hub](https://hub.continue.dev) and [create a new agent](https://hub.continue.dev/new?type=agent).\n      </Step>\n\n      <Step title=\"Add Klavis AI Strata MCP\">\n        Add the following YAML configuration to your agent:\n\n        ```yaml title=\"config.yaml\"\n            name: klavis-strata\n            version: 0.0.1\n            schema: v1\n\n            mcpServers:\n              - name: klavis-strata\n                command: npx\n                args:\n                  - \"mcp-remote\"\n                  - \"https://strata.klavis.ai/mcp/?strata_id=your_strata_id_here\"\n        ```\n\n        **Notes:**\n        - Keep your Klavis Strata URL secret and secure\n        - Ensure your Klavis account has Slack and Gmail integrations activated\n        - Complete OAuth flows for both Slack and Gmail in the Klavis dashboard\n      </Step>\n\n      <Step title=\"Test the Connection\">\n        Launch Continue and ask:\n        ```\n        Check my latest 5 emails and summarize them in a Slack message to #general.\n        ```\n      </Step>\n    </Steps>\n\n    <Info>\n      **Pro tip**: Learn how to integrate and use [Klavis Strata](https://www.klavis.ai/docs/knowledge-base/use-mcp-server/continue) with Continue through CLI and IDE.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"CLI Setup\">\n    <Steps>\n      <Step title=\"Set Environment Variables\">\n        ```bash\n        export KLAVIS_API_KEY=\"your_api_key_here\"\n        ```\n      </Step>\n\n      <Step title=\"Launch Continue with Klavis MCP\">\n        From your project root:\n        ```bash\n        cn --agent your-agent-name\n        ```\n        Now try:\n        ```\n        \"Send a Slack message to #general saying 'Hello from Continue!'\"\n        ```\n      </Step>\n    </Steps>\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n  To use Klavis AI Strata MCP with Continue CLI, you need either:\n  - **Continue CLI Pro Plan** with the models add-on, OR\n  - **Your own API keys** added to Continue Hub secrets\n\n  The agent will automatically detect and use your configuration along with the Klavis AI Strata MCP for multi-tool operations.\n</Accordion>\n\n---\n\n## Slack Workflows\n\nUse natural language to send messages, manage channels, and coordinate team communications through Slack.\n\n<Info>\n  **Where to run these workflows:**\n  - **IDE Extensions**: Use Continue in VS Code, JetBrains, or other supported IDEs\n  - **Terminal (TUI mode)**: Run `cn` to enter interactive mode, then type your prompts\n  - **CLI (headless mode)**: Use `cn -p \"your prompt\" --auto` for automation\n\n  To run any of the example prompts below in headless mode, use `cn -p \"prompt\" --auto`\n</Info>\n\n### Message Management\n\n<Card title=\"Send Channel Message\" icon=\"message\">\n  Post messages to Slack channels.\n\n**Prompt:**\n```\nSend a message to #engineering channel:\n\"Deploy completed successfully! Version 2.1.0 is now live.\"\n```\n</Card>\n\n<Card title=\"Send Direct Message\" icon=\"user\">\n  Message team members directly.\n\n**Prompt:**\n```\nSend a direct message to @sarah.chen:\n\"Hi Sarah, could you review the PR for the authentication refactor when you have a chance? It's ready for review.\"\n```\n</Card>\n\n<Card title=\"Thread Reply\" icon=\"reply\">\n  Respond to existing Slack threads.\n\n**Prompt:**\n```\nFind the thread about \"Q1 Planning\" in #product-strategy from the last 2 days.\nReply with: \"I've updated the roadmap document with the Q1 priorities we discussed. Link: [doc_url]\"\n```\n</Card>\n\n### Channel and Workspace Management\n\n<Card title=\"List Channels\" icon=\"list\">\n  Discover available channels.\n\n**Prompt:**\n```\nList all public channels in the workspace that contain \"engineering\" or \"dev\" in the name.\n```\n</Card>\n\n<Card title=\"Create Channel\" icon=\"plus\">\n  Set up new communication channels.\n\n**Prompt:**\n```\nCreate a new Slack channel called #klavis-strata\n```\n</Card>\n\n<Card title=\"Archive Old Channels\" icon=\"archive\">\n  Clean up inactive channels.\n\n**Prompt:**\n```\nFind and archive all channels with no messages in the last 90 days.\n```\n</Card>\n\n---\n\n## Gmail Workflows\n\nManage email communications, automate responses, and coordinate notifications through Gmail.\n\n### Email Management\n\n<Card title=\"Send Email\" icon=\"envelope\">\n  Compose and send emails with a single prompt.\n\n**Prompt:**\n```\nSend an email to team@company.com regarding Weekly Status Update.\n```\n</Card>\n\n<Card title=\"Read Recent Emails\" icon=\"inbox\">\n  Check and summarize recent messages.\n\n**Prompt:**\n```\nRead all unread emails from the last 24 hours.\n```\n</Card>\n\n<Card title=\"Search Email Thread\" icon=\"magnifying-glass\">\n  Find specific conversations.\n\n**Prompt:**\n```\nSearch my emails from the last 30 days containing \"invoice\" or \"payment\".\n```\n</Card>\n\n### Advanced Email Operations\n\n<Card title=\"Send with Attachments\" icon=\"paperclip\">\n  Attach files to outgoing emails.\n\n**Prompt:**\n```\nSend an email to client@company.com with subject \"Monthly Report 2025\" and attach the file: reports/monthly-2025-analytics.pdf\n```\n</Card>\n\n---\n\n## Cross-Platform Workflows\n\nCombine Slack and Gmail for powerful multi-channel communication automation.\n\n### Notification Coordination\n\n<Card title=\"Slack + Gmail Announcements\" icon=\"bullhorn\">\n  Send coordinated messages across platforms.\n\n**Prompt:**\n```\nSend an announcement about Scheduled Maintenance @ Sat 2-6 AM EST to #engineering channel and also email to engineering@klavis.ai\n```\n</Card>\n\n<Card title=\"Issue Escalation\" icon=\"triangle-exclamation\">\n  Coordinate urgent communications.\n\n**Prompt:**\n```\nA critical bug has been reported (issue #2847 - payment processing failure). Send this urgent message to #engineering-alerts with issue description, assessment, and immediate action items. Finally, create a Slack thread for coordination and updates.\n```\n</Card>\n\n### Meeting and Event Coordination\n\n<Card title=\"Meeting Invitation\" icon=\"calendar\">\n  Coordinate meeting schedules across platforms.\n\n**Prompt:**\n```\nSchedule a sprint planning meeting:\n\nGmail invitations to: team@company.com\nSubject: \"Sprint 24 Planning - Q2 Roadmap\"\nTime: Next Monday, 10 AM EST\nDuration: 2 hours\n\nSlack notification to #engineering:\n\"Sprint Planning scheduled for Monday 10 AM EST\nPlease review the backlog before the meeting: [link]\nAdd agenda items to the shared doc by Friday.\"\n```\n</Card>\n\n<Card title=\"Status Updates\" icon=\"chart-line\">\n  Share progress updates across channels.\n\n**Prompt:**\n```\nGenerate and distribute weekly status update:\n\n1. Analyze my recent work (commits, PRs, issues closed)\n2. Draft status summary with:\n   - Completed work\n   - In-progress items\n   - Blockers\n   - Next week's priorities\n\n3. Send to Slack #team-updates (formatted for Slack)\n4. Email to manager@company.com (formal email format)\n```\n</Card>\n\n### Customer Communication\n\n<Card title=\"Support Ticket Response\" icon=\"headset\">\n  Coordinate customer support across channels.\n\n**Prompt:**\n```\nA customer reported an issue via email (support ticket #5234 - login problems).\n\n1. Send acknowledgment email to customer@client.com:\n   \"We've received your report about login issues. Investigating now.\n    Ticket #5234 - Expected resolution: 4 hours\"\n\n2. Alert Slack #customer-support:\n   \"New Priority 1 Ticket #5234: Login issues affecting Customer XYZ\n    Assigned to: @on-call-engineer\n    Details: [summary]\"\n\n3. Create Slack thread for internal coordination\n```\n</Card>\n\n---\n\n## Multi-Tool Integration Workflows\n\nLeverage Klavis AI Strata's integrations for comprehensive automation workflows.\n\n<Info>\n  Klavis AI Strata supports integrations with Notion, Linear, GitHub, Asana, Salesforce, HubSpot, and more. Combine these with Slack and Gmail to create powerful cross-platform workflows.\n</Info>\n\n### Development Workflows\n\n<Card title=\"PR Review Notifications\" icon=\"code-pull-request\">\n  Automate code review coordination.\n\n**Prompt:**\n```\nMonitor GitHub for new pull requests in the main repository.\n\nWhen a new PR is created:\n1. Send Slack message to #code-reviews:\n   \"New PR ready for review: [PR title]\n    Author: [name]\n    Changes: [summary]\n    Link: [github_url]\"\n\n2. If PR is labeled \"urgent\":\n   - Send Gmail to tech-leads@company.com\n   - Ping reviewer in Slack DM\n```\n</Card>\n\n<Card title=\"Deployment Notifications\" icon=\"rocket\">\n  Coordinate deployment communications.\n\n**Prompt:**\n```\nAfter successful deployment to production:\n\n1. Update Notion deployment log with:\n   - Version number\n   - Deployment timestamp\n   - Changes included\n   - Rollback procedure\n\n2. Announce in Slack #announcements:\n   \"Production deployment complete: v2.5.0\n    Release notes: [notion_link]\"\n\n3. Email stakeholders@company.com with:\n   - Executive summary of changes\n   - User-facing improvements\n   - Link to full documentation\n```\n</Card>\n\n### Project Management Integration\n\n<Card title=\"Task Sync Workflow\" icon=\"tasks\">\n  Synchronize tasks across platforms.\n\n**Prompt:**\n```\nDaily task synchronization:\n\n1. Fetch high-priority tasks from Linear (assigned to me, due this week)\n2. Create Notion task list with priorities and deadlines\n3. Send Slack DM reminder about tasks due today\n4. If any tasks are overdue, email manager@company.com with status update\n```\n</Card>\n\n<Card title=\"Sprint Report Generation\" icon=\"chart-pie\">\n  Automate sprint reporting.\n\n**Prompt:**\n```\nGenerate end-of-sprint report:\n\n1. Analyze completed work:\n   - Linear: Closed issues and story points\n   - GitHub: Merged PRs and code changes\n   - Notion: Sprint goals completion status\n\n2. Create comprehensive report in Notion\n3. Send summary to Slack #sprint-reviews\n4. Email detailed report to leadership@company.com with:\n   - Velocity metrics\n   - Burndown chart\n   - Highlights and challenges\n```\n</Card>\n\n---\n\n## Automation Examples\n\n<Card title=\"Ready-to-Use Workflows\" icon=\"clipboard-check\">\n  These examples have been designed for common use cases. Customize them to fit your team's needs.\n</Card>\n\n### Daily Standup Automation\n\n```bash\ncn -p --auto \"\nGenerate and distribute daily standup update:\n\n1. Review my activity:\n   - GitHub commits and PRs from yesterday\n   - Linear tasks completed\n   - Calendar meetings attended\n\n2. Draft standup summary:\n   - Yesterday: [completed work]\n   - Today: [planned work]\n   - Blockers: [any issues]\n\n3. Post to Slack #daily-standups\n4. If there are blockers, also send email to team-lead@company.com\n\"\n```\n\n### On-Call Alert System\n\n```bash\ncn -p --auto \"\nWhen critical alert is triggered:\n\n1. Send urgent Slack message to #on-call with:\n   - Alert details\n   - Severity level\n   - Affected systems\n   - Runbook link\n\n2. Send SMS-style urgent email to on-call engineer\n3. If not acknowledged in 5 minutes, escalate to backup on-call\n4. Create incident thread in Slack for coordination\n\"\n```\n\n### Customer Onboarding\n\n```bash\ncn -p --auto \"\nNew customer onboarding workflow:\n\n1. Send welcome email via Gmail:\n   - Getting started guide\n   - Support contact information\n   - Link to documentation\n\n2. Create Slack Connect channel with customer\n3. Post introduction message in Slack with team members\n4. Create customer record in Notion with:\n   - Contact information\n   - Onboarding status\n   - Key milestones\n\n5. Add reminder in Linear for 7-day check-in call\n\"\n```\n\n### Weekly Team Digest\n\n```bash\ncn -p --auto \"\nGenerate weekly team digest (run every Friday at 4 PM):\n\n1. Collect data:\n   - GitHub: PRs merged this week\n   - Linear: Issues closed, velocity metrics\n   - Slack: Key discussions and decisions\n   - Gmail: Important external communications\n\n2. Create digest in Notion with sections:\n   - Accomplishments\n   - Challenges\n   - Next week's priorities\n   - Team shoutouts\n\n3. Share to Slack #team-updates\n4. Email summary to remote team members\n\"\n```\n\n## Troubleshooting\n\n<AccordionGroup>\n  <Accordion title=\"Authentication Issues\">\n    - **Klavis Strata URL not recognized**: Verify the Strata URL is active and correctly set in environment variables or Hub secrets\n    - **Slack OAuth failed**: Re-authenticate through the Klavis AI dashboard and ensure workspace permissions are granted\n    - **Gmail OAuth failed**: Check Google Cloud Console for API access and ensure the Gmail API is enabled\n    - **Token expired**: Klavis AI manages token refresh automatically, but you may need to re-authorize if it fails\n  </Accordion>\n\n  <Accordion title=\"Message Delivery Issues\">\n    - **Slack message not sent**: Verify the bot has permission to post in the target channel\n    - **Gmail rate limiting**: Google imposes sending limits‚Äîmonitor your quota\n    - **Channel not found**: Check channel name spelling and ensure it's public or the bot has been invited\n    - **User not found**: Verify Slack username format (@username) and that the user exists in the workspace\n  </Accordion>\n\n  <Accordion title=\"Workflow Execution Issues\">\n    - **Multi-step workflow incomplete**: Check each step individually to identify where the failure occurred\n    - **Rate limits hit**: Klavis manages rate limiting, but external APIs (Slack, Gmail) have their own limits\n    - **Permission denied errors**: Ensure your authenticated user has the necessary permissions for the requested operation\n    - **Integration not available**: Verify the integration is active in your Klavis dashboard at klavis.ai/integrations\n  </Accordion>\n</AccordionGroup>\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered communication automation system** that:\n\n- ‚úÖ **Uses natural language** ‚Äî Simple prompts instead of complex commands\n- ‚úÖ **Automates Slack**: Send messages, manage channels, and team coordination\n- ‚úÖ **Manages Gmail**: Read, send, and triage emails with intelligent automation\n- ‚úÖ **Cross-platform coordination**: Unified workflows across Slack, Gmail, and other tools\n- ‚úÖ **Scales reliably**: Handles complex multi-app workflows efficiently\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your communication workflows now operate at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine communications and coordinates multi-platform workflows\n  with optional human oversight and approval.\n</Card>\n\n## Klavis Strata MCP Capabilities\n\n<CardGroup cols={2}>\n  <Card title=\"Scalable Tool Integration\" icon=\"layer-group\">\n    Handle **thousands of tools** with no context overload.\n  </Card>\n\n  <Card title=\"Progressive Discovery\" icon=\"network-wired\">\n    Strata guides AI agents to **discover relevant categories** first, avoiding tool overload.\n  </Card>\n\n  <Card title=\"Smart Navigation\" icon=\"compass\">\n    AI drills down **layer by layer** to find the exact tool needed for the task.\n  </Card>\n\n  <Card title=\"Precise Execution\" icon=\"bullseye\">\n    Once identified, **Strata** pulls API details and executes with correct parameters.\n  </Card>\n</CardGroup>\n\n## Next Steps\n\n1. **Explore integrations** ‚Äî Try Slack and Gmail prompts on your workspace\n2. **Add more tools** ‚Äî Integrate Notion, Linear, GitHub, or other services\n3. **Automate notifications** ‚Äî Set up cross-platform announcement workflows\n4. **Monitor performance** ‚Äî Track automation accuracy and time savings\n5. **Scale up** ‚Äî Add scheduled workflows and event-driven automation\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"Klavis AI Platform\" icon=\"link\" href=\"https://www.klavis.ai/docs/knowledge-base/use-mcp-server/overview\">\n    Integrate Klavis AI with AI platforms\n  </Card>\n  <Card title=\"Strata MCP Documentation\" icon=\"book\" href=\"https://www.klavis.ai/docs/concepts/strata\">\n    Learn about Klavis AI's Strata MCP platform architecture\n  </Card>\n  <Card title=\"MCP in Continue\" icon=\"puzzle-piece\" href=\"/customize/deep-dives/mcp\">\n    Learn how MCP works with Continue agents\n  </Card>\n  <Card title=\"Continue Hub\" icon=\"globe\" href=\"https://hub.continue.dev\">\n    Create and manage your Continue agents\n  </Card>\n</CardGroup>"}
{"source":"github","repo":"continue","path":"docs/guides/sanity-mcp-continue-cookbook.mdx","content":"---\ntitle: \"Content Management with Sanity MCP and Continue\"\ndescription: \"Set up an AI-powered content management workflow that helps you manage schemas, run GROQ queries, handle documentation, and perform migrations using natural language commands.\"\nsidebarTitle: \"Sanity CMS with Continue\"\n---\n\n<Card title=\"What You'll Build\" icon=\"database\">\n  An AI-powered content management system workflow that uses Continue's AI agent with Sanity\n  MCP to manage schemas, execute GROQ queries, handle migrations, and maintain documentation - all through simple natural language prompts\n</Card>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Continue account with **Hub access**\n  - Read: [Understanding Configs ‚Äî How to get started with Hub configs](/hub/configs/intro)\n- Node.js 20+ installed locally\n- A [Sanity account](https://www.sanity.io/) and project (free tier works)\n- Basic understanding of content management systems\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n\n<Step title=\"Install Sanity CLI\">\n  ```bash\n  npm install -g @sanity/cli\n  ```\n  </Step>\n</Steps>\n <Warning>\n  To use agents in headless mode, you need a [Continue API key](https://hub.continue.dev/settings/api-keys) and proper environment variable configuration.\n  </Warning>\n\n## Getting Started with Sanity\n\n<Accordion title=\"New to Sanity? Start Here\">\n\nNew to Sanity? Follow the [Sanity Getting Started Guide](https://www.sanity.io/docs/getting-started) to set up your first project.\n\n**Quick setup:**\n```bash\nnpm create sanity@latest\ncd your-project-name\nnpm run dev\n```\n\n**Key resources:**\n- [Sanity Studio](https://www.sanity.io/docs/sanity-studio) - Content editing interface\n- [Schema Types](https://www.sanity.io/docs/schema-types) - Define content structure\n- [GROQ](https://www.sanity.io/docs/groq) - Query language for fetching content\n- [Content Lake](https://www.sanity.io/docs/datastore) - Real-time data store\n\n</Accordion>\n\n## Sanity MCP Workflow Options\n\n<Card title=\"Fastest Path to Success\" icon=\"rocket\">\n  Skip the manual setup and use our pre-built Sanity Assistant agent that includes\n      the Sanity MCP and optimized content management workflows for more consistent results. You can [remix this agent](/guides/understanding-configs#how-to-get-started-with-hub-configs) to customize it for your specific needs.\n</Card>\n\n\nAfter ensuring you meet the **Prerequisites** above, you have two paths to get started:\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    <Steps>\n      <Step title=\"Load the Pre-Built Agent\">\n        Visit the [Sanity Assistant Agent](https://hub.continue.dev/continuedev/sanity-assistant-agent) on Continue Mission Control and click **\"Install Config\"** or run:\n\n        ```bash\n        cn --agent continuedev/sanity-assistant-agent\n        ```\n\n        This agent includes:\n        - **[Sanity MCP](https://hub.continue.dev/sanity/sanity-mcp)** pre-configured and ready to use\n        - **Content management rules** for best practices\n        - **Schema optimization** guidelines\n      </Step>\n\n      <Step title=\"Your First MCP Conversation\">\n        Start with these beginner-friendly prompts to explore your content:\n\n        ```bash\n        # Content exploration\n        \"Show me all blog posts published in the last month\"\n\n        # Schema understanding\n        \"Show me all the document types in my Sanity schema and explain their relationships\"\n\n        # Content management\n        \"Create a new product page for our upcoming feature\"\n        ```\n\n        That's it! The agent handles everything automatically. For more starter prompts and examples, see the [official Sanity MCP blog post](https://www.sanity.io/blog/introducing-sanity-model-context-protocol-server).\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** The pre-built Sanity Assistant agent provides consistent content management workflows and handles MCP configuration automatically, making it easier to get started with AI-powered CMS operations. Results are more consistent and debugging is easier thanks to the [Sanity MCP](https://hub.continue.dev/sanity/sanity-mcp) integration and pre-tested prompts.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n<Steps>\n  <Step title=\"Create a New Agent via the Continue Mission Control\">\n    Go to the [Continue Mission Control](https://hub.continue.dev) and [create a new agent](https://hub.continue.dev/new?type=agent).\n    </Step>\n\n  <Step title=\"Connect Sanity MCP via Continue Mission Control\">\n    Visit the [Sanity MCP on Continue Mission Control](https://hub.continue.dev/sanity/sanity-mcp) and click **\"Install\"** to add it to the agent you created in the step above.\n\n    This will add Sanity MCP to your agent's available tools. The Mission Control listing automatically configures the MCP connection.\n\n    <Tip>\n      **Alternative installation methods:**\n      1. **Quick CLI install**: `cn --mcp sanity/sanity-mcp`\n      2. **With config**: Use [Sanity MCP Config](https://hub.continue.dev/sanity/sanity-mcp-config) for environment variable setup\n      3. **Manual configuration**: Add the MCP to your agent configuration\n\n      Once installed, Sanity MCP tools become available to your Continue agent for all prompts.\n    </Tip>\n\n    <Info>\n      **Authentication Options:**\n      - **Interactive mode**: OAuth authentication via browser (expires after 7 days)\n      - **Headless/CI mode**: Uses environment variables (SANITY_API_TOKEN, SANITY_PROJECT_ID, etc.)\n\n      See [Sanity MCP Config](https://hub.continue.dev/sanity/sanity-mcp-config) for environment variable setup.\n    </Info>\n\n  </Step>\n\n        <Step title=\"Your First MCP Conversation\">\n        Start with these beginner-friendly prompts:\n        ```bash\n        cn\n        # Try these starter prompts:\n        # \"Show me all blog posts published in the last month\"\n        # \"Show me all the document types in my Sanity schema and explain their relationships\"\n        # \"Create a new product page for our upcoming feature\"\n        ```\n\n        For more examples, see the [Sanity MCP blog post](https://www.sanity.io/blog/introducing-sanity-model-context-protocol-server).\n  </Step>\n</Steps>\n\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use the pre-built Sanity Assistant agent, you need either:\n      - **Continue CLI Pro Plan** with the models add-on, OR\n      - **Your own API keys** added to Continue Mission Control secrets\n\n    The agent will automatically detect and use your configuration along with the pre-configured Sanity MCP for content operations. Note that OAuth authentication will be required on first use.\n\n</Accordion>\n\n---\n\n## Sanity MCP Capabilities\n\n<Card title=\"Understanding the Capabilities\" icon=\"info-circle\">\n  **Sanity MCP** provides comprehensive tools for content management:\n  - Execute [GROQ queries](https://www.sanity.io/docs/groq) to fetch and analyze content\n  - Explore and modify [document schemas](https://www.sanity.io/docs/schema-types)\n  - Manage [content releases](https://www.sanity.io/docs/release-schedules) and versions\n  - Handle content [migrations](https://www.sanity.io/docs/migrating-data) between environments\n  - Automate documentation generation\n  - Perform bulk content operations\n  - Manage [localization](https://www.sanity.io/docs/localization) and translations\n\n  The MCP integrates seamlessly with your existing Sanity workspace, providing AI-powered assistance for both development and content operations.\n</Card>\n\n---\n\n## Your First MCP Conversation\n\nWith everything set up, you're ready for your first AI-powered content conversation! Try these beginner-friendly starter prompts:\n\n<CardGroup cols={2}>\n  <Card title=\"Content Discovery\" icon=\"search\">\n    ```\n    \"Show me all blog posts published in the last month\"\n    ```\n  </Card>\n  <Card title=\"Content Creation\" icon=\"plus\">\n    ```\n    \"Create a new product page for our upcoming feature\"\n    ```\n  </Card>\n  <Card title=\"Content Updates\" icon=\"edit\">\n    ```\n    \"Update our pricing information across all service pages\"\n    ```\n  </Card>\n  <Card title=\"Content Scheduling\" icon=\"calendar\">\n    ```\n    \"Schedule the Easter marketing campaign content release for next Tuesday\"\n    ```\n  </Card>\n</CardGroup>\n\n<Info>\n**New to MCP?** These prompts demonstrate the power of natural language content management. For more examples and detailed explanations, check out the [official Sanity MCP blog post](https://www.sanity.io/blog/introducing-sanity-model-context-protocol-server).\n</Info>\n\n## Content Management Recipes\n\nNow you can use natural language prompts to manage your Sanity content and schemas. The Continue agent automatically calls the appropriate Sanity MCP tools.\n\n<Info>\nYou can add prompts to your agent's configuration for easy access in future sessions. Go to your agent in the [Continue Mission Control](https://hub.continue.dev), click **Edit**, and add prompts under the **Prompts** section.\n</Info>\n\n<Info>\n  **Where to run these workflows:**\n  - **IDE Extensions**: Use Continue in VS Code, JetBrains, or other supported IDEs\n  - **Terminal (TUI mode)**: Run `cn` to enter interactive mode, then type your prompts\n  - **CLI (headless mode)**: Use `cn -p \"your prompt\"` for headless commands\n\n  **Test in Plan Mode First**: Before running operations that might make\n  changes, test your prompts in plan mode (see the [Plan Mode\n  Guide](/guides/plan-mode-guide); press **Shift+Tab** to switch modes in TUI/IDE). This\n  shows you what the agent will do without executing it.\n\n  To run any of the example prompts below in headless mode, use `cn -p \"prompt\"`\n</Info>\n\n<Info>\n\n  **About the --auto flag**: The `--auto` flag enables tools to run continuously without manual confirmation. This is essential for headless mode where the agent needs to execute multiple tools automatically to complete tasks like schema exploration, GROQ execution, and content migration.\n\n</Info>\n\n### Schema Management\n\n<Card title=\"Explore Document Types\" icon=\"diagram-project\">\n  Review and understand your content schema structure.\n\n**Prompt:**\n```\nShow me all document types in my Sanity schema with their fields,\nvalidation rules, and relationships to other types.\n```\n\n</Card>\n\n### GROQ Queries\n\n<Card title=\"Execute GROQ Queries\" icon=\"magnifying-glass\">\n  Run GROQ queries to fetch and analyze content.\n\n**Prompt:**\n```\nRun a GROQ query to fetch all articles published in the last 30 days,\nincluding their titles, authors, categories, and view counts.\n```\n\n</Card>\n\n### Content Operations\n\n<Card title=\"Bulk Content Updates\" icon=\"arrows-rotate\">\n  Perform bulk operations on your content.\n\n**Prompt:**\n```\nFind all blog posts with the category \"News\" and update their\nstatus to \"archived\" if they are older than 6 months.\n```\n\n</Card>\n\n### Localization\n\n<Card title=\"Add Localization Support\" icon=\"language\">\n  Set up and manage content translations.\n\n**Prompt:**\n```\nAdd localization support to my article document type for\nSpanish and French languages with appropriate field configurations.\n```\n\n</Card>\n\n### Content Migration\n\n<Card title=\"Migrate Content Structure\" icon=\"right-left\">\n  Migrate content between different schemas or environments.\n\n**Prompt:**\n```\nHelp me migrate my blog posts from the old schema structure\nto the new one, mapping the deprecated fields to the new format.\n```\n\n</Card>\n\n### Documentation Generation\n\n<Card title=\"Generate Schema Documentation\" icon=\"book\">\n  Automatically generate documentation for your content model.\n\n**Prompt:**\n```\nGenerate comprehensive documentation for my Sanity schema including\nall document types, their purposes, field descriptions, and usage examples.\n```\n\n</Card>\n\n### Content Releases\n\n<Card title=\"Manage Content Releases\" icon=\"rocket\">\n  Work with Sanity's content release feature.\n\n**Prompt:**\n```\nList all active releases in my dataset and show me the content\nchanges scheduled for each release.\n```\n\n</Card>\n\n### Performance Analysis\n\n<Card title=\"Analyze Query Performance\" icon=\"gauge-high\">\n  Optimize GROQ queries for better performance.\n\n**Prompt:**\n```\nAnalyze the performance of my most frequent GROQ queries and\nsuggest optimizations to improve response times.\n```\n\n</Card>\n\n## Continuous Content Management with GitHub Actions\n\n  This example demonstrates a **Continuous AI workflow** where content validation and schema checks run automatically in your CI/CD pipeline in headless mode using the Sanity agent config. Consider [remixing this agent](/guides/understanding-configs#how-to-get-started-with-hub-configs) to add your organization's specific content governance rules.\n\n### Add GitHub Secrets\n\nNavigate to **Repository Settings ‚Üí Secrets and variables ‚Üí Actions** and add:\n\n- `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n- `SANITY_PROJECT_ID`: Your Sanity project ID\n- `SANITY_DATASET`: Your Sanity dataset name (usually \"production\")\n- `SANITY_API_TOKEN`: Your Sanity API token with appropriate permissions\n- `MCP_USER_ROLE`: Your MCP user role (typically \"admin\" or \"editor\")\n\n<Info>\n  The workflow uses the [Sanity Assistant Agent](https://hub.continue.dev/continuedev/sanity-assistant-agent) with environment variable authentication via [Sanity MCP Config](https://hub.continue.dev/sanity/sanity-mcp-config). This enables headless mode operation without OAuth browser authentication.\n</Info>\n\n### Create Workflow File\n\nThis workflow automatically validates your Sanity schemas and content on pull requests using the Continue CLI in [headless mode](/cli/overview#headless-mode%3A-production-automation). It checks schema integrity, validates content relationships, and posts a summary report as a PR comment.\n\nCreate `.github/workflows/sanity-content-validation.yml` in your repository:\n\n```yaml\nname: Sanity Content Validation with MCP\n\non:\n  pull_request:\n    branches: [main]\n  workflow_dispatch:\n\njobs:\n  validate-content:\n    runs-on: ubuntu-latest\n    env:\n      CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n      SANITY_PROJECT_ID: ${{ secrets.SANITY_PROJECT_ID }}\n      SANITY_DATASET: ${{ secrets.SANITY_DATASET }}\n      SANITY_API_TOKEN: ${{ secrets.SANITY_API_TOKEN }}\n      MCP_USER_ROLE: ${{ secrets.MCP_USER_ROLE }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: \"18\"\n\n      - name: Install Sanity CLI\n        run: |\n          npm install -g @sanity/cli\n          echo \"‚úÖ Sanity CLI installed\"\n\n      - name: Install Continue CLI\n        run: |\n          npm install -g @continuedev/cli\n          echo \"‚úÖ Continue CLI installed\"\n\n      - name: Validate Schema Structure\n        run: |\n          echo \"üîç Validating schema structure...\"\n          cn --agent continuedev/sanity-assistant-agent \\\n             -p \"Analyze the Sanity schema for any structural issues,\n                 missing required fields, or broken references between document types.\" \\\n             --auto\n\n      - name: Check Content Integrity\n        run: |\n          echo \"üìä Checking content integrity...\"\n          cn --agent continuedev/sanity-assistant-agent \\\n             -p \"Run GROQ queries to identify any orphaned documents,\n                 broken references, or missing required fields in the content.\" \\\n             --auto\n\n      - name: Generate Schema Documentation\n        run: |\n          echo \"üìù Generating schema documentation...\"\n          cn --agent continuedev/sanity-assistant-agent \\\n             -p \"Generate a markdown summary of all schema changes\n                 in this PR and their potential impact on existing content.\" \\\n             --auto > schema-changes.md\n\n      - name: Comment Report on PR\n        if: always() && github.event_name == 'pull_request'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          REPORT=$(          cn --agent continuedev/sanity-assistant-agent \\\n             -p \"Generate a concise summary (200 words or less) of:\n                 - Schema validation results\n                 - Content integrity checks\n                 - Any breaking changes detected\n                 - Recommended actions before merging\" \\\n             --auto)\n\n          gh pr comment ${{ github.event.pull_request.number }} --body \"$REPORT\"\n```\n\n<Info>\n  Environment variables enable the MCP to authenticate without OAuth browser prompts. The [Sanity MCP Config](https://hub.continue.dev/sanity/sanity-mcp-config) documentation provides detailed setup instructions for all required variables.\n</Info>\n\n## Content Management Best Practices\n\nImplement automated content quality checks using Continue's rule system. See the [Rules deep dive](/customize/deep-dives/rules) for authoring tips.\n\n<Card title=\"Schema Validation\" icon=\"check-circle\">\n  ```bash\n  \"Before deploying schema changes, validate that all required fields\n  are present and that no breaking changes affect existing content.\"\n  ```\n</Card>\n\n<Card title=\"Content Consistency\" icon=\"shield-exclamation\">\n  ```bash\n  \"When creating new content types, ensure they follow the existing\n  naming conventions and field patterns established in the schema.\"\n  ```\n</Card>\n\n<Card title=\"Query Optimization\" icon=\"gauge-high\">\n  ```bash\n  \"Review GROQ queries for performance issues and suggest indexes\n  or query restructuring to improve response times.\"\n  ```\n</Card>\n\n<Card title=\"Localization Standards\" icon=\"check-double\">\n  ```bash\n  \"Ensure all user-facing content fields have proper localization\n  support configured for the required languages.\"\n  ```\n</Card>\n\n## Troubleshooting\n\n### Authentication Issues\n\n```bash\n\"Check if I'm properly authenticated with Sanity.\nIf not, help me set up OAuth or API token authentication.\"\n```\n\n### Schema Not Found\n\n```bash\n\"Verify that the Sanity project is properly configured\nand that the schema files are accessible in the current directory.\"\n```\n\n### GROQ Query Errors\n\n```bash\n\"Debug this GROQ query and explain why it's failing,\nthen provide a corrected version that achieves the intended result.\"\n```\n\n### Migration Conflicts\n\n<Check>\n  **Verification Steps:**\n  - Sanity MCP is installed via [Continue Mission Control](https://hub.continue.dev/sanity/sanity-mcp)\n  - Project is authenticated with Sanity\n  - Schema files are present and valid\n  - Dataset permissions are correctly configured\n</Check>\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered content management system** that:\n\n- ‚úÖ Uses natural language ‚Äî Simple prompts instead of complex CMS commands\n- ‚úÖ Manages schemas automatically ‚Äî AI handles schema evolution and migrations\n- ‚úÖ Runs continuously ‚Äî Automated validation in CI/CD pipelines\n- ‚úÖ Ensures quality ‚Äî Content checks prevent broken references and invalid data\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your content management workflow now operates at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine content operations and schema management with human oversight\n  through review and approval of changes.\n</Card>\n\n## Next Steps\n\n1. **Explore your schema** - Try the schema exploration prompt on your current project\n2. **Run GROQ queries** - Use natural language to query your content\n3. **Set up CI validation** - Add the GitHub Actions workflow to your repo\n4. **Create documentation** - Generate comprehensive docs for your content model\n5. **Optimize performance** - Analyze and improve query performance\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"Sanity Documentation\" icon=\"book\" href=\"https://www.sanity.io/docs\">\n    Complete Sanity platform documentation\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"plug\" href=\"https://hub.continue.dev\">\n    Explore more MCP integrations and agents\n  </Card>\n  <Card\n    title=\"GROQ Documentation\"\n    icon=\"code\"\n    href=\"https://www.sanity.io/docs/groq\"\n  >\n    Learn GROQ query language\n  </Card>\n  <Card\n    title=\"Sanity MCP Blog Post\"\n    icon=\"newspaper\"\n    href=\"https://www.sanity.io/blog/introducing-sanity-model-context-protocol-server\"\n  >\n    Official Sanity MCP introduction with examples\n  </Card>\n  <Card\n    title=\"Sanity MCP Setup\"\n    icon=\"puzzle-piece\"\n    href=\"https://www.sanity.io/docs/compute-and-ai/mcp-server\"\n  >\n    Official Sanity MCP documentation\n  </Card>\n</CardGroup>"}
{"source":"github","repo":"continue","path":"docs/guides/snyk-mcp-continue-cookbook.mdx","content":"---\ntitle: \"Automated Security Scanning with Snyk MCP and Continue\"\ndescription: \"Set up an AI-powered security workflow that automatically scans your code, dependencies, infrastructure, and containers using natural language commands.\"\nsidebarTitle: \"Snyk Security Scanning with Continue\"\n---\n\n<Note>\n\n  **üöÄ New: Snyk Mission Control Integration with Continuous AI**\n  \n  The new [Snyk Mission Control Integration](/mission-control/integrations/snyk) enables **[Continuous AI](/guides/continuous-ai)** - where AI agents autonomously detect, analyze, and fix vulnerabilities. When Snyk detects an issue, your agent automatically generates a fix, creates a PR, and validates the solution without manual intervention.\n  \n  **Mission Control Benefits:**\n  - **One-click OAuth** - No tokens, no configuration files, just connect and go\n  - **Centralized workflows** - Manage all security Tasks and automations from one dashboard\n  - **Webhook triggers** - Automatically respond to new vulnerabilities as they're discovered\n  - **Metrics & intervention rates** - Track how often agents successfully fix issues without human help\n  \n  This represents **[Level 2 Continuous AI](/guides/continuous-ai#the-continuous-ai-maturity-model)**: AI handles routine security work autonomously while developers focus on complex problems. As outlined in our [Continuous AI guide](/guides/continuous-ai#best-practices-for-sustainable-continuous-ai), this reduces intervention rates and accelerates secure development.\n  \n  **Get Started:** Use this cookbook to understand the fundamentals, then [enable Mission Control](/mission-control/integrations/snyk) to deploy autonomous security agents across your organization.\n\n</Note>\n\n<Card title=\"What You'll Build\" icon=\"shield-halved\">\n  An automated security scanning system that uses Continue's AI agent with Snyk\n  MCP to identify vulnerabilities in code, dependencies, infrastructure, and\n  containers - all through simple natural language prompts\n</Card>\n\n## Demo Video\n\n<iframe\n  width=\"100%\"\n  height=\"400\"\n  src=\"https://www.youtube.com/embed/cwVnKOf3tVg\"\n  title=\"Snyk MCP Continue Cookbook Demo\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Continue account with **Hub access**\n- Read: [Understanding Configs ‚Äî How to get started with Hub configs](/guides/understanding-configs#how-to-get-started-with-hub-configs)\n- Node.js 18+ installed locally\n- [Snyk account](https://snyk.io/) (free tier works)\n- A local project to scan for vulnerabilities\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n\n<Step title=\"Add Your Project to Snyk\">\n\n  1. Sign up for a Snyk account at [snyk.io](https://snyk.io/)\n  2. Create a new project in Snyk by importing your code repository (Git \n  provider or manual upload)\n  3. Install and authenticate the Snyk CLI locally:\n     ```bash\n     npm install -g snyk\n     snyk auth\n     ```\n     This will open your browser to authenticate with your Snyk account.\n  </Step>\n</Steps>\n\n<Note>\n  **Important**: The Snyk MCP requires the Snyk CLI to be authenticated locally. Run `snyk auth` to authenticate before using the Continue agent with Snyk MCP.\n</Note>\n <Warning>\n  To use agents in headless mode, you need a [Continue API key](https://hub.continue.dev/settings/api-keys).\n  </Warning>\n\n## Snyk Continuous AI Workflow Options\n\n<Card title=\"üöÄ Fastest Path to Success\" icon=\"zap\">\n  Skip the manual setup and use our pre-built Snyk Continuous AI agent that includes\n  the Snyk MCP and optimized security scanning workflows for more consistent results.\n</Card>\n\n\nAfter ensuring you meet the **Prerequisites** above, you have two paths to get started:\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    <Steps>\n      <Step title=\"Load the Pre-Built Agent\">\n        Navigate to your project directory and run:\n        ```bash\n        cn --agent continuedev/snyk-continuous-ai-agent\n        ```\n\n        This agent includes:\n        - **Snyk MCP** pre-configured and ready to use\n        - **Security-focused rules** for best practices\n      </Step>\n\n      <Step title=\"Run Your First Security Scan\">\n        From your project directory, start with a comprehensive security scan:\n        ```bash\n         # Headless mode\n        cn -p \"Run a complete security scan on this project including code vulnerabilities, dependencies, and any IaC files. Summarize findings by severity.\" --auto\n        ```\n\n        That's it! The agent handles everything automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** The pre-built agent provides consistent security scanning workflows and handles MCP configuration automatically, making it easier to get started with AI-powered security scanning.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n<Steps>\n  <Step title=\"Create a New Agent via the Continue Mission Control\">\n    Go to the [Continue Mission Control](https://hub.continue.dev) and [create a new agent](https://hub.continue.dev/new?type=agent).\n    </Step>\n\n  <Step title=\"Connect Snyk MCP via Continue Mission Control\">\n    Visit the [Snyk Continuous AI Agent](https://hub.continue.dev/continuedev/snyk-continuous-ai-agent) and click **Install** to add it to the agent you created in the step above.\n\n    This will add Snyk MCP to your agent's available tools. The Mission Control listing automatically configures the MCP command:\n    ```bash\n    npx -y snyk@latest mcp -t stdio\n    ```\n\n    <Tip>\n      **Alternative installation methods:**\n      1. **Quick CLI install**: `cn --mcp snyk/snyk-mcp`\n      2. **Manual configuration**: Add the MCP to your `~/.continue/config.json` under the `mcpServers` section\n\n      Once installed, Snyk MCP tools become available to your Continue agent for all prompts.\n    </Tip>\n\n    <Info>\n      The MCP will request authentication and folder trust permissions when first used.\n      This is handled automatically by the Continue agent.\n    </Info>\n\n  </Step>\n\n  <Step title=\"Add Secure-at-Inception Rules\">\n    Install the [Snyk Secure-at-Inception rules](https://hub.continue.dev/snyk/secure-at-inception) from Mission Control to enable automatic security scanning.\n\n    **How to add rules to your agent:**\n    1. Visit the rules link above and click **Install**\n    2. The rules will be added to your agent configuration automatically\n    3. Rules apply globally to all your Continue sessions\n\n    These rules configure your agent to:\n    - **Run [SAST](https://snyk.io/learn/application-security/sast/) scans** on newly generated or modified code\n    - **Check dependencies** when adding or updating packages\n    - **Auto-fix issues** using Snyk's recommendations, then rescan\n\n  </Step>\n\n  <Step title=\"Run Your First Security Scan\">\n    Start with a comprehensive security scan:\n    ```bash\n    # TUI mode\n    cn \"Run a complete security scan on this project including code vulnerabilities, dependencies, and any IaC files. Summarize findings by severity.\"\n    ```\n  </Step>\n</Steps>\n\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use the pre-built agent, you need either: \n      - **Continue CLI Pro Plan** with the models add-on, OR \n      - **Your own API keys** added to Continue Mission Control secrets (same as manual setup)\n      \n    The agent will automatically detect and use your configuration along with the pre-configured Snyk MCP for security scanning operations.\n\n</Accordion>\n\n---\n\n## Security Scanning Recipes\n\nNow you can use natural language prompts to run comprehensive security scans. The Continue agent automatically calls the appropriate Snyk MCP tools.\n\n<Info>\nYou can add prompts to your agent's configuration for easy access in future sessions. Go to your agent in the [Continue Mission Control](https://hub.continue.dev), click **Edit**, and add prompts under the **Prompts** section.\n</Info>\n\n<Info>\n  **Where to run these workflows:**\n  - **IDE Extensions**: Use Continue in VS Code, JetBrains, or other supported IDEs\n  - **Terminal (TUI mode)**: Run `cn` to enter interactive mode, then type your prompts\n  - **CLI (headless mode)**: Use `cn -p \"your prompt\" --auto` for headless commands\n\n  **Test in Plan Mode First**: Before running security scans that might make\n  changes, test your prompts in plan mode (see the [Plan Mode\n  Guide](/guides/plan-mode-guide); press **Shift+Tab** to switch modes in TUI/IDE). This\n  shows you what the agent will do without executing it. For example: `\"Run a\n  Snyk Code scan and fix the top 3 issues\"`\n</Info>\n\n### Code Vulnerability Scanning ([SAST](https://snyk.io/learn/application-security/sast/))\n\n<Card title=\"Static Application Security Testing\" icon=\"code\">\n  Scan your source code for security vulnerabilities and code quality issues.\n\n**TUI Mode Prompt:**\n```\nRun a Snyk Code scan on this repo with severity threshold medium.\nSummarize issues with file:line. Propose minimal diffs for the top 3\nand rerun to verify.\n```\n\n**Headless Mode Prompt:**\n```bash\ncn -p \"Run a Snyk Code scan on this repo with severity threshold medium. Summarize issues with file:line. Propose minimal diffs for the top 3 and rerun to verify.\" --auto\n```\n\n</Card>\n\n### Dependency Scanning ([SCA](https://snyk.io/learn/software-composition-analysis-sca/))\n\n<Card title=\"Software Composition Analysis\" icon=\"cube\">\n  Check open source dependencies for known vulnerabilities.\n\n**TUI Mode Prompt:**\n```\nRun Snyk Open Source on this repo (include dev deps).\nSummarize vulnerable paths and propose a minimal-risk upgrade plan.\nRe-test after the plan (dry run).\n```\n\n**Headless Mode Prompt:**\n```bash\ncn -p \"Run Snyk Open Source on this repo (include dev deps). Summarize vulnerable paths and propose a minimal-risk upgrade plan. Re-test after the plan (dry run).\" --auto\n```\n\n</Card>\n\n### Infrastructure as Code ([IaC](https://snyk.io/learn/infrastructure-as-code-iac/))\n\n<Card title=\"IaC Security\" icon=\"cloud\">\n  Scan Terraform, CloudFormation, and Kubernetes configs for misconfigurations.\n\n**TUI Mode Prompt:**\n```\nScan ./infra with Snyk IaC. Report high/critical misconfigs\nwith exact files/lines. Provide code changes and re-scan to confirm.\n```\n\n**Headless Mode Prompt:**\n```bash\ncn -p \"Scan ./infra with Snyk IaC. Report high/critical misconfigs with exact files/lines. Provide code changes and re-scan to confirm.\" --auto\n```\n\n</Card>\n\n### Container Scanning\n\n<Card title=\"Container Security\" icon=\"docker\">\n  Analyze Docker images for vulnerabilities in base images and packages.\n\n**TUI Mode Prompt:**\n```\nScan image my-api:latest. Exclude base image vulns.\nPrint dependency tree. Recommend a safer base image or upgrades.\nRe-test after the change (dry run).\n```\n\n**Headless Mode Prompt:**\n```bash\ncn -p \"Scan image my-api:latest. Exclude base image vulns. Print dependency tree. Recommend a safer base image or upgrades. Re-test after the change (dry run).\" --auto\n```\n\n</Card>\n\n### Pull Request Scanning\n\n<Card title=\"Changed Files Only\" icon=\"code-branch\">\n  Focus scanning on modified files to catch issues before merging.\n\n**TUI Mode Prompt:**\n```\nScan only files changed since origin/main with Snyk Code.\nBlock if new high issues would be introduced. Show deltas.\n```\n\n**Headless Mode Prompt:**\n```bash\ncn -p \"Scan only files changed since origin/main with Snyk Code. Block if new high issues would be introduced. Show deltas.\" --auto\n```\n\n</Card>\n\n### Security Learning\n\n<Card title=\"Snyk Learn Integration\" icon=\"graduation-cap\">\n  Access security education resources based on identified vulnerabilities ([CWE](https://cwe.mitre.org/)).\n\n**TUI Mode Prompt:**\n```\nOpen Snyk Learn lessons related to the top CWE(s) from this scan.\n```\n\n**Headless Mode Prompt:**\n```bash\ncn -p \"Open Snyk Learn lessons related to the top CWE(s) from this scan.\" --auto\n```\n\n</Card>\n\n## Continuous Security with GitHub Actions\n\nThis example demonstrates a **Continuous AI workflow** where security scanning runs automatically on pull requests, generates AI-powered mitigation suggestions, and posts them as PR comments.\n\n<Info>\n\n  **About the --auto flag**: The `--auto` flag enables tools to run continuously without manual confirmation. This is essential for headless mode where the agent needs to execute multiple tools automatically to complete tasks like security scanning, vulnerability analysis, and fix validation.\n\n</Info>\n\n### Add GitHub Secrets\n\nNavigate to **Repository Settings ‚Üí Secrets and variables ‚Üí Actions** and add:\n\n- `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n- `SNYK_TOKEN`: Your Snyk authentication token from [app.snyk.io/account](https://app.snyk.io/account)\n\n### Create Workflow File\n\nCreate `.github/workflows/snyk-security.yml` in your repository:\n\n```yaml\nname: Snyk Security Scanning\n\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0 # Fetch all history for git diff\n\n      - name: Get Changed Files\n        id: changed-files\n        run: |\n          echo \"üìù Getting changed files since main branch...\"\n          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | tr '\\n' ' ')\n          echo \"changed_files=$CHANGED_FILES\" >> $GITHUB_OUTPUT\n          echo \"Changed files: $CHANGED_FILES\"\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '18'\n\n      - name: Install Snyk CLI\n        run: |\n          npm install -g snyk\n          echo \"‚úÖ Snyk CLI installed\"\n\n      - name: Install Continue CLI\n        run: |\n          npm install -g @continuedev/cli\n          echo \"‚úÖ Continue CLI installed\"\n\n      - name: Validate Secrets\n        run: |\n          if [ -z \"${{ secrets.SNYK_TOKEN }}\" ]; then\n            echo \"‚ùå Error: SNYK_TOKEN secret is not set\"\n            exit 1\n          fi\n          if [ -z \"${{ secrets.CONTINUE_API_KEY }}\" ]; then\n            echo \"‚ö†Ô∏è Warning: CONTINUE_API_KEY not set - AI mitigation suggestions will be skipped\"\n          fi\n          echo \"‚úÖ Required secrets validated\"\n\n      - name: Authenticate Snyk\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        run: |\n          snyk auth \"$SNYK_TOKEN\"\n          echo \"‚úÖ Snyk authenticated\"\n\n      - name: Run Security Scans\n        id: security-scan\n        continue-on-error: true\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n          CHANGED_FILES: ${{ steps.changed-files.outputs.changed_files }}\n        run: |\n          SCAN_FAILED=0\n\n          if [ -n \"$CHANGED_FILES\" ]; then\n            echo \"üîç Running targeted scan on changed files...\"\n            echo \"Changed files: $CHANGED_FILES\"\n            FILE_ARGS=\"\"\n            for file in $CHANGED_FILES; do\n              if [[ \"$file\" =~ \\.(js|jsx|ts|tsx|py|java|go|rb)$ ]]; then\n                FILE_ARGS=\"$FILE_ARGS --file=$file\"\n              fi\n            done\n\n            if [ -n \"$FILE_ARGS\" ]; then\n              echo \"üîç Running Snyk Code scan on changed files...\"\n              snyk code test $FILE_ARGS --severity-threshold=high --json > snyk-code-results.json || {\n                echo \"‚ùå Snyk Code found high severity issues in changed files\"\n                SCAN_FAILED=1\n              }\n            else\n              echo \"‚ö†Ô∏è No scannable code files changed, skipping Snyk Code scan\"\n              echo '{\"runs\": [{\"results\": []}]}' > snyk-code-results.json\n            fi\n          else\n            echo \"‚ö†Ô∏è No changed files detected, creating empty results\"\n            echo '{\"runs\": [{\"results\": []}]}' > snyk-code-results.json\n          fi\n\n          echo \"üì¶ Checking dependencies...\"\n          snyk test --severity-threshold=high --json > snyk-oss-results.json || {\n            echo \"‚ùå Snyk Open Source found high severity issues\"\n            SCAN_FAILED=1\n          }\n\n          if [ $SCAN_FAILED -eq 1 ]; then\n            echo \"scan_status=failed\" >> $GITHUB_OUTPUT\n            echo \"‚ö†Ô∏è Scans completed with issues - continuing to generate mitigation suggestions\"\n          else\n            echo \"scan_status=passed\" >> $GITHUB_OUTPUT\n            echo \"‚úÖ All security scans passed\"\n          fi\n\n      - name: Generate AI Mitigation Suggestions\n        if: always() && steps.security-scan.outputs.scan_status == 'failed'\n        continue-on-error: true\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        run: |\n          echo \"ü§ñ Generating AI-powered mitigation suggestions...\"\n\n          # Create a summary of findings for Continue CLI\n          FINDINGS_SUMMARY=$(cat snyk-code-results.json snyk-oss-results.json | jq -r '\n            if .runs then\n              .runs[0].results[] | \"Code Issue: \\(.message.text) in \\(.locations[0].physicalLocation.artifactLocation.uri) (Severity: \\(.level))\"\n            elif .vulnerabilities then\n              .vulnerabilities[] | \"Dependency Issue: \\(.title) in \\(.packageName)@\\(.version) (Severity: \\(.severity))\"\n            else\n              empty\n            end\n          ' | head -20)\n\n          if [ -n \"$FINDINGS_SUMMARY\" ]; then\n            echo \"üìã Security findings to analyze:\"\n            echo \"$FINDINGS_SUMMARY\"\n            echo \"\"\n\n            # Use Continue CLI to generate mitigation suggestions\n            PROMPT=\"Analyze these Snyk security findings and provide specific, actionable mitigation steps for each issue. Focus on: 1) Root cause, 2) Immediate fix, 3) Long-term prevention. Findings: $FINDINGS_SUMMARY. Provide clear, prioritized recommendations.\"\n\n            cn --agent continuedev/snyk-continuous-ai-agent -p \"$PROMPT\" --auto > mitigation-suggestions.md || {\n              echo \"‚ö†Ô∏è Warning: Could not generate AI suggestions\"\n              exit 0\n            }\n\n            if [ -f mitigation-suggestions.md ]; then\n              echo \"‚úÖ AI mitigation suggestions generated\"\n              echo \"\"\n              echo \"--- Mitigation Suggestions ---\"\n              cat mitigation-suggestions.md\n            fi\n          else\n            echo \"‚ö†Ô∏è No findings to analyze\"\n          fi\n\n      - name: Post Mitigation Summary to PR\n        if: steps.security-scan.outputs.scan_status == 'failed' && hashFiles('mitigation-suggestions.md') != ''\n        continue-on-error: true\n        env:\n          GH_TOKEN: ${{ github.token }}\n        run: |\n          if [ -f mitigation-suggestions.md ]; then\n            echo \"üí¨ Posting mitigation summary to PR...\"\n\n            # Create PR comment with mitigation suggestions\n            cat > pr-comment.md <<'EOF'\n          ## üõ°Ô∏è Snyk Mitigation Summary\n\n          Snyk has identified security issues in this PR. Here are AI-generated mitigation recommendations:\n\n          EOF\n\n            cat mitigation-suggestions.md >> pr-comment.md\n\n            cat >> pr-comment.md <<EOF\n\n          ---\n          **Scan Details:**\n          - üìä Full report available in workflow artifacts\n          - üîç Review the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for complete details\n          - ü§ñ Generated with Continue CLI + Snyk\n\n          _This is an automated security analysis. Please review and address the findings before merging._\n          EOF\n\n            gh pr comment ${{ github.event.pull_request.number }} --body-file pr-comment.md\n\n            echo \"‚úÖ Mitigation summary posted to PR\"\n          else\n            echo \"‚ö†Ô∏è No mitigation suggestions file found\"\n          fi\n\n      - name: Generate Security Report\n        if: always()\n        continue-on-error: true\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        run: |\n          echo \"üìä Generating security report...\"\n          {\n            echo \"# Security Scan Report\"\n            echo \"\"\n            echo \"**Date:** $(date -u +\"%Y-%m-%d %H:%M:%S UTC\")\"\n            echo \"**Branch:** ${{ github.ref_name }}\"\n            echo \"**Commit:** ${{ github.sha }}\"\n            echo \"**Status:** ${{ steps.security-scan.outputs.scan_status }}\"\n            echo \"\"\n\n            echo \"## Snyk Code Scan Results\"\n            if [ -f snyk-code-results.json ]; then\n              jq -r '.runs[0].results[] | \"- **\\(.level | ascii_upcase)**: \\(.message.text) in \\(.locations[0].physicalLocation.artifactLocation.uri)\"' snyk-code-results.json || echo \"No code issues found\"\n            else\n              echo \"No scan results available\"\n            fi\n            echo \"\"\n\n            echo \"## Snyk Open Source Scan Results\"\n            if [ -f snyk-oss-results.json ]; then\n              jq -r '.vulnerabilities[] | \"- **\\(.severity | ascii_upcase)**: \\(.title) (\\(.packageName)@\\(.version))\"' snyk-oss-results.json || echo \"No dependency issues found\"\n            else\n              echo \"No scan results available\"\n            fi\n            echo \"\"\n\n            if [ -f mitigation-suggestions.md ]; then\n              echo \"## AI-Powered Mitigation Suggestions\"\n              echo \"\"\n              cat mitigation-suggestions.md\n            fi\n          } > scan-results.md\n\n          if [ -f scan-results.md ]; then\n            echo \"‚úÖ Security report generated successfully\"\n            cat scan-results.md\n          else\n            echo \"‚ö†Ô∏è Warning: scan-results.md was not created\"\n          fi\n\n      - name: Upload Security Report\n        if: always()\n        continue-on-error: true\n        uses: actions/upload-artifact@v4\n        with:\n          name: security-scan-results\n          path: |\n            scan-results.md\n            snyk-code-results.json\n            snyk-oss-results.json\n            mitigation-suggestions.md\n          if-no-files-found: warn\n\n      - name: Fail if Security Issues Found\n        if: steps.security-scan.outputs.scan_status == 'failed'\n        run: |\n          echo \"‚ùå Security scan failed - high severity issues found\"\n          echo \"üìã Review the security report artifact for details and mitigation suggestions\"\n          exit 1\n```\n\n<Info>\n  **About SNYK_TOKEN**: The workflow uses the SNYK_TOKEN in two ways:\n  1. **Direct Snyk CLI authentication** - Authenticates the Snyk CLI for running scans\n  2. **Continue CLI access** - Available as an environment variable when Continue generates AI mitigation suggestions\n\n  The `cn` agent automatically uses the SNYK_TOKEN when needed for Snyk MCP operations.\n</Info>\n\n<Info>\n  This workflow demonstrates several advanced features:\n  - **Changed Files Detection**: Only scans files modified in the PR\n  - **AI Mitigation**: Uses Continue CLI to generate actionable mitigation steps\n  - **PR Comments**: Automatically posts mitigation suggestions as PR comments\n  - **Comprehensive Reporting**: Generates detailed security reports with artifacts\n</Info>\n\n## Security Guardrails\n\nImplement automated security policies using Continue's rule system. See the [Rules deep dive](/customize/deep-dives/rules) for authoring tips.\n\n<Note>\n  **Coming Soon**: These security guardrail prompts will be available as pre-configured rules on the Continue Mission Control for easy installation.\n</Note>\n\n<Card title=\"Pre-commit Scanning\" icon=\"lock\">\n  ```bash\n  \"Always run Snyk Code before committing newly\n  generated code; refuse to proceed if high\n  issues remain.\"\n  ```\n</Card>\n\n<Card title=\"Dependency Safety\" icon=\"shield-check\">\n  ```bash\n  \"When adding/updating a dependency, run Snyk Open Source, choose the\n  lowest-risk upgrade, and re-test.\"\n  ```\n</Card>\n\n<Card title=\"Container Hardening\" icon=\"box\">\n  ```bash\n  \"Before building containers, scan base images and recommend\n  security-hardened alternatives.\"\n  ```\n</Card>\n\n<Card title=\"IaC Compliance\" icon=\"clipboard-list\">\n  ```bash\n  \"Scan all Terraform changes for compliance\n  violations before applying infrastructure.\"\n  ```\n</Card>\n\n<Tip>\n  Enable the **Secure-at-Inception** rules from Mission Control to automatically apply\n  these guardrails to all code generation and modifications.\n</Tip>\n\n## Troubleshooting\n\n### Authentication Issues\n\n```bash\n\"Check Snyk auth status and current org. If not authenticated,\nhelp me authenticate. Then run a quick Code scan on ./\nwith severity medium and print one example issue.\"\n```\n\n### Fix Validation\n\n```bash\n\"Propose minimal diffs only in affected files,\nthen rerun the same Snyk scan to confirm resolution.\"\n```\n\n### Connection Problems\n\n<Check>\n  **Verification Steps:** - Snyk MCP is installed via [Continue\n  Hub](https://hub.continue.dev/snyk/snyk-mcp) - Secure-at-Inception rules are\n  [enabled](https://hub.continue.dev/snyk/secure-at-inception) - Authentication\n  completed successfully - Project folder has been trusted\n</Check>\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered security system** that:\n\n- ‚úÖ Uses natural language ‚Äî Simple prompts instead of complex CLI commands\n- ‚úÖ Fixes automatically ‚Äî AI suggests and validates security fixes\n- ‚úÖ Runs continuously ‚Äî Automated scanning in CI/CD pipelines\n- ‚úÖ Enforces guardrails ‚Äî Security rules prevent vulnerable code from shipping\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your security workflow now operates at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine security scanning and remediation with human oversight\n  through review and approval of fixes.\n</Card>\n\n## Next Steps\n\n1. **Run your first scan** - Try the [SAST](https://snyk.io/learn/application-security/sast/) prompt on your current project\n2. **Review findings** - Analyze the security report and implement fixes\n3. **Set up CI pipeline** - Add the GitHub Actions workflow to your repo\n4. **Customize rules** - Add project-specific security policies\n5. **Monitor trends** - Track [vulnerability](https://snyk.io/learn/security-vulnerability/) reduction over time\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"Snyk Documentation\" icon=\"book\" href=\"https://docs.snyk.io\">\n    Complete Snyk platform documentation\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"plug\" href=\"https://hub.continue.dev\">\n    Explore more MCP integrations and agents\n  </Card>\n  <Card\n    title=\"Security Best Practices\"\n    icon=\"shield\"\n    href=\"https://snyk.io/learn\"\n  >\n    Learn about secure coding practices\n  </Card>\n  <Card\n    title=\"MCP Concepts\"\n    icon=\"puzzle-piece\"\n    href=\"https://docs.snyk.io/integrations/developer-guardrails-for-agentic-workflows\"\n  >\n    Understanding MCP architecture\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/guides/dlt-mcp-continue-cookbook.mdx","content":"---\ntitle: \"Building Data Pipelines with dlt MCP and Continue\"\ndescription: \"Set up an AI-powered data engineering workflow that helps you develop, debug, and inspect dlt data pipelines using natural language commands.\"\nsidebarTitle: \"dlt Data Pipelines with Continue\"\n---\n\n<Card title=\"What You'll Build\" icon=\"database\">\n  An AI-powered data pipeline development system that uses Continue's AI agent with dlt\n  MCP to inspect pipeline execution, retrieve schemas, analyze datasets, and debug load errors - all through simple natural language prompts\n</Card>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Continue account with **Hub access**\n  - Read: [Understanding Configs ‚Äî How to get started with Hub configs](/guides/understanding-configs#how-to-get-started-with-hub-configs)\n- Python 3.8+ installed locally\n- A dlt pipeline project (or create one during this guide)\n- Basic understanding of data pipelines\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n\n<Step title=\"Install dlt\">\n  ```bash\n  pip install dlt\n  ```\n  </Step>\n</Steps>\n <Warning>\n  To use agents in headless mode, you need a [Continue API key](https://hub.continue.dev/settings/api-keys).\n  </Warning>\n\n## dlt MCP Workflow Options\n\n<Card title=\"üöÄ Fastest Path to Success\" icon=\"zap\">\n  Skip the manual setup and use our pre-built [dlt Agent](https://hub.continue.dev/continuedev/dlt-agent) that includes\n      the dlt MCP and optimized data pipeline workflows for more consistent results. You can [remix this agent](/guides/understanding-configs#how-to-get-started-with-hub-configs) to customize it for your specific needs.\n</Card>\n\n\nAfter ensuring you meet the **Prerequisites** above, you have two paths to get started:\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    <Steps>\n      <Step title=\"Load the Pre-Built Agent\">\n        Navigate to your pipeline project directory and run:\n        ```bash\n        cn --agent continuedev/dlt-agent\n        ```\n\n        This agent includes:\n        - **dlt MCP** pre-configured and ready to use\n        - **Pipeline-focused rules** for data engineering best practices\n      </Step>\n\n      <Step title=\"Run Your First Pipeline Inspection\">\n        Start with a comprehensive pipeline check:\n        ```bash\n        # TUI mode\n        Inspect the execution of my dlt pipeline and summarize the load info, including timing and file sizes.\n        ```\n\n        That's it! The agent handles everything automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** The pre-built [dlt Agent](https://hub.continue.dev/continuedev/dlt-agent) provides consistent pipeline development workflows and handles MCP configuration automatically, making it easier to get started with AI-powered data engineering. You can [remix and customize this agent](/guides/understanding-configs#how-to-get-started-with-hub-configs) later to fit your team's specific workflow.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n<Steps>\n  <Step title=\"Create a New Agent via the Continue Mission Control\">\n    Go to the [Continue Mission Control](https://hub.continue.dev) and [create a new agent](https://hub.continue.dev/new?type=agent).\n    </Step>\n\n  <Step title=\"Connect dlt MCP via Continue Mission Control\">\n    Visit the [dlt MCP on Continue Mission Control](https://hub.continue.dev/dlthub/dlt-mcp) and click **Install** to add it to the agent you created in the step above.\n\n    This will add dlt MCP to your agent's available tools. The Mission Control listing automatically configures the MCP command.\n\n    <Tip>\n      **Alternative installation methods:**\n      1. **Quick CLI install**: `cn --mcp dlthub/dlt-mcp`\n      2. **Manual configuration**: Add the MCP to your `~/.continue/config.json` under the `mcpServers` section\n\n      Once installed, dlt MCP tools become available to your Continue agent for all prompts.\n    </Tip>\n\n    <Info>\n      The MCP will work with your existing dlt pipelines in your current directory.\n    </Info>\n\n  </Step>\n\n  <Step title=\"Run Your First Pipeline Inspection\">\n    Start with a comprehensive pipeline check:\n    ```bash\n    # TUI mode\n    cn\n    # Then type: Inspect the execution of my dlt pipeline and summarize the load info, including timing and file sizes.\n    ```\n  </Step>\n</Steps>\n\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use the pre-built [dlt Agent](https://hub.continue.dev/continuedev/dlt-agent), you need either:\n      - **Continue CLI Pro Plan** with the models add-on, OR\n      - **Your own API keys** added to Continue Mission Control secrets (same as manual setup)\n\n    The agent will automatically detect and use your configuration along with the pre-configured dlt MCP for pipeline operations.\n\n</Accordion>\n\n---\n\n## dlt MCP vs dlt+ MCP\n\n<Card title=\"Understanding the Difference\" icon=\"info-circle\">\n  **dlt MCP** is focused on local pipeline development and inspection. It provides tools to:\n  - Inspect pipeline execution and load information\n  - Retrieve schema metadata from your local pipelines\n  - Query dataset records from destination databases\n  - Analyze load errors, timings, and file sizes\n\n  **[dlt+ MCP](https://hub.continue.dev/dlthub/dlt-plus-mcp)** extends these capabilities with cloud-based features for production deployments:\n  - Connect to dlt+ Projects and manage deployments\n  - Monitor pipeline runs across multiple environments\n  - Access centralized logging and observability\n  - Collaborate with team members on pipeline development\n\n  For local development and getting started, **[dlt MCP](https://hub.continue.dev/dlthub/dlt-mcp)** is the right choice. Consider **[dlt+ MCP](https://hub.continue.dev/dlthub/dlt-plus-mcp)** when you need production deployment features and team collaboration.\n</Card>\n\n---\n\n## Pipeline Development Recipes\n\nNow you can use natural language prompts to develop and debug your dlt pipelines. The Continue agent automatically calls the appropriate dlt MCP tools.\n\n<Info>\nYou can add prompts to your agent's configuration for easy access in future sessions. Go to your agent in the [Continue Mission Control](https://hub.continue.dev), click **Edit**, and add prompts under the **Prompts** section.\n</Info>\n\n<Info>\n  **Where to run these workflows:**\n  - **IDE Extensions**: Use Continue in VS Code, JetBrains, or other supported IDEs\n  - **Terminal (TUI mode)**: Run `cn` to enter interactive mode, then type your prompts\n  - **CLI (headless mode)**: Use `cn -p \"your prompt\"` for headless commands\n\n  **Test in Plan Mode First**: Before running pipeline operations that might make\n  changes, test your prompts in plan mode (see the [Plan Mode\n  Guide](/guides/plan-mode-guide); press **Shift+Tab** to switch modes in TUI/IDE). This\n  shows you what the agent will do without executing it.\n  \n  To run any of the example prompts below in headless mode, use `cn -p \"prompt\"`\n</Info>\n\n<Info>\n\n  **About the --auto flag**: The `--auto` flag enables tools to run continuously without manual confirmation. This is essential for headless mode where the agent needs to execute multiple tools automatically to complete tasks like pipeline inspection, schema retrieval, and error analysis.\n\n</Info>\n\n### Pipeline Inspection\n\n<Card title=\"Inspect Pipeline Execution\" icon=\"magnifying-glass\">\n  Review pipeline execution details including load timing and file sizes.\n\n**Prompt:**\n```\nInspect my dlt pipeline execution and provide a summary of the load info.\nShow me the timing breakdown and file sizes for each table.\n```\n\n</Card>\n\n### Schema Management\n\n<Card title=\"Retrieve Schema Metadata\" icon=\"diagram-project\">\n  Get detailed schema information for your pipeline's tables.\n\n**Prompt:**\n```\nShow me the schema for my users table including all columns,\ndata types, and any constraints.\n```\n\n</Card>\n\n### Data Exploration\n\n<Card title=\"Query Dataset Records\" icon=\"table\">\n  Retrieve and analyze records from your destination database.\n\n**Prompt:**\n```\nGet the last 10 records from my orders table and show me\nthe distribution of order statuses.\n```\n\n</Card>\n\n### Error Debugging\n\n<Card title=\"Analyze Load Errors\" icon=\"triangle-exclamation\">\n  Investigate and understand pipeline load errors.\n\n**Prompt:**\n```\nCheck for any load errors in my last pipeline run. If there are errors,\nexplain what went wrong and suggest fixes.\n```\n\n</Card>\n\n### Pipeline Creation\n\n<Card title=\"Build New Pipeline\" icon=\"plus\">\n  Create a new dlt pipeline from an API or data source.\n\n**Prompt:**\n```\nHelp me create a new dlt pipeline that loads data from the\nJSONPlaceholder API users endpoint into DuckDB.\n```\n\n</Card>\n\n### Schema Evolution\n\n<Card title=\"Handle Schema Changes\" icon=\"code-branch\">\n  Review and manage schema evolution in your pipelines.\n\n**Prompt:**\n```\nCheck if my pipeline schema has evolved since the last run.\nShow me what columns were added or modified.\n```\n\n</Card>\n\n## Continuous Data Pipelines with GitHub Actions\n\n  This example demonstrates a **Continuous AI workflow** where data pipeline validation runs automatically in your CI/CD pipeline in headless mode using the [dlt Assistant agent](https://hub.continue.dev/dlthub/dlt-assistant). Consider [remixing this agent](/guides/understanding-configs#how-to-get-started-with-hub-configs) to add your organization's specific validation rules.\n\n### Add GitHub Secrets\n\nNavigate to **Repository Settings ‚Üí Secrets and variables ‚Üí Actions** and add:\n\n- `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n- Any required database credentials for your destination\n\n<Info>\n      The workflow uses the pre-built [dlt Agent](https://hub.continue.dev/continuedev/dlt-agent) with `--agent continuedev/dlt-agent`. This agent comes pre-configured with the dlt MCP and optimized rules for pipeline operations. You can [remix this agent](/guides/understanding-configs#how-to-get-started-with-hub-configs) to customize the validation rules and prompts for your specific pipeline requirements.\n</Info>\n\n### Create Workflow File\n\nThis workflow automatically validates your dlt data pipelines on pull requests using the Continue CLI in [headless mode](/cli/overview#headless-mode%3A-production-automation). It inspects pipeline schemas, checks for errors, and posts a summary report as a PR comment. The workflow can also be triggered manually via `workflow_dispatch`.\n\nCreate `.github/workflows/dlt-pipeline-validation.yml` in your repository:\n\n```yaml\nname: Data Pipeline Validation with dlt MCP\n\non:\n  pull_request:\n    branches: [main]\n  workflow_dispatch:\n\njobs:\n  validate-pipeline:\n    runs-on: ubuntu-latest\n    env:\n      CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: \"18\"\n\n      - name: Install dlt\n        run: |\n          pip install dlt\n          echo \"‚úÖ dlt installed\"\n\n      - name: Install Continue CLI\n        run: |\n          npm install -g @continuedev/cli\n          echo \"‚úÖ Continue CLI installed\"\n\n      - name: Validate Pipeline Schema\n        run: |\n          echo \"üîç Validating pipeline schema...\"\n          cn --agent continuedev/dlt-agent \\\n             -p \"Inspect the pipeline schema and verify all required tables\n                 and columns are present. Flag any missing or unexpected changes.\" \\\n             --auto\n\n      - name: Check Pipeline Health\n        run: |\n          echo \"üìä Checking pipeline health...\"\n          cn --agent continuedev/dlt-agent \\\n             -p \"Analyze the last pipeline run for errors or warnings.\n                 Report any issues that need attention.\" \\\n             --auto\n\n      - name: Comment Pipeline Report on PR\n        if: always() && github.event_name == 'pull_request'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          REPORT=$(cn --agent continuedev/dlt-agent \\\n             -p \"Generate a concise summary (200 words or less) of:\n                 - Pipeline schemas and row counts\n                 - Any load errors or warnings\n                 - Performance metrics (timing, file sizes)\n                 - Recommended improvements\" \\\n             --auto)\n\n          gh pr comment ${{ github.event.pull_request.number }} --body \"$REPORT\"\n```\n\n<Info>\n  The dlt MCP works with your local pipeline state. Make sure your CI environment\n  has access to the necessary pipeline configuration and credentials.\n</Info>\n\n## Pipeline Development Best Practices\n\nImplement automated pipeline quality checks using Continue's rule system. See the [Rules deep dive](/customize/deep-dives/rules) for authoring tips.\n\n<Card title=\"Schema Validation\" icon=\"check-circle\">\n  ```bash\n  \"Before committing pipeline changes, verify the schema\n  matches expectations and flag any unexpected modifications.\"\n  ```\n</Card>\n\n<Card title=\"Error Handling\" icon=\"shield-exclamation\">\n  ```bash\n  \"When load errors occur, analyze the error details and\n  suggest specific code fixes to handle the data issues.\"\n  ```\n</Card>\n\n<Card title=\"Performance Monitoring\" icon=\"gauge-high\">\n  ```bash\n  \"Track pipeline execution times and file sizes. Alert if\n  performance degrades significantly from baseline.\"\n  ```\n</Card>\n\n<Card title=\"Data Quality\" icon=\"check-double\">\n  ```bash\n  \"After each pipeline run, validate row counts and check for\n  null values in critical columns.\"\n  ```\n</Card>\n\n## Troubleshooting\n\n### Pipeline Not Found\n\n```bash\n\"Check if there's a dlt pipeline in the current directory.\nIf not, help me initialize a new pipeline.\"\n```\n\n### Destination Connection Issues\n\n```bash\n\"Verify the destination connection and credentials for my pipeline.\nTest the connection and report any issues.\"\n```\n\n### Schema Inference Problems\n\n<Check>\n  **Verification Steps:**\n  - dlt MCP is installed via [Continue Mission Control](https://hub.continue.dev/dlthub/dlt-mcp)\n  - Pipeline directory is accessible\n  - Destination database credentials are configured\n  - Pipeline has been run at least once\n</Check>\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered data pipeline development system** that:\n\n‚úÖ Uses natural language ‚Äî Simple prompts instead of complex pipeline commands\n‚úÖ Debugs automatically ‚Äî AI analyzes errors and suggests fixes\n‚úÖ Runs continuously ‚Äî Automated validation in CI/CD pipelines\n‚úÖ Ensures quality ‚Äî Pipeline checks prevent bad data from shipping\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your data pipeline workflow now operates at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine pipeline inspection and debugging with human oversight\n  through review and approval of changes.\n</Card>\n\n## Next Steps\n\n1. **Inspect your first pipeline** - Try the pipeline inspection prompt on your current project\n2. **Debug load errors** - Use the error analysis prompt to fix any issues\n3. **Set up CI pipeline** - Add the GitHub Actions workflow to your repo\n4. **Create new pipelines** - Use AI to scaffold new data sources\n5. **Monitor performance** - Track pipeline execution metrics over time\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"dlt Documentation\" icon=\"book\" href=\"https://dlthub.com/docs\">\n    Complete dlt platform documentation\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"plug\" href=\"https://hub.continue.dev\">\n    Explore more MCP integrations and agents\n  </Card>\n  <Card\n    title=\"dlt Blog: MCP Deep Dive\"\n    icon=\"newspaper\"\n    href=\"https://dlthub.com/blog/deep-dive-assistants-mcp-continue\"\n  >\n    Learn about AI agents, MCP, and Continue integration\n  </Card>\n  <Card\n    title=\"MCP Concepts\"\n    icon=\"puzzle-piece\"\n    href=\"https://dlthub.com/blog/deep-dive-assistants-mcp-continue\"\n  >\n    Deep dive into dlt MCP integration\n  </Card>\n</CardGroup>"}
{"source":"github","repo":"continue","path":"docs/guides/netlify-mcp-continuous-deployment.mdx","content":"---\ntitle: \"Netlify Performance Optimization Cookbook\"\ndescription: \"Optimize web performance with A/B testing, automated monitoring, and data-driven improvements using Netlify MCP and Continue.\"\nsidebarTitle: \"Using Netlify MCP for Performance Optimization\"\n---\n\n<Card title=\"Ship Faster Code, Not Slower Sites\" icon=\"gauge-high\">\n  Use AI to automatically monitor performance metrics, run A/B tests between\n  branches, and get actionable optimization suggestions based on real user data\n  from Netlify Analytics\n</Card>\n\n<Info>\n  **Did You Know?** Netlify is more than just static hosting! It offers:\n  - [Split Testing](https://docs.netlify.com/site-deploys/split-testing/) for A/B testing branches\n  - [Analytics](https://docs.netlify.com/analytics/get-started/) with Core Web Vitals tracking\n  - [Edge Functions](https://docs.netlify.com/edge-functions/overview/) for personalization at the edge\n  - [Build Plugins](https://docs.netlify.com/integrations/build-plugins/) ecosystem with 100+ integrations\n  - [Forms](https://docs.netlify.com/forms/setup/) with built-in spam protection\n  - [Identity](https://docs.netlify.com/visitor-access/identity/) for user authentication\n  - [Large Media](https://docs.netlify.com/large-media/overview/) for Git LFS support\n\nThis guide shows you how to leverage these features through natural language with Continue CLI!\n\n</Info>\n\n## What You'll Learn\n\nThis cookbook teaches you to:\n\n- Run [A/B tests](https://docs.netlify.com/site-deploys/split-testing/) between branches to measure performance impact\n- Monitor [Core Web Vitals](https://docs.netlify.com/analytics/get-started/#core-web-vitals) and build performance metrics\n- Automatically block deploys that degrade performance using [Deploy Contexts](https://docs.netlify.com/site-deploys/overview/#deploy-contexts)\n- Optimize build times with [Build Plugins](https://docs.netlify.com/integrations/build-plugins/) and bundle sizes with AI assistance\n\n## Prerequisites\n\n- GitHub repository with a web project\n- [Netlify account](https://netlify.com) (free tier works)\n- Node.js 22+ installed (required for Netlify)\n- [Continue CLI](https://docs.continue.dev/guides/cli) (`npm i -g @continuedev/cli`)\n- [Netlify MCP](https://hub.continue.dev/netlify/netlify-mcp) configured\n- [Netlify Development Rules](https://hub.continue.dev/netlify/netlify-development) (recommended)\n\n## Quick Setup\n\n<Info>\n  The Netlify Development Rules bundle includes guardrails for:\n  - Proper `.gitignore` configuration\n  - Function structure and placement\n  - Edge function constraints\n  - Local development best practices\n\n</Info>\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n\n<Step title=\"Install Netlify CLI\">\n\n  1. Install Netlify CLI: `npm i -g netlify-cli` \n  2. Authenticate with Netlify:\n  `netlify login`\n  </Step>\n</Steps>\n\n## Netlify Continuous AI Workflow Options\n\n<Card title=\"üöÄ Fastest Path to Success\" icon=\"zap\">\n  Skip the manual setup and use our pre-built Netlify Continuous AI agent that includes\n  optimized prompts, rules, and the Netlify MCP for more consistent results.\n</Card>\n\nAfter completing **Quick Setup** above, you have two paths to get started:\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n\n    <Steps>\n      <Step title=\"Add the Pre-Built Agent\">\n        Visit the [Netlify Continuous AI Agent](https://hub.continue.dev/continuedev/netlify-continuous-ai-agent) on Continue Mission Control and click **\"Install Agent\"** or run:\n        \n        ```bash\n        cn --agent continuedev/netlify-continuous-ai-agent\n        ```\n\n        This agent includes:\n        - **Built-in rules** for consistent formatting and error handling\n        - **Netlify MCP** for more reliable API interactions\n      </Step>\n\n      <Step title=\"Verify Your Setup\">\n        The agent uses the Netlify MCP automatically and includes best practice rules for deployment and performance optimization.\n      </Step>\n\n      <Step title=\"Run Performance Analysis\">\n        From your project directory, run:\n        ```bash\n        cn \"Analyze my Netlify site's performance and optimize it for better Core Web Vitals.\"\n        ```\n\n        That's it! The agent handles everything automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** Results are more consistent and debugging is easier thanks to the Netlify MCP integration and pre-tested prompts. You can remix the agent later to customize it to your needs.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n<Steps>\n\n<Step title=\"Configure Netlify MCP\">\n  1. Authenticate with Netlify:\n  `netlify login`\n  2. Visit [Netlify MCP on Continue\n  Hub](https://hub.continue.dev/netlify/netlify-mcp)\n  3. Follow the configuration instructions for your editor\n\n</Step>\n\n  <Step title=\"Add Development Rules\">\n    Install the [Netlify Development Rules](https://hub.continue.dev/netlify/netlify-development) bundle for best practices:\n\n    1. Visit the bundle page on Continue Mission Control\n    2. Click **\"Install Rules\"**\n    3. Rules automatically apply to your agent\n\n  </Step>\n\n  <Step title=\"Verify Setup\">\n    Test the connection with cn CLI:\n\n    ```bash\n    cn\n    # Then in TUI mode:\n    \"Check my Netlify auth and list sites\"\n    ```\n\n  </Step>\n\n \n</Steps>\n <Info>\n    You're all set! Now you can use cn CLI to interact with Netlify using natural language prompts. Check out the examples below to get started.\n  </Info>\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use the pre-built agent, you need either: \n      - **Continue CLI Pro Plan** with\n      the models add-on, OR \n      - **Your own API keys** added to Continue Mission Control secrets\n      The agent will automatically detect and use your\n      configuration along with the Netlify MCP for deployment operations.\n\n</Accordion>\n\n---\n\n## Performance Optimization Steps\n\n### Step 1: Baseline Performance Metrics\n\nEstablish your current performance baseline using cn CLI:\n\n```bash\n# Start cn in TUI mode\ncn\n\n# Then ask:\n\"Show my site's Core Web Vitals and build times\"\n```\n\n<Tip>\n  **Test in Plan Mode First**: Before making performance optimizations that\n  might affect your site, test your prompts in plan mode (see the [Plan Mode\n  Guide](/guides/plan-mode-guide); press **Shift+Tab** to switch modes). This\n  shows you what the agent will do without executing it. For example: `\"Set up\n  A/B testing between main and feature branch with performance monitoring\"`\n</Tip>\n\n<Info>\n\n    Netlify automatically tracks: \n    - **Build Performance**: Compile times, cache\n    hits \n    - **Runtime Performance**: Core Web Vitals, Time to Interactive \n    -  **Resource Usage**: Bandwidth, function execution times\n    \n</Info>\n\n### Step 2: A/B Test Branch Performance\n\nCompare performance between branches:\n\n```bash\n# In cn TUI mode:\n\"Set up A/B test between main and feature branch on Netlify:\n- Split traffic 50/50 between branches\n- Track Core Web Vitals for each variant\n- Monitor conversion metrics and bounce rate\n- Enable analytics to measure performance impact\n- Configure cookie-based visitor persistence\n- Set test duration for 1000 unique visitors\n- Auto-conclude test when statistical significance reached\nReport winner based on performance + conversion metrics\"\n```\n\n<Info>\n  **Enhanced Analytics**: Combine Netlify's A/B testing with PostHog session\n  recordings to understand not just which variant performs better, but why users\n  behave differently. See our [PostHog session analysis\n  guide](/guides/posthog-github-continuous-ai) to set up session tracking and\n  create a complete continuous AI analytics workflow.\n</Info>\n\n### Step 3: Advanced Build Optimization with Netlify\n\nLeverage Netlify's powerful build features to dramatically reduce build times:\n\n```bash\n# In cn TUI mode:\n\"Optimize my Netlify build performance using these features:\n\n1. Enable Netlify Cache Plugin for dependency caching\n   - Configure @netlify/plugin-cache for node_modules\n   - Set up custom cache directories for .next/cache or .nuxt\n   - Enable Gatsby's incremental builds if applicable\n\n2. Set up Build Plugins from Netlify's ecosystem:\n   - Install @netlify/plugin-lighthouse for performance monitoring\n   - Add netlify-plugin-checklinks to prevent broken links\n   - Configure netlify-plugin-submit-sitemap for SEO\n\n3. Implement Conditional Builds:\n   - Skip builds when only docs change (ignore: /docs/**)\n   - Use build.ignore script for custom logic\n   - Set up monorepo-specific build triggers\n\n4. Configure concurrent builds for monorepos\n   - Set base directory per package\n   - Use pnpm workspaces or yarn workspaces\n\nShow me the netlify.toml configuration and explain each optimization\"\n```\n\n<Info>\n  **Netlify Build Features You're Getting**:\n  - [Build Caching](https://docs.netlify.com/configure-builds/build-caching/) - Persist dependencies between builds\n  - [Build Plugins](https://docs.netlify.com/integrations/build-plugins/) - Extend build process with 100+ plugins\n  - [Conditional Builds](https://docs.netlify.com/configure-builds/ignore-builds/) - Skip unnecessary builds\n  - [Monorepo Support](https://docs.netlify.com/configure-builds/monorepos/) - Optimize multi-package repos\n\nThese features can reduce build times by 50-70% for most projects!\n\n</Info>\n\n<Tip>\n  **Discover more optimization prompts!** The Netlify community has documented dozens of build optimization strategies in their [Support Guide: How can I optimize my Netlify build time](https://answers.netlify.com/t/support-guide-how-can-i-optimize-my-netlify-build-time/3907). \n  \n  Use this guide as inspiration for cn CLI prompts not covered in this cookbook, such as:\n  - `\"Configure my builds to skip Dependabot PRs automatically\"`\n  - `\"Set up custom ignore patterns for documentation-only changes\"`\n  - `\"Optimize my Contentful webhooks to prevent duplicate builds\"`\n  - `\"Show me how to use build hooks instead of automatic git triggers\"`\n  - `\"Help me choose between Astro and Hugo based on build performance\"`\n  \n  The community guide contains real-world scenarios that you can turn into AI-assisted solutions - just describe what you want to achieve and let cn CLI handle the implementation!\n</Tip>\n\n### Step 4: Bundle Analysis with Netlify's Built-in Tools\n\nUse Netlify's bundle analyzer and optimization features:\n\n```bash\n# In cn TUI mode:\n\"Analyze and optimize my bundle using Netlify's tools:\n\n1. Enable Netlify Bundle Analyzer:\n   - Add @netlify/plugin-bundle-analyzer to plugins\n   - Configure size thresholds in netlify.toml\n   - Generate visual bundle reports\n\n2. Set up Asset Optimization:\n   - Enable automatic JS minification\n   - Configure CSS optimization\n   - Turn on HTML minification\n   - Set up image processing pipeline\n\n3. Implement Smart Code Splitting:\n   - Analyze current chunks with the bundle analyzer\n   - Identify components over 50KB for splitting\n   - Configure webpack/vite for optimal chunking\n   - Set up route-based code splitting\n\n4. Configure Netlify's CDN for optimal delivery:\n   - Set cache headers for static assets\n   - Enable Brotli compression\n   - Configure edge caching rules\n\nGenerate a full report with before/after bundle sizes\"\n```\n\n<Card title=\"Netlify's Performance Arsenal\" icon=\"rocket\">\n\n    **Features Available Through Netlify MCP**: \n    - [Bundle\n    Analyzer](https://docs.netlify.com/configure-builds/build-plugins/bundle-analyzer/): Visualize your JavaScript bundles \n    - [Asset\n    Optimization](https://docs.netlify.com/configure-builds/post-processing/):\n    Automatic minification and compression \n    - [Edge\n    Network](https://docs.netlify.com/platform/edge-network/): Global CDN with\n    smart caching \n    - [Deploy\n    Previews](https://docs.netlify.com/site-deploys/deploy-previews/): Test\n    optimizations before production\n  \n</Card>\n\n### Step 5: Image Optimization\n\nOptimize images for better performance:\n\n```bash\n# In cn TUI mode:\n\"Set up Cloudinary image optimization for my Netlify site:\n- Install @cloudinary/netlify-plugin via MCP\n- Auto-convert images to WebP with fallbacks\n- Generate responsive sizes (320w, 640w, 1024w, 1920w)\n- Add lazy loading for all images\n- Configure blur-up placeholders\n- Update netlify.toml with Cloudinary settings\nTarget: Reduce image payload by 60-80% and improve LCP\"\n```\n\n## Continuous Performance Monitoring\n\n### Step 6: Performance Budget Enforcement with Lighthouse CI\n\nSet and enforce performance budgets with automated testing:\n\n```bash\n# In cn TUI mode:\n\"Set up Lighthouse CI with performance budgets for my Netlify site:\n\nRequirements:\n- LCP must be < 2.5 seconds\n- JavaScript bundle must be < 200KB\n- Total size < 500KB\n- Performance score >= 90\n\nSetup needed:\n1. Install @lhci/cli and @netlify/plugin-lighthouse\n2. Create lighthouserc.js with these budget assertions\n3. Configure netlify.toml to run checks on all deploys\n4. Add GitHub status checks to block PRs that exceed budgets\n5. Create performance dashboard at /lighthouse-reports\n\nPlease configure the complete Lighthouse CI setup with these budgets.\"\n```\n\n<Tip>\n  This comprehensive setup will automatically test every deploy preview and\n  production deployment, blocking any changes that violate your performance\n  budgets.\n</Tip>\n\n### Step 7: Real User Monitoring with Netlify Analytics\n\nLeverage Netlify's built-in analytics and integrate advanced RUM solutions:\n\n```bash\n# In cn TUI mode:\n\"Set up comprehensive Real User Monitoring for my Netlify site:\n\n1. Configure Netlify Analytics Pro (requires Pro account):\n   - Set up server-side analytics (no JS required)\n   - Track Core Web Vitals (LCP, FID, CLS, INP)\n   - Monitor top pages by performance score\n   - Create custom performance alerts\n   - Configure weekly performance reports\n\n2. Integrate Web Vitals tracking:\n   - Install web-vitals library for detailed metrics\n   - Send metrics to Netlify Functions endpoint\n   - Store performance data in Netlify Blobs\n   - Create performance dashboard at /metrics\n\n3. Set up Performance Alerts:\n   - Alert when P75 LCP > 3 seconds\n   - Notify if CLS increases by 20%\n   - Monitor JavaScript error rates\n   - Track 404s and broken resources\n   - Send alerts to Slack via Netlify Functions\n\n4. Configure Geographic Performance Monitoring:\n   - Use Netlify Edge Functions to track region-specific metrics\n   - Identify slow regions with Edge geo data\n   - Compare performance across CDN nodes\n   - Optimize edge caching for slow regions\n\n5. Create Custom Performance Dashboard:\n   - Build dashboard page using Netlify Functions\n   - Display real-time Core Web Vitals\n   - Show performance trends over time\n   - Include browser and device breakdowns\n\nShow me the complete implementation with all code and configurations\"\n```\n\n<Warning>\n  **Note**: Netlify Analytics Pro requires a paid Netlify Pro account ($19/month\n  per site). The prompt above assumes you have this plan. For free tier users,\n  focus on steps 2-5 which use Netlify Functions and Edge Functions to build\n  custom analytics.\n</Warning>\n\n<Info>\n  **Netlify Analytics Advantages**:\n  - [Server-side Analytics](https://docs.netlify.com/analytics/get-started/) - No client-side JavaScript needed\n  - [Core Web Vitals](https://docs.netlify.com/analytics/core-web-vitals/) - Automatic tracking of Google's metrics\n  - [Custom Metrics](https://docs.netlify.com/functions/logs/) - Track any metric via Functions\n  - [Edge Insights](https://docs.netlify.com/edge-functions/overview/#geolocation) - Geographic performance data\n  - [No Cookie Banner Required](https://docs.netlify.com/analytics/get-started/#privacy) - GDPR compliant by default\n\nUnlike Google Analytics, Netlify Analytics:\n\n- Doesn't slow down your site (server-side)\n- Captures 100% of traffic (no ad blockers)\n- Respects user privacy (no cookies)\n- Shows bot traffic separately\n</Info>\n\n<Tip>\n\n    **Pro Tip**\n    Combine Netlify Analytics with Edge Functions to create a\n    powerful RUM solution that: \n    - Tracks performance by user segment \n    - A/B tests\n    performance optimizations \n    - Personalizes content based on connection speed \n    - Automatically serves lighter assets to slow connections\n  \n</Tip>\n\n## Automated Performance Checks\n\n### Add GitHub Secrets\n\nNavigate to **Repository Settings ‚Üí Secrets and variables ‚Üí Actions** and add:\n\n- `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n- `NETLIFY_AUTH_TOKEN`: Your Netlify personal access token\n- `NETLIFY_SITE_ID`: Your Netlify site ID\n\n### GitHub Actions Performance Guard\n\nBlock PRs that degrade performance:\n\n```yaml\nname: Performance Check\n\non:\n  pull_request:\n\njobs:\n  performance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: \"22\"\n\n      - name: Install Continue CLI\n        run: |\n          npm install -g @continuedev/cli\n          echo \"‚úÖ Continue CLI installed\"\n\n      - name: Authenticate Continue CLI\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n        run: |\n          cn auth login --api-key \"$CONTINUE_API_KEY\"\n          echo \"‚úÖ Continue CLI authenticated\"\n\n      - name: Deploy and Test Performance\n        id: perf\n        env:\n          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n        run: |\n          echo \"üöÄ Deploying PR preview and analyzing performance...\"\n          cn -p \"Deploy PR preview and run Lighthouse.\n                 Compare scores with main branch.\n                 Output JSON with score deltas.\" > performance.json\n\n      - name: Comment Performance Results\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const perf = JSON.parse(fs.readFileSync('performance.json'));\n\n            const emoji = perf.score_delta < -10 ? 'üî¥' :\n                         perf.score_delta < 0 ? 'üü°' : 'üü¢';\n\n            const comment = `## ${emoji} Performance Impact\n\n            | Metric | Main | PR | Delta |\n            |--------|------|----|---------|\n            | Performance Score | ${perf.main_score} | ${perf.pr_score} | ${perf.score_delta > 0 ? '+' : ''}${perf.score_delta} |\n            | LCP | ${perf.main_lcp}s | ${perf.pr_lcp}s | ${perf.lcp_delta > 0 ? '+' : ''}${perf.lcp_delta}s |\n            | Bundle Size | ${perf.main_bundle}KB | ${perf.pr_bundle}KB | ${perf.bundle_delta > 0 ? '+' : ''}${perf.bundle_delta}KB |\n\n            ${perf.score_delta < -10 ? '‚ö†Ô∏è **This PR significantly degrades performance. Please optimize before merging.**' : ''}\n\n            [View Full Report](${perf.report_url})`;\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comment\n            });\n\n            // Fail check if performance degrades significantly\n            if (perf.score_delta < -10) {\n              core.setFailed('Performance degraded by more than 10 points');\n            }\n```\n\n<Warning>\n  This workflow will: \n  - **Block merge** if performance score drops >10 points \n  - **Warn** on any performance regression \n  - **Celebrate** improvements with green indicators\n\n</Warning>\n\n## Performance Testing Locally\n\n### Step 8: Local Performance Testing\n\nTest performance before deploying:\n\n```bash\n# In cn TUI mode:\n\"Run production build and measure bundle sizes\"\n```\n\n### Step 9: Pre-commit Performance Checks\n\nPrevent performance regressions before they happen:\n\n```bash\n# In cn TUI mode:\n\"Add pre-commit hooks for bundle size limits\"\n```\n\n## Performance Troubleshooting\n\n### Debug Performance Issues\n\nIdentify and fix performance bottlenecks:\n\n```bash\n# In cn TUI mode:\n\"Why did my performance score drop?\"\n```\n\n### Performance Issue Quick Fixes\n\n| Issue         | Quick Fix Command (in cn TUI)     |\n| ------------- | --------------------------------- |\n| Slow LCP      | `\"Preload critical resources\"`    |\n| High CLS      | `\"Add size attributes to images\"` |\n| Large bundles | `\"Implement code splitting\"`      |\n| Slow builds   | `\"Enable build caching\"`          |\n| Poor caching  | `\"Configure cache headers\"`       |\n\n## What You've Accomplished\n\n<Card title=\"Performance-First Development\" icon=\"gauge-high\">\n\n    You've built an AI-powered performance optimization system that: \n    - Automatically monitors Core Web Vitals \n    - Runs A/B tests between branches \n    - Blocks deployments that degrade performance \n    - Provides actionable optimization\n    suggestions\n  \n</Card>\n\n## Discover Netlify's Hidden Performance Gems\n\nFeatures many developers don't know Netlify offers:\n\n<CardGroup cols={2}>\n  <Card title=\"Edge Functions\" icon=\"bolt\">\n  \n      **[Netlify Edge Functions](https://docs.netlify.com/edge-functions/overview/)**\n      - Run code at the edge, closer to users\n      - Transform responses on-the-fly\n      - A/B test at the edge level\n      - Personalize content without client-side JS\n    \n  </Card>\n\n<Card title=\"Netlify Graph\" icon=\"diagram-project\">\n\n    **[Netlify Graph](https://docs.netlify.com/graph/overview/)** \n    - Unified GraphQL gateway for all your APIs \n    - Automatic TypeScript generation \n    - Built-in authentication handling \n    - Zero client-side API keys needed\n    \n</Card>\n\n<Card title=\"On-Demand Builders\" icon=\"hammer\">\n  **[On-Demand\n  Builders](https://docs.netlify.com/configure-builds/on-demand-builders/)** \n  - Generate pages only when requested\n  - Cache dynamically generated content\n  - Perfect for large sites (10k+ pages)\n  - Reduce build times dramatically\n\n</Card>\n\n  <Card title=\"Netlify Functions\" icon=\"function\">\n    **[Background Functions](https://docs.netlify.com/functions/background-functions/)**\n    - Run tasks up to 15 minutes\n    - Process webhooks asynchronously\n    - Handle heavy computations\n    - No timeout worries\n  </Card>\n</CardGroup>\n\n```bash\n# Try these advanced features in cn TUI mode:\n\"Show me how to use Netlify Edge Functions for geo-based personalization\"\n\"Set up On-Demand Builders for my blog with 5000 posts\"\n\"Configure Background Functions for image processing\"\n```\n\n## Performance Best Practices\n\nThe Netlify Performance Rules enforce:\n\n<CardGroup cols={3}>\n  <Card title=\"Build Optimization\" icon=\"hammer\">\n    - Dependency caching enabled\n    - Parallel builds when possible\n    - Incremental compilation\n    - Smart cache invalidation\n  </Card>\n\n  <Card title=\"Runtime Performance\" icon=\"bolt\">\n    - Automatic image optimization\n    - Efficient resource hints (preload, prefetch)\n    - Optimal cache headers\n    - CDN configuration\n  </Card>\n\n  <Card title=\"Monitoring Standards\" icon=\"chart-line\">\n    - Performance budgets enforced\n    - Core Web Vitals tracking\n    - Real user monitoring\n    - Automated alerts\n  </Card>\n</CardGroup>\n\n## Advanced Performance Strategies\n\n### Progressive Enhancement\n\n```bash\n# In cn TUI mode:\n\"Implement progressive enhancement with basic HTML first\"\n```\n\n### Multi-variant Testing\n\n```bash\n# In cn TUI mode:\n\"Test 3 bundle strategies and auto-select winner\"\n```\n\n### Predictive Prefetching\n\n```bash\n# In cn TUI mode:\n\"Analyze navigation patterns and prefetch next pages\"\n```\n\n## Next Steps\n\n- Install [Netlify MCP](https://hub.continue.dev/netlify/netlify-mcp) from Continue Mission Control\n- Set up [Performance Monitoring](https://docs.netlify.com/analytics/get-started/)\n- Configure [A/B Testing](https://docs.netlify.com/split-testing/overview/)\n- Join the [Continue Discord](https://discord.gg/continue) for support\n\n## Resources\n\n- [Netlify MCP on Continue Mission Control](https://hub.continue.dev/netlify/netlify-mcp)\n- [Core Web Vitals Guide](https://web.dev/vitals/)\n- [Netlify Analytics Documentation](https://docs.netlify.com/analytics/)\n- [Continue Performance Guides](https://docs.continue.dev/guides)\n"}
{"source":"github","repo":"continue","path":"docs/guides/sentry-mcp-error-monitoring.mdx","content":"---\ntitle: \"Automated Error Analysis with Sentry MCP\"\ndescription: \"Build an AI-powered error monitoring workflow that analyzes Sentry issues, identifies patterns, and creates actionable GitHub issues automatically.\"\nsidebarTitle: \"Sentry Automated Error Analysis with MCP\"\n---\n\n<Note>\n\n  **üöÄ New: Sentry Mission Control Integration with Continuous AI**\n  \n  The new [Sentry Mission Control Integration](/mission-control/integrations/sentry) enables **[Continuous AI](/guides/continuous-ai)** for error monitoring - where AI agents autonomously detect, analyze, and fix production errors. When Sentry detects an issue, your agent automatically analyzes it, generates a fix, creates a PR, and validates the solution without manual intervention.\n  \n  **Mission Control Benefits:**\n  - **Webhook-based triggers** - Automatically respond when new errors occur in production\n  - **Centralized error workflows** - Manage all error monitoring Tasks from one dashboard\n  - **Dedicated inbox view** - Monitor all Sentry-related agent activity at [hub.continue.dev/inbox?view=sentry](https://hub.continue.dev/inbox?view=sentry)\n  - **Metrics & intervention rates** - Track how often agents successfully fix errors without human help\n  \n  This represents **[Level 2 Continuous AI](/guides/continuous-ai#the-continuous-ai-maturity-model)**: AI handles routine error analysis and fixes autonomously while developers focus on complex problems. As outlined in our [Continuous AI guide](/guides/continuous-ai#best-practices-for-sustainable-continuous-ai), this reduces mean time to resolution (MTTR) and improves application reliability.\n  \n  **Get Started:** Use this cookbook to understand error monitoring fundamentals, then [enable Mission Control](/mission-control/integrations/sentry) to deploy autonomous error-fixing agents across your organization.\n\n</Note>\n\n<Card title=\"What You'll Build\" icon=\"bug\">\n  An automated error monitoring system that uses Continue CLI with Sentry MCP to analyze production errors, identify root causes with AI, and create detailed GitHub issues with suggested fixes.\n</Card>\n\n## What You'll Learn\n\nThis cookbook teaches you to:\n\n- Use [Sentry MCP](https://docs.sentry.io/product/sentry-mcp/) to access [issues](https://docs.sentry.io/product/issues/)\n- Analyze error patterns and stack traces with AI\n\n- Automatically create GitHub issues with root cause analysis\n- Set up continuous error monitoring with GitHub Actions\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- GitHub repository where you want to create issues\n- [Sentry account](https://sentry.io) with an active project collecting errors\n- Node.js 18+ installed locally\n- [Continue CLI](https://docs.continue.dev/guides/cli) with **active credits** (required for API usage)\n- [GitHub CLI](https://cli.github.com/) installed (`gh` command)\n\n<Steps>\n  <Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n\n  <Step title=\"Set up Continue CLI Account & API Key\">\n    1. Visit [Continue Organizations](https://hub.continue.dev/settings/organizations)\n    2. Sign up or log in to your Continue account\n    3. Navigate to your organization settings\n    4. Click **\"API Keys\"** and then **\"+ New API Key\"**\n    5. Copy the API key immediately (you won't see it again!)\n    6. Login to the CLI: `cn login`\n  </Step>\n</Steps>\n\n<Tip>\n  Continue CLI handles complex error analysis and API interactions - you just need to provide the right prompts!\n</Tip>\n\n## Step 1: Set Up Your Credentials\n\nFirst, you'll need to gather your Sentry and GitHub API credentials.\n\n<Tabs>\n  <Tab title=\"Configure Sentry MCP\">\n\n    <Tip>\n      See [Sentry MCP Documentation](https://docs.sentry.io/product/sentry-mcp/) for detailed configuration options\n    </Tip>\n\n    The Sentry MCP supports multiple configuration methods. For Continue CLI, OAuth is recommended:\n\n    **Option 1: OAuth Configuration (Recommended)**\n\n    The Sentry MCP will prompt for OAuth authentication when first used. Simply follow the authorization flow.\n\n    **Option 2: STDIO Mode with Auth Token**\n\n    For local development or self-hosted Sentry installations, you can use STDIO mode:\n    ```bash\n    npx @sentry/mcp-server@latest --access-token=YOUR_SENTRY_TOKEN --host=sentry.io\n    ```\n\n    Or use environment variables:\n    ```bash\n    SENTRY_ACCESS_TOKEN=your-token SENTRY_HOST=sentry.io\n    ```\n\n    <Info>\n      The `--host` parameter is required and should point to your Sentry instance (e.g., `sentry.io` or `sentry.example.com` for self-hosted).\n    </Info>\n\n  </Tab>\n  <Tab title=\"Sentry API Credentials\">\n    You'll need a **Sentry User Auth Token** to access issues and error data:\n\n    1. Go to [User Auth Tokens](https://sentry.io/settings/account/api/auth-tokens/) in Sentry\n       - For self-hosted Sentry, use: `https://YOUR-SENTRY-DOMAIN/settings/account/api/auth-tokens/`\n    2. Click **Create New Token**\n    3. Name it \"Continue CLI Error Analysis\"\n    4. **Select these permission scopes** (required for full functionality):\n       - `org:read` - **Required** - Access organization information\n       - `project:read` - **Required** - Read project configurations\n       - `project:releases` - **Required** - Access release information for deployment tracking\n       - `event:read` - **Required** - Read detailed error event data and stack traces\n       - `event:write` - Optional - Update error events (for marking as resolved)\n       - `member:read` - Recommended - Read team member information for auto-assignment\n       - `team:read` - Recommended - Access team data for routing issues\n    5. Copy the token immediately (you won't see it again!)\n    6. Note your organization slug (found in your Sentry URL: `https://sentry.io/organizations/YOUR-ORG-SLUG`)\n    7. Note your Sentry host URL (typically `https://sentry.io` or your self-hosted domain)\n\n    <Info>\n      **Sentry MCP Connection**: The MCP server connects via OAuth to `https://mcp.sentry.dev/mcp` and handles authentication securely. For local development, you can use STDIO mode with your auth token.\n    </Info>\n\n  </Tab>\n  <Tab title=\"Set Up GitHub CLI Authentication\">\n   GitHub CLI handles authentication automatically - no manual PAT needed:\n\n    1. Install GitHub CLI if not already installed\n    2. Run `gh auth login` and follow the prompts\n    3. Choose authentication method (browser or token)\n    4. Grant necessary permissions when prompted (`issues:write` is **required** for creating issues)\n\n  </Tab>\n</Tabs>\n## Sentry Error Monitoring Workflow Options\n\n<Card title=\"Fastest Path to Success\" icon=\"rocket\">\n  Skip the manual setup and use our pre-built Sentry Continuous AI agent that includes\n  optimized prompts, rules, and the Sentry MCP for more consistent results.\n</Card>\n\n<Info>\n  **How Sentry MCP Works**:\n  - Connects to your Sentry organization via OAuth\n  - Provides tools for accessing issues, projects, teams, and DSNs\n  - Supports both hosted (`https://mcp.sentry.dev`) and self-hosted Sentry instances\n  - Automatically handles authentication and API interactions\n</Info>\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    **Perfect for:** Immediate error analysis with AI-powered root cause detection and built-in debugging\n\n    <Steps>\n      <Step title=\"Add the Pre-Built Agent\">\n        Visit the [Sentry Continuous AI Agent](https://hub.continue.dev/continuedev/sentry-continuous-ai-agent) on Continue Mission Control and click **\"Install Agent\"** or run:\n        \n        ```bash\n        cn --agent continuedev/sentry-continuous-ai-agent\n        ```\n\n        This agent includes:\n        - **Optimized prompts** for Sentry error analysis and GitHub issue creation\n        - **Built-in rules** for consistent formatting and error handling\n        - **[Sentry MCP](https://docs.sentry.io/product/sentry-mcp/)** for more reliable API interactions\n        - **Automatic authentication** via OAuth flow\n      </Step>\n\n      <Step title=\"Run Error Analysis\">\n        Navigate to your project directory and enter this prompt in the Continue CLI TUI:\n        \n        ```\n        Analyze recent Sentry errors and create GitHub issues for critical bugs with suggested fixes\n        ```\n\n        That's it! The agent handles everything automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** Results are more consistent and debugging is easier thanks to the Sentry MCP integration and pre-tested prompts.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n    <Steps>\n      <Step title=\"Add Sentry MCP to Continue CLI\">\n        Configure the [Sentry MCP](https://docs.sentry.io/product/sentry-mcp/) using OAuth:\n\n        The MCP server will automatically prompt for OAuth authentication when you first use it.\n      </Step>\n\n      <Step title=\"Verify Sentry Connection\">\n        Test your Sentry MCP connection with this prompt:\n        \n        ```\n        List my Sentry organizations and projects\n        ```\n      </Step>\n\n      <Step title=\"Create Custom Error Analysis Prompts\">\n        Use this prompt template with Continue CLI to analyze Sentry errors:\n\n        ```\n        Analyze Sentry errors from the past 24 hours:\n        - Group errors by root cause\n        - Identify the top 5 most critical issues by frequency and impact\n        - For each critical issue, provide:\n          * Stack trace analysis\n          * Affected user count\n          * First seen and last seen timestamps\n          * Suggested fix based on error context\n        - Create GitHub issues using gh CLI with:\n          * Title format: 'üêõ [Sentry] [Error Type]: Brief description'\n          * Labels: 'bug', 'sentry', 'production'\n          * Priority labels based on severity\n          * Full error context and suggested fix in the body\n        Execute the commands and confirm each issue was created.\n        ```\n      </Step>\n    </Steps>\n\n  </Tab>\n</Tabs>\n\n<Info>\n  **Why GitHub CLI over GitHub MCP**: While GitHub MCP is available, it can be\n  token-expensive to run. The `gh` CLI is more efficient, requires no API tokens\n  (authenticated via `gh auth login`), and provides a cleaner command-line\n  experience. GitHub MCP remains an option if you prefer full MCP integration.\n</Info>\n\n<Accordion title=\"Agent Requirements\">\n  To use the pre-built agent, you need either:\n  - **Continue CLI Pro Plan** with the models add-on, OR\n  - **Your own API keys** added to Continue Mission Control secrets (same as Step 1)\n  \n  The agent will automatically detect and use your configuration. For Sentry MCP:\n  - **Sentry account** with at least one project\n  - **User Auth Token** with appropriate scopes (or OAuth flow)\n  - The MCP works with both Sentry's hosted service (`sentry.io`) and self-hosted instances\n</Accordion>\n\n---\n\n<Warning>\n  **Repository Labels Required**: Make sure your GitHub repository has these labels:\n  - `bug`, `sentry`, `production`\n  - `critical`, `high-priority`, `medium-priority`, `low-priority`\n  - `needs-investigation`, `has-fix`\n\n  Create missing labels in your repo at: **Settings ‚Üí Labels ‚Üí New label**\n</Warning>\n\n## Step 2: Analyze Sentry Errors with AI\n\nUse Continue CLI to perform intelligent error analysis. Enter these prompts in the Continue CLI TUI:\n\n<Info>\nTo run any of the example prompts below in headless mode, use `cn -p \"prompt\"`\n</Info>\n\n<Tabs>\n  <Tab title=\"Recent Errors Analysis\">\n    **Prompt:**\n    ```\n    Show me Sentry errors from the past 7 days, grouped by error type, with frequency counts\n    ```\n  </Tab>\n\n  <Tab title=\"Critical Error Investigation\">\n    **Prompt:**\n    ```\n    Find the most critical Sentry error affecting the most users in production and provide:\n    - Full stack trace analysis\n    - Affected user count and browser/OS breakdown\n    - Timeline of when the error started occurring\n    - Similar historical issues from Sentry\n    - Root cause hypothesis based on code context\n    - Suggested fix with code examples\n    ```\n  </Tab>\n\n  <Tab title=\"Performance Issue Detection\">\n    **Prompt:**\n    ```\n    Analyze Sentry performance data to identify:\n    - Slowest transactions in the past 24 hours\n    - Database queries with high latency\n    - API endpoints with degraded performance\n    - Suggested optimizations for each issue\n    ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Available Sentry MCP Tools**:\n  - **Organizations**: Access org-level data and settings\n  - **Projects**: Query projects and their configurations\n  - **Issues**: Search and analyze error issues\n  - **Teams**: Manage team assignments\n  - **DSNs**: Retrieve project DSN configurations\n\n</Info>\n\n## Step 3: Automate GitHub Issue Creation\n\nCreate actionable GitHub issues from Sentry errors. Enter this prompt in the Continue CLI TUI:\n\n**Prompt:**\n```\nFor each unresolved Sentry error with 'critical' or 'high' severity:\n1. Analyze the error using Sentry MCP\n2. Create a GitHub issue with gh CLI:\n   - Title: 'üêõ [Sentry] [Error Type]: Brief description'\n   - Body with:\n     * Error summary and impact (affected users, frequency)\n     * Full stack trace\n     * Environment details (browser, OS, release version)\n     * Link to Sentry issue\n     * Root cause analysis from AI\n     * Suggested fix with code snippets\n     * Related Sentry issues\n   - Labels: 'bug', 'sentry', 'production', and severity label\n   - Assignees: Team member based on code ownership\n3. Update Sentry issue with GitHub issue link\n4. Confirm creation with GitHub issue URL\n```\n\n<Tip>\n  **Best Practice**: Link GitHub issues back to Sentry for full traceability. This creates a bidirectional connection between your error monitoring and issue tracking.\n</Tip>\n\n## Step 4: Set Up Continuous Monitoring with GitHub Actions\n\nAutomate error monitoring with the [Sentry Release GitHub Action](https://docs.sentry.io/product/releases/setup/release-automation/github-actions/) and Continue CLI to create comprehensive, AI-powered issue descriptions:\n\n<Tip>\n  **Why Combine Sentry Releases with Continue CLI?**\n  - **Release Tracking**: Associate errors with specific deployments\n  - **AI-Powered Analysis**: Continue CLI generates detailed issue descriptions with root cause analysis\n  - **Better Context**: Link errors to commits and pull requests\n  - **Automated Workflows**: Create issues with full stack traces and suggested fixes\n</Tip>\n\n```yaml\nname: Sentry Error Monitoring\n\non:\n  push:\n    branches:\n      - main\n  schedule:\n    # Run every 6 hours\n    - cron: \"0 */6 * * *\"\n  workflow_dispatch: # Allow manual triggers\n\njobs:\n  monitor-errors:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: \"22\"\n\n      # Create Sentry release for better error tracking\n      - name: Create Sentry release\n        uses: getsentry/action-release@v1\n        env:\n          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}\n          SENTRY_ORG: ${{ secrets.SENTRY_ORG }}\n          SENTRY_PROJECT: ${{ secrets.SENTRY_PROJECT }}\n        with:\n          environment: production\n\n      - name: Install Continue CLI\n        run: |\n          npm install -g @continuedev/cli\n          echo \"‚úÖ Continue CLI installed\"\n\n      - name: Authenticate GitHub CLI\n        run: |\n          echo \"${{ secrets.GITHUB_TOKEN }}\" | gh auth login --with-token\n          echo \"‚úÖ GitHub CLI authenticated\"\n\n      - name: Analyze Sentry Errors and Create Issues\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}\n          SENTRY_ORG: ${{ secrets.SENTRY_ORG }}\n          SENTRY_PROJECT: ${{ secrets.SENTRY_PROJECT }}\n        run: |\n          echo \"üîç Analyzing Sentry errors...\"\n          \n          # Use Continue CLI to analyze errors and generate comprehensive issue descriptions\n          cn -p \"Using Sentry MCP, analyze errors from the past 6 hours for project $SENTRY_PROJECT:\n                 1. Filter for unresolved errors with high or critical severity\n                 2. Group similar errors to avoid duplicates\n                 3. For each unique critical error:\n                    - Generate a comprehensive issue description including:\n                      * Error summary with frequency and user impact metrics\n                      * Full stack trace with highlighted problem areas\n                      * Environment details (browser, OS, release version)\n                      * Link to Sentry issue dashboard\n                      * Root cause analysis using AI\n                      * Step-by-step reproduction if available\n                      * Suggested fix with code examples\n                      * Related errors or patterns\n                    - Check if a GitHub issue already exists for this error\n                    - If not, create a new issue with the generated description\n                    - Use gh CLI: gh issue create --title '[Sentry] [Error Type]: Brief description' --body 'AI-generated comprehensive analysis' --label 'bug,sentry,critical,needs-investigation'\n                    - Link the GitHub issue back to Sentry\n                 4. Generate a summary report with:\n                    - Total errors analyzed\n                    - New issues created with URLs\n                    - Errors skipped (already tracked)\n                    - Release correlation if available\n\n                 Sentry Organization: $SENTRY_ORG\n                 Project: $SENTRY_PROJECT\n                 Only process errors not already tracked in GitHub.\"\n\n      - name: Post workflow summary\n        if: always()\n        run: |\n          echo \"## üìä Sentry Error Monitoring Summary\" >> $GITHUB_STEP_SUMMARY\n          echo \"‚úÖ Workflow completed at $(date)\" >> $GITHUB_STEP_SUMMARY\n          echo \"\" >> $GITHUB_STEP_SUMMARY\n          echo \"Check the 'Analyze Sentry Errors' step above for:\" >> $GITHUB_STEP_SUMMARY\n          echo \"- Number of errors analyzed\" >> $GITHUB_STEP_SUMMARY\n          echo \"- GitHub issues created\" >> $GITHUB_STEP_SUMMARY\n          echo \"- Errors already tracked\" >> $GITHUB_STEP_SUMMARY\n```\n\n<Warning>\n  **Required GitHub Secrets**:\n  - `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n  - `SENTRY_AUTH_TOKEN`: Your Sentry User Auth Token (needs scopes: `org:read`, `project:read`, `project:releases`, `event:read`)\n  - `SENTRY_ORG`: Your Sentry organization slug\n  - `SENTRY_PROJECT`: Your Sentry project slug\n  - `GITHUB_TOKEN`: Automatically provided by GitHub Actions\n\n  Add these at: **Repository Settings ‚Üí Secrets and variables ‚Üí Actions**\n</Warning>\n\n<Info>\n  **Workflow Best Practices**:\n  - Run every 6 hours to catch critical errors quickly\n  - Create Sentry releases on push to track error-to-deployment correlation\n  - Use Continue CLI to generate comprehensive, AI-powered issue descriptions\n  - Use duplicate detection to avoid creating multiple issues for the same error\n  - Filter by severity to focus on high-impact issues\n  - Include full error context and suggested fixes in issues\n  - Tag issues with appropriate labels for team routing\n  - Link GitHub issues back to Sentry for bidirectional tracking\n</Info>\n\n## What You've Built\n\nAfter completing this guide, you have a complete **Sentry-powered error monitoring system** that:\n\n- **Monitors production errors** - Automatically fetches and analyzes Sentry issues every 6 hours\n\n- **Identifies critical bugs** - Uses AI to spot high-impact errors\n\n- **Creates actionable tasks** - Generates GitHub issues with root cause analysis and suggested fixes\n\n- **Runs autonomously** - Operates continuously without manual intervention using GitHub Actions\n\n- **Scales with your app** - Handles growing error volumes and complexity automatically\n\n<Card title=\"Continuous AI Error Monitoring\" icon=\"shield-check\">\n  Your system now operates at **[Level 2 Continuous AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** - AI handles routine error analysis with human oversight through GitHub issue review and resolution.\n</Card>\n\n## Advanced Error Analysis Prompts\n\nEnhance your workflow with these advanced Continue CLI prompts:\n\n<CardGroup cols={2}>\n  <Card title=\"Release Impact Analysis\" icon=\"rocket\">\n    Compare error rates before and after the latest [Sentry release](https://docs.sentry.io/product/releases/) to identify regressions introduced in the deployment\n  </Card>\n  <Card title=\"Error Trend Detection\" icon=\"chart-line\">\n    Analyze Sentry error trends over the past 30 days and identify emerging issues before they become critical\n  </Card>\n  <Card title=\"User Impact Assessment\" icon=\"users\">\n    Identify which errors are affecting the most unique users and prioritize fixes based on user impact\n  </Card>\n  <Card title=\"Performance Correlation\" icon=\"gauge\">\n    Cross-reference Sentry [performance issues](https://docs.sentry.io/product/performance/) with error spikes to identify root causes\n  </Card>\n</CardGroup>\n\n## Security Best Practices\n\n<Warning>\n  **Protect Your API Keys**:\n  - Store all credentials as GitHub Secrets, never in code\n  - Use Continue CLI's secure secret storage\n  - Limit Sentry token scopes to minimum required permissions\n  - Rotate API keys regularly (every 90 days recommended)\n  - Monitor token usage for unusual activity\n  - Use OAuth when possible for better security\n</Warning>\n\n## Troubleshooting\n\n### Sentry MCP Connection Issues\n\nIf you encounter connection issues:\n\n1. Verify OAuth authentication is complete\n2. Check your Sentry organization access\n3. Ensure the MCP server URL is correct (`https://mcp.sentry.dev/mcp`)\n4. For self-hosted Sentry, verify your host URL is configured correctly\n\nSee the [Sentry MCP GitHub Issues](https://github.com/getsentry/sentry-mcp/issues) for known issues and solutions.\n\n### Common Error Analysis Issues\n\n| Issue | Solution |\n|:------|:---------|\n| No errors returned | Verify your Sentry project has collected errors recently |\n| OAuth prompt not appearing | Check that Continue CLI has proper MCP configuration |\n| Duplicate GitHub issues | Implement duplicate detection in your prompts |\n| Missing error context | Ensure your Sentry token has `event:read` scope |\n\n## Next Steps\n\n\n- Set up [Sentry performance monitoring](https://docs.sentry.io/product/performance/)\n- Configure [Sentry release tracking](https://docs.sentry.io/product/releases/) for deployment correlation\n- Integrate [Slack MCP](https://hub.continue.dev/slack/slack-mcp) for error alerts\n- Join the [Continue Discord](https://discord.gg/continue) for support\n\n## Resources\n\n- [Sentry MCP Documentation](https://docs.sentry.io/product/sentry-mcp/)\n- [Sentry API Documentation](https://docs.sentry.io/api/)\n- [Sentry Issues Guide](https://docs.sentry.io/product/issues/)\n- [Sentry Performance Monitoring](https://docs.sentry.io/product/performance/)\n- [Sentry Release Tracking](https://docs.sentry.io/product/releases/)\n\n- [Sentry MCP GitHub Repository](https://github.com/getsentry/sentry-mcp)\n- [GitHub CLI Documentation](https://cli.github.com/)\n- [Continue CLI Guide](https://docs.continue.dev/guides/cli)\n- [Continuous AI Best Practices](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)\n"}
{"source":"github","repo":"continue","path":"docs/guides/notion-continue-guide.mdx","content":"---\ntitle: \"Developer & Team Workflows with Notion + Continue CLI\"\ndescription: \"Use Continue CLI with Notion to generate docs, manage tasks, and automate project workflows ‚Äì all through natural-language prompts.\"\nsidebarTitle: \"Notion with Continue\"\n---\n\n<Card title=\"What You'll Build\" icon=\"notion\">\n\n  A workflow that lets you query, update, and create Notion pages or database\n  entries from natural-language prompts. Generate PRDs, sprint tasks, meeting\n  notes, or status reports automatically ‚Äì perfect for individual developers\n  and cross-functional teams.\n\n</Card>\n\n## What You'll Learn\n\nThis guide teaches you to:\n- Use natural language to connect to the Notion API directly with Continue CLI for powerful automation\n- Configure Notion API access with proper permissions and security\n- Run prompts in both TUI (interactive) and headless modes\n- Create automated workflows that generate docs, manage tasks, and sync data\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- [Continue CLI](https://docs.continue.dev/cli/overview) installed (`npm i -g @continuedev/cli`)\n- A [Notion workspace](https://notion.so) with Editor (or higher) access\n- Node.js 18+ installed locally\n- [Continue account](https://hub.continue.dev) with **Hub access**\n\n<Warning>\n\n  **Agent usage requires credits** ‚Äì create a Continue API key at\n  [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n  and store it as a secret.\n\n</Warning>\n\n<Steps>\n  <Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n    Verify installation:\n    ```bash\n    cn --version\n    ```\n  </Step>\n\n  <Step title=\"Create Notion Integration & Get API Key\">\n    1. Go to **[Notion Integrations](https://www.notion.so/my-integrations)**\n    2. Click **+ New integration** ‚Üí give it a name (e.g. \"Continue Integration\")\n    3. Select your workspace\n    4. Under **Content Capabilities**, enable:\n       - ‚úÖ Read content\n       - ‚úÖ Update content\n       - ‚úÖ Insert content\n    5. Under **Comment Capabilities**, enable:\n       - ‚úÖ Read comments\n       - ‚úÖ Insert comments\n    6. Under **User Capabilities**, select:\n       - ‚úÖ Read user information including email addresses\n    7. Click **Submit** and copy the **Internal Integration Secret** (starts with `secret_`)\n\n    <Info>\n\n      This token is your `NOTION_API_KEY`.\n      Keep it safe ‚Äì you won't be able to view it again.\n\n    </Info>\n\n    8. In Notion, open each database or top-level page you want accessible ‚Üí **Share** ‚Üí **Invite** your new integration ‚Üí **Full access**.\n  </Step>\n\n  <Step title=\"Configure API Access\">\n    Set your Notion API key as an environment variable in your terminal:\n    \n    ```bash\n    export NOTION_API_KEY=\"secret_xxx\"\n    ```\n\n    <Tip>\n\n    Running this command sets your API key for the current terminal session only.  \n    When you close the terminal, the variable won't persist. To test your API key, run the curl command below in the same session.\n\n     ```bash\n    curl -H \"Authorization: Bearer $NOTION_API_KEY\" \\\n         -H \"Notion-Version: 2022-06-28\" \\\n         https://api.notion.com/v1/databases\n    ```\n    </Tip>\n\n  </Step>\n  <Step title=\"Add Workspace Keys to Terminal Session\">\n    Depending on what you want `cn` to accomplish, you'll need to add your Notion workspace keys to the terminal session. The workspace key is your Notion database ID. Run the following command:\n\n      ```bash\n      export NOTION_DATABASE_ID=\"your_database_id\"\n      ```\n    <Tip>\n\n     You can find your database ID by:\n        1. Opening your Notion changelog database\n        2. Looking at the URL - it will be something like:\n        https://www.notion.so/your-workspace/DATABASE_ID?v=...\n        3. The DATABASE_ID is the long string of characters between the last / and the ?\n\n    </Tip>\n  </Step>\n</Steps>\n\n## Running Continue CLI with Notion API\n\n<Card title=\"üöÄ Choose Your Interface\" icon=\"zap\">\n\n  Continue CLI offers two powerful modes for Notion automation:\n  **TUI mode** for interactive workflows and **Headless mode** for automated scripts.\n\n</Card>\n\n<Tabs>\n  <Tab title=\"üñ•Ô∏è TUI Mode (Interactive)\">\n\n    <Steps>\n      <Step title=\"Launch Interactive Mode\">\n        Navigate to your project directory and run:\n        ```bash\n        cn \n        ```\n      </Step>\n\n      <Step title=\"Run Your First Prompt\">\n        In the TUI interface, enter:\n        ```\n        1. Fetch my Notion databases using the Notion API key\n        and Database ID stored in this terminal session.\n        2. Look at the last week of Merged GitHub PRs, and create\n        a changelog entry in Notion summarizing the features and breaking changes.\n        ```\n\n        <Tip>\n\n          TUI mode lets you review and approve each action before execution,\n          perfect for learning and debugging.\n\n        </Tip>\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"ü§ñ Headless Mode (Automated)\">\n    <Steps>\n      <Step title=\"Run Automated Commands\">\n        Execute prompts directly from the command line:\n        ```bash\n        cn -p --auto \"\n        1. Fetch my Notion databases using the Notion API key\n        and Database ID stored in this terminal session.\n        2. Analyze all merged GitHub PRs from the past week.\n        3. Extract feature descriptions and breaking changes.\n        4. Create a technical changelog in Notion with PR links.\"\n        ```\n\n        **Flags explained:**\n        - `-p`: Run without TUI interface\n        - `--auto`: Execute without manual approval\n\n        <Warning>\n\n          Always test prompts in TUI mode first before running\n          them with `--auto` in production.\n\n        </Warning>\n      </Step>\n    </Steps>\n  </Tab>\n</Tabs>\n\n<Accordion title=\"API Connection Notes\">\n\n  - Environment variable `NOTION_API_KEY` must be set before running Continue CLI\n  - Continue automatically uses the API key to authenticate with Notion\n  - No need for manual curl commands - just reference \"the API key stored in this session\"\n  - For complex workflows, Continue maintains the API connection throughout\n  - Consider creating aliases or scripts for frequently used prompts\n\n</Accordion>\n\n---\n\n## Quick Start Example\n\nWorking example that demonstrates the power of Continue with Notion API:\n\n<Card title=\"üìä Weekly Sprint Summary\" icon=\"chart-line\">\n\n  **This exact command has been tested and works:**\n\n  ```bash\n  # Weekly Sprint Summary\n  cn -p --auto \"\n  1. Fetch Notion Sprint database\n  2. Analyze completed tasks vs planned\n  3. Generate sprint retrospective page\n  4. Add velocity metrics and burndown chart\"\n  ```\n\n  **What this does:**\n  1. Connects to your Notion workspace using the API key\n  2. Analyzes your sprint data\n  3. Calculates completion metrics\n  4. Creates a comprehensive retrospective with visualizations\n\n</Card>\n\n\n## Example Prompts & Workflows\n\nWith the Notion API configured, you can use natural language prompts to automate your workspace. Here are examples for both TUI and headless modes:\n\n<CardGroup cols={1}>\n  <Card title=\"API Documentation\" icon=\"code\">\n    **TUI Mode:**\n    ```bash\n    cn \"\n    1. Fetch my Notion databases using the API key and secrets stored in this session\n    2. Generate API documentation for all endpoints \n    in src/api/routes with request/response schemas\n    3. Create a page in Technical Docs database\n    \"\n    ```\n    \n    **Headless Mode:**\n    ```bash\n    cn -p --auto \"Fetch my Notion databases using the API key and secrets stored in this session. Generate API docs \n    for src/api/routes and save to Notion in Technical Docs database\"\n    ```\n  </Card>\n  \n  <Card title=\"Sprint Planning from Code\" icon=\"list-checks\">\n    **Headless Mode (Automated):**\n    ```bash\n    cn -p --auto \"\n    1. Fetch my Notion databases using the API key stored in this session\n    2. Scan codebase for TODO and FIXME comments\n    3. Create tasks in my Notion Engineering Backlog database\n    4. Include file paths and complexity estimates\"\n    ```\n  </Card>\n  \n  <Card title=\"Changelog from PRs\" icon=\"rocket\">\n    ```bash\n        cn -p --auto \"\n        1. Fetch my Notion databases using the API key stored in this session\n        2. Analyze all merged GitHub PRs from past month\n        3. In my Notion Launch Database, review the Launch Template.\n        4. Create an October Launch doc in Notion with a week of launches and materials based on the Launch documents based on the PR analysis and launch documents.\"\n    ```\n \n  </Card>\n  \n  <Card title=\"Daily Standup Automation\" icon=\"comments\">\n    ```bash\n    cn -p --auto \"\n    1. Fetch my Tasks database from Notion using the API key\n    2. Find tasks completed yesterday\n    3. Find tasks in progress\n    4. Create standup note with: completed, \n       in-progress, and blockers sections\"\n    ```\n  </Card>\n</CardGroup>\n\n## Advanced Workflows\n\n<CardGroup cols={1}>\n  <Card title=\"Notion + GitHub\" icon=\"github\">\n    ```bash\n    cn -p --auto \"\n    1. Connect to Notion using the API key in this session\n    2. Get merged PRs from last 7 days using GitHub\n    3. Extract feature descriptions and changes\n    4. Create formatted changelog in Notion\n    5. Add links back to GitHub PRs\"\n    ```\n    \n    <Info>\n\n      Requires GitHub repository access. To add GitHub access, update your [integration settings](https://hub.continue.dev/settings/integrations).\n\n    </Info>\n  </Card>\n  \n  <Card title=\"Test Coverage Report\" icon=\"chart-bar\">\n    ```bash\n    cn -p --auto \"\n    1. Connect to Notion using the API key in this session\n    2. Run test coverage report (npm test -- --coverage)\n    3. Parse coverage metrics\n    4. Update Test Metrics database in Notion\n    5. Flag files with coverage below 80%\"\n    ```\n  </Card>\n  \n  <Card title=\"Blog Post Review\" icon=\"magnifying-glass\">\n    ```bash\n    cn \"\n    1. Connect to Notion using the API key in this session\n    2. In my Blog Posts Database, find drafts tagged 'Review'\n    3. Review all blog posts for grammar and clarity and suggest improvements through comments.\n    ```\n  </Card>\n  \n  <Card title=\"Weekly Report Generation\" icon=\"list\">\n    ```bash\n    cn -p --auto \"\n    1. Connect to Notion databases using API key\n    2. Aggregate completed tasks from the week\n    3. Calculate velocity and burndown metrics\n    4. Generate weekly report with charts\n    5. Share link in Slack #team-updates\"\n    ```\n  </Card>\n</CardGroup>\n\n## Security Best Practices\n\n<Warning>\n\n  **Protect Your API Keys:**\n  - Never commit `NOTION_API_KEY` to version control\n  - Use environment variables or secure secret managers\n  - Rotate API keys every 90 days\n  - Grant integration access only to required databases/pages\n  - Monitor API usage through Notion's integration dashboard\n  - Use `.env` files with `.gitignore` for local development\n\n</Warning>\n\n\n## Next Steps\n\n- Create a **GitHub Actions** workflow to automate changelog generation on releases\n- Build a **daily standup bot** that runs every morning and posts to Slack\n- Set up **database templates** in Notion for consistent formatting\n- Explore **batch operations** to update multiple pages efficiently\n- Implement **error handling** for API rate limits and network issues\n\n---\n\n## Troubleshooting\n\n<Accordion title=\"Common Issues and Solutions\">\n\n  **API Key Not Found:**\n  - Ensure `NOTION_API_KEY` is exported in your current shell session\n  - Check for typos in the environment variable name\n  - Verify the key starts with `secret_`\n\n  **Database Access Denied:**\n  - Share the specific database with your integration in Notion\n  - Ensure the integration has the correct permissions (Read, Write, Insert)\n\n  **Connection Issues:**\n  - Verify Continue CLI has internet access\n  - Check network connectivity to api.notion.com\n  - Ensure your Notion workspace allows API access\n\n  **Rate Limiting:**\n  - Notion API has rate limits (3 requests per second)\n  - Implement exponential backoff for automated scripts\n  - Consider batching operations when possible\n\n</Accordion>\n"}
{"source":"github","repo":"continue","path":"docs/guides/custom-code-rag.mdx","content":"---\ntitle: \"How to Build Custom Code RAG\"\ndescription: \"Build a custom retrieval-augmented generation (RAG) system for faster and more cost-efficient code search across large codebases. This guide is for advanced users who need to index code a single time across all users or include custom logic.\"\nkeywords: [RAG, vector database, embeddings, code search, MCP]\n---\n\n## Step 1: How to Choose an Embeddings Model\n\nIf possible, we recommend using [`voyage-code-3`](https://docs.voyageai.com/docs/embeddings), which will give the most accurate answers of any existing embeddings model for code. You can obtain an API key [here](https://dash.voyageai.com/api-keys). Because their API is [OpenAI-compatible](https://docs.voyageai.com/reference/embeddings-api), you can use any OpenAI client by swapping out the URL.\n\n## Step 2: How to Choose a Vector Database\n\nThere are a number of available vector databases, but because most vector databases will be able to performantly handle large codebases, we would recommend choosing one for ease of setup and experimentation.\n\n[LanceDB](https://lancedb.github.io/lancedb/basic/) is a good choice for this because it can run in-memory with libraries for both Python and Node.js. This means that in the beginning you can focus on writing code rather than setting up infrastructure. If you have already chosen a vector database, then using this instead of LanceDB is also a fine choice.\n\n## Step 3: How to Choose a \"Chunking\" Strategy\n\nMost embeddings models can only handle a limited amount of text at once. To get around this, we \"chunk\" our code into smaller pieces.\n\nIf you use `voyage-code-3`, it has a maximum context length of 16,000 tokens, which is enough to fit most files. This means that in the beginning you can get away with a more naive strategy of truncating files that exceed the limit. In order of easiest to most comprehensive, 3 chunking strategies you can use are:\n\n1. Truncate the file when it goes over the context length: in this case you will always have 1 chunk per file.\n2. Split the file into chunks of a fixed length: starting at the top of the file, add lines in your current chunk until it reaches the limit, then start a new chunk.\n3. Use a recursive, abstract syntax tree (AST)-based strategy: this is the most exact, but most complex. In most cases you can achieve high quality results by using (1) or (2), but if you'd like to try this you can find a reference example in [our code chunker](https://github.com/continuedev/continue/blob/main/core/indexing/chunk/code.ts) or in [LlamaIndex](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/code/).\n\nAs usual in this guide, we recommend starting with the strategy that gives 80% of the benefit with 20% of the effort.\n\n## Step 4: How to Put Together an Indexing Script\n\nIndexing, in which we will insert your code into the vector database in a retrievable format, happens in three steps:\n\n1. Chunking\n2. Generating embeddings\n3. Inserting into the vector database\n\nWith LanceDB, we can do steps 2 and 3 simultaneously, as demonstrated [in their docs](https://lancedb.github.io/lancedb/basic/#using-the-embedding-api). If you are using Voyage AI for example, it would be configured like this:\n\n```\nfrom lancedb.pydantic import LanceModel, Vectorfrom lancedb.embeddings import get_registrydb = lancedb.connect(\"/tmp/db\")func = get_registry().get(\"openai\").create(    name=\"voyage-code-3\",    base_url=\"https://api.voyageai.com/v1/\",    api_key=os.environ[\"VOYAGE_API_KEY\"],)class CodeChunks(LanceModel):    filename: str    text: str = func.SourceField()    # 1024 is the default dimension for `voyage-code-3`: https://docs.voyageai.com/docs/embeddings#model-choices    vector: Vector(1024) = func.VectorField()table = db.create_table(\"code_chunks\", schema=CodeChunks, mode=\"overwrite\")table.add([    {\"text\": \"print('hello world!')\", filename: \"hello.py\"},    {\"text\": \"print('goodbye world!')\", filename: \"goodbye.py\"}])query = \"greetings\"actual = table.search(query).limit(1).to_pydantic(CodeChunks)[0]print(actual.text)\n```\n\n<Check>\n  If you are indexing more than one repository, it is best to store these in\n  separate \"tables\" (terminology used by LanceDB) or \"collections\" (terminology\n  used by some other vector DBs). The alternative of adding a \"repository\" field\n  and then filtering by this is less performant.\n</Check>\n\nRegardless of which database or model you have chosen, your script should iterate over all of the files that you wish to index, chunk them, generate embeddings for each chunk, and then insert all of the chunks into your vector database.\n\n## Step 5: How to Run Your Indexing Script\n\n<Check>\n  In a perfect production version, you would want to build \"automatic, incremental indexing\", so that you whenever a file changes, that file and nothing else is automatically re-indexed. This has the benefits of perfectly up-to-date embeddings and lower cost.\n\nThat said, we highly recommend first building and testing the pipeline before attempting this. Unless your codebase is being entirely rewritten frequently, an incremental refresh of the index is likely to be sufficient and reasonably cheap.\n\n</Check>\n\nAt this point, you've written your indexing script and tested that you can make queries from your vector database. Now, you'll want a plan for when to run the indexing script.\n\nIn the beginning, you should probably run it by hand. Once you are confident that your custom RAG is providing value and is ready for the long-term, then you can set up a cron job to run it periodically. Because codebases are largely unchanged in short time frames, you won't want to re-index more than once a day. Once per week or month is probably even sufficient.\n\n## Step 6: How to set up an MCP server\n\nTo integrate your custom RAG system with Continue, you'll create an MCP (Model Context Protocol) server. MCP provides a standardized way for AI tools to access external resources.\n\n### Create your MCP server\n\nHere's a reference implementation using Python that queries your vector database:\n\n```python\n\"\"\"Custom RAG MCP server for code retrieval\"\"\"\nimport asyncio\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import Tool, TextContent\nimport lancedb\n\n# Initialize your vector database connection\ndb = lancedb.connect(\"/path/to/your/db\")\ntable = db.open_table(\"code_chunks\")\n\napp = Server(\"custom-rag-server\")\n\n@app.tool()\nasync def search_codebase(query: str, limit: int = 10) -> list[TextContent]:\n    \"\"\"\n    Search the codebase using vector similarity.\n    \n    Args:\n        query: The search query\n        limit: Maximum number of results to return\n    \"\"\"\n    # Query your vector database\n    results = table.search(query).limit(limit).to_list()\n    \n    # Format results for Continue\n    formatted_results = []\n    for result in results:\n        formatted_results.append(TextContent(\n            type=\"text\",\n            text=f\"File: {result['filename']}\\n\\n{result['text']}\"\n        ))\n    \n    return formatted_results\n\n@app.tool()\nasync def get_file_context(filename: str) -> list[TextContent]:\n    \"\"\"\n    Get all chunks from a specific file.\n    \n    Args:\n        filename: The name of the file to retrieve\n    \"\"\"\n    results = table.where(f\"filename = '{filename}'\").to_list()\n    \n    return [TextContent(\n        type=\"text\",\n        text=\"\\n\".join([r['text'] for r in results])\n    )]\n\nif __name__ == \"__main__\":\n    stdio_server(app).run()\n```\n\n### Configure Continue to use your MCP server\n\nAdd your MCP server to Continue's configuration:\n\n**config.yaml:**\n```yaml\nmcpServers:\n  - name: custom-rag\n    command: python\n    args:\n      - /path/to/your/mcp_server.py\n    env:\n      VOYAGE_API_KEY: ${VOYAGE_API_KEY}\n```\n\n**config.json:**\n```json\n{\n  \"mcpServers\": [\n    {\n      \"name\": \"custom-rag\",\n      \"command\": \"python\",\n      \"args\": [\"/path/to/your/mcp_server.py\"],\n      \"env\": {\n        \"VOYAGE_API_KEY\": \"${VOYAGE_API_KEY}\"\n      }\n    }\n  ]\n}\n```\n\n## Step 7 (Bonus): How to Set Up Reranking\n\nIf you'd like to improve the quality of your results, a great first step is to add reranking. This involves retrieving a larger initial pool of results from the vector database, and then using a reranking model to order them from most to least relevant. This works because the reranking model can perform a slightly more expensive calculation on the small set of top results, and so can give a more accurate ordering than similarity search, which has to search over all entries in the database.\n\nIf you wish to return 10 total results for each query for example, then you would:\n\n1. Retrieve \\~50 results from the vector database using similarity search\n2. Send all of these 50 results to the reranker API along with the query in order to get relevancy scores for each\n3. Sort the results by relevancy score and return the top 10\n\nWe recommend using the `rerank-2` model from Voyage AI, which has examples of usage [here](https://docs.voyageai.com/docs/reranker).\n"}
{"source":"github","repo":"continue","path":"docs/guides/how-to-self-host-a-model.mdx","content":"---\ntitle: \"How to Self-Host a Model\"\ndescription: \"Learn how to deploy and self-host open-source language models using HuggingFace TGI, vLLM, SkyPilot, Anyscale Private Endpoints, or Lambda for use with Continue\"\n---\n\n- [HuggingFace TGI](https://github.com/continuedev/deploy-os-code-llm#tgi)\n- [vLLM](https://github.com/continuedev/deploy-os-code-llm#vllm)\n- [SkyPilot](https://github.com/continuedev/deploy-os-code-llm#skypilot)\n- [Anyscale Private Endpoints](https://github.com/continuedev/deploy-os-code-llm#anyscale-private-endpoints) (OpenAI compatible API)\n- [Lambda](https://github.com/continuedev/deploy-os-code-llm#lambda)\n\n## How to Self-Host an Open-Source Model\n\nFor many cases, either Continue will have a built-in provider or the API you use will be OpenAI-compatible, in which case you can use the \"openai\" provider and change the \"baseUrl\" to point to the server.\n\nHowever, if neither of these are the case, you will need to wire up a new LLM object.\n\n## How to Set Up Authentication\n\nBasic authentication can be done with any provider using the `apiKey` field:\n\n- YAML\n- JSON\n\nconfig.yaml\n\n```\nmodels:\n  - name: Ollama\n    provider: ollama\n    model: llama2-7b\n    apiKey: <YOUR_CUSTOM_OLLAMA_SERVER_API_KEY>\n```\n\nconfig.json\n\n```json\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama2-7b\",\n      \"apiKey\": \"<YOUR_CUSTOM_OLLAMA_SERVER_API_KEY>\"\n    }\n  ]\n}\n```\n\nThis translates to the header `\"Authorization\": \"Bearer xxx\"`.\n\nIf you need to send custom headers for authentication, you may use the `requestOptions.headers` property like in this example with Ollama:\n\n- YAML\n- JSON\n\nconfig.yaml\n\n```\nmodels:\n  - name: Ollama\n    provider: ollama\n    model: llama2-7b\n    requestOptions:\n      headers:\n        X-Auth-Token: xxx\n```\n\nconfig.json\n\n```json\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama2-7b\",\n      \"requestOptions\": { \"headers\": { \"X-Auth-Token\": \"xxx\" } }\n    }\n  ]\n}\n```\n\nSimilarly if your model requires a Certificate for authentication, you may use the `requestOptions.clientCertificate` property like in the example below:\n\n- YAML\n- JSON\n\nconfig.yaml\n\n```\nmodels:\n  - name: Ollama\n    provider: ollama\n    model: llama2-7b\n    requestOptions:\n      clientCertificate:\n        cert: C:\\tempollama.pem\n        key: C:\\tempollama.key\n        passphrase: c0nt!nu3\n```\n\nconfig.json\n\n```json\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama2-7b\",\n      \"requestOptions\": {\n        \"clientCertificate\": {\n          \"cert\": \"C:\\\\tempollama.pem\",\n          \"key\": \"C:\\\\tempollama.key\",\n          \"passphrase\": \"c0nt!nu3\"\n        }\n      }\n    }\n  ]\n}\n```\n"}
{"source":"github","repo":"continue","path":"docs/guides/ollama-guide.mdx","content":"---\ntitle: \"Using Ollama with Continue: A Developer's Guide\"\ndescription: \"Complete guide to setting up Ollama with Continue for local AI development. Learn installation, configuration, model selection, performance optimization, and troubleshooting for privacy-focused offline coding assistance\"\n---\n\n## What Are the Prerequisites for Using Ollama\n\nBefore getting started, ensure your system meets these requirements:\n\n- Operating System: macOS, Linux, or Windows\n- RAM: Minimum 8GB (16GB+ recommended)\n- Storage: At least 10GB free space\n- Continue extension installed\n\n## How to Install Ollama - Step-by-Step\n\n### Step 1: Install Ollama\n\nChoose the installation method for your operating system:\n\n```\n# macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows\n# Download from ollama.ai\n```\n\n### Step 2: Start Ollama Service\n\nAfter installation, start the Ollama service:\n\n```bash\n# Check Ollama version - verify it's installed\nollama --version\n\n# Start Ollama (runs in background)\nollama serve\n\n# Verify it's running\ncurl http://localhost:11434\n# Should return \"Ollama is running\"\n```\n\n### Step 3: Download Models\n\n<Warning>\n  **Important**: Always use `ollama pull` instead of `ollama run` to download\n  models. The `run` command starts an interactive session which isn't needed for\n  Continue.\n</Warning>\n\nDownload models using the exact tag specified:\n\n```bash\n# Pull models with specific tags\nollama pull deepseek-r1:32b       # 32B parameter version\nollama pull deepseek-r1:latest     # Latest/default version\nollama pull mistral:latest\nollama pull qwen2.5-coder:1.5b\n\n# List all downloaded models\nollama list\n```\n\n**Common Model Tags:**\n\n- `:latest` - Default version (used if no tag specified)\n- `:32b`, `:7b`, `:1.5b` - Parameter count versions\n- `:instruct`, `:base` - Model variants\n\n<Note>\n  If a model page shows `deepseek-r1:32b` on Ollama's website, you must pull it\n  with that exact tag. Using just `deepseek-r1` will pull `:latest` which may be\n  a different size.\n</Note>\n\n## How to Configure Ollama with Continue\n\nThere are multiple ways to configure Ollama models in Continue:\n\n### Method 1: Using Hub Model Blocks in Local config.yaml\n\nThe easiest way is to use [pre-configured model blocks](/reference#local-blocks) from the Continue Mission Control in your local configuration:\n\n```yaml title=\"~/.continue/configs/config.yaml\"\nname: My Local Config\nversion: 0.0.1\nschema: v1\nmodels:\n  - uses: ollama/deepseek-r1-32b\n  - uses: ollama/qwen2.5-coder-7b\n  - uses: ollama/gpt-oss-20b\n```\n\n<Warning>\n  **Important**: Blocks only provide configuration - you still need to pull\n  the model locally. The block `ollama/deepseek-r1-32b` configures Continue\n  to use `model: deepseek-r1:32b`, but the actual model must be installed:\n  ```bash\n  # Check what the block expects (view on hub.continue.dev)\n  # Then pull that exact model tag locally\n  ollama pull deepseek-r1:32b  # Required for ollama/deepseek-r1-32b hub block\n  ```\n  If the model isn't installed, Ollama will return:\n  `404 model \"deepseek-r1:32b\" not found, try pulling it first`\n</Warning>\n\n### Method 2: Using Autodetect\n\nContinue can automatically detect available Ollama models. You can configure this in your YAML:\n\n```yaml title=\"~/.continue/config.yaml\"\nmodels:\n  - name: Autodetect\n    provider: ollama\n    model: AUTODETECT\n    roles:\n      - chat\n      - edit\n      - apply\n      - rerank\n      - autocomplete\n```\n\nOr use it through the GUI:\n\n1. Click on the model selector dropdown\n2. Select \"Autodetect\" option\n3. Continue will scan for available Ollama models\n4. Select your desired model from the detected list\n\n<Note>\n  The Autodetect feature scans your local Ollama installation and lists all\n  available models. When set to `AUTODETECT`, Continue will dynamically populate\n  the model list based on what's installed locally via `ollama list`. This is\n  useful for quickly switching between models without manual configuration. For\n  any roles not covered by the detected models, you may need to manually\n  configure them.\n</Note>\n\nYou can update `apiBase` with the IP address of a remote machine serving Ollama.\n\n### Method 3: Manual Configuration\n\nFor custom configurations or models not in Mission Control:\n\n```yaml\nmodels:\n  - name: DeepSeek R1 32B\n    provider: ollama\n    model: deepseek-r1:32b # Must match exactly what `ollama list` shows\n    apiBase: http://localhost:11434\n    roles:\n      - chat\n      - edit\n    capabilities: # Add if not auto-detected\n      - tool_use\n  - name: Qwen2.5-Coder 1.5B\n    provider: ollama\n    model: qwen2.5-coder:1.5b\n    roles:\n      - autocomplete\n```\n\n### Model Capabilities and Tool Support\n\nSome Ollama models support tools (function calling) which is required for Agent mode. However, not all models that claim tool support work correctly:\n\n#### Checking Tool Support\n\n```yaml\nmodels:\n  - name: DeepSeek R1\n    provider: ollama\n    model: deepseek-r1:latest\n    capabilities:\n      - tool_use # Add this to enable tools\n```\n\n<Warning>\n  **Known Issue**: Some models like DeepSeek R1 may show \"Agent mode is not\n  supported\" or \"does not support tools\" even with capabilities configured. This\n  is a known limitation where the model's actual tool support differs from its\n  advertised capabilities.\n</Warning>\n\n#### If Agent Mode Shows \"Not Supported\"\n\n![agent not supported](/images/guides/images/agent-not-supported.png)\n\n1. First, add `capabilities: [tool_use]` to your model config\n2. If you still get errors, the model may not actually support tools despite documentation\n3. Use a different model known to work with tools (e.g., Llama 3.1, Mistral)\n4. Alternatively, you can turn on [System Message tools](/ide-extensions/agent/model-setup#how-system-message-tools-work)\n\nSee the [Model Capabilities guide](/customize/deep-dives/model-capabilities) for more details.\n\n### How to Configure Advanced Settings\n\nFor optimal performance, consider these advanced configuration options:\n\n```yaml\nmodels:\n  - name: Optimized DeepSeek\n    provider: ollama\n    model: deepseek-r1:32b\n    contextLength: 8192 # Adjust context window (default varies by model)\n    completionOptions:\n      temperature: 0.7 # Controls randomness (0.0-1.0)\n      top_p: 0.9 # Nucleus sampling threshold\n      top_k: 40 # Top-k sampling\n      num_predict: 2048 # Max tokens to generate\n    # Ollama-specific options (set via environment or modelfile)\n    # num_gpu: 35        # Number of GPU layers to offload\n    # num_thread: 8      # CPU threads to use\n```\n\nFor GPU acceleration and memory tuning, create an Ollama Modelfile:\n\n```\n# Create custom model with optimizations\nFROM deepseek-r1:32b\nPARAMETER num_gpu 35\nPARAMETER num_thread 8\nPARAMETER num_ctx 4096\n```\n\n## What Are the Best Practices for Ollama\n\n### How to Choose the Right Model\n\nChoose models based on your specific needs (see [recommended models](/customize/models#recommended-models) for more options):\n\n1. **Code Generation**:\n\n   - `qwen2.5-coder:7b` - Excellent for code completion\n   - `codellama:13b` - Strong general coding support\n   - `deepseek-coder:6.7b` - Fast and efficient\n\n2. **Chat & Reasoning**:\n\n   - `llama3.1:8b` - Latest Llama with tool support\n   - `mistral:7b` - Fast and versatile\n   - `deepseek-r1:32b` - Advanced reasoning capabilities\n\n3. **Autocomplete**:\n\n   - `qwen2.5-coder:1.5b` - Lightweight and fast\n   - `starcoder2:3b` - Optimized for code completion\n\n4. **Memory Requirements**:\n   - 1.5B-3B models: ~4GB RAM\n   - 7B models: ~8GB RAM\n   - 13B models: ~16GB RAM\n   - 32B models: ~32GB RAM\n\n### How to Optimize Performance\n\nTo get the best performance from Ollama:\n\n- Monitor system resources with `ollama ps` to see memory usage\n- Adjust context window size based on available RAM\n- Use appropriate model sizes for your hardware\n- Enable GPU acceleration when available (NVIDIA CUDA or AMD ROCm)\n- Use `ollama logs` to debug performance issues\n\n## How to Troubleshoot Ollama Issues\n\n### Common Configuration Problems\n\n#### \"404 model not found, try pulling it first\"\n\nThis error occurs when the model isn't installed locally:\n\n**Problem**: Using a hub block or config that references a model not yet pulled\n**Solution**:\n\n```bash\n# Check what models you have\nollama list\n\n# Pull the exact model version needed\nollama pull model-name:tag  # e.g., deepseek-r1:32b\n```\n\n#### Model Tag Mismatches\n\n**Problem**: `ollama pull deepseek-r1` installs `:latest` but hub block expects `:32b`\n**Solution**: Always pull with the exact tag:\n\n```bash\n# Wrong - pulls :latest\nollama pull deepseek-r1\n\n# Right - pulls specific version\nollama pull deepseek-r1:32b\n```\n\n#### \"Agent mode is not supported\"\n\n**Problem**: Model doesn't support tools/function calling\n**Solutions**:\n\n1. Add `capabilities: [tool_use]` to your model config\n2. If still not working, the model may not actually support tools\n3. Switch to a model with confirmed tool support (Llama 3.1, Mistral)\n\n#### Using Hub Blocks in Local Config\n\n**Problem**: Unclear how to use hub models locally\n**Solution**: Create a local agent file:\n\n```yaml\n# ~/.continue/configs/config.yaml\nname: Local Config\nversion: 0.0.1\nschema: v1\nmodels:\n  - uses: ollama/model-name\n```\n\n### How to Fix Connection Problems\n\n- Verify Ollama is running: `curl http://localhost:11434`\n- Check service status: `systemctl status ollama` (Linux)\n- Ensure port 11434 is not blocked by firewall\n- For remote connections, set `OLLAMA_HOST=0.0.0.0:11434`\n\n### How to Resolve Performance Issues\n\n- Insufficient RAM: Use smaller models (7B instead of 32B)\n- Model too large: Check available memory with `ollama ps`\n- GPU issues: Verify CUDA/ROCm installation for GPU acceleration\n- Slow generation: Adjust `num_gpu` layers in model configuration\n- Check system diagnostics: `ollama ps` for active models and memory usage\n\n## What Are Example Workflows with Ollama\n\n### How to Use Ollama for Code Generation\n\n```python\n# Example: Generate a FastAPI endpoint\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\n@app.post(\"/users/\")\nasync def create_user(user: User):\n    # Continue will help complete this implementation\n    # Use Cmd+I (Mac) or Ctrl+I (Windows/Linux) to generate code\n    pass\n```\n\n### How to Use Ollama for Code Review\n\nUse Continue with Ollama to:\n\n- Analyze code quality\n- Suggest improvements\n- Identify potential bugs\n- Generate documentation\n\n## Conclusion\n\nOllama with Continue provides a powerful local development environment for AI-assisted coding. You now have complete control over your AI models, ensuring privacy and enabling offline development workflows.\n\n---\n\n_This guide is based on Ollama v0.11.x and Continue v1.1.x. Please check for updates regularly._\n"}
{"source":"github","repo":"continue","path":"docs/guides/atlassian-mcp-continue-cookbook.mdx","content":"---\ntitle: \"Jira Issues and Confluence Pages with Atlassian MCP and Continue\"\ndescription: \"Use Continue and the Atlassian Rovo MCP to search, summarize, and manage Jira issues, Confluence pages, and Compass components with natural language prompts.\"\nsidebarTitle: \"Atlassian Workflows with Continue\"\n---\n\n<Card title=\"What You'll Build\" icon=\"atlassian\">\n  An Atlassian workflow assistant that uses Continue with the Atlassian Rovo MCP to:\n  - Search and summarize Jira issues across projects\n  - Find and digest Confluence documentation\n  - Create and update Jira issues with natural language\n  - Query Compass components and service dependencies\n  - Automate Atlassian workflows with headless CLI runs\n</Card>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Continue account with **Hub access**\n  - Read: [Understanding Configs ‚Äî How to get started with Hub configs](/guides/understanding-configs#how-to-get-started-with-hub-configs)\n- Node.js 18+ installed locally\n- An Atlassian Cloud site with Jira, Confluence, and/or Compass\n- Access to the Atlassian products you want to integrate with\n\n<Note>\n  The Atlassian Rovo MCP Server is currently in **Beta**. Core functionality is available, but some features are still under development.\n</Note>\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n  ```bash\n  npm i -g @continuedev/cli\n  ```\n  </Step>\n\n<Step title=\"Understand OAuth Flow\">\n  The Atlassian MCP uses **OAuth 2.1** for authentication. The first time you connect, you'll complete a browser-based authorization flow that respects your existing Atlassian permissions.\n  </Step>\n</Steps>\n\n<Warning>\n  To use agents in headless mode, you need a [Continue API key](https://hub.continue.dev/settings/api-keys).\n  All data access respects your existing Jira, Confluence, and Compass user permissions.\n</Warning>\n\n## Choose Your Atlassian Agent\n\n<Card title=\"üöÄ Two Specialized Agents\" icon=\"rocket\">\n  We've created two ready-to-use agents to help you get started fast - they come pre-configured with optimized prompts, rules, and the Atlassian MCP. You don't have to use these agents, but they're designed to give you the best experience right away. Choose the agent that matches your needs, or create your own custom agent.\n</Card>\n\n### Agent Quick Reference\n\n| Agent | Best For | Use Cases | Link |\n|-------|----------|-----------|------|\n| **Jira Agent** | Issue Management | Search issues, create stories, sprint planning, status updates, bulk operations | [View Agent](https://hub.continue.dev/continuedev/atlassian-continuous-ai-jira-agent) |\n| **Confluence Agent** | Documentation | Search pages, summarize docs, create/update pages, manage spaces | [View Agent](https://hub.continue.dev/continuedev/atlassian-continuous-ai-confluence-agent) |\n\n<Note>\n  **Cross-product workflows**: Both agents can work with Jira, Confluence, and Compass. Choose based on your primary focus area, or create your own agent using the [Atlassian MCP](https://hub.continue.dev/atlassian/atlassian-mcp). The Atlassian MCP can work with Jira, Compass, or Confluence.\n</Note>\n\n<Tabs>\n  <Tab title=\"üìã Jira Agent (Recommended for Issue Management)\">\n    <Steps>\n      <Step title=\"Use the Jira Agent\">\n        Visit the [Atlassian Continuous AI - Jira Agent](https://hub.continue.dev/continuedev/atlassian-continuous-ai-jira-agent) on Continue Mission Control. This agent is optimized for:\n        - Searching and filtering Jira issues\n        - Creating and updating issues\n        - Sprint planning and workload analysis\n        - Issue transitions and status updates\n\n        No installation needed - you can start using it immediately from the command line!\n      </Step>\n\n      <Step title=\"Complete OAuth Authorization\">\n        On first use, you'll be prompted to authorize the MCP server in your browser. This is a one-time setup that grants access based on your Atlassian permissions.\n      </Step>\n\n      <Step title=\"Run Your First Command\">\n        From anywhere in your terminal:\n        ```bash\n        cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Show me my open issues assigned to me\"\n        ```\n        The agent will connect to your Atlassian site and return results.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Pro tip**: Create a shell alias for the Jira agent:\n      ```bash\n      alias jira-ai='cn --agent continuedev/atlassian-continuous-ai-jira-agent'\n      ```\n      Then use: `jira-ai \"your prompt\"`\n    </Info>\n  </Tab>\n\n  <Tab title=\"üìÑ Confluence Agent (Recommended for Documentation)\">\n    <Steps>\n      <Step title=\"Use the Confluence Agent\">\n        Visit the [Atlassian Continuous AI - Confluence Agent](https://hub.continue.dev/continuedev/atlassian-continuous-ai-confluence-agent) on Continue Mission Control. This agent is optimized for:\n        - Searching and summarizing documentation\n        - Creating and updating Confluence pages\n        - Managing spaces and content\n        - Documentation discovery\n\n        No installation needed - you can start using it immediately from the command line!\n      </Step>\n\n      <Step title=\"Complete OAuth Authorization\">\n        On first use, you'll be prompted to authorize the MCP server in your browser. This is a one-time setup that grants access based on your Atlassian permissions.\n      </Step>\n\n      <Step title=\"Run Your First Command\">\n        From anywhere in your terminal:\n        ```bash\n        cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"Summarize the Q2 planning page\"\n        ```\n        The agent will connect to your Atlassian site and return results.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Pro tip**: Create a shell alias for the Confluence agent:\n      ```bash\n      alias confluence-ai='cn --agent continuedev/atlassian-continuous-ai-confluence-agent'\n      ```\n      Then use: `confluence-ai \"your prompt\"`\n    </Info>\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n    <Steps>\n      <Step title=\"Create a New Agent on Continue Mission Control\">\n        Go to the [Continue Mission Control](https://hub.continue.dev) and [create a new agent](https://hub.continue.dev/new?type=agent).\n      </Step>\n\n      <Step title=\"Add Atlassian MCP\">\n        Install from Hub (recommended) or add YAML manually. Minimal YAML example:\n\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        mcpServers:\n          - name: Atlassian Rovo MCP\n            type: sse\n            url: https://mcp.atlassian.com/v1/sse\n            connectionTimeout: 30\n        ```\n\n        Notes:\n        - The Atlassian MCP uses **Server-Sent Events (SSE)** transport type\n        - OAuth is handled automatically on first connection\n        - No API keys or tokens needed in the config\n      </Step>\n\n      <Step title=\"Test the Connection\">\n        Launch Continue and ask:\n        ```\n        List my Jira projects\n        ```\n        You'll be prompted to authorize in your browser if this is your first time.\n      </Step>\n    </Steps>\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use Atlassian MCP with Continue CLI, you need either:\n      - **Continue CLI Pro Plan** with the models add-on, OR\n      - **Your own API keys** added to Continue Mission Control secrets\n\n    The agent will automatically detect and use your configuration along with the Atlassian MCP for Jira, Confluence, and Compass operations.\n\n</Accordion>\n\n---\n\n## Jira Workflows\n\n<Note>\n  **Using the Jira Agent**: All examples below use the [Jira Agent](https://hub.continue.dev/continuedev/atlassian-continuous-ai-jira-agent) which is optimized for issue management.\n</Note>\n\nUse natural language to explore, triage, and manage Jira issues. The agent calls Atlassian MCP tools under the hood.\n\n<Info>\n  **Running in headless mode**: Add `-p` flag before your prompt and `--auto` flag at the end:\n  ```bash\n  cn --agent continuedev/atlassian-continuous-ai-jira-agent -p \"your prompt\" --auto\n  ```\n  **Important**: Complete browser OAuth authentication first before using headless mode.\n</Info>\n\n### Search and Discovery\n\n<AccordionGroup>\n  <Accordion title=\"Find Open Bugs\" icon=\"bug\">\n    Search for bugs across your projects.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Find all open bugs in Project Alpha\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"My Assigned Issues\" icon=\"user\">\n    Get a list of issues assigned to you.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Show me all issues assigned to me that are in progress\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Sprint Planning\" icon=\"calendar\">\n    Analyze sprint workload and priorities.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"List all issues in the current sprint. Group by assignee and priority. Summarize the workload distribution and flag any overloaded team members.\"\n    ```\n  </Accordion>\n</AccordionGroup>\n\n### Create and Update\n\n<AccordionGroup>\n  <Accordion title=\"Create Story from Description\" icon=\"plus\">\n    Turn natural language into a Jira story.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Create a story titled 'Redesign onboarding flow' in Project Alpha. Description: We need to simplify the user registration process by reducing steps from 5 to 3. Target completion: Q3.\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Bulk Issue Creation\" icon=\"list\">\n    Create multiple issues from meeting notes or specs.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Create five Jira issues from these meeting notes: 1) Fix login timeout bug 2) Update API documentation 3) Implement dark mode toggle 4) Review security audit findings 5) Optimize database queries\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Update Issue Status\" icon=\"arrows-rotate\">\n    Transition issues with natural language.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Move PROJ-123 to In Progress and add a comment: Starting work on this today. ETA: end of week.\"\n    ```\n  </Accordion>\n</AccordionGroup>\n\n---\n\n## Confluence Workflows\n\n<Note>\n  **Using the Confluence Agent**: All examples below use the [Confluence Agent](https://hub.continue.dev/continuedev/atlassian-continuous-ai-confluence-agent) which is optimized for documentation management.\n</Note>\n\nAccess, search, and manage your team's documentation directly from Continue.\n\n<Info>\n  **Running in headless mode**: Add `-p` flag before your prompt and `--auto` flag at the end:\n  ```bash\n  cn --agent continuedev/atlassian-continuous-ai-confluence-agent -p \"your prompt\" --auto\n  ```\n  **Important**: Complete browser OAuth authentication first before using headless mode.\n</Info>\n\n### Search and Summarize\n\n<AccordionGroup>\n  <Accordion title=\"Summarize Documentation\" icon=\"book\">\n    Get a quick summary of a Confluence page.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"Summarize the Q2 planning page from the Engineering space\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Find Documentation\" icon=\"magnifying-glass\">\n    Search for specific information across Confluence.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"Find all pages about API authentication in our developer docs\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"List Available Spaces\" icon=\"folder\">\n    Discover what Confluence spaces you have access to.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"What Confluence spaces do I have access to?\"\n    ```\n  </Accordion>\n</AccordionGroup>\n\n### Create and Update\n\n<AccordionGroup>\n  <Accordion title=\"Create Documentation Page\" icon=\"file-plus\">\n    Generate a new Confluence page with structured content.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"Create a page titled 'Team Goals Q3' in the Engineering space. Include sections for: Objectives, Key Results, and Timeline.\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Update Existing Page\" icon=\"pen-to-square\">\n    Modify content on an existing page.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"Update the 'Onboarding Guide' page to add a new section about our code review process\"\n    ```\n  </Accordion>\n</AccordionGroup>\n\n---\n\n## Compass Workflows\n\n<Note>\n  **Using Either Agent**: Compass workflows work with both agents. Use the Jira agent for component-to-issue workflows, or the Confluence agent for component-to-docs workflows.\n</Note>\n\nQuery and manage your service architecture with Compass integration.\n\n<Info>\n  **Running in headless mode**: Add `-p` flag before your prompt and `--auto` flag at the end. Complete browser OAuth authentication first before using headless mode.\n</Info>\n\n### Service Discovery\n\n<AccordionGroup>\n  <Accordion title=\"Query Dependencies\" icon=\"diagram-project\">\n    Understand service relationships.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"What services depend on the api-gateway component?\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Create Service Component\" icon=\"server\">\n    Register a new service in Compass.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Create a service component for the current repository. Use the package.json to infer details.\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Bulk Import Components\" icon=\"upload\">\n    Import multiple components from structured data.\n\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Import Compass components and custom fields from this CSV: [paste CSV data]\"\n    ```\n\n    Or reference a file:\n    ```bash\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Import Compass components from services.csv in the current directory\"\n    ```\n  </Accordion>\n</AccordionGroup>\n\n---\n\n## Combined Workflows\n\n<Note>\n  **Cross-Product Workflows**: These workflows span multiple Atlassian products. Choose the agent based on your primary focus:\n  - Use **Jira Agent** when the workflow is issue-centric\n  - Use **Confluence Agent** when the workflow is documentation-centric\n</Note>\n\nIntegrate actions across Jira, Confluence, and Compass for powerful cross-product workflows.\n\n<Info>\n  **Running in headless mode**: Add `-p` flag before your prompt and `--auto` flag at the end. Complete browser OAuth authentication first before using headless mode.\n</Info>\n\n<AccordionGroup>\n  <Accordion title=\"Link Jira to Confluence\" icon=\"link\">\n    Connect related content across products.\n\n    ```bash\n    # Using Jira Agent for issue-centric workflow\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Link Jira tickets PROJ-123, PROJ-456, and PROJ-789 to the 'Sprint 23 Release Plan' Confluence page\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Service Documentation Lookup\" icon=\"search\">\n    Find documentation for a specific component.\n\n    ```bash\n    # Using Confluence Agent for docs-centric workflow\n    cn --agent continuedev/atlassian-continuous-ai-confluence-agent \"Fetch the Confluence documentation page linked to the user-authentication Compass component\"\n    ```\n  </Accordion>\n\n  <Accordion title=\"Release Notes Generation\" icon=\"file-lines\">\n    Create release documentation from completed work.\n\n    ```bash\n    # Using Jira Agent to pull issue data and create in Confluence\n    cn --agent continuedev/atlassian-continuous-ai-jira-agent \"Generate release notes for all Jira issues completed in Sprint 23. Create a Confluence page in the Release Notes space with: New features (grouped by epic), Bug fixes, Known issues, Deployment instructions\"\n    ```\n  </Accordion>\n</AccordionGroup>\n\n---\n\n## Troubleshooting\n\n<AccordionGroup>\n  <Accordion title=\"OAuth Authorization Issues\">\n    - **\"Your site admin must authorize this app\"**: A site admin needs to complete the OAuth flow first before other users can connect\n    - **Can't complete browser authorization**: Ensure you're logged into the correct Atlassian Cloud site and have necessary permissions\n    - **Connection timeout**: Increase `connectionTimeout` in your MCP config or check your network connection\n  </Accordion>\n\n  <Accordion title=\"Permission Errors\">\n    - **\"Access denied\" or \"Forbidden\"**: The MCP respects your existing Atlassian permissions. Verify you have access to the project/space/component you're trying to access\n    - **Can't create issues**: Ensure you have \"Create issues\" permission in the target Jira project\n    - **Can't edit Confluence pages**: Verify you have edit permissions in the target Confluence space\n  </Accordion>\n\n  <Accordion title=\"Rate Limiting\">\n    - **Standard plan**: Moderate usage thresholds\n    - **Premium/Enterprise plans**: Higher quotas (1,000 requests/hour plus per-user limits)\n    - If you hit rate limits, reduce query frequency or batch operations\n  </Accordion>\n\n  <Accordion title=\"Beta Limitations\">\n    - Bulk operations may be constrained by rate limits\n    - Custom Jira fields may not be recognized without explicit setup\n    - Workspace/site switching not available within a single session\n    - Check [Atlassian Community](https://community.atlassian.com/) for known issues\n  </Accordion>\n</AccordionGroup>\n\n### Admin Considerations\n\n<Card title=\"For Atlassian Site Admins\" icon=\"shield\">\n  **Installation**: The Atlassian Rovo MCP is automatically installed on first OAuth authorization (JIT installation) - no Marketplace installation needed.\n\n  **Managing Access**:\n  - View connected apps in [Admin Hub](https://admin.atlassian.com)\n  - Users can revoke access from their [profile settings](https://id.atlassian.com/manage-profile/apps)\n  - Block user-installed apps via \"user-installed apps\" control in Admin Hub\n\n  **Security**: All traffic is encrypted via HTTPS (TLS 1.2+), and access respects existing user permissions.\n</Card>\n\n---\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered Atlassian workflow system** that:\n\n- ‚úÖ Uses natural language ‚Äî Simple prompts for complex Atlassian operations\n- ‚úÖ Spans multiple products ‚Äî Seamlessly works across Jira, Confluence, and Compass\n- ‚úÖ Respects permissions ‚Äî Secure OAuth-based access with existing role enforcement\n- ‚úÖ Runs headlessly ‚Äî Integrate into CI/CD pipelines and automation scripts\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your Atlassian workflows now operate at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine project management tasks with human oversight through\n  review and approval.\n</Card>\n\n## Next Steps\n\n1. **Explore your projects** - Try the Jira search and triage prompts\n2. **Organize documentation** - Use Confluence workflows to summarize and create pages\n3. **Map your services** - Query Compass for service dependencies\n4. **Customize prompts** - Tailor workflows to your team's specific needs\n5. **Monitor usage** - Track how AI improves your Atlassian workflows\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"Atlassian Rovo MCP\" icon=\"link\" href=\"https://support.atlassian.com/atlassian-rovo-mcp-server/docs/getting-started-with-the-atlassian-remote-mcp-server/\">\n    Official Atlassian MCP documentation\n  </Card>\n  <Card title=\"MCP in Continue\" icon=\"book\" href=\"/customize/deep-dives/mcp\">\n    How MCP works with Continue agents\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"globe\" href=\"https://hub.continue.dev\">\n    Create and manage your agents\n  </Card>\n  <Card title=\"Atlassian Community\" icon=\"users\" href=\"https://community.atlassian.com/\">\n    Get help and share feedback\n  </Card>\n</CardGroup>\n\n## Example Use Cases\n\n<CardGroup cols={2}>\n  <Card title=\"Release Management\" icon=\"rocket\">\n    Automate release notes generation, link issues to documentation, and track deployment status across Jira and Confluence.\n  </Card>\n  <Card title=\"Sprint Planning\" icon=\"calendar-days\">\n    Analyze workload distribution, identify blockers, and generate sprint summaries automatically.\n  </Card>\n  <Card title=\"Service Ownership\" icon=\"sitemap\">\n    Map service dependencies in Compass, link to technical documentation in Confluence, and track related issues in Jira.\n  </Card>\n  <Card title=\"Onboarding Automation\" icon=\"user-plus\">\n    Create standardized Jira onboarding tickets, link to Confluence guides, and track new hire progress.\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/guides/plan-mode-guide.mdx","content":"---\ntitle: \"Using Plan Mode with Continue\"\ndescription: \"Plan Mode gives you a safe, read-only environment to explore your codebase, map out solutions, and collaborate with AI before making any changes. Think of it as your sandbox for understanding and strategy.\"\n---\n\nPlan Mode prevents unintentional file modifications, letting you think through solutions and build confidence before committing to action. The core principle is understand before you change, giving you a research-first approach that prevents costly mistakes and creates better architectural decisions.\n\n<Accordion title=\"Learn how to choose the right mode\" icon=\"lightbulb\">\n     <Columns cols={3}>\n  <Card title=\"Chat Mode\" icon=\"comments\">\n      *Learn and discuss without changing code.*\n\n    **Mental Model:** Talking to a knowledgeable colleague\n    **Best For:** Explaining concepts, comparing approaches, code review discussions.\n\n  </Card>\n\n  <Card title=\"Plan Mode\" icon=\"binoculars\">\n      *Safely explore and plan with read-only tools.*\n\n    **Mental Model:** Architect surveying before renovation\n    **Best For:** Understanding a codebase, bug investigation, planning implementations.\n\n  </Card>\n\n  <Card title=\"Agent Mode\" icon=\"hammer\">\n  *Make actual changes with full tool access.*\n\n    **Mental Model:** Contractor executing approved blueprints\n    **Best For:** Implementing features, fixing bugs, running tests and commands.\n\n  </Card>\n</Columns>\n  </Accordion>\n\n## What Are the Use Cases for Plan Mode\n\nPlan Mode excels in four key scenarios where understanding before acting prevents expensive mistakes:\n\n<Columns cols={2}>\n  <Card title=\"Codebase Exploration\" icon=\"map\">\n    Navigate unfamiliar systems and trace data flows without modification risk.\n    \n    [See exploration prompts ‚Üí](#prompt-library)\n  </Card>\n\n  <Card title=\"Implementation Planning\" icon=\"clipboard-list\">\n    Map dependencies and sequence complex changes before execution. <br />\n\n[See planning prompts ‚Üí](#prompt-library)\n\n  </Card>\n\n  <Card title=\"Issue Investigation\" icon=\"magnifying-glass\">\n    Debug systematically by tracing execution paths and analyzing root causes.{\" \"}\n\n[See debugging prompts ‚Üí](#prompt-library)\n\n  </Card>\n\n  <Card title=\"Architecture Analysis\" icon=\"building\">\n    Assess system health, identify bottlenecks, and plan improvements.\n    \n    [See analysis prompts ‚Üí](#prompt-library)\n  </Card>\n</Columns>\n\n## What Are the Best Practices for Plan Mode?\n\n### How to Plan Faster\n\n1. **Scope your requests**: `Focus analysis on the user authentication module only, ignore the admin features`\n2. **Use targeted context**: `Analyze @Files and its direct imports for security issues`\n\n### How to Create Higher Quality Plans\n\n1. **Provide business context**: `This feature needs to handle Black Friday traffic (10x normal load). Plan accordingly.`\n2. **Share technical constraints**: `We're on AWS with strict security requirements. Plan a file upload system that meets SOC2 compliance.`\n3. **Ask for risk analysis**: `What could go wrong with this database migration? Plan rollback procedures.`\n4. **Request multiple perspectives**: `Show me 3 different approaches to implementing caching in this API, with pros and cons for each.`\n\n## How to Enable Plan Mode\n\nYou can switch to `Plan` in the mode selector below the chat input box.\n\n![How to select plan mode](../images/plan-mode-selector.png)\n\n## What Tools Are Available in Plan Mode?\n\n| Tool                              | Available | Description                                          |\n| :-------------------------------- | :-------- | :--------------------------------------------------- |\n| **File/directory reading**        | ‚úÖ        | Browse and read any file or folder in your workspace |\n| **Grep/glob search**              | ‚úÖ        | Search for patterns across files and directories     |\n| **Repository structure analysis** | ‚úÖ        | Understand codebase organization and dependencies    |\n| **Git history/diffs**             | ‚úÖ        | Review commits, branches, and changes                |\n| **Web content fetching**          | ‚úÖ        | Access external documentation and resources          |\n| **External API access**           | ‚úÖ        | Read-only calls to external services                 |\n| **MCP tools**                     | ‚úÖ        | Model Context Protocol integrations                  |\n| **Database schema examination**   | ‚úÖ        | Analyze database structure (read-only)               |\n| **File creation/editing**         | ‚ùå        | Creating or modifying files                          |\n| **Terminal command execution**    | ‚ùå        | Running shell commands                               |\n| **System modifications**          | ‚ùå        | Changing system settings or configurations           |\n| **Package installation**          | ‚ùå        | Installing dependencies or packages                  |\n| **Git commits/pushes**            | ‚ùå        | Making changes to version control                    |\n| **Database modifications**        | ‚ùå        | Altering database data or structure                  |\n\n## How Context Integration Works in Plan Mode\n\nContext is the foundation of effective planning. Without proper context, AI models fall back on generic patterns, leading to plans that don't fit your specific system. Continue's [context system](/ide-extensions/chat/context-selection) transforms broad suggestions into actionable strategies:\n\n| Context Type         | Usage                                                  | Best For                      |\n| :------------------- | :----------------------------------------------------- | :---------------------------- |\n| **Highlighted Code** | `cmd/ctrl + L` (VS Code) or `cmd/ctrl + J` (JetBrains) | Component-specific analysis   |\n| **Active File**      | `opt/alt + enter` when sending request                 | Current file context          |\n| **@Files**           | `@Files package.json tsconfig.json`                    | Specific file analysis        |\n| **@Terminal**        | `@Terminal`                                            | Debugging with output         |\n| **@Git Diff**        | `@Git Diff`                                            | Change impact analysis        |\n\n<Accordion title=\"Advanced Context Examples\">\n<AccordionGroup>\n  <Accordion title=\"Component Analysis\">\n    **After highlighting a React component**:\n    `Analyze this component for performance bottlenecks and plan optimization strategies.`\n  </Accordion>\n\n<Accordion title=\"Configuration Review\">\n  `@Files docker-compose.yml Dockerfile - Review our containerization setup and\n  suggest improvements for production deployment.`\n</Accordion>\n\n<Accordion title=\"API Design Review\">\n  `@Folder src/api - Evaluate our REST API design patterns and identify\n  opportunities for better consistency.`\n</Accordion>\n\n  <Accordion title=\"Cross-Platform Migration\">\n    `@Codebase - Plan a strategy to migrate this web app to also support mobile using React Native.`\n  </Accordion>\n</AccordionGroup>\n</Accordion>\n\n## Prompt Library\n\n### Codebase Exploration\n\n<AccordionGroup>\n  <Accordion title=\"Understanding Systems\">\n    **Data Flow Analysis**:\n    `Help me understand how state management works in this Redux app. Map out the data flow from actions to components.`\n\n    **Microservices Communication**:\n    `Analyze how our microservices communicate and identify potential bottlenecks in service-to-service calls.`\n\n    **Database Schema Review**:\n    `Examine the database schema and relationships to understand how user data is structured and accessed.`\n\n  </Accordion>\n\n  <Accordion title=\"Legacy Code Investigation\">\n    **System Mapping**:\n    `Map out this legacy codebase architecture. What are the main components and how do they communicate?`\n\n    **Technical Debt Assessment**:\n    `Analyze this codebase for technical debt hotspots and maintenance pain points.`\n\n    **Modernization Planning**:\n    `Assess this legacy system and recommend modernization opportunities that provide the highest impact with lowest risk.`\n\n  </Accordion>\n</AccordionGroup>\n\n### Implementation Planning\n\n<AccordionGroup>\n  <Accordion title=\"Migrations & Upgrades\">\n    **Cloud Migration**:\n    `Plan a migration from on-premise servers to AWS, considering our current Node.js application architecture.`\n\n    **Package Manager Migration**:\n    `Create a plan to migrate from npm to pnpm, analyzing dependencies and potential breaking changes.`\n\n    **Monorepo Conversion**:\n    `Plan how to convert our multiple repositories into a single monorepo using Nx or Lerna.`\n\n  </Accordion>\n\n  <Accordion title=\"Refactoring & Integration\">\n    **Service Extraction**:\n    `Plan a refactor to extract shared authentication logic into a reusable service across these microservices.`\n\n    **Payment Integration**:\n    `We need to add Stripe payments to this e-commerce app. Plan the integration considering our current architecture.`\n\n    **API Migration**:\n    `@Codebase - Plan a migration from REST to GraphQL across our entire API.`\n\n  </Accordion>\n</AccordionGroup>\n\n### Issue Investigation\n\n<AccordionGroup>\n  <Accordion title=\"Performance Problems\">\n    **Dashboard Performance**:\n    `Users report slow page loads on the dashboard. Investigate performance bottlenecks in the @Files and related components.`\n\n    **API Optimization**:\n    `Plan a performance optimization strategy for this Node.js API, focusing on database queries and caching.`\n\n    **Bundle Analysis**:\n    `Analyze this React app for performance bottlenecks. What are the most expensive operations?`\n\n  </Accordion>\n\n  <Accordion title=\"Bug Investigation\">\n    **Race Condition Analysis**:\n    `Users occasionally see stale data in the UI. Investigate potential race conditions in our async data fetching.`\n\n    **Production Incidents**:\n    `@Git Diff We're seeing 500 errors in production. Analyze recent changes and identify what might be causing the issue.`\n\n    **Deployment Issues**:\n    `Our staging environment works fine, but production deployments fail. Analyze environment differences.`\n\n  </Accordion>\n</AccordionGroup>\n\n### Architecture Analysis\n\n<AccordionGroup>\n  <Accordion title=\"Scalability & Performance\">\n    **Traffic Planning**:\n    `Analyze this @codebase architecture and identify potential bottlenecks for handling 10x more traffic.`\n\n    **CDN Implementation**:\n    `Plan a CDN strategy for our global user base, considering asset optimization and edge caching.`\n\n    **Caching Strategy**:\n    `Show me 3 different approaches to implementing caching in this API, with pros and cons for each.`\n\n  </Accordion>\n\n  <Accordion title=\"Security & Compliance\">\n    **Security Audit**:\n    `Audit this API for security vulnerabilities, focusing on authentication, authorization, and data validation.`\n\n    **Threat Modeling**:\n    `Model potential security threats for this user registration flow and plan mitigation strategies.`\n\n    **Compliance Planning**:\n    `Plan GDPR compliance implementation for this user data handling system.`\n\n  </Accordion>\n</AccordionGroup>\n\n## How to Transition From Plan to Execution\n\n### When to Transition to Agent Mode\n\nMove to Agent Mode when you have:\n\n‚úÖ **Clear understanding** of the current system  \n‚úÖ **Detailed implementation plan** with specific steps  \n‚úÖ **Risk assessment** and mitigation strategies  \n‚úÖ **Team approval** (if required)  \n‚úÖ **Success criteria** defined\n\n### Key Takeaways\n\nThe three-mode system‚ÄîChat mode for learning, Plan mode for strategy, and Agent mode for execution‚Äîprovides a complete development workflow that scales from simple bug fixes to complex system architecture.\n\n**Remember:**\n\n- Choose the right mode for each task\n- Start broad, then focus your planning sessions for better results\n- Transition to Agent mode with clear execution steps\n\nThe best code is planned code.\n"}
{"source":"github","repo":"continue","path":"docs/guides/overview.mdx","content":"---\ntitle: \"How to Use Continue Guides\"\ndescription: \"Comprehensive collection of practical guides for Continue including model setup, local development with Ollama, offline usage, self-hosting, custom context providers, and advanced customization tutorials\"\n---\n\n## What Model & Setup Guides Are Available\n\n- [Using Ollama with Continue](/guides/ollama-guide) - Local AI development with Ollama\n- [How to Self-Host a Model](/guides/how-to-self-host-a-model) - Self-hosting AI models\n- [Running Continue Without Internet](/guides/running-continue-without-internet) - Offline development setup\n\n## Continuous AI\n\n- [Continuous AI: A Developer's Guide](/guides/continuous-ai) - Integrating AI into development workflows\n- [How to Use Continue CLI (cn)](/guides/cli) - Command-line interface for Continue\n- [Continuous AI Readiness Assessment](/guides/continuous-ai-readiness-assessment) - Evaluate team readiness for Continuous AI adoption\n- [Notion + Continue Guide](/guides/notion-continue-guide) - Automate docs, tasks, and release workflows\n- [Pull Request Review Bot with GitHub Actions](/guides/github-pr-review-bot) - Set up automated, privacy-first code reviews using Continue CLI\n\n## MCP Integration Cookbooks\n\nStep-by-step guides for integrating Model Context Protocol (MCP) servers with Continue:\n\n<CardGroup cols={3}>\n\n  <Card title=\"Continue Docs MCP Cookbook\" icon=\"book\" href=\"/guides/continue-docs-mcp-cookbook\">\n    Use the Continue Docs MCP to write cookbooks, guides, and documentation with AI-powered workflows\n  </Card>\n\n  <Card title=\"GitHub MCP Cookbook\" icon=\"github\" href=\"/guides/github-mcp-continue-cookbook\">\n    Use GitHub MCP to list, filter, and summarize open issues and merged PRs, and post AI-generated comments\n  </Card>\n\n  <Card title=\"Atlassian MCP Cookbook\" icon=\"atlassian\" href=\"/guides/atlassian-mcp-continue-cookbook\">\n    Use Atlassian Rovo MCP to search and manage Jira issues, Confluence pages, and Compass components with natural language\n  </Card>\n\n  <Card title=\"PostHog Session Analysis Cookbook\" icon=\"chart-line\" href=\"/guides/posthog-github-continuous-ai\">\n    Analyze user behavior data to optimize your codebase with automatic issue creation\n  </Card>\n\n  <Card title=\"Netlify Performance Optimization Cookbook\" icon=\"globe\" href=\"/guides/netlify-mcp-continuous-deployment\">\n    Optimize web performance with A/B testing and automated monitoring using Netlify MCP\n  </Card>\n\n  <Card title=\"Chrome DevTools Performance Cookbook\" icon=\"gauge\" href=\"/guides/chrome-devtools-mcp-performance\">\n    Measure and optimize web performance with automated traces, Core Web Vitals monitoring, and performance budgets\n  </Card>\n\n  <Card title=\"Sanity CMS Integration Cookbook\" icon=\"database\" href=\"/guides/sanity-mcp-continue-cookbook\">\n    Manage headless CMS content with AI-powered workflows using Sanity MCP\n  </Card>\n\n  <Card title=\"Sentry Error Monitoring Cookbook\" icon=\"bug\" href=\"/guides/sentry-mcp-error-monitoring\">\n    Automated error analysis with Sentry MCP to identify patterns and create actionable GitHub issues\n  </Card>\n\n  <Card title=\"Snyk + Continue Mission Control Agent Cookbook (MCP)\" icon=\"shield\" href=\"/guides/snyk-mcp-continue-cookbook\">\n    Integrate Snyk MCP via Continue Mission Control to scan code, deps, IaC, and containers\n  </Card>\n\n  <Card title=\"Supabase Database Workflow Cookbook\" icon=\"database\" href=\"/guides/supabase-mcp-database-workflow\">\n    Audit Row Level Security in your Supabase database, identify vulnerabilities, and automatically generate fixes using Supabase MCP\n  </Card>\n\n  <Card title=\"dlt Data Pipelines Cookbook\" icon=\"pipe\" href=\"/guides/dlt-mcp-continue-cookbook\">\n    Build AI-powered data pipelines with dlt MCP for pipeline inspection, schema management, and debugging\n  </Card>\n\n  <Card title=\"Klavis AI Cookbook\" icon=\"K\" href=\"/guides/klavis-mcp-continue-cookbook\">\n    Use Continue and Klavis AI's Strata MCP to automate communication workflows across Slack and Gmail\n  </Card>\n  \n</CardGroup>\n\n## What Advanced Tutorials Are Available\n\n- [Codebase and Documentation Awareness](/guides/codebase-documentation-awareness) - Make agent mode aware of codebases and documentation\n- [Custom Code RAG](/guides/custom-code-rag) - Advanced: Build custom retrieval-augmented generation for large codebases\n\n## How to Contribute to Guides\n\nHave a guide idea or found an issue? We welcome contributions! Check our [GitHub repository](https://github.com/continuedev/continue) to get involved.\n"}
{"source":"github","repo":"continue","path":"docs/guides/running-continue-without-internet.mdx","content":"---\ntitle: \"How to Run Continue Without Internet\"\ndescription: \"Learn how to set up Continue for air-gapped or offline environments using local models, including steps to disable telemetry and configure local model providers\"\n---\n\n1. Download the latest .vsix file from the [GitHub Releases page](https://github.com/continuedev/continue/releases) and [install it to VS Code](https://code.visualstudio.com/docs/editor/extension-marketplace#_install-from-a-vsix).\n2. Turn off \"Allow Anonymous Telemetry\" in the user settings. This will stop Continue from attempting requests to PostHog for [anonymous telemetry](https://docs.continue.dev/reference/telemetry).\n3. In your `config.yaml` file (or through the Continue UI), set the default model to a local model. You can find available local model options [here](https://docs.continue.dev/reference/model-providers/ollama).\n4. Restart VS Code to ensure that the changes to `config.yaml` take effect.\n"}
{"source":"github","repo":"continue","path":"docs/guides/chrome-devtools-mcp-performance.mdx","content":"---\ntitle: \"Chrome DevTools Performance Optimization Cookbook\"\ndescription: \"Measure and optimize web performance with Chrome DevTools MCP, automated performance traces, and Core Web Vitals monitoring using Continue.\"\nsidebarTitle: \"Chrome DevTools MCP for Performance\"\n---\n\n<Card title=\"Debug Performance Like a Pro\" icon=\"gauge-high\">\n  Use AI to automatically trace performance, analyze Core Web Vitals, diagnose bottlenecks, and get actionable optimization suggestions directly from Chrome DevTools\n</Card>\n\n<Info>\n  **Did You Know?** Chrome DevTools MCP brings the full power of browser debugging to your AI workflow:\n  - [Performance Traces](https://developer.chrome.com/docs/devtools/performance) with automated analysis\n  - [Network Request Monitoring](https://developer.chrome.com/docs/devtools/network) for bottleneck detection\n  - [Console Debugging](https://developer.chrome.com/docs/devtools/console) with error pattern recognition\n  - [CPU & Network Throttling](https://developer.chrome.com/docs/devtools/performance/reference#throttling) to simulate real-world conditions\n  - [Performance Insights](https://developer.chrome.com/docs/devtools/performance-insights) with AI-powered analysis\n  - [Screenshot Debugging](https://developer.chrome.com/docs/devtools/device-mode) for visual regression detection\n\nThis guide shows you how to leverage these features through natural language with Continue CLI!\n\n</Info>\n\n## What You'll Learn\n\nThis cookbook teaches you to:\n\n- Run automated [performance traces](https://developer.chrome.com/docs/devtools/performance) to capture runtime metrics\n- Analyze [Core Web Vitals](https://web.dev/vitals/) (LCP, FID, CLS, INP) and performance insights\n- Diagnose performance bottlenecks using network and console analysis\n- Test performance under different network and CPU conditions with [throttling](https://developer.chrome.com/docs/devtools/performance/reference#throttling)\n- Automate visual regression testing with [screenshots](https://developer.chrome.com/docs/devtools/device-mode)\n\n## Prerequisites\n\n- Chrome browser installed\n- Web project with a running development server (or deployed URL)\n- Node.js 20+ installed\n- [Continue CLI](https://docs.continue.dev/guides/cli) (`npm i -g @continuedev/cli`)\n- [Chrome DevTools MCP](https://hub.continue.dev) configured\n\n## Quick Setup\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Step>\n</Steps>\n\n## Chrome DevTools MCP Workflow Options\n\n<Card title=\"Fastest Path to Success\" icon=\"rocket\">\n  Skip the manual setup and use our pre-built Chrome DevTools agent that includes\n  optimized prompts, rules, and the Chrome DevTools MCP for more consistent results.\n</Card>\n\nAfter completing **Quick Setup** above, you have two paths to get started:\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    **Perfect for:** Immediate results with optimized prompts and built-in performance analysis\n\n    <Steps>\n      <Step title=\"Add the Pre-Built Agent\">\n        Visit the [Chrome Dev Tools Agent](https://hub.continue.dev/continuedev/chrome-dev-tools-agent) on Continue Mission Control and click **\"Install Agent\"** or run:\n        \n        ```bash\n        cn --agent continuedev/chrome-dev-tools-agent\n        ```\n\n        This agent includes:\n        - **Optimized prompts** for performance analysis and debugging\n        - **Built-in rules** for consistent formatting and error handling\n        - **[Chrome DevTools MCP](https://hub.continue.dev/google/chrome-devtools-mcp)** for reliable browser automation\n      </Step>\n\n      <Step title=\"Run Performance Analysis\">\n        In the TUI that opens, type:\n        ```\n        Analyze performance of http://localhost:3000 and provide optimization recommendations\n        ```\n\n        That's it! The agent handles Chrome automation automatically.\n      </Step>\n    </Steps>\n\n    <Info>\n      **Why Use the Agent?** Results are more consistent and debugging is easier thanks to the Chrome DevTools MCP integration and pre-tested prompts.\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n    <Steps>\n      <Step title=\"Add Chrome DevTools MCP to Your Assistant\">\n        Visit the [Chrome DevTools MCP](https://hub.continue.dev/google/chrome-devtools-mcp) on Continue Mission Control and add it to your assistant, or add this to your configuration:\n\n        ```yaml\n        name: Chrome DevTools MCP\n        version: 0.0.1\n        schema: v1\n        mcpServers:\n          - name: Chrome DevTools MCP\n            command: npx\n            args:\n              - chrome-devtools-mcp@latest\n        ```\n\n        The MCP will automatically launch Chrome and connect to DevTools when needed.\n      </Step>\n\n      <Step title=\"Verify Setup\">\n        Test the connection:\n\n        **TUI Mode Prompt:**\n        ```\n        Open web.dev and take a screenshot\n        ```\n      </Step>\n\n      <Step title=\"Run Performance Analysis\">\n        Navigate to your project directory:\n\n        **TUI Mode Prompt:**\n        ```\n        Navigate to http://localhost:3000 and record a performance trace. Analyze LCP, FID, and CLS.\n        ```\n      </Step>\n    </Steps>\n\n    <Info>\n      **Manual Setup**: While you can configure the MCP manually, the pre-built agent provides optimized prompts and better error handling for performance analysis workflows.\n    </Info>\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use the pre-built agent, you need either:\n      - **Continue CLI Pro Plan** with the models add-on, OR\n      - **Your own API keys** added to Continue Mission Control secrets\n      - **Chrome browser** installed on your system\n      - **Node.js 20+** to run the MCP via npx\n\n    The agent will automatically detect and use your configuration.\n</Accordion>\n\n---\n\n## Performance Measurement Workflows\n\nThe Chrome DevTools MCP enables natural language performance analysis. Here are workflows adapted from real-world use cases:\n\n### Quick Performance Checks\n\n<Card title=\"Verify Code Changes\" icon=\"check\">\n**TUI Mode Prompt:**\n```\nVerify in the browser that your change works as expected.\n```\n</Card>\n\n<Card title=\"Debug Broken Images\" icon=\"image\">\n**TUI Mode Prompt:**\n```\nA few images on localhost:8080 are not loading. What's happening?\n```\n</Card>\n\n<Card title=\"Debug Form Submission Issues\" icon=\"rectangle-list\">\n**TUI Mode Prompt:**\n```\nWhy does submitting the form fail after entering an email address?\n```\n</Card>\n\n<Card title=\"Debug Layout Issues\" icon=\"layer-group\">\n**TUI Mode Prompt:**\n```\nThe page on localhost:8080 looks strange and off. Check what's happening there.\n```\n</Card>\n\n<Card title=\"Performance Optimization\" icon=\"gauge-high\">\n**TUI Mode Prompt:**\n```\nLocalhost:8080 is loading slowly. Make it load faster.\n```\n</Card>\n\n<Card title=\"Check Core Web Vitals\" icon=\"chart-line\">\n**TUI Mode Prompt:**\n```\nPlease check the LCP of web.dev.\n```\n</Card>\n\n---\n\n## Performance Analysis Recipes\n\nNow you can use natural language prompts to analyze web performance. The Continue agent automatically calls the appropriate Chrome DevTools MCP tools.\n\n<Info>\n  **Where to run these workflows:**\n  - **IDE Extensions**: Use Continue in VS Code, JetBrains, or other supported IDEs\n  - **Terminal (TUI mode)**: Run `cn` to enter interactive mode, then type your prompts\n  - **CLI (headless mode)**: Use `cn -p \"your prompt\"` for headless commands\n\n  **Test in Plan Mode First**: Before running performance measurements, test your prompts in plan mode (see the [Plan Mode Guide](/guides/plan-mode-guide); press **Shift+Tab** to switch modes). This shows you what the agent will do without executing it.\n</Info>\n\n### Step 1: Baseline Performance Trace\n\nEstablish your current performance baseline:\n\n**TUI Mode Prompt:**\n```\nNavigate to http://localhost:3000 and record a performance trace with page reload. Analyze the trace and show me the LCP, FID, CLS, and total blocking time.\n```\n\n\n<Info>\n\n    Chrome DevTools Performance Panel automatically tracks:\n    - **Core Web Vitals**: LCP, FID, CLS, INP\n    - **Loading Performance**: DOMContentLoaded, Load events, First Paint\n    - **Runtime Performance**: JavaScript execution time, layout shifts, paint events\n    - **Resource Usage**: Memory consumption, CPU utilization\n\n</Info>\n\n### Step 2: Analyze Performance Insights\n\nDeep dive into specific performance issues:\n\n**TUI Mode Prompt:**\n```\nRecord a performance trace for http://localhost:3000 and analyze all Performance Insights. For each insight found, provide detailed information about:\n- The specific issue (e.g., DocumentLatency, SlowCSS)\n- Root cause analysis\n- Affected resources or code\n- Recommended optimizations\n- Expected impact on Core Web Vitals\n```\n\n<Info>\n  **Performance Insights**: Chrome DevTools automatically detects common performance issues like:\n  - **Render Blocking Resources**: CSS and JavaScript that delay first paint\n  - **Layout Shifts**: Elements that move during page load\n  - **Long Tasks**: JavaScript execution blocking the main thread\n  - **Slow Network Requests**: Resources taking too long to load\n  - **Unused CSS/JavaScript**: Dead code increasing bundle size\n</Info>\n\n### Step 3: Network Performance Analysis\n\nIdentify slow network requests and optimization opportunities:\n\n**TUI Mode Prompt:**\n```\nNavigate to https://my-site.com and:\n1. List all network requests made during page load\n2. Identify the 5 slowest requests and their sizes\n3. Find any requests over 1MB\n4. Detect render-blocking resources\n5. Check for inefficient caching (missing cache headers)\n6. Suggest specific optimizations for each issue found\n```\n\n### Step 4: Throttling Performance Tests\n\nTest performance under real-world network and CPU conditions:\n\n**TUI Mode Prompt:**\n```\nTest my site's performance under different conditions:\n\n1. Navigate to http://localhost:3000\n2. Emulate 4x CPU slowdown and Fast 3G network\n3. Record a performance trace with reload\n4. Analyze LCP and Total Blocking Time\n5. Take a screenshot when page is fully loaded\n\nThen repeat the test with:\n- Slow 3G network + 6x CPU slowdown\n- Offline network (to test service worker)\n\nCompare results and identify which conditions cause the worst performance degradation.\"\n```\n\n<Info>\n  **Available Network Presets**:\n  - No throttling\n  - Fast 3G (1.6 Mbps down, 0.75 Mbps up)\n  - Slow 3G (400 Kbps down, 400 Kbps up)\n  - Offline\n\n  **CPU Throttling**: 4x, 6x, or custom slowdown to simulate low-end devices\n</Info>\n\n### Step 5: JavaScript Performance Analysis\n\nIdentify expensive JavaScript operations:\n\n**TUI Mode Prompt:**\n```\nRecord a performance trace for http://localhost:3000 and:\n1. Identify all long tasks (>50ms) blocking the main thread\n2. Find the specific JavaScript functions causing these tasks\n3. Measure total JavaScript execution time\n4. Detect unused JavaScript being loaded\n5. Show the call stack for the longest task\n6. Recommend code splitting or lazy loading opportunities\"\n```\n\n## Automated Performance Monitoring\n\n### Step 6: Core Web Vitals Dashboard\n\nCreate automated monitoring for Core Web Vitals:\n\n**TUI Mode Prompt:**\n```\nCreate a performance monitoring script that:\n1. Opens my site at http://localhost:3000\n2. Records a performance trace with reload\n3. Extracts Core Web Vitals (LCP, FID, CLS, INP)\n4. Checks console for JavaScript errors\n5. Takes a screenshot\n6. Saves results to performance-report.json with format:\n   {\n     'timestamp': 'ISO date',\n     'lcp': number,\n     'fid': number,\n     'cls': number,\n     'inp': number,\n     'errors': array,\n     'screenshot': 'path'\n   }\n\nSave this as scripts/performance-monitor.js that I can run regularly\"\n```\n\n### Step 7: Visual Regression Detection\n\nDetect unintended visual changes:\n\n**TUI Mode Prompt:**\n```\nSet up visual regression testing:\n1. Navigate to http://localhost:3000\n2. Take full-page screenshots at:\n   - Desktop viewport (1920x1080)\n   - Tablet viewport (768x1024)\n   - Mobile viewport (375x667)\n3. Save screenshots to screenshots/baseline/\n4. Create a script to compare future screenshots against baseline\n5. Highlight any pixel differences over 5%\n6. Generate a visual diff report\"\n```\n\n### Step 8: Performance Budget Enforcement\n\nSet and enforce performance budgets:\n\n**TUI Mode Prompt:**\n```\nCreate a performance budget checker that:\n\nRequirements:\n- LCP must be < 2.5 seconds\n- FID must be < 100ms\n- CLS must be < 0.1\n- Total JavaScript < 300KB\n- Total page size < 1MB\n- No console errors\n\nImplementation:\n1. Navigate to http://localhost:3000\n2. Record performance trace with reload\n3. List all network requests and calculate total sizes\n4. Check console messages for errors\n5. Validate all metrics against budgets\n6. Exit with code 1 if any budget is exceeded\n7. Generate a detailed report showing pass/fail for each metric\n\nSave as scripts/performance-budget.js for CI/CD integration\"\n```\n\n## Continuous Performance Testing with GitHub Actions\n\nThis example demonstrates a **Continuous AI workflow** where performance validation runs automatically in your CI/CD pipeline using Chrome DevTools MCP in headless mode.\n\n### Add GitHub Secrets\n\nNavigate to **Repository Settings ‚Üí Secrets and variables ‚Üí Actions** and add:\n\n- `CONTINUE_API_KEY`: Your Continue API key from [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n\n### Create Workflow File\n\nThis workflow automatically validates web performance on pull requests using the Continue CLI in headless mode. It records performance traces, extracts Core Web Vitals, and posts a summary report as a PR comment.\n\nCreate `.github/workflows/performance-check.yml` in your repository:\n\n```yaml\nname: Performance Check\n\non:\n  pull_request:\n\njobs:\n  performance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: \"20\"\n\n      - name: Install Dependencies\n        run: |\n          npm install -g @continuedev/cli\n          npm ci\n\n      - name: Build Project\n        run: npm run build\n\n      - name: Start Dev Server\n        run: |\n          npm run dev &\n          npx wait-on http://localhost:3000\n\n      - name: Run Performance Tests\n        env:\n          CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n        run: |\n          cn --agent continuedev/chrome-dev-tools-agent \\\n             -p \"Navigate to http://localhost:3000 and:\n             1. Record performance trace with reload\n             2. Extract LCP, FID, CLS values\n             3. List network requests and calculate total bundle size\n             4. Output results as JSON to performance.json\" \\\n             --auto\n\n      - name: Check Performance Budgets\n        run: |\n          node << 'EOF'\n          const fs = require('fs');\n          const perf = JSON.parse(fs.readFileSync('performance.json'));\n\n          const budgets = {\n            lcp: 2.5,\n            fid: 100,\n            cls: 0.1,\n            bundleSize: 300000\n          };\n\n          let failed = false;\n          const results = [];\n\n          if (perf.lcp > budgets.lcp) {\n            results.push(`‚ùå LCP: ${perf.lcp}s (budget: ${budgets.lcp}s)`);\n            failed = true;\n          } else {\n            results.push(`‚úÖ LCP: ${perf.lcp}s`);\n          }\n\n          if (perf.fid > budgets.fid) {\n            results.push(`‚ùå FID: ${perf.fid}ms (budget: ${budgets.fid}ms)`);\n            failed = true;\n          } else {\n            results.push(`‚úÖ FID: ${perf.fid}ms`);\n          }\n\n          if (perf.cls > budgets.cls) {\n            results.push(`‚ùå CLS: ${perf.cls} (budget: ${budgets.cls})`);\n            failed = true;\n          } else {\n            results.push(`‚úÖ CLS: ${perf.cls}`);\n          }\n\n          if (perf.bundleSize > budgets.bundleSize) {\n            results.push(`‚ùå Bundle: ${perf.bundleSize}KB (budget: ${budgets.bundleSize}KB)`);\n            failed = true;\n          } else {\n            results.push(`‚úÖ Bundle: ${perf.bundleSize}KB`);\n          }\n\n          console.log(results.join('\\n'));\n\n          if (failed) {\n            process.exit(1);\n          }\n          EOF\n\n      - name: Comment Performance Results\n        if: always()\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const perf = JSON.parse(fs.readFileSync('performance.json'));\n\n            const comment = `## üìä Performance Report\n\n            | Metric | Value | Budget | Status |\n            |--------|-------|--------|--------|\n            | LCP | ${perf.lcp}s | 2.5s | ${perf.lcp <= 2.5 ? '‚úÖ' : '‚ùå'} |\n            | FID | ${perf.fid}ms | 100ms | ${perf.fid <= 100 ? '‚úÖ' : '‚ùå'} |\n            | CLS | ${perf.cls} | 0.1 | ${perf.cls <= 0.1 ? '‚úÖ' : '‚ùå'} |\n            | Bundle Size | ${perf.bundleSize}KB | 300KB | ${perf.bundleSize <= 300 ? '‚úÖ' : '‚ùå'} |\n\n            ${perf.lcp > 2.5 || perf.fid > 100 || perf.cls > 0.1 || perf.bundleSize > 300\n              ? '‚ö†Ô∏è **Performance budgets exceeded. Please optimize before merging.**'\n              : '‚úÖ **All performance budgets met!**'}`;\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comment\n            });\n```\n\n<Info>\n  The Chrome DevTools MCP works in headless Chrome environments. Make sure your CI environment has Chrome installed (it's pre-installed on GitHub Actions ubuntu-latest runners).\n</Info>\n\n## Advanced Performance Workflows\n\n### Step 9: Competitor Comparison\n\nCompare your site's performance with competitors:\n\n**TUI Mode Prompt:**\n```\nCompare performance between my site and competitors:\n\nSites to test:\n- http://localhost:3000 (my site)\n- https://competitor1.com\n- https://competitor2.com\n\nFor each site:\n1. Navigate to homepage\n2. Record performance trace with reload\n3. Extract LCP, FID, CLS, Total Blocking Time\n4. List network requests and calculate total page size\n5. Count JavaScript and CSS files\n\nCreate a comparison table showing:\n- Site name\n- All Core Web Vitals\n- Total page size\n- Number of requests\n- Which site performs best for each metric\n\nProvide specific recommendations for how my site can improve based on what competitors do better.\"\n```\n\n### Step 10: Performance Testing Across Routes\n\nTest performance consistency across your entire site:\n\n**TUI Mode Prompt:**\n```\nTest performance across all major routes:\n\nRoutes to test:\n- / (homepage)\n- /products\n- /products/[id] (pick a sample product)\n- /checkout\n- /blog\n\nFor each route:\n1. Navigate to the URL\n2. Record performance trace\n3. Extract LCP, Total Blocking Time, bundle size\n4. Check console for errors\n5. Take a screenshot\n\nGenerate a report showing:\n- Which routes have the worst LCP\n- Routes with the most JavaScript errors\n- Bundle size variations across routes\n- Recommendations for route-specific optimizations\"\n```\n\n### Step 11: Mobile Performance Analysis\n\nFocus on mobile-specific performance issues:\n\n**TUI Mode Prompt:**\n```\nAnalyze mobile performance for http://localhost:3000:\n\n1. Resize viewport to mobile (375x667)\n2. Emulate Slow 3G network + 4x CPU slowdown\n3. Record performance trace with reload\n4. Analyze Performance Insights for mobile-specific issues\n5. Check for:\n   - Touch target sizes too small\n   - Horizontal scrolling issues\n   - Oversized images not optimized for mobile\n   - Excessive JavaScript on mobile\n6. Take mobile screenshot\n7. Provide mobile-specific optimization recommendations\"\n```\n\n## Performance Troubleshooting\n\n### Debug Performance Regressions\n\nQuickly identify what caused a performance regression:\n\n**TUI Mode Prompt:**\n```\nDebug performance regression on http://localhost:3000:\n\n1. Record performance trace\n2. Analyze all Performance Insights\n3. List console errors and warnings\n4. Check network requests for:\n   - Failed requests\n   - Slow requests (>1s)\n   - Large requests (>500KB)\n5. Identify the top 3 performance bottlenecks\n6. For each bottleneck, suggest:\n   - Root cause\n   - Specific code or resource causing it\n   - Step-by-step fix\n   - Expected performance improvement\"\n```\n\n### Performance Issue Quick Reference\n\n| Issue | Quick Fix Command (in cn TUI) |\n| :---- | :---------------------------- |\n| Slow LCP | `\"Find render-blocking resources and suggest preloading or deferring\"` |\n| High CLS | `\"Detect layout shifts and identify unsized images or dynamic content\"` |\n| Long Tasks | `\"Find JavaScript tasks over 50ms and suggest code splitting\"` |\n| Large Bundles | `\"List all JavaScript files, identify largest ones, suggest lazy loading\"` |\n| Slow Network | `\"Find requests over 500KB and suggest compression or optimization\"` |\n| Console Errors | `\"List all console errors and suggest fixes\"` |\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered performance analysis system** that:\n\n- ‚úÖ **Uses natural language** ‚Äî Simple prompts instead of complex DevTools commands\n- ‚úÖ **Analyzes automatically** ‚Äî AI interprets performance traces and suggests fixes\n- ‚úÖ **Runs continuously** ‚Äî Automated validation in CI/CD pipelines\n- ‚úÖ **Ensures quality** ‚Äî Performance checks prevent regressions from shipping\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your performance workflow now operates at **[Level 2 Continuous AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** - AI handles routine performance analysis and debugging with human oversight through review and approval of changes.\n</Card>\n\n## Chrome DevTools MCP Capabilities\n\n<CardGroup cols={2}>\n  <Card title=\"Performance Tracing\" icon=\"chart-line\">\n\n      **Tools Available**\n      - `performance_start_trace`: Start recording with auto-reload\n      - `performance_stop_trace`: Stop and analyze trace\n      - `performance_analyze_insight`: Deep dive into specific issues\n\n  </Card>\n\n  <Card title=\"Network Debugging\" icon=\"network-wired\">\n\n      **Tools Available**\n      - `list_network_requests`: See all requests and sizes\n      - `get_network_request`: Inspect specific request details\n      - `emulate_network`: Test under various network conditions\n\n  </Card>\n\n  <Card title=\"Console Analysis\" icon=\"terminal\">\n\n      **Tools Available**\n      - `list_console_messages`: Get all console logs and errors\n      - `evaluate_script`: Run JavaScript in page context\n      - Automatic error pattern detection\n\n  </Card>\n\n  <Card title=\"Visual Testing\" icon=\"camera\">\n\n      **Tools Available**\n      - `take_screenshot`: Capture full or partial page\n      - `take_snapshot`: Get accessibility tree snapshot\n      - `resize_page`: Test responsive layouts\n\n  </Card>\n</CardGroup>\n\n## Performance Best Practices\n\nKey metrics to monitor for optimal web performance:\n\n<CardGroup cols={3}>\n  <Card title=\"Core Web Vitals\" icon=\"gauge-high\">\n    - LCP < 2.5s (good)\n    - FID < 100ms (good)\n    - CLS < 0.1 (good)\n    - INP < 200ms (good)\n  </Card>\n\n  <Card title=\"Loading Performance\" icon=\"bolt\">\n    - Total page size < 1MB\n    - JavaScript < 300KB\n    - Time to Interactive < 3.8s\n    - First Contentful Paint < 1.8s\n  </Card>\n\n  <Card title=\"Runtime Performance\" icon=\"microchip\">\n    - No long tasks > 50ms\n    - 60 FPS during interactions\n    - No memory leaks\n    - Efficient event listeners\n  </Card>\n</CardGroup>\n\n## Advanced Testing Scenarios\n\n### A/B Test Performance Impact\n\n**TUI Mode Prompt:**\n```\nCompare performance of two implementations:\n1. Test variant A at http://localhost:3000?variant=A\n2. Test variant B at http://localhost:3000?variant=B\nRun each test 5 times and calculate average LCP, FID, CLS\nDetermine which variant has better performance and by how much\n```\n\n### Lighthouse Score Tracking\n\n**TUI Mode Prompt:**\n```\nCreate a script that runs daily performance audits:\n1. Navigate to production site\n2. Record performance trace\n3. Calculate Lighthouse-style performance score based on:\n   - FCP (10%)\n   - SI (10%)\n   - LCP (25%)\n   - TTI (10%)\n   - TBT (30%)\n   - CLS (15%)\n4. Track score over time in performance-history.json\n5. Alert if score drops below 90\n```\n\n### Performance Regression Detection\n\n**TUI Mode Prompt:**\n```\nSet up automated regression detection:\n1. Record baseline performance for main branch\n2. Save baseline metrics to baseline-perf.json\n3. On each PR, run performance tests\n4. Compare PR metrics with baseline\n5. Flag regressions over 10% for any metric\n6. Generate visual diff report with screenshots\n```\n\n## Next Steps\n\n1. **Analyze your first site** - Try the baseline performance trace on your current project\n2. **Debug bottlenecks** - Use the network analysis prompt to fix slow requests\n3. **Set up CI pipeline** - Add the GitHub Actions workflow to your repo\n4. **Test throttling** - Measure performance under real-world network conditions\n5. **Monitor trends** - Track Core Web Vitals over time\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"Chrome DevTools MCP\" icon=\"plug\" href=\"https://github.com/ChromeDevTools/chrome-devtools-mcp\">\n    Official Chrome DevTools MCP repository\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"circle-nodes\" href=\"https://hub.continue.dev\">\n    Explore more MCP integrations\n  </Card>\n  <Card title=\"DevTools Documentation\" icon=\"book\" href=\"https://developer.chrome.com/docs/devtools\">\n    Complete Chrome DevTools documentation\n  </Card>\n  <Card title=\"Web Vitals Guide\" icon=\"gauge-high\" href=\"https://web.dev/vitals/\">\n    Learn about Core Web Vitals\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/guides/github-mcp-continue-cookbook.mdx","content":"---\ntitle: \"GitHub Issues and PRs with GitHub MCP and Continue\"\ndescription: \"Use Continue and the GitHub MCP to list, summarize, and act on open issues and recently merged pull requests with natural language prompts.\"\nsidebarTitle: \"GitHub Issues with Continue\"\n---\n\n<Card title=\"What You'll Build\" icon=\"github\">\n  A GitHub workflow assistant that uses Continue with the GitHub MCP to:\n  - List, filter, and summarize open issues\n  - Review and summarize recently merged PRs\n  - Post comments with AI-generated summaries or checklists\n  - Automate routine GitHub maintenance with headless CLI runs\n</Card>\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Continue account with **Hub access**\n  - Read: [Understanding Configs ‚Äî How to get started with Hub configs](/guides/understanding-configs#how-to-get-started-with-hub-configs)\n- Node.js 22+ installed locally\n- A GitHub account and a repository to work with\n- A GitHub token with the appropriate scopes:\n  - For read-only: `repo:read`\n  - To comment on issues/PRs: `public_repo` for public repos or `repo` for private repos\n\nFor all options, first:\n<Steps>\n<Step title=\"Install Continue CLI\">\n  ```bash\n  npm i -g @continuedev/cli\n  ```\n  </Step>\n\n<Step title=\"Configure GitHub Token\">\n  Add your `GITHUB_TOKEN` to your [Continue Mission Control agent's environment variables](https://hub.continue.dev/settings).\n  </Step>\n</Steps>\n\n<Warning>\n  To use agents in headless mode, you need a [Continue API key](https://hub.continue.dev/settings/api-keys).\n  For write actions (e.g., posting comments), your token must include the relevant GitHub scopes.\n</Warning>\n\n## GitHub MCP Workflow Options\n\n<Card title=\"üöÄ Fastest Path to Success\" icon=\"rocket\">\n  Use the GitHub MCP from Continue Mission Control for one-click setup, or add it via CLI.\n</Card>\n\nAfter ensuring you meet the **Prerequisites** above, you have two paths to get started:\n\n<Tabs>\n  <Tab title=\"‚ö° Quick Start (Recommended)\">\n    <Steps>\n      <Step title=\"Install GitHub MCP via Continue Mission Control\">\n        Visit the [GitHub Project Manager Agent](https://hub.continue.dev/continuedev/github-project-manager-agent) on Continue Mission Control and click **Install** to add it to your agent.\n\n        The listing provides a pre-configured MCP block; add your `GITHUB_TOKEN` in Hub.\n      </Step>\n\n      <Step title=\"Launch the Agent\">\n        From your repo root:\n        ```bash\n        cn --agent continuedev/github-project-manager-agent\n        ```\n        Now try: \"List my open issues labeled bug and summarize priorities.\"\n      </Step>\n    </Steps>\n\n    <Info>\n      You can also attach an MCP to a one-off session: `cn --mcp anthropic/github-mcp`.\n    </Info>\n  </Tab>\n\n  <Tab title=\"üõ†Ô∏è Manual Setup\">\n    <Steps>\n      <Step title=\"Create a New Agent on Continue Mission Control\">\n        Go to the [Continue Mission Control](https://hub.continue.dev) and [create a new agent](https://hub.continue.dev/new?type=agent).\n      </Step>\n\n      <Step title=\"Add GitHub MCP\">\n        Install from Hub (recommended) or add YAML manually. Minimal YAML example:\n\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        mcpServers:\n          - name: GitHub MCP\n            command: npx\n            args:\n              - \"-y\"\n              - \"@modelcontextprotocol/server-github\"\n            env:\n              GITHUB_TOKEN: ${env:GITHUB_TOKEN}\n            connectionTimeout: 30\n        ```\n\n        Notes:\n        - The exact `command`/`args` may differ based on the MCP you choose on Hub. Hub templates prefill these for you.\n        - Provide `GITHUB_TOKEN` via environment or Hub secrets.\n      </Step>\n\n      <Step title=\"Test the Connection\">\n        Launch Continue and ask:\n        ```\n        List the 5 most recently updated open issues in this repository.\n        ```\n      </Step>\n    </Steps>\n  </Tab>\n</Tabs>\n\n<Accordion title=\"Agent Requirements\">\n    To use GitHub MCP with Continue CLI, you need either:\n      - **Continue CLI Pro Plan** with the models add-on, OR\n      - **Your own API keys** added to Continue Mission Control secrets\n\n    The agent will automatically detect and use your configuration along with the GitHub MCP for issue and PR operations.\n\n</Accordion>\n\n---\n\n## Issue Workflows\n\nUse natural language to explore, triage, and act on open issues. The agent calls GitHub MCP tools under the hood.\n\n<Info>\n  **Where to run these workflows:**\n  - **IDE Extensions**: Use Continue in VS Code, JetBrains, or other supported IDEs\n  - **Terminal (TUI mode)**: Run `cn` to enter interactive mode, then type your prompts\n  - **CLI (headless mode)**: Use `cn -p \"your prompt\" --auto` for automation\n  \n  To run any of the example prompts below in headless mode, use `cn -p \"prompt\"`\n</Info>\n\n### Triage and Summaries\n\n<Card title=\"Summarize Open Issues\" icon=\"clipboard-list\">\n  Get a prioritized overview of current open issues.\n\n**Prompt:**\n```\nList the 20 most recently updated open issues in this repo.\nCluster by label and severity. Summarize top priorities.\n```\n</Card>\n\n<Card title=\"Owner/Label Filters\" icon=\"filter\">\n  Narrow by label or assignee.\n\n**Prompt:**\n```\nShow open issues labeled bug or security, assigned to @me.\nSummarize blockers and suggest next steps.\n```\n</Card>\n\n### Taking Action\n\n<Card title=\"Comment on an Issue\" icon=\"comment\">\n  Post an AI-generated status update or checklist.\n\n**Prompt:**\n```\nFind the issue with the most engagement (comments, reactions) in the last 30 days.\nCome up with a triage plan and draft a comment with:\n- current hypothesis\n- next 2 steps\n- owner and ETA\nThen post the comment.\n```\n\n<Note>\n  Requires a token with permission to comment (`public_repo` or `repo`). The agent will confirm before posting unless `--auto` is used.\n</Note>\n</Card>\n\n<Card title=\"Find Stale Issues\" icon=\"clock\">\n  Identify issues that need attention.\n\n**Prompt:**\n```\nFind open issues with no activity in the last 30 days.\nSuggest action (close, needs-repro, or prioritize) and draft comments.\n```\n</Card>\n\n---\n\n## Merged PR Workflows\n\nAnalyze recently merged PRs for change awareness, release notes, and quality signals.\n\n### Recently Merged Overview\n\n<Card title=\"Merged PR Digest\" icon=\"code-merge\">\n  Summarize merged PRs over a time window.\n\n**Prompt:**\n```\nList PRs merged in the last 7 days.\nGroup by area (label or path). Summarize impact and notable changes.\n```\n</Card>\n\n### Release Notes\n\n<Card title=\"Generate Release Notes\" icon=\"file-lines\">\n  Turn merged PRs into crisp release notes.\n\n**Prompt:**\n```\nGenerate release notes for PRs merged since tag v1.2.0.\nUse keep-a-changelog sections and include PR numbers/authors.\n```\n</Card>\n\n### Deep Dives\n\n<Card title=\"Summarize a PR\" icon=\"magnifying-glass\">\n  Get a human-readable summary of a recently merged PR with active discussion.\n\n**Prompt:**\n```\nFind the last merged PR with a significant amount of comments.\nSummarize the conversation, including goal, key changes,\nrisk areas, and any follow-ups.\n```\n</Card>\n\n---\n\n## Automate with GitHub Actions\n\nRun headless commands on a schedule or in PRs to keep teams informed.\n\n### Add GitHub Secrets\n\nRepository Settings ‚Üí Secrets and variables ‚Üí Actions:\n\n- `CONTINUE_API_KEY`: From [hub.continue.dev/settings/api-keys](https://hub.continue.dev/settings/api-keys)\n- `GITHUB_TOKEN`: A token with permissions to read issues/PRs and post comments\n\n### Example Workflow\n\nCreate `.github/workflows/github-mcp-reports.yml`:\n\n```yaml\nname: GitHub MCP Reports\n\non:\n  schedule:\n    - cron: \"0 13 * * 1\"  # Mondays 13:00 UTC\n  workflow_dispatch:\n  pull_request:\n    types: [opened, synchronize]\n\njobs:\n  report:\n    runs-on: ubuntu-latest\n    env:\n      CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: \"18\"\n      - name: Install Continue CLI\n        run: npm i -g @continuedev/cli\n\n      - name: Weekly Issue Triage Summary\n        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'\n        run: |\n          cn --agent continuedev/github-project-manager-agent \\\n             -p \"List open issues updated in the last 14 days. Group by label and priority. Propose top 5 actions and include issue links.\" \\\n             --auto > issue_summary.txt\n\n      - name: Post PR Context Comment\n        if: github.event_name == 'pull_request'\n        env:\n          PR_NUMBER: ${{ github.event.pull_request.number }}\n        run: |\n          REPORT=$(        cn --agent continuedev/github-project-manager-agent \\\n            -p \"Summarize the context for PR #${PR_NUMBER}: related open issues, risk areas, and test suggestions. Keep under 200 words.\" \\\n            --auto)\n          gh pr comment ${PR_NUMBER} --body \"$REPORT\"\n```\n\n<Info>\n  Use `continuedev/github-project-manager-agent` consistently throughout your automation as the recommended agent configuration.\n</Info>\n\n---\n\n## Troubleshooting\n\n- **Missing or invalid token:** Ensure `GITHUB_TOKEN` is set and has the required scopes. Try a minimal test: ‚ÄúList 3 open issues in this repo‚Äù.\n- **Permissions errors on comment:** Your token must include `public_repo` (public) or `repo` (private) to write comments.\n- **Rate limiting:** Reduce frequency or filter queries; consider using a PAT distinct from the default Actions token when running in CI.\n- **MCP connection timeout:** If using custom YAML, increase `connectionTimeout` or verify the `command`/`args` from Mission Control listing.\n\n## What You've Built\n\nAfter completing this guide, you have a complete **AI-powered GitHub workflow system** that:\n\n- ‚úÖ Uses natural language ‚Äî Simple prompts for complex GitHub operations\n- ‚úÖ Automates issue triage ‚Äî AI analyzes and summarizes open issues\n- ‚úÖ Generates release notes ‚Äî Automatic PR digests and changelogs\n- ‚úÖ Runs continuously ‚Äî Automated reports via GitHub Actions\n\n<Card title=\"Continuous AI\" icon=\"rocket\">\n  Your GitHub workflow now operates at **[Level 2 Continuous\n  AI](https://blog.continue.dev/what-is-continuous-ai-a-developers-guide/)** -\n  AI handles routine issue triage and PR summaries with human oversight\n  through review and approval.\n</Card>\n\n## Next Steps\n\n1. **Explore your issues** - Try the issue triage prompts on your repository\n2. **Generate release notes** - Use the merged PR prompts to create changelogs\n3. **Set up automation** - Add the GitHub Actions workflow to your repo\n4. **Customize prompts** - Tailor the prompts to your team's workflow\n5. **Monitor progress** - Track issue resolution metrics over time\n\n## Additional Resources\n\n<CardGroup cols={2}>\n  <Card title=\"GitHub MCP (Anthropic)\" icon=\"link\" href=\"https://hub.continue.dev/anthropic/github-mcp\">\n    Anthropic GitHub MCP on Continue Mission Control\n  </Card>\n  <Card title=\"MCP in Continue\" icon=\"book\" href=\"/customize/deep-dives/mcp\">\n    How MCP works with Continue agents\n  </Card>\n  <Card title=\"GitHub Project Manager Agent\" icon=\"robot\" href=\"https://hub.continue.dev/continuedev/github-project-manager-agent\">\n    Pre-configured agent on Continue Mission Control\n  </Card>\n  <Card title=\"GitHub MCP Server\" icon=\"link\" href=\"https://github.com/github/github-mcp-server/blob/main/README.md\">\n    Official GitHub MCP server README\n  </Card>\n</CardGroup>\n\n"}
{"source":"github","repo":"continue","path":"docs/guides/doc-writing-agent-cli.mdx","content":"---\ntitle: \"Automating Documentation Updates with Continue CLI\"\nsidebarTitle: \"Automating Documentation Updates with Continue CLI\"\ndescription: \"Learn how to create automated documentation generation workflows using Continue CLI. Set up AI agents to analyze code changes and generate or update documentation automatically in GitHub workflows or local development.\"\n---\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/rJ2taa8OLvY\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\n# Getting Started\n\nThis guide demonstrates how to create automated documentation generation based on code updates in a git branch using the Continue CLI, either as part of your local workflow or as part of a GitHub workflow.\n\nThis process utilizes the **Continue CLI** (`cn`) in **headless mode** to analyze changes and generate the necessary documentation, and commit and push the changes. The goal is to keep the workflow as simple as possible by using straightforward shell commands, Continue CLI prompts, and basic git operations.\n\n## Why Use Continue CLI for Documentation?\n\n<CardGroup cols={2}>\n  <Card title=\"Intelligent Analysis\" icon=\"brain\">\n    AI agents understand your codebase and documentation patterns, analyzing git diffs to identify what needs documenting.\n  </Card>\n\n  <Card title=\"Automated Workflows\" icon=\"robot\">\n    Integrate seamlessly into CI/CD pipelines or local development workflows with minimal setup.\n  </Card>\n\n  <Card title=\"Contextual Understanding\" icon=\"files\">\n    Agents can read files, explore projects, and access Git history to generate accurate, relevant documentation.\n  </Card>\n\n  <Card title=\"Controlled Permissions\" icon=\"shield\">\n    Restrict agent actions to specific files and operations, ensuring safe automated documentation updates.\n  </Card>\n</CardGroup>\n\n## Prerequisites\n\n<CardGroup cols={1}>\n  <Card title=\"Node.js 18+\" icon=\"node-js\">\n    Continue CLI requires Node.js 18 or higher. Install globally with:\n    ```bash\n    npm i -g @continuedev/cli\n    ```\n  </Card>\n\n  <Card title=\"Continue API Key\" icon=\"key\">\n    Get your API key from [Continue Mission Control](https://hub.continue.dev/settings/api-keys) and set:\n    ```bash\n    export CONTINUE_API_KEY=your_key_here\n    ```\n    <Info>\n    You can use the Continue CLI in headless mode without interactive login by setting the `CONTINUE_API_KEY` environment variable.\n    </Info>\n  </Card>\n\n  <Card title=\"Git Repository\" icon=\"code\">\n    A project with code and documentation, or use an open-source project to experiment with the workflow.\n  </Card>\n</CardGroup>\n\n# Documentation Generation Workflow\n\n## Workflow Overview\n\nThe documentation generation process follows these sequential steps:\n\n<Steps>\n  <Step title=\"Environment Setup\">\n    Validate environment, install Continue CLI, and set up authentication with API keys.\n  </Step>\n\n  <Step title=\"Change Analysis\">\n    Generate git diff context and analyze code changes between branches to identify new functionality.\n  </Step>\n\n  <Step title=\"Branch Creation\">\n    Create a dedicated documentation branch following the pattern `{original-branch}-docs-update-{timestamp}`.\n  </Step>\n\n  <Step title=\"AI Documentation Generation\">\n    Use Continue CLI with custom rules to analyze changes and generate or update documentation files.\n    <Tip>\n    Use an agent configuration with rules specific for documentation writing in your project and fine-tune it to work for your team's standards.\n    </Tip>\n  </Step>\n\n  <Step title=\"Review & Commit\">\n    Review generated documentation, commit changes to the docs directory, and push to origin.\n  </Step>\n\n  <Step title=\"Cleanup\">\n    Remove temporary files and output completion summary with branch information.\n  </Step>\n</Steps>\n\n\n\n## Implementation\n\n<Tabs>\n  <Tab title=\"GitHub Workflow\">\n\n### GitHub Actions Implementation\n\n<Info>\nThis example uses a manual workflow dispatch that requires two inputs: the repository name and the branch containing your code changes. This is helpful when you want to generate documentation for a feature branch before creating a pull request.\n</Info>\n\n**Required Inputs:**\n- **repository:** The repository you are operating on (format: `owner/repo`)\n- **branch_name:** The name of the branch you have code changes on and want to generate documentation for\n- **continue_config:** The Continue agent configuration to use\n  - Consider setting a default value if you have a default config your'd like to use\n- **continue_org:** The Continue org to use\n  - Consider setting a default value if you have a default org your'd like to use\n\n\n```yaml title=\".github/workflows/generate-docs.yml\"\nname: Generate Docs for Branch\n\non:\n  workflow_dispatch:\n    inputs:\n      repository:\n        description: 'Repository (owner/repo)'\n        required: true\n        default: 'owner/repo'\n      branch_name:\n        description: 'Branch name to generate docs for'\n        required: true\n      continue_config:\n        description: 'Continue agent configuration to use'\n        required: true\n        # Set a default value if you have a default config your'd like to use\n        # default: 'agent-config-name'\n      continue_org:\n        description: 'Continue org to use'\n        required: true\n        # Set a default value if you have a default org your'd like to use\n        # default: 'your-org-name'\n\njobs:\n  write-docs:\n    runs-on: ubuntu-latest\n    name: Write Documentation for Branch\n    env:\n      CONTINUE_API_KEY: ${{ secrets.CONTINUE_API_KEY }}\n      CONTINUE_ORG: ${{ github.event.inputs.continue_org }}\n      CONTINUE_CONFIG: ${{ github.event.inputs.continue_config }}\n\n    steps:\n\n    - name: Checkout fork repository\n      uses: actions/checkout@v4\n      with:\n        repository: ${{ github.event.inputs.repository || 'owner/repo' }}\n        token: ${{ secrets.GH_PAT }}\n        fetch-depth: 0  # Full history needed for sync\n\n    - name: Setup git configuration\n      run: |\n        git config user.name \"github-actions[doc-writer-bot]\"\n        git config user.email \"yourname@email.com\"\n\n    - name: Checkout target branch\n      run: |\n        git fetch origin ${{ github.event.inputs.branch_name }}\n        git checkout ${{ github.event.inputs.branch_name }}\n\n    - name: Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: '18'\n\n    - name: Install Continue CLI\n      run: |\n        echo \"Installing Continue CLI...\"\n        npm i -g @continuedev/cli\n\n    - name: Verify Continue CLI installation\n      run: |\n        echo \"Checking Continue CLI version...\"\n        cn --version || exit 1\n\n    - name: Set branch name\n      id: branch\n      run: |\n        BRANCH_NAME=\"${{ github.event.inputs.branch_name }}-docs-update-$(date +%Y-%m-%d-%H-%M-%S)\"\n        echo \"branch_name=$BRANCH_NAME\" >> $GITHUB_OUTPUT\n        echo \"Branch name: $BRANCH_NAME\"\n\n    - name: Generate git diff context\n      run: |\n        echo \"Git changes:\" > context.txt\n        git diff origin/main..HEAD --stat >> context.txt\n        echo -e \"\\n\\nFile list:\" >> context.txt\n        git diff origin/main..HEAD >> context.txt\n        echo \"Generated context file:\"\n        cat context.txt\n\n    - name: Create documentation branch\n      run: |\n        echo \"Creating branch: ${{ steps.branch.outputs.branch_name }}\"\n        git checkout -b \"${{ steps.branch.outputs.branch_name }}\"\n\n    - name: Generate documentation with Continue CLI\n      run: |\n        echo \"Running Continue agent to generate documentation...\"\n        cn --config <continue-user>/<agent-config> \\\n           --auto \\\n           --allow Write \\\n           -p \\\n           --prompt ./context.txt \\\n           \"Analyze the provided git diff and identify any new functionality introduced. Summarise the new features in a few sentences. Then search the existing documentation in the site docs/ directory to determine whether these features are already documented in a way that enables users to use them. If documentation is missing or incomplete, create or modify Markdown files under the site diretory (without changing any code) to add clear explanations, usage instructions and examples for the new features in the same style and format as the existing documentation. Finally, print a brief summary of the documentation changes you made.\"\n\n    - name: Clean up temporary files\n      run: rm -f context.txt\n\n    - name: Commit and push documentation changes\n      run: |\n        echo \"Review git status...\"\n        git status\n\n        echo \"Adding files edited in site directory to git...\"\n        git add site/\n\n        echo \"Committing changes...\"\n        git commit -s -m \"docs: update for new functionality from branch ${{ github.event.inputs.branch_name }}\"\n\n        echo \"Pushing changes to origin...\"\n        git push --set-upstream origin \"${{ steps.branch.outputs.branch_name }}\"\n\n```\n\n  </Tab>\n  <Tab title=\"Local Development\">\n\n### Local Development Implementation\n\n<Warning>\nMake sure to set your `CONTINUE_API_KEY` environment variable before running local scripts to enable headless mode.\n</Warning>\n\nWhen using the Continue CLI on your local machine, you can build workflows in various ways, one of which is by simply creating shell scripts that you can run, which call the CLI.\n\nThe below shell script snippet shows the final part of a docs updating shell script that can be used to generate documentation for the code changes in a branch of a git repository.\n\nFor the example snippet below to work you will need to set the following variables:\n- `CONTINUE_ORG`\n- `CONTINUE_CONFIG`\n- `BASE_BRANCH`\n- `COMPARE_BRANCH`\n- `DOCS_BRANCH_NAME`\n\nFor example you can either set these as environment variables or as command line arguments to be used by the script.\n\n**Example shell script snippet for generating documentation**\n```bash title=\"generate-docs.sh\"\n#!/bin/bash\n# The script below shows the final part of a docs updating shell script that can be used to generate documentation for the code changes in a branch of a git repository.\n\n# Generate git diff context\necho \"Generating git diff context...\"\necho \"Git changes:\" > context.txt\ngit diff \"$BASE_BRANCH..$COMPARE_BRANCH\" --stat >> context.txt\necho -e \"\\n\\nFile list:\" >> context.txt\ngit diff \"$BASE_BRANCH..$COMPARE_BRANCH\" >> context.txt\necho \"Generated context file:\"\ncat context.txt\n\n# Create documentation branch\necho \"Creating branch: $DOCS_BRANCH_NAME\"\ngit checkout -b \"$DOCS_BRANCH_NAME\"\n\n# Generate documentation with Continue CLI\necho \"Running Continue agent to generate documentation...\"\ncn -config \"$CONTINUE_ORG/$CONTINUE_CONFIG\" \\\n   --auto \\\n   --allow Write \\\n   -p \\\n   --prompt ./context.txt \\\n   \"Analyze the provided git diff and identify any new functionality introduced. Summarise the new features in a few sentences. Then search the existing documentation in the site docs/ directory to determine whether these features are already documented in a way that enables users to use them. If documentation is missing or incomplete, create or modify Markdown files under the site diretory (without changing any code) to add clear explanations, usage instructions and examples for the new features in the same style and format as the existing documentation. Finally, print a brief summary of the documentation changes you made.\"\n\n# Clean up temporary files\necho \"Cleaning up temporary files...\"\nrm -f context.txt\n\n# Commit and push documentation changes\necho \"Reviewing git status...\"\ngit status\n\n# Only adding files that have been added and modified in the site directory\necho \"Adding files edited in site directory to git...\"\ngit add site/\n\nif git diff --cached --quiet; then\n    echo \"No documentation changes to commit\"\nelse\n    echo \"Committing changes...\"\n    git commit -s -m \"docs: update for new functionality from branch $BRANCH_NAME\"\n\n    echo \"Pushing changes to origin...\"\n    git push --set-upstream origin \"$DOCS_BRANCH_NAME\"\n\n    echo \"Documentation branch created and pushed: $DOCS_BRANCH_NAME\"\nfi\n```\n\n  </Tab>\n</Tabs>\n\n## Enhancement Ideas\n\nThe workflow above is a basic example and can be enhanced in various ways to fit your needs. Here are some ideas:\n\n<CardGroup cols={2}>\n  <Card title=\"Change Analysis Agent\" icon=\"magnifying-glass\">\n    Define a specialized agent for analyzing changes and generating targeted prompts for documentation writers, improving output quality.\n  </Card>\n\n  <Card title=\"Auto-Documentation on Merge\" icon=\"code-compare\">\n    Create GitHub workflows that automatically generate documentation PRs when new features are merged to main.\n  </Card>\n\n  <Card title=\"Documentation Gap Analysis\" icon=\"clipboard\">\n    Build an agent that reviews older merged PRs to identify undocumented features and generates missing documentation.\n  </Card>\n\n  <Card title=\"Copy Editor Agent\" icon=\"pencil\">\n    Add a post-processing agent to enhance writing quality with rules like \"use short sentences and simple words.\"\n  </Card>\n</CardGroup>\n\n## Next Steps\n\nReady to implement automated documentation with Continue CLI? Here are some helpful resources to get you started:\n\n<CardGroup cols={2}>\n  <Card title=\"Continue CLI Guide\" icon=\"terminal\" href=\"/guides/cli\">\n    Learn the fundamentals of using Continue CLI for automated coding tasks and headless workflows.\n  </Card>\n\n  <Card title=\"Understanding Configs\" icon=\"robot\" href=\"/guides/understanding-configs\">\n    Discover how to configure and customize AI configs for your specific documentation needs.\n  </Card>\n\n  <Card title=\"Video: Leverage AI to help with your docs\" icon=\"video\" href=\"https://www.youtube.com/watch?v=rJ2taa8OLvY\">\n    Checkout this video from Tetrate about using Continue Agents to help with writing your docs.\n  </Card>\n\n  <Card title=\"Continue Mission Control\" icon=\"users\" href=\"https://hub.continue.dev\">\n    Browse pre-built agents and configurations from the Continue community.\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/guides/continuous-ai-readiness-assessment.mdx","content":"---\ntitle: Assessing Your Team's Readiness for Continuous AI\ndescription: Complete framework to evaluate team readiness for Continuous AI adoption with maturity levels, assessment criteria, and implementation roadmap.\nsidebarTitle: Continuous AI Readiness Assessment\n---\n\n<Info>\n  **TL;DR:** Use this assessment framework to determine if your team is ready to\n  move from individual AI tool usage to automated Continuous AI workflows.\n  Covers technical infrastructure, processes, culture, and organizational\n  support.\n</Info>\n\n## Assessing Continuous AI Readiness\n\nContinuous AI can dramatically improve development velocity and code quality, but successful implementation requires careful evaluation across four key dimensions.\n\n<Warning>\n  Rushing into Continuous AI without proper foundations leads to frustration and\n  failed initiatives. Use this framework to identify gaps before scaling.\n</Warning>\n\n### 1. Identify Your Current Maturity Level\n\nDetermine where your team falls on the Continuous AI maturity spectrum:\n\n<CardGroup cols={1}>\n  <Card title=\"Level 1: Manual AI Assistance\" icon=\"user\">\n    Developers use AI tools inconsistently with highly variable results.\n\n    **Characteristics:**\n      - High rejection rates of AI-generated code (>50%)\n      - No shared standards or prompting rules\n      - AI tools lack context about your codebase\n      - Ad-hoc usage without team coordination\n\n  </Card>\n\n<Card title=\"Level 2: Workflow Automation\" icon=\"gear\">\n  AI is systematically integrated into team workflows and CI/CD pipelines.\n\n**Characteristics:**\n\n- Consistent adoption across 80%+ of team members\n- AI integrated into code reviews and deployment processes\n- Documented standards for prompts and tool usage\n- Basic metrics tracking AI impact\n\n</Card>\n\n  <Card title=\"Level 3: Zero-Intervention Workflows\" icon=\"robot\">\n    Certain development processes run autonomously with minimal human oversight.\n\n    **Characteristics:**\n    - Human intervention rates below 15%\n    - Robust monitoring and automated rollback systems\n    - Measurable ROI from automation initiatives\n    - Advanced context awareness and learning loops\n\n  </Card>\n</CardGroup>\n\n### 2. Evaluate Readiness Across Four Key Dimensions\n\nAssess your team's strengths and potential risks across these critical areas:\n\n<AccordionGroup>\n  <Accordion title=\"Technical Infrastructure Assessment\">\n    **Key Questions:** \n    - Do our development tools integrate reliably? \n    - Can we measure AI effectiveness and impact? \n    - Are security policies compatible with AI workflows?\n\n    **üü¢ Green Flags:**\n    - Stable tool integrations with >99.5% uptime\n    - Comprehensive monitoring and observability\n    - Security policies that support AI tool usage\n    - Automated testing and deployment pipelines\n\n    **üî¥ Red Flags:**\n    - Frequent integration breakdowns\n    - No performance tracking or metrics\n    - Restrictive security policies blocking AI tools\n    - Manual deployment processes\n\n  </Accordion>\n\n<Accordion title=\"Process Maturity Assessment\">\n  **Key Questions:** \n  - Are our development workflows consistent and documented?\n  - Do we have quality gates and review processes? \n  - Can we reproduce builds and deployments reliably? \n \n **üü¢ Green Flags:** \n  - Clear coding standards and style guides\n  - Automated CI/CD with quality gates\n  - Documented, repeatable processes\n  - Consistent code review practices\n  \n  **üî¥ Red Flags:** \n  - Inconsistent code reviews\n  - Ad-hoc deployment processes\n  - \"Works on my machine\" culture\n  - Undocumented tribal knowledge\n\n</Accordion>\n\n<Accordion title=\"Team Culture & Skills Assessment\">\n  **Key Questions:** \n  - Are developers open to adopting new AI-powered workflows?\n  - How does the team handle experimentation and failure?\n  - Do team members collaborate effectively on new initiatives?\n  \n  **üü¢ Green Flags:** \n  - High curiosity and willingness to experiment\n  - Collaborative problem-solving culture\n  - Constructive feedback and learning mindset\n  - Active knowledge sharing practices\n\n**üî¥ Red Flags:**\n\n- Strong resistance to workflow changes\n- Blame culture around mistakes\n- Perfectionism blocking experimentation\n- Siloed work with minimal collaboration\n\n</Accordion>\n\n  <Accordion title=\"Organizational Support Assessment\">\n    **Key Questions:** \n    - Does leadership provide budget and resources for AI initiatives?\n    - Is there tolerance for experimentation and learning?\n    - Are expectations realistic for ROI timelines?\n\n    **üü¢ Green Flags:**\n    - Executive buy-in and strategic alignment\n    - Dedicated budget for training and tools\n    - 3-6 month ROI expectations\n    - Support for calculated risk-taking\n\n    **üî¥ Red Flags:**\n    - Pressure for immediate ROI (weeks)\n    - No allocated budget for AI initiatives\n    - High risk aversion culture\n    - Lack of leadership engagement\n\n  </Accordion>\n</AccordionGroup>\n\n### 3. Critical Warning Signs\n\n<Warning>\n  **Stop and address these issues before scaling Continuous AI:**\n</Warning>\n\n<CardGroup cols={2}>\n  <Card title=\"Technical Red Flags\" icon=\"triangle-exclamation\">\n    - Builds breaking regularly (>5% failure rate)\n    - Unstable deployments or rollback frequency >10%\n    - No monitoring or observability systems\n    - Critical security policy conflicts\n\n  </Card>\n\n<Card title=\"Team & Culture Red Flags\" icon=\"users-slash\">\n  - More than 30% of team opposed to AI tools\n  - No established feedback or learning culture\n  - History of failed automation initiatives\n  - Resistance to changing existing workflows\n\n</Card>\n\n<Card title=\"Process Red Flags\" icon=\"exclamation-triangle\">\n  - Inconsistent development workflows\n  - No quality gates or review processes\n  - Manual deployment and testing processes\n  - Lack of documentation and standards\n\n</Card>\n\n  <Card title=\"Organizational Red Flags\" icon=\"building\">\n    - Leadership expecting ROI in weeks vs months\n    - No allocated budget for AI initiatives\n    - High pressure, low experimentation tolerance\n    - Lack of strategic alignment on AI adoption\n    \n  </Card>\n</CardGroup>\n\n### 4. Implementation Roadmap\n\nBased on your assessment results, follow this step-by-step approach:\n\n<Steps>\n  <Step title=\"Establish Baseline Metrics\">\n    Document current performance across key areas: \n    \n    - Development velocity (story\n    points, cycle time) \n    - Code quality metrics (bug rates, technical debt) \n    - Review times and approval rates \n    - Developer satisfaction and productivity\n    scores\n\n  </Step>\n\n<Step title=\"Select Initial Automation Target\">\n  Choose one high-impact, low-risk workflow to automate first:\n\n    - **Code Review:** Automated analysis and suggestions\n    - **Documentation:** Auto-generated API docs and README updates\n    - **Testing:** Automated test generation and maintenance\n    - **Refactoring:** Systematic code improvement suggestions\n\n  </Step>\n\n<Step title=\"Standardize Team AI Usage\">\n  Create and document consistent practices: \n    - AI tool selection and configuration guidelines\n\n    - Prompting standards and best practices\n\n    - Quality gates and review processes\n\n    - Security and compliance requirements\n\n  </Step>\n\n<Step title=\"Pilot and Measure Impact\">\n  Run controlled experiments with success criteria: \n    - Start with 2-3 team members for 2-4 weeks\n    - Track metrics against baseline performance\n    - Gather qualitative feedback on developer experience\n    - Document lessons learned and optimization opportunities\n\n</Step>\n\n  <Step title=\"Scale Deliberately\">\n    Expand successful pilots across the organization: \n      - Roll out to additional team members gradually\n      - Implement monitoring and alerting systems\n      - Establish feedback loops for continuous improvement\n      - Plan next automation targets based on results\n\n  </Step>\n</Steps>\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Building Async Agents with Continue CLI\"\n    icon=\"blog\"\n    href=\"https://blog.continue.dev/building-async-agents-with-continue-cli/\"\n  >\n    Comprehensive explanation of maturity levels and organizational readiness\n    factors\n  </Card>\n\n  <Card\n    title=\"Developer's Guide\"\n    icon=\"book\"\n    href=\"https://docs.continue.dev/guides/continuous-ai\"\n  >\n    Technical implementation details and best practices for Continuous AI\n    workflows\n  </Card>\n</CardGroup>\n\n## Quick Assessment Checklist\n\n<Tip>\n  **Ready to get started?** Use this quick checklist to gauge your immediate\n  readiness:\n</Tip>\n\n**Technical Foundation (Score: \\_\\_\\_/4)**\n\n- Stable CI/CD pipelines with \\<5% failure rate\n- Monitoring and observability systems in place\n- Security policies support AI tool integration\n- Development environment standardization\n\n**Process Maturity (Score: \\_\\_\\_/4)**\n\n- Documented coding standards and review processes\n- Consistent deployment and rollback procedures\n- Quality gates and automated testing\n- Regular retrospectives and process improvement\n\n**Team Culture (Score: \\_\\_\\_/4)**\n\n- \\<30% resistance to AI tool adoption\n- Active experimentation and learning culture\n- Collaborative problem-solving approach\n- Constructive feedback and knowledge sharing\n\n**Organizational Support (Score: \\_\\_\\_/4)**\n\n- Leadership buy-in and strategic alignment\n- Dedicated budget for AI initiatives and training\n- 3-6 month ROI expectations (not weeks)\n- Support for calculated risk-taking\n\n---\n\n**Overall Readiness Score: \\_\\_\\_/16**\n\n- **12-16:** Ready to begin Continuous AI implementation\n- **8-11:** Address gaps in 1-2 areas before scaling\n- **\\<8:** Focus on foundational improvements first\n"}
{"source":"github","repo":"continue","path":"docs/guides/continuous-ai.mdx","content":"---\ntitle: \"Continuous AI: A Developer's Guide\"\ndescription: \"Learn how to integrate intelligent automation into development workflows, making AI assistance as natural as syntax highlighting. Implement systematic AI workflows that compound productivity gains over time.\"\n---\n\nContinuous AI is the integration of intelligent automation directly into development workflows, making AI assistance as natural and reliable as syntax highlighting or code completion. Its purpose is to amplify developer intent at every stage of the coding process.\n\nThink of it this way: DevOps automated the mechanical aspects of software delivery‚Äîbuilding, testing, deploying. Continuous AI automates the intelligence aspects‚Äîunderstanding context, making suggestions, adapting to patterns, and learning from developer feedback.\n\n## Why Continuous AI Matters Now\n\nThe same market forces that made DevOps inevitable are now driving Continuous AI adoption:\n\n<CardGroup>\n  <Card title=\"Developer Behavior is Shifting\" icon=\"chart-line\">\n    Engineering teams are rapidly adopting AI tools, with many seeing\n    significant productivity improvements in their workflows.\n  </Card>\n\n<Card title=\"Economic Pressure\" icon=\"dollar-sign\">\n  AI-assisted coding is contributing measurable increases in developer output,\n  creating competitive advantages for early adopters.\n</Card>\n\n  <Card title=\"Tooling Maturity\" icon=\"wrench\">\n    Unlike DevOps, Continuous AI can be implemented incrementally on existing\n    development stacks without major infrastructure changes.\n  </Card>\n</CardGroup>\n\nThe teams that implement systematic AI workflows first create advantages that compound over time.\n\n## The Continuous AI Maturity Model\n\nTeams typically progress through three stages when adopting Continuous AI:\n\n<Accordion title=\"Level 1: Manual AI Assistance\">\nYou prompt the AI when you remember, and it completes the task. This is great for quick productivity boosts but remains highly manual and inconsistent.\n\n**Example**: Using AI to draft a function or suggest a test case only when you think to ask for it.\n\n**Continue Implementation**: Using [Chat](/ide-extensions/chat/quick-start) or [Edit](/ide-extensions/edit/quick-start) mode for one-off coding tasks.\n\n</Accordion>\n\n<Accordion title=\"Level 2: Workflow Automation\">\nAI handles routine tasks with human oversight. This is where teams start seeing compounding gains.\n\n**Examples**:\n\n- AI adds missing documentation during PR review\n- Automatic code formatting and style corrections\n- Generated unit tests for new functions\n- Updated issue tracking when branches are merged\n\n**Continue Implementation**: Using [Continue CLI](/guides/cli) with custom rules and integrations into CI/CD pipelines.\n\n</Accordion>\n\n<Accordion title=\"Level 3: Zero-Intervention Workflows\">\nAI autonomously completes processes end-to-end without human input, but only for very specific, low-risk workflows.\n\n**Examples**:\n\n- AI merges safe dependency updates after automated tests pass\n- Automatic documentation updates when code changes\n- Self-healing test suites that fix themselves based on failure patterns\n\n**Continue Implementation**: Fully automated [agents](/ide-extensions/agent/quick-start) with strict permissions and safety guardrails.\n\n</Accordion>\n\n## Building Your Continuous AI Workflow\n\n### Start with Level 1 ‚Üí Level 2: Pick One Workflow\n\nDon't try to automate everything at once. Choose a specific daily friction point:\n\n```bash\n# Example: Automated code review comments\ngit diff | cn -p \"review this diff and suggest improvements following our team standards\"\n\n# Example: Generate missing tests\ncn -p \"create unit tests for the functions in src/auth.js\"\n\n# Example: Update documentation\ncn -p \"update the README with the new API endpoints from the recent changes\"\n```\n\n### Configure Team-Wide Intelligence\n\nThe most effective Continuous AI is tuned to your codebase, standards, and practices:\n\n```yaml config.yaml\nrules:\n  - name: code-review\n    description: \"Review code following team standards\"\n    rule: \"Review following our TypeScript style guide and security practices\"\n    context:\n      - \"docs/style-guide.md\"\n      - \"security-checklist.md\"\n```\n\n### Implement Progressive Permissions\n\nUse Continue CLI's permission system to gradually expand AI capabilities:\n\n```yaml ~/.continue/permissions.yaml\npermissions:\n  - allow: \"Bash(git*)\" # Git commands are safe\n  - ask: \"Write(**/*.ts)\" # Ask before modifying TypeScript files\n  - deny: \"Bash(rm*)\" # Never allow deletions\n```\n\n### Measure What Matters: Intervention Rate\n\nTrack how often you need to correct AI output. Lower intervention rates mean higher trust and compounding productivity gains.\n\n## Real-World Implementation Patterns\n\n<AccordionGroup>\n  <Accordion title=\"Pattern 1: The Async Triage Bot\">\n    Imagine setting up an AI agent that checks new GitHub issues every morning and leaves the first helpful response. This lightens the load for maintainers and ensures community members feel heard quickly.\n  </Accordion>\n\n    <Accordion title=\"Pattern 2: The PR Review Agent\">\n    AI can automatically review new pull requests for security, performance, and style issues. The reviewer still has the final say, but the agent highlights common problems and speeds up the feedback loop.\n\n  </Accordion>\n    <Accordion title=\"Pattern 3: The Documentation Guardian\">\n    Whenever code changes, AI can scan for mismatches in documentation and suggest updates. This keeps docs current without relying on developers to remember every detail.\n  </Accordion>\n</AccordionGroup>\n## Best Practices for Sustainable Continuous AI\n\n<CardGroup>\n  <Card title=\"Human-AI Collaboration\" icon=\"handshake\">\n    AI should amplify human intelligence, not replace it. Always validate AI\n    suggestions rather than blindly accepting them.\n  </Card>\n\n<Card title=\"Start Small, Scale Thoughtfully\" icon=\"seedling\">\n  Begin with low-risk, high-value automations. Gradually expand as you build\n  trust and understanding.\n</Card>\n\n<Card title=\"Customize for Your Context\" icon=\"cog\">\n  Generic AI suggestions are often wrong or irrelevant. Configure AI to\n  understand your specific patterns and requirements.\n</Card>\n\n  <Card title=\"Build Safety Guardrails\" icon=\"shield\">\n    Use permission systems, code review processes, and testing to ensure AI\n    actions are safe and reversible.\n  </Card>\n</CardGroup>\n\n## Common Pitfalls to Avoid\n\n<Warning>\n  **Over-automation**: Don't automate processes you don't fully understand\n</Warning>\n\n<Warning>\n  **Ignoring Context**: AI works best when it understands your codebase and team\n  practices\n</Warning>\n\n<Warning>\n  **Skipping Safety**: Always implement proper permissions and review processes\n</Warning>\n\n<Warning>\n  **Vanity Metrics**: Focus on intervention rate and actual time saved, not \"AI\n  suggestions generated\"\n</Warning>\n\n## Getting Started Today\n\n<Steps>\n  <Step title=\"Install Continue CLI\">\n    ```bash npm i -g @continuedev/cli ```\n  </Step>\n\n<Step title=\"Pick One Workflow\">Choose a daily friction point to automate</Step>\n\n<Step title=\"Set Permissions\">Configure safe boundaries for AI actions</Step>\n\n<Step title=\"Measure Impact\">Track intervention rates and time saved</Step>\n\n  <Step title=\"Iterate and Expand\">Gradually add more automated workflows</Step>\n</Steps>\n\n## The Competitive Advantage\n\nTeams implementing Continuous AI are coding faster and building institutional intelligence that scales with their organization. While others manually perform routine tasks, your AI handles the repetitive work so your team can focus on innovation and complex problem-solving.\n\n## What's Next?\n\nAs AI capabilities continue to improve and tooling matures, we're moving toward a world where intelligent assistance is as fundamental to development as version control or IDEs. The teams that start building these capabilities now will have refined systems, cultural readiness, and institutional knowledge when Continuous AI becomes the industry standard.\n\nReady to amplify your development workflow with Continuous AI? Start with one simple automation and build from there.\n\n<Card title=\"Want to dive deeper?\" icon=\"rocket\">\n      Check out our guides on [Continue CLI](/guides/cli) and [Understanding\n    Configs](/guides/understanding-configs).\n</Card>\n"}
{"source":"github","repo":"continue","path":"docs/guides/instinct.mdx","content":"---\ntitle: \"Using Instinct with Ollama in Continue\"\ndescription: \"Learn how to run Instinct, Continue's leading open Next Edit model, on your own hardware with Ollama\"\n---\n\n<Warning>\n  Instinct is a 7 billion parameter model. You should expect slow responses if\n  running on a laptop. To learn how to inference Instinct on a GPU, see our\n  [HuggingFace model card](https://huggingface.co/continuedev/instinct).\n</Warning>\n\nWe recently released Instinct, a state-of-the-art open Next Edit model. Robustly fine-tuned from Qwen2.5-Coder-7B, Instinct intelligently predicts your next move to keep you in flow. To learn more about the model, check out [our blog post](https://blog.continue.dev/instinct/).\n\n<Frame>\n  <img src=\"/images/instinct.gif\" />\n</Frame>\n\n### 1. Install Ollama\n\nIf you haven't already installed Ollama, see our guide [here](./ollama-guide).\n\n### 2. Download Instinct\n\n```bash\nollama run nate/instinct\n```\n\n### 3. Update your `config.yaml`\n\nOpen your `config.yaml` and add Instinct to the models section:\n\n```yaml\n# ... rest of config.yaml ...\n\nmodels:\n  - uses: continuedev/instinct\n```\n\nAlternatively, you can just click to add the block at https://hub.continue.dev/continuedev/instinct.\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/xAI.mdx","content":"---\ntitle: xAI\nslug: ../xai\n---\n\n<Tip>**Discover xAI models [here](https://hub.continue.dev/xai)**</Tip>\n\n<Info>Get an API key from the [xAI Console](https://console.x.ai/)</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: xai\n      model: <MODEL_ID>\n      apiKey: <YOUR_XAI_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"xai\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_XAI_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration\n  [here](https://hub.continue.dev/xai/grok-code-fast-1?view=config)**\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/morph.mdx","content":"---\ntitle: \"Morph\"\ndescription: \"Configure Morph with Continue to access their optimized models for code application, embeddings, and reranking, designed for fast and accurate integration of AI-generated code changes\"\n---\n\nMorph provides a fast apply model that helps you quickly and accurately apply code changes from chat suggestions to your files. It's optimized for speed and precision when integrating generated code into your existing codebase. You can sign up for Morph's generous free tier [here](https://morphllm.com/dashboard). Then, update your configuration file as follows:\n\n<Tabs>\n   <Tab title=\"YAML\">\n   ```yaml title=\"config.yaml\"\n   name: My Config\n   version: 0.0.1\n   schema: v1\n\n   models:\n     - uses: morphllm/morph-v0\n       with:\n         MORPH_API_KEY: ${{ secrets.MORPH_API_KEY }}\n```\nor\n```yaml title=\"config.yaml\"\n     name: My Config\n     version: 0.0.1\n     schema: v1\n\n     - name: Morph Fast Apply\n       provider: openai\n       model: morph-v2\n       apiKey: <YOUR_MORPH_API_KEY>\n       apiBase: https://api.morphllm.com/v1/\n       roles:\n         - apply\n       promptTemplates:\n         apply: \"<code>{{{ original_code }}}</code>\\n<update>{{{ new_code }}}</update>\"\n   ```\n   </Tab>\n   <Tab title=\"JSON\">\n   ```json title=\"config.json\"\n   {\n     \"models\": [\n       {\n         \"title\": \"Morph Fast Apply\",\n         \"provider\": \"openai\",\n         \"model\": \"morph-v2\",\n         \"apiKey\": \"<YOUR_MORPH_API_KEY>\",\n         \"apiBase\": \"https://api.morphllm.com/v1/\",\n         \"roles\": [\"apply\"],\n         \"promptTemplates\": {\n           \"apply\": \"<code>{{{ original_code }}}</code>\\n<update>{{{ new_code }}}</update>\"\n         }\n       }\n     ]\n   }\n   ```\n   </Tab>\n</Tabs>\n\n## Embeddings model\n\nWe recommend configuring **morph-embedding-v2** as your embeddings model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Morph Embeddings\n      provider: openai\n      model: morph-embedding-v2\n      apiKey: <YOUR_MORPH_API_KEY>\n      apiBase: https://api.morphllm.com/v1/\n      roles:\n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"openai\",\n      \"model\": \"morph-embedding-v2\",\n      \"apiKey\": \"<YOUR_MORPH_API_KEY>\",\n      \"apiBase\": \"https://api.morphllm.com/v1/\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Reranking model\n\nWe recommend configuring **morph-rerank-v2** as your reranking model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Morph Reranker\n      provider: cohere\n      model: morph-rerank-v2\n      apiKey: <YOUR_MORPH_API_KEY>\n      apiBase: https://api.morphllm.com/v1/\n      roles:\n        - rerank\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"reranker\": {\n      \"name\": \"cohere\",\n      \"params\": {\n        \"model\": \"morph-rerank-v2\",\n        \"apiKey\": \"<YOUR_MORPH_API_KEY>\",\n        \"apiBase\": \"https://api.morphllm.com/v1/\"\n      }\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/relace.mdx","content":"---\ntitle: \"Relace\"\ndescription: \"Configure Relace with Continue to access their Fast Apply model, which helps you quickly and reliably apply chat suggestions to your codebase\"\n---\n\nRelace provides a fast apply model through their API that helps you reliably and almost instantly apply chat suggestions to your codebase. You can sign up and obtain an API key [here](https://app.relace.ai/settings/api-keys). Then, change your configuration file to look like this:\n\n<Tabs>\n   <Tab title=\"YAML\">\n   ```yaml title=\"Package or config.yaml\"\n   models:\n     - name: Relace Fast Apply\n       provider: relace\n       model: Fast-Apply\n       apiKey: <YOUR_RELACE_API_KEY>\n       roles:\n         - apply\n       promptTemplates:\n         apply: \"{{{ new_code }}}\"\n   ```\n   </Tab>\n   <Tab title=\"JSON\">\n\t```json title=\"config.json\"\n\t{\n      \"models\": [\n        {\n          \"title\": \"Relace Fast Apply\",\n          \"provider\": \"relace\",\n          \"model\": \"Fast-Apply\",\n          \"apiKey\": \"<YOUR_RELACE_API_KEY>\"\n        }\n      ]\n\t}\n    ```\n   </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Relace.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/ncompass.mdx","content":"---\ntitle: \"NCompass\"\ndescription: \"Configure NCompass Technologies with Continue to access their fast inference engine for open-source models like Google's Gemma 3 Coder\"\n---\n\nThe nCompass Technologies API exposes an extremely fast inference engine for open-source language models. You can sign up [here](https://app.ncompass.tech/api-settings), copy your API key on the initial welcome screen, and then hit the play button on any model from the [nCompass Models list](https://ncompass.tech/models). Change `~/.continue/config.json` to look like this:\n\n<Tabs>\n   <Tab title=\"YAML\">\n   ```yaml title=\"Package or config.yaml\"\n   models:\n     - name: Ncompass Gemma 3 Coder\n       provider: ncompass\n       model: google/gemma-3-27b-it\n       apiKey: <YOUR_NCOMPASS_API_KEY>\n   ```\n   </Tab>\n   <Tab title=\"JSON\">\n\t```json title=\"config.json\"\n\t{\n      \"models\": [\n        {\n          \"title\": \"Ncompass Gemma 3 Coder\",\n          \"provider\": \"ncompass\",\n          \"model\": \"google/gemma-3-27b-it\",\n          \"apiKey\": \"<YOUR_NCOMPASS_API_KEY>\"\n        }\n      ]\n\t}\n    ```\n   </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/NCompass.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/cloudflare.mdx","content":"---\ntitle: \"Cloudflare\"\ndescription: \"Configure Cloudflare Workers AI with Continue to access various models for chat and autocomplete, including Llama 3 8B and DeepSeek Coder through Cloudflare's serverless AI platform\"\n---\n\nCloudflare Workers AI can be used for both chat and tab autocompletion in Continue. Here is an example of Cloudflare Workers AI configuration:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Llama 3 8B\n      provider: cloudflare\n      apiKey: <YOUR_CLOUDFARE_API_KEY>\n      model: \"@cf/meta/llama-3-8b-instruct\"\n      contextLength: 2400\n      defaultCompletionOptions:\n        maxTokens: 500\n      roles:\n        - chat\n      env:\n        accountId: YOUR CLOUDFLARE ACCOUNT ID \n    - name: DeepSeek Coder 6.7b Instruct\n      provider: cloudflare\n      apiKey: <YOUR_CLOUDFARE_API_KEY>\n      model: \"@hf/thebloke/deepseek-coder-6.7b-instruct-awq\"\n      contextLength: 2400\n      defaultCompletionOptions:\n        maxTokens: 500\n      roles:\n        - chat\n      env:\n        accountId: YOUR CLOUDFLARE ACCOUNT ID\n    - name: DeepSeek 7b\n      provider: cloudflare\n      apiKey: <YOUR_CLOUDFARE_API_KEY>\n      model: \"@hf/thebloke/deepseek-coder-6.7b-base-awq\"\n      roles:\n        - autocomplete\n      env:\n        accountId: YOUR CLOUDFLARE ACCOUNT ID\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Llama 3 8B\",\n        \"provider\": \"cloudflare\",\n        \"accountId\": \"YOUR CLOUDFLARE ACCOUNT ID\",\n        \"apiKey\": \"YOUR CLOUDFLARE API KEY\",\n        \"contextLength\": 2400,\n        \"completionOptions\": {\n          \"maxTokens\": 500\n        },\n        \"model\": \"@cf/meta/llama-3-8b-instruct\"\n      },\n      {\n        \"title\": \"DeepSeek Coder 6.7b Instruct\",\n        \"provider\": \"cloudflare\",\n        \"accountId\": \"YOUR CLOUDFLARE ACCOUNT ID\",\n        \"apiKey\": \"YOUR CLOUDFLARE API KEY\",\n        \"contextLength\": 2400,\n        \"completionOptions\": {\n          \"maxTokens\": 500\n        },\n        \"model\": \"@hf/thebloke/deepseek-coder-6.7b-instruct-awq\"\n      }\n    ],\n    \"tabAutocompleteModel\": {\n      \"title\": \"DeepSeek 7b\",\n      \"provider\": \"cloudflare\",\n      \"accountId\": \"YOUR CLOUDFLARE ACCOUNT ID\",\n      \"apiKey\": \"YOUR CLOUDFLARE API KEY\",\n      \"model\": \"@hf/thebloke/deepseek-coder-6.7b-base-awq\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\nVisit the [Cloudflare dashboard](https://dash.cloudflare.com/) to [create an API key](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/).\n\nReview [available models](https://developers.cloudflare.com/workers-ai/models/) on Workers AI\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Cloudflare.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/siliconflow.mdx","content":"---\ntitle: \"SiliconFlow\"\ndescription: \"Configure SiliconFlow with Continue to access their AI model platform, featuring Qwen's Coder models for chat and autocomplete, along with various embedding and reranking models\"\n---\n\n<Info>\n  You can get an API key from the [Silicon\n  Cloud](https://cloud.siliconflow.cn/account/ak).\n</Info>\n\n## Chat model\n\nWe recommend configuring **Qwen/Qwen2.5-Coder-32B-Instruct** as your chat model.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Qwen\n        provider: siliconflow\n        model: Qwen/Qwen2.5-Coder-32B-Instruct\n        apiKey: <YOUR_SILICONFLOW_API_KEY>\n        roles:\n          - chat\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Qwen\",\n          \"provider\": \"siliconflow\",\n          \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n          \"apiKey\": \"<YOUR_SILICONFLOW_API_KEY>\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## Autocomplete model\n\nWe recommend configuring **Qwen/Qwen2.5-Coder-7B-Instruct** as your autocomplete model.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Qwen\n        provider: siliconflow\n        model: Qwen/Qwen2.5-Coder-32B-Instruct\n        apiKey: <YOUR_SILICONFLOW_API_KEY>\n        roles: \n          - autocomplete\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Qwen\",\n          \"provider\": \"siliconflow\",\n          \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n          \"apiKey\": \"<YOUR_SILICONFLOW_API_KEY>\"\n        }\n      ]\n      \"tabAutocompleteModel\": {\n        \"title\": \"Qwen\",\n        \"provider\": \"siliconflow\",\n        \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n        \"apiKey\": \"<YOUR_SILICONFLOW_API_KEY>\"\n      }\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## Embeddings model\n\nSiliconFlow provide some embeddings models. [Click here](https://siliconflow.cn/models) to see a list of embeddings models.\n\n## Reranking model\n\nSiliconFlow provide some reranking models. [Click here](https://siliconflow.cn/models) to see a list of reranking models.\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/flowise.mdx","content":"---\ntitle: \"Flowise\"\ndescription: \"Configure Flowise with Continue to integrate with this low-code/no-code drag & drop tool for building and visualizing LLM applications\"\n---\n\n[Flowise](https://flowiseai.com/) is a low-code/no-code drag & drop tool with the aim to make it easy for people to visualize and build LLM apps. Continue can then be configured to use the `Flowise` LLM class, like the example here:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Flowise\n      provider: flowise\n      model: <MODEL>\n      apiBase: <API_BASE>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"provider\": \"flowise\",\n        \"title\": \"Flowise\",\n        \"model\": \"<MODEL>\",\n        \"apiBase\": \"<API_BASE>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Flowise.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/watsonx.mdx","content":"---\ntitle: \"IBM WatsonX\"\ndescription: \"How to configure IBM's watsonx models in Continue, including authentication methods, deployment options, and support for chat, autocomplete, embeddings, and reranking models\"\n---\n\nwatsonx, developed by IBM, offers a variety of pre-trained AI foundation models that can be used for natural language processing (NLP), computer vision, and speech recognition tasks.\n\n## Setup\n\nAccessing watsonx models can be done either through watsonx SaaS on IBM Cloud or using a dedicated watsonx.ai Software instance.\n\n### watsonx.ai SaaS - IBM Cloud\n\nTo get started with watsonx SaaS, visit the [registration page](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx). If you do not have an existing IBM Cloud account, you can sign up for a free trial.\n\nTo authenticate to watsonx.ai SaaS with Continue, you will need to create a project and [set up an API key](https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=cli-creating-your-cloud-api-key). Then, in continue:\n\n- Set **apiBase** to your watsonx SaaS endpoint, e.g. `https://us-south.ml.cloud.ibm.com` for US South region.\n- Set **projectId** to your watsonx project ID.\n- Set **apiKey** to your watsonx API Key.\n\n### watsonx.ai Software\n\nTo authenticate to your watsonx.ai Software instance with Continue, you can use either `username/password` or `ZenApiKey` method:\n\n1. _Option 1_ (Recommended): using `ZenApiKey` authentication:\n   - Set **apiBase** to your watsonx software endpoint, e.g. `https://cpd-watsonx.apps.example.com`.\n   - Set **projectId** to your watsonx project ID.\n   - Set **apiKey** to your watsonx Zen API Key. To generate it:\n     1. Log in to the CPD web client.\n     2. From the toolbar, click your avatar.\n     3. Click **Profile and settings**.\n     4. Click **API key** > **Generate new key**.\n     5. Click **Generate**.\n     6. Click **Copy** and save your key somewhere safe. You cannot recover this key if you lose it.\n     7. Generate your ZenApiKey by running the following command in your preferred terminal: `echo \"<username>:<apikey>\" | base64`, replacing `<username>` with your CPD username and `<apikey>` with the API Key you just created.\n2. _Option 2_: using `username/password` authentication:\n   - Set **apiBase** to your watsonx software endpoint, e.g. `https://cpd-watsonx.apps.example.com`.\n   - Set **projectId** to your watsonx project ID.\n   - Set **API Key** to your watsonx Username and Password using `username:password` as format.\n\n## Configuration\n\nAdd the following configuration:\n\n<Tabs>\n\t<Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: watsonx - Model Name\n      provider: watsonx\n      model: model ID\n      apiBase: https://us-south.ml.cloud.ibm.com\n      apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n      env:\n        projectId: PROJECT_ID\n        apiVersion: 2024-03-14\n  ```\n\t</Tab>\n\t<Tab title=\"JSON\">\n\t```json title=\"config.json\"\n  {\n\t\t\"models\": [\n\t\t  {\n\t\t    \"model\": \"model ID\",\n\t\t    \"title\": \"watsonx - Model Name\",\n\t\t    \"provider\": \"watsonx\",\n\t\t    \"apiBase\": \"https://us-south.ml.cloud.ibm.com\",\n\t\t    \"projectId\": \"PROJECT_ID\",\n\t\t    \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n\t\t    \"apiVersion\": \"2024-03-14\"\n\t\t  }\n\t\t]\n  }\n\t```\n\t</Tab>\n</Tabs>\n\n`apiVersion` is optional and defaults to the latest version.\n\nIf you are using a custom deployment endpoint, set `deploymentID` to the model's deployment ID. You can find it in the watsonx.ai Prompt Lab UI by selecting the corresponding model and opening the `</>` tab on the right, which will display the endpoint's URL containing the deployment ID.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: watsonx - Model Name\n        provider: watsonx\n        model: model ID\n        apiBase: watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\n        apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n        env:\n          apiVersion: 2024-03-14\n          deploymentId: DEPLOYMENT_ID\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"model\": \"model ID\",\n          \"title\": \"watsonx - Model Name\",\n          \"provider\": \"watsonx\",\n          \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n          \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n          \"apiVersion\": \"2024-03-14\",\n          \"deploymentId\": \"DEPLOYMENT_ID\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n### Configuration Options\n\nMake sure to specify a template name, such as `granite` or `llama3`, and to set the `contextLength` to the model's context window size.\nYou can also configure generation parameters, such as temperature, topP, topK, frequency penalty, and stop sequences:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Granite Code 20b\n      provider: watsonx\n      model: ibm/granite-20b-code-instruct\n      apiBase: watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\n      apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n      template: granite\n      defaultCompletionOptions:\n        contextLength: 8000\n        temperature: 0.1\n        topP: 0.3\n        topK: 20\n        maxTokens: 2000\n        frequencyPenalty: 1.1\n        stop:\n          - Question:\n          - \"\\n\\n\\n\"\n      env:\n        projectId: PROJECT_ID\n        apiVersion: 2024-03-14\n        \n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"model\": \"ibm/granite-20b-code-instruct\",\n        \"title\": \"Granite Code 20b\",\n        \"provider\": \"watsonx\",\n        \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n        \"projectId\": \"PROJECT_ID\",\n        \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n        \"apiVersion\": \"2024-03-14\",\n        \"template\": \"granite\",\n        \"contextLength\": 8000,\n        \"completionOptions\": {\n          \"temperature\": 0.1,\n          \"topP\": 0.3,\n          \"topK\": 20,\n          \"maxTokens\": 2000,\n          \"frequencyPenalty\": 1.1,\n          \"stop\": [\"Question:\", \"\\n\\n\\n\"]\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Tab Auto Complete Model\n\nGranite models are recommended for tab auto complete. The configuration is similar to that of the chat models:\n\n<Tabs>\n\t<Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Granite Code 8b\n      provider: watsonx\n      model: ibm/granite-8b-code-instruct\n      apiBase: watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\n      projectId: PROJECT_ID\n      apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n      apiVersion: 2024-03-14\n      roles:\n        - autocomplete\n  ```\n\t</Tab>\n\t<Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"tabAutocompleteModel\": {\n      \"model\": \"ibm/granite-8b-code-instruct\",\n      \"title\": \"Granite Code 8b\",\n      \"provider\": \"watsonx\",\n      \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n      \"projectId\": \"PROJECT_ID\",\n      \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n      \"apiVersion\": \"2024-03-14\",\n      \"contextLength\": 4000\n    }\n  }\n  ```\n\t</Tab>\n</Tabs>\n\n## Embeddings Model\n\nTo view the list of available embeddings models, visit [this page](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx&pos=2#ibm-provided).\n\n<Tabs>\n\t<Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Watsonx Embedder\n      provider: watsonx\n      model: ibm/slate-30m-english-rtrvr-v2\n      apiBase: https://us-south.ml.cloud.ibm.com\n      projectId: PROJECT_ID\n      apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n      apiVersion: 2024-03-14\n      roles:\n        - embed\n  ```\n\t</Tab>\n\t<Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"watsonx\",\n      \"model\": \"ibm/slate-30m-english-rtrvr-v2\",\n      \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n      \"projectId\": \"PROJECT_ID\",\n      \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n      \"apiVersion\": \"2024-03-14\"\n    }\n  }\n  ```\n\t</Tab>\n</Tabs>\n\n## Reranker\n\n<Tabs>\n\t<Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Watsonx Reranker\n      provider: watsonx\n      model: cross-encoder/ms-marco-minilm-l-12-v2\n      apiBase: https://us-south.ml.cloud.ibm.com\n      projectId: PROJECT_ID\n      apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n      apiVersion: 2024-03-14\n  ```\n\t</Tab>\n\t<Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"reranker\": {\n      \"name\": \"watsonx\",\n      \"params\": {\n        \"model\": \"cross-encoder/ms-marco-minilm-l-12-v2\",\n        \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n        \"projectId\": \"PROJECT_ID\",\n        \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n        \"apiVersion\": \"2024-03-14\"\n      }\n    }\n  }\n  ```\n\t</Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/replicatellm.mdx","content":"---\ntitle: \"Replicate\"\ndescription: \"Configure Replicate with Continue to access newly released language models or deploy your own through their platform, with support for various models including CodeLLama\"\n---\n\nReplicate is a great option for newly released language models or models that you've deployed through their platform. Sign up for an account [here](https://replicate.ai/), copy your API key, and then select any model from the [Replicate Streaming List](https://replicate.com/collections/streaming-language-models). Change `~/.continue/config.json` to look like this:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Replicate CodeLLama\n      provider: replicate\n      model: codellama-13b\n      apiKey: <YOUR_REPLICATE_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Replicate CodeLLama\",\n        \"provider\": \"replicate\",\n        \"model\": \"codellama-13b\",\n        \"apiKey\": \"<YOUR_REPLICATE_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\nIf you don't specify the `model` parameter, it will default to `replicate/llama-2-70b-chat:58d078176e02c219e11eb4da5a02a7830a283b14cf8f94537af893ccff5ee781`.\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Replicate.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/venice.mdx","content":"---\ntitle: \"Venice AI\"\ndescription: \"Configure Venice AI with Continue to access this privacy-focused generative AI platform that supports open-source LLMs without storing private user data\"\n---\n\nVenice.AI is a privacy focused generative AI platform, allowing users to interact with open-source LLMs without storing any private user data. To get started with Venice's API, either purchase a pro account, stake $VVV to obtain daily inference allotments or fund your account with USD and head over to https://venice.ai/settings/api. Venice hosts state of the art open-source AI models and supports the OpenAI API standard, allowing users to easily interact with the platform. Learn more about the Venice API at https://venice.ai/api.\n\nChange `~/.continue/config.json` to look like the following.\n\n```json title=\"config.json\"\n{\n  \"models\": [\n    {\n      \"provider\": \"venice\",\n      \"title\": \"Autodetect\",\n      \"model\": \"AUTODETECT\",\n      \"apiKey\": \"...\"\n    }\n  ]\n}\n```\n\nTo utilize features offered by the venice API that are not apart of the standard OpenAI API schema, include these as `venice_parameters`\n\nFor example, to use models with no system prompt, you can explicitly turn off the feature like so:\n\n```json title=\"config.json\"\n{\n  \"completionOptions\": {\n    \"venice_parameters\": {\n        \"include_venice_system_prompt\" : false\n      },\n    ...\n  }\n}\n```\n\nLearn more about available settings [here](https://docs.venice.ai/api-reference/api-spec).\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/lemonade.mdx","content":"---\ntitle: \"Lemonade Server\"\ndescription: \"Configure Lemonade Server with Continue for refreshingly fast local LLM inference on GPUs and NPUs\"\n---\n\n<Info>\n  Get started with [Lemonade Server](https://lemonade-server.ai/) - Refreshingly fast LLMs on GPUs and NPUs with seamless Continue integration\n</Info>\n\n## Overview\n\nLemonade Server provides optimized local LLM inference with support for GPU and NPU hardware acceleration. It offers an OpenAI-compatible API that seamlessly integrates with Continue and other open-source platforms.\n\n## Installation\n\nDownload and install Lemonade Server from [lemonade-server.ai](https://lemonade-server.ai/).\n\n## Configuration\n\nLemonade Server is available directly in the Continue UI as a provider. You can select it from the model provider dropdown without manual configuration.\n\n### Option 1: Using the Continue UI (Recommended)\n\n1. Click on the model selector dropdown in Continue\n2. Select \"Add Model\"\n3. Choose \"Lemonade Server\" from the provider list\n4. Continue will automatically configure the connection\n\n### Option 2: Manual Configuration\n\nIf you need custom settings, you can manually configure Lemonade:\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Lemonade\n        provider: lemonade\n        model: <MODEL_NAME>\n        apiBase: http://localhost:8000/api/v1/\n    ```\n    </Tab>\n    <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Lemonade\",\n          \"provider\": \"lemonade\",\n          \"model\": \"<MODEL_NAME>\",\n          \"apiBase\": \"http://localhost:8000/api/v1/\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## Getting Started\n\n1. **Install Lemonade Server**: Download from [lemonade-server.ai](https://lemonade-server.ai/)\n2. **Start the server**: Launch Lemonade Server (runs on `http://localhost:8000/api/v1/` by default)\n3. **Add to Continue**: Select Lemonade Server from the model provider dropdown in Continue\n4. **Load a model**: Choose your preferred model through the interface\n\n## Hardware Support\n\nLemonade Server automatically detects and optimizes for available hardware:\n- **NPU**: Neural Processing Unit acceleration for efficient inference\n- **GPU**: Full GPU acceleration support\n- **CPU**: Optimized CPU fallback when accelerators are unavailable\n\n## Key Features\n\n- OpenAI-compatible API for seamless integration\n- Support for popular model formats\n- Automatic hardware detection and optimization\n- Integration with Continue, Open WebUI, Gaia, and AnythingLLM\n- Active open-source community\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Lemonade.ts)"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/groq.mdx","content":"---\ntitle: \"How to Configure Groq with Continue\"\nsidebarTitle: \"Groq\"\n---\n\n<Info>\n  Get your API key from the [Groq Console](https://console.groq.com/docs/models)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: groq\n      model: <MODEL_ID>\n      apiKey: <YOUR_GROQ_API_KEY>\"\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"groq\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_GROQ_API_KEY>\"\",\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/novita.mdx","content":"---\ntitle: \"Novita\"\ndescription: \"Configure Novita AI with Continue to access their affordable and reliable inference platform for language models like Llama 3.1, offering scalable LLM API services\"\n---\n\n[Novita AI](https://novita.ai?utm_source=github_continuedev&utm_medium=github_readme&utm_campaign=github_link) offers an affordable, reliable, and simple inference platform with scalable [LLM API](https://novita.ai/docs/model-api/reference/introduction.html), empowering developers to build AI applications. Try the [Novita AI Llama 3 API Demo](https://novita.ai/model-api/product/llm-api/playground/meta-llama-llama-3.1-70b-instruct?utm_source=github_continuedev&utm_medium=github_readme&utm_campaign=github_link) today!. You can sign up [here](https://novita.ai/user/login?&redirect=/&utm_source=github_continuedev&utm_medium=github_readme&utm_campaign=github_link), copy your API key on the [Key Management](https://novita.ai/settings/key-management?utm_source=github_continuedev&utm_medium=github_readme&utm_campaign=github_link), and then hit the play button on any model from the [Novita AI Models list](https://novita.ai/llm-api?utm_source=github_continuedev&utm_medium=github_readme&utm_campaign=github_link). Change `~/.continue/config.json` to look like this:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Llama 3.1 8B\n      provider: novita\n      model: meta-llama/llama-3.1-8b-instruct\n      apiKey: <YOUR_NOVITA_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Llama 3.1 8B\",\n        \"provider\": \"novita\",\n        \"model\": \"meta-llama/llama-3.1-8b-instruct\",\n        \"apiKey\": \"<YOUR_NOVITA_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Novita.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/textgenwebui.mdx","content":"---\ntitle: \"Text Generation WebUI\"\ndescription: \"Configure Text Generation WebUI with Continue to use its comprehensive open-source language model UI and local server through its OpenAI-compatible API\"\n---\n\nTextGenWebUI is a comprehensive, open-source language model UI and local server. You can set it up with an OpenAI-compatible server plugin, and then configure it like this:\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Text Generation WebUI\n        provider: text-gen-webui\n        apiBase: http://localhost:5000/v1\n        model: MODEL_NAME\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Text Generation WebUI\",\n          \"provider\": \"text-gen-webui\",\n          \"apiBase\": \"http://localhost:5000/v1\",\n          \"model\": \"MODEL_NAME\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/cohere.mdx","content":"---\ntitle: \"Cohere\"\ndescription: \"Configure Cohere's AI models with Continue, including setup for Command A for chat, embed-v4.0 for embeddings, and rerank-v3.5 for reranking capabilities\"\n---\n\nBefore using Cohere, visit the [Cohere dashboard](https://dashboard.cohere.com/api-keys) to create an API key.\n\n## Chat model\n\nWe recommend configuring **Command A** as your chat model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Command A 03-2025\n      provider: cohere\n      model: command-a-03-2025\n      apiKey: <YOUR_COHERE_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Command A 03-2025\",\n        \"provider\": \"cohere\",\n        \"model\": \"command-a-03-2025\",\n        \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Autocomplete model\n\nCohere currently does not offer any autocomplete models.\n\n[Click here](../../model-roles/autocomplete) to see a list of autocomplete model providers.\n\n## Embeddings model\n\nWe recommend configuring **embed-v4.0** as your embeddings model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Cohere Embed v4.0\n      provider: cohere\n      model: embed-v4.0\n      apiKey: <YOUR_COHERE_API_KEY>\n      roles:\n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"cohere\",\n      \"model\": \"embed-v4.0\",\n      \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Reranking model\n\nWe recommend configuring **rerank-v3.5** as your reranking model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Cohere Rerank v3.5\n      provider: cohere\n      model: rerank-v3.5\n      apiKey: <YOUR_COHERE_API_KEY>\n      roles:\n        - rerank\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"reranker\": {\n      \"name\": \"cohere\",\n      \"params\": {\n        \"model\": \"rerank-v3.5\",\n        \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n      }\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/openvino_model_server.mdx","content":"---\ntitle: \"OpenVINO Model Server\"\ndescription: \"Configure OpenVINO Model Server with Continue to use Intel-optimized models for CPU, iGPU, GPU and NPU via the OpenAI-compatible API, supporting code completion with models like CodeLlama and Qwen\"\n---\n\n<Info>\n  [**OpenVINO‚Ñ¢ Mode Server**](https://github.com/openvinotoolkit/model_server)\n  is scalable inference server for models optimized with OpenVINO‚Ñ¢ for Intel\n  CPU, iGPU, GPU and NPU.\n</Info>\n\nOpenVINO‚Ñ¢ Mode Server supports text generation via OpenAI Chat Completions API. Simply select OpenAI provider to point `apiBase` to running OVMS instance. Refer [to this demo](https://docs.openvino.ai/2025/model-server/ovms_demos_code_completion_vsc.html) on official OVMS documentation to easily set up your own local server.\n\nExample configuration once OVMS is launched:\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 0.0.1\nschema: v1\n\nmodels:\n  - name: OVMS CodeLlama-7b-Instruct-hf\n    provider: openai\n    model: codellama/CodeLlama-7b-Instruct-hf\n    apiKey: unused\n    apiBase: http://localhost:5555/v3\n    roles:\n      - chat\n      - edit\n      - apply\n  - name: OVMS Qwen2.5-Coder-1.5B\n    provider: openai\n    model: Qwen/Qwen2.5-Coder-1.5B\n    apiKey: unused\n    apiBase: http://localhost:5555/v3\n    roles:\n      - autocomplete\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/llamastack.mdx","content":"---\ntitle: \"Llama Stack\"\nkeywords: [llama, stack, local, api, inference]\n---\n\n<Info>\n  Get started with [Lllama Stack](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html)\n</Info>\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: llamastack\n      model: <MODEL_ID>\n      apiBase: http://<LLAMA_STACK_ENDPOINT>/v1/openai/v1/\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"llamastack\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiBase\": \"http://<LLAMA_STACK_ENDPOINT>/v1/openai/v1/\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/vllm.mdx","content":"---\ntitle: \"vLLM\"\ndescription: \"Configure vLLM's high-performance inference library with Continue for chat, autocomplete, and embeddings, including setup instructions for Llama3.1, Qwen2.5-Coder, and Nomic Embed models\"\n---\n\nvLLM is an open-source library for fast LLM inference which typically is used to serve multiple users at the same time. It can also be used to run a large model on multiple GPU:s (e.g. when it doesn¬¥t fit in a single GPU). Run their OpenAI-compatible server using `vllm serve`. See their [server documentation](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html) and the [engine arguments documentation](https://docs.vllm.ai/en/latest/usage/engine_args.html).\n\n```shell\nvllm serve meta-llama/Meta-Llama-3.1-8B-Instruct\n```\n\n## Chat model\n\nWe recommend configuring **Llama3.1 8B** as your chat model.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Llama3.1 8B Instruct\n        provider: vllm\n        model: meta-llama/Meta-Llama-3.1-8B-Instruct\n        apiBase: http://<vllm chat endpoint>/v1\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Llama3.1 8B Instruct\",\n          \"provider\": \"vllm\",\n          \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n          \"apiBase\": \"http://<vllm chat endpoint>/v1\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## Autocomplete model\n\nWe recommend configuring **Qwen2.5-Coder 1.5B** as your autocomplete model.\n\n<Tabs>\n   <Tab title=\"YAML\">\n   ```yaml title=\"config.yaml\"\n   name: My Config\n   version: 0.0.1\n   schema: v1\n\n   models:\n     - name: Qwen2.5-Coder 1.5B\n       provider: vllm\n       model: Qwen/Qwen2.5-Coder-1.5B\n       apiBase: http://<vllm autocomplete endpoint>/v1\n       roles:\n         - autocomplete\n   ```\n   </Tab>\n   <Tab title=\"JSON\">\n   ```json title=\"config.json\"\n   {\n     \"tabAutocompleteModel\": {\n        \"title\": \"Qwen2.5-Coder 1.5B\",\n        \"provider\": \"vllm\",\n        \"model\": \"Qwen/Qwen2.5-Coder-1.5B\",\n        \"apiBase\": \"http://<vllm autocomplete endpoint>/v1\"\n     }\n   }\n   ```\n   </Tab>\n</Tabs>\n\n## Embeddings model\n\nWe recommend configuring **Nomic Embed Text** as your embeddings model.\n\n<Tabs>\n   <Tab title=\"YAML\">\n   ```yaml title=\"config.yaml\"\n   name: My Config\n   version: 0.0.1\n   schema: v1\n\n   models:\n     - name: VLLM Nomad Embed Text \n       provider: vllm\n       model: nomic-ai/nomic-embed-text-v1\n       apiBase: http://<vllm embed endpoint>/v1\n       roles:\n         - embed\n   ```\n   </Tab>\n   <Tab title=\"JSON\">\n   ```json title=\"config.json\"\n   {\n     \"embeddingsProvider\": {\n       \"provider\": \"vllm\",\n       \"model\": \"nomic-ai/nomic-embed-text-v1\",\n       \"apiBase\": \"http://<vllm embed endpoint>/v1\"\n     }\n   }\n   ```\n   </Tab>\n</Tabs>\n\n## Reranking model\n\nContinue automatically handles vLLM's response format (which uses `results` instead of `data`).\n\n[Click here](../../model-roles/reranking) to see a list of reranking model providers.\n\nThe continue implementation uses [OpenAI](../top-level/openai) under the hood. [View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Vllm.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/moonshot.mdx","content":"---\ntitle: \"Moonshot AI\"\ndescription: \"Configure Moonshot AI's language models with Continue, including setup instructions for their 8K, 32K, and 128K context window models with competitive pricing\"\n---\n\n[Moonshot AI](https://platform.moonshot.cn/) provides high-quality large language model services with competitive pricing and excellent performance.\n\n## Configuration\n\nTo use Moonshot AI models, you need to:\n\n1. Get an API key from [Moonshot AI Platform](https://platform.moonshot.cn/)\n2. Add the following configuration:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Moonshot\n      provider: moonshot\n      model: moonshot-v1-8k\n      apiKey: <YOUR_MOONSHOT_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Moonshot\",\n        \"provider\": \"moonshot\",\n        \"model\": \"moonshot-v1-8k\",\n        \"apiKey\": \"<YOUR_MOONSHOT_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Available Models\n\nMoonshot AI currently provides the following models:\n\n- `moonshot-v1-8k`: Base model with 8K context window\n- `moonshot-v1-32k`: Base model with 32K context window\n- `moonshot-v1-128k`: Base model with 128K context window\n\n## Configuration Options\n\n| Option    | Description       | Default                       |\n| --------- | ----------------- | ----------------------------- |\n| `apiKey`  | Moonshot API key  | Required                      |\n| `apiBase` | API base URL      | `https://api.moonshot.cn/v1/` |\n| `model`   | Model name to use | `moonshot-v1-8k`              |\n\n## Example\n\nHere's a complete configuration example:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Moonshot-8K\n      provider: moonshot\n      model: moonshot-v1-8k\n      apiKey: <YOUR_MOONSHOT_API_KEY>\n      defaultCompletionOptions:\n        temperature: 0.7\n        topP: 0.95\n        maxTokens: 2048\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Moonshot-8K\",\n        \"provider\": \"moonshot\",\n        \"model\": \"moonshot-v1-8k\",\n        \"apiKey\": \"<YOUR_MOONSHOT_API_KEY>\",\n        \"completionOptions\": {\n          \"temperature\": 0.7,\n          \"topP\": 0.95,\n          \"maxTokens\": 2048\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/deepseek.mdx","content":"---\ntitle: DeepSeek\nslug: ../deepseek\n---\n\n<Tip>\n  **Discover DeepSeek models [here](https://hub.continue.dev/hub?q=DeepSeek)**\n</Tip>\n\n<Info>\n  You can get an API key from the [DeepSeek Console](https://www.deepseek.com/).\n</Info>\n\n## Confiugration\n\n<Tabs>\n   <Tab title=\"YAML\">\n   ```yaml title=\"config.yaml\"\n   name: My Config\n   version: 0.0.1\n   schema: v1\n\n   models:\n     - name: <MODEL_NAME>\n       provider: deepseek\n       model: <MODEL_ID>\n       apiKey: <YOUR_DEEPSEEK_API_KEY>\n   ```\n   </Tab>\n   <Tab title=\"JSON\">\n   ```json title=\"config.json\"\n   {\n     \"models\": [\n       {\n         \"title\": \"<MODEL_NAME>\",\n         \"provider\": \"deepseek\",\n         \"model\": \"<MODEL_ID>\",\n         \"apiKey\": \"<YOUR_DEEPSEEK_API_KEY>\"\n       }\n     ]\n   }\n   ```\n   </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/nebius.mdx","content":"---\ntitle: \"Nebius\"\ndescription: \"Configure Nebius AI Studio with Continue to access their language model offerings, including DeepSeek R1 for chat and BAAI embeddings models\"\n---\n\nYou can get an API key from the [Nebius AI Studio API keys page](https://studio.nebius.ai/settings/api-keys)\n\n## Availible models\n\nAvailable models can be found on the [Nebius AI Studio models page](https://studio.nebius.ai/models/text2text)\n\n## Chat model\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: DeepSeek R1\n      provider: nebius\n      model: deepseek-ai/DeepSeek-R1\n      apiKey: <YOUR_NEBIUS_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"DeepSeek R1\",\n        \"provider\": \"nebius\",\n        \"model\": \"deepseek-ai/DeepSeek-R1\",\n        \"apiKey\": \"<YOUR_NEBIUS_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Embeddings model\n\nAvailable models can be found on the [Nebius AI Studio embeddings page](https://studio.nebius.ai/models/embedding)\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: BAAI Embedder\n      provider: nebius\n      model: BAAI/bge-en-icl\n      apiKey: <YOUR_NEBIUS_API_KEY>\n      roles:\n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"nebius\",\n      \"model\": \"BAAI/bge-en-icl\",\n      \"apiKey\": \"<YOUR_NEBIUS_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/scaleway.mdx","content":"---\ntitle: \"Scaleway\"\ndescription: \"Configure Scaleway Generative APIs with Continue to access AI models hosted in European data centers, offering low latency, data privacy, and EU AI Act compliance with models like Qwen2.5-Coder and BGE-Multilingual-Gemma2\"\n---\n\n<Info>\n  Scaleway Generative APIs give you instant access to leading AI models hosted\n  in European data centers, ideal for developers requiring low latency, full\n  data privacy, and compliance with EU AI Act. You can generate your API key in\n  [Scaleway's console](https://console.scaleway.com/generative-api/models). Read\n  the [quickstart documentation\n  here](https://www.scaleway.com/en/docs/ai-data/generative-apis/quickstart/).\n</Info>\n\n## Chat model\n\nWe recommend configuring **Qwen2.5-Coder-32B-Instruct** as your chat model.\n[Click here](https://www.scaleway.com/en/docs/ai-data/generative-apis/reference-content/supported-models/) to see the list of available chat models.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Qwen2.5-Coder-32B-Instruct\n        provider: scaleway\n        model: qwen2.5-coder-32b-instruct\n        apiKey: <YOUR_SCALEWAY_API_KEY>\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Qwen2.5-Coder-32B-Instruct\",\n          \"provider\": \"scaleway\",\n          \"model\": \"qwen2.5-coder-32b-instruct\",\n          \"apiKey\": \"<YOUR_SCALEWAY_API_KEY>\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## Autocomplete model\n\nScaleway currently does not offer any autocomplete models.\n\n[Click here](../../model-roles/autocomplete) to see a list of autocomplete model providers.\n\n## Embeddings model\n\nWe recommend configuring **BGE-Multilingual-Gemma2** as your embeddings model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: BGE Multilingual Gemma2\n      provider: scaleway\n      model: bge-multilingual-gemma2\n      apiKey: <YOUR_SCALEWAY_API_KEY>\n      roles: \n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"scaleway\",\n      \"model\": \"bge-multilingual-gemma2\",\n      \"apiKey\": \"<YOUR_SCALEWAY_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/function-network.mdx","content":"---\ntitle: \"Function Network\"\ndescription: \"Configure Function Network with Continue to access private and affordable AI models, including Llama 3.1, Qwen2.5-Coder, and various embedding models for a user-owned AI experience\"\n---\n\n> Private, Affordable User-Owned AI\n\n<Info>\n\nTo get an API key, login to the Function Network Developer Platform. If you don't have an account, you can create one [here](https://www.function.network/join-waitlist).\n\n</Info>\n\n## Chat model\n\nFunction Network supports a number of models for chat. We recommend using LLama 3.1 70b or Qwen2.5-Coder-32B-Instruct.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Llama 3.1 70b\n      provider: function-network\n      model: meta/llama-3.1-70b-instruct\n      apiKey: <YOUR_FUNCTION_NETWORK_API_KEY>\n      roles:\n        - chat\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Llama 3.1 70b\",\n        \"provider\": \"function-network\",\n        \"model\": \"meta/llama-3.1-70b-instruct\",\n        \"apiKey\": \"<YOUR_FUNCTION_NETWORK_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[Click here](https://docs.function.network/models-supported/chat-and-code-completion) to see a list of chat model providers.\n\n## Autocomplete model\n\nFunction Network supports a number of models for autocomplete. We recommend using Llama 3.1 8b or Qwen2.5-Coder-1.5B.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Deepseek Coder 6.7b\n      provider: function-network\n      model: thebloke/deepseek-coder-6.7b-base-awq\n      apiKey: <YOUR_FUNCTION_NETWORK_API_KEY>\n      roles:\n        - autocomplete\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"tabAutocompleteModel\": {\n      \"title\": \"Deepseek Coder 6.7b\",\n      \"provider\": \"function-network\",\n      \"model\": \"thebloke/deepseek-coder-6.7b-base-awq\",\n      \"apiKey\": \"<YOUR_FUNCTION_NETWORK_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Embeddings model\n\nFunction Network supports a number of models for embeddings. We recommend using baai/bge-base-en-v1.5.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: BGE Base En\n      provider: function-network\n      model: baai/bge-base-en-v1.5\n      apiKey: <YOUR_FUNCTION_NETWORK_API_KEY>\n      roles:\n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"function-network\",\n      \"model\": \"baai/bge-base-en-v1.5\",\n      \"apiKey\": \"<YOUR_FUNCTION_NETWORK_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[Click here](https://docs.function.network/models-supported/embeddings) to see a list of embeddings model providers.\n\n## Reranking model\n\nFunction Network currently does not offer any reranking models.\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/cerebras.mdx","content":"---\ntitle: \"Cerebras\"\ndescription: \"Configure Cerebras Inference with Continue for fast model inference using specialized silicon, including setup instructions for Llama 3.1 70B model\"\n---\n\nCerebras Inference uses specialized silicon to provides fast inference.\n\n1. Create an account in the portal [here](https://cloud.cerebras.ai/).\n2. Create and copy the API key for use in Continue.\n3. Update your Continue config file:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Cerebras Llama 3.1 70B\n      provider: cerebras\n      model: llama3.1-70b\n      apiKey:  <YOUR_CEREBRAS_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Cerebras Llama 3.1 70B\",\n        \"provider\": \"cerebras\",\n        \"model\": \"llama3.1-70b\",\n        \"apiKey\": \"<YOUR_CEREBRAS_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/together.mdx","content":"---\ntitle: \"Together AI\"\nsidebarTitle: \"Together AI\"\n---\n\n<Tip>\n  **Discover Together AI models [here](https://hub.continue.dev/togetherai)**\n</Tip>\n\n<Info>\n  Get an API key from the [Together AI](https://api.together.ai)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: together\n      model: <MODEL_ID>\n      apiKey: <YOUR_TOGETHER_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"together\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_TOGETHER_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/togetherai/qwen3-coder-480b-a35b-instruct-fp8?view=config)**\n</Info>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/mistral.mdx","content":"---\ntitle: Mistral\nslug: ../mistral\n---\n\n<Tip>\n  **Discover Mistral models [here](https://hub.continue.dev/mistral)**\n</Tip>\n\n<Info>\n  Get an API key from the [Mistral Dashboard](https://console.mistral.ai)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: mistral\n      model: <MODEL_ID>\n      apiKey: <YOUR_MISTRAL_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"mistral\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_MISTRAL_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/mistral/codestral?view=config)**\n</Info>\n\n<Note>\n  The API key for `codestral.mistral.ai` is different from `api.mistral.ai`. \n  If you are using a Codestral API key, you should set the `apiBase` to `https://codestral.mistral.ai/v1`.\n  Otherwise, we will default to using `https://api.mistral.ai/v1`.\n</Note>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/ipex_llm.mdx","content":"---\ntitle: \"Intel Extension for PyTorch\"\ndescription: \"Configure Intel Extension for PyTorch (IPEX-LLM) with Continue to run language models with very low latency on Intel CPUs and GPUs, leveraging accelerated Ollama backend\"\n---\n\n<Info>\n  [**IPEX-LLM**](https://github.com/intel-analytics/ipex-llm) is a PyTorch\n  library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU,\n  discrete GPU such as Arc A-Series, Flex and Max) with very low latency.\n</Info>\n\nIPEX-LLM supports accelerated Ollama backend to be hosted on Intel GPU. Refer to [this guide](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html) from IPEX-LLM official documentation about how to install and run Ollama serve accelerated by IPEX-LLM on Intel GPU. You can then configure Continue to use the IPEX-LLM accelerated `\"ollama\"` provider as follows:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: IPEX-LLM\n      provider: ollama\n      model: AUTODETECT\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"IPEX-LLM\",\n        \"provider\": \"ollama\",\n        \"model\": \"AUTODETECT\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\nIf you would like to reach the Ollama service from another machine, make sure you set or export the environment variable `OLLAMA_HOST=0.0.0.0` before executing the command `ollama serve`. Then, in the Continue configuration, set `'apiBase'` to correspond with the IP address / port of the remote machine. That is, Continue can be configured to be:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: IPEX-LLM\n      provider: ollama\n      model: AUTODETECT\n      apiBase: http://your-ollama-service-ip:11434\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"IPEX-LLM\",\n        \"provider\": \"ollama\",\n        \"model\": \"AUTODETECT\",\n        \"apiBase\": \"http://your-ollama-service-ip:11434\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  If you would like to preload the model before your first conversation with\n  that model in Continue, you could refer to\n  [here](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/continue_quickstart.html#pull-and-prepare-the-model)\n  for more information.\n</Tip>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/nvidia.mdx","content":"---\ntitle: \"NVIDIA\"\n---\n\n<Info>\n  Get an API key from the [NVIDIA](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html#option-1-from-api-catalog)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: nvidia\n      model: <MODEL_ID>\n      apiKey: <YOUR_NVIDIA_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"nvidia\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_NVIDIA_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/asksage.mdx","content":"---\ntitle: \"Ask Sage\"\nslug: ../asksage\n---\n\n<Tip>\n  **Discover Ask Sage models [here](https://hub.continue.dev/hub?q=Ask%20Sage)**\n</Tip>\n\n<Info>\n  You can get an API key from the [Ask Sage](https://www.asksage.ai/).\n</Info>\n\n\n## Overview\n\nAsk Sage provides secure, government-compliant access to LLMs. This guide explains how to set up and configure Ask Sage models, including support for DoD certificates.\n\n## 1. Prerequisites\n\n- **Ask Sage Account:**  \n  Sign up or log in at [Ask Sage Platform](https://chat.asksage.ai/).\n\n- **API Key:**  \n  Follow the [API Key Documentation](https://docs.asksage.ai/docs/api-documentation/api-documentation.html) to generate your key.\n\n\n## 2. Configuration\n\nAdd your Ask Sage model to your Continue configuration file.\n\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: GPT-4 gov\n      provider: askSage\n      model: gpt4-gov\n      apiBase: https://api.asksage.ai/server/\n      apiKey: <YOUR_ASK_SAGE_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"GPT-4 gov\",\n        \"provider\": \"askSage\",\n        \"model\": \"gpt4-gov\",\n        \"apiBase\": \"https://api.asksage.ai/server/\",\n        \"apiKey\": \"<YOUR_ASK_SAGE_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## 3. Using DoD Certificates\n\nFor secure environments, specify your DoD CA bundle path:\n\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: GPT-4 gov\n      provider: askSage\n      model: gpt4-gov\n      apiBase: https://api.asksage.ai/server/\n      apiKey: <YOUR_ASK_SAGE_API_KEY>\n      requestOptions:\n        caBundlePath: /path/to/dod/certificates\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"GPT-4 gov\",\n        \"provider\": \"askSage\",\n        \"model\": \"gpt4-gov\",\n        \"apiBase\": \"https://api.asksage.ai/server/\",\n        \"apiKey\": \"<YOUR_ASK_SAGE_API_KEY>\",\n        \"requestOptions\": {\n          \"caBundlePath\": \"/path/to/dod/certificates\"\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\nReplace `/path/to/dod/certificates` with your actual CA bundle file path.\n\n---\n\n## 4. Final Steps\n\n- Save your configuration file.\n- Restart Continue to apply changes.\n\nYour Ask Sage model will now be available in Continue.\n\n## 5. Usage\n\nSupported features:\n\n- **Chat:** Interact and iterate on code in the sidebar.\n- **Edit:** Modify code in place.\n\n## 6. Support\n\n- Refer to [Ask Sage Docs](https://docs.asksage.ai/docs/api-documentation/api-documentation.html) for more details.\n- Email the Ask Sage support team for assistance at support@asksage.ai.\n\n---\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/deepinfra.mdx","content":"---\ntitle: \"DeepInfra\"\ndescription: \"Configure DeepInfra with Continue to access low-cost inference for open-source models like Mixtral-8x7B-Instruct, including API setup instructions\"\n---\n\n<Tip>\n  **Discover Deep Infra models [here](https://hub.continue.dev/deepinfra)**\n</Tip>\n\n<Info>\n  Get an API key from the [Deep Infra](https://deepinfra.com/)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: deepinfra\n      model: <MODEL_ID>\n      apiKey: <YOUR_DEEPINFRA_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"deepinfra\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_DEEPINFRA_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/deepinfra/qwen-qwen2.5-coder-32b-instruct?view=config)**\n</Info>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/sagemaker.mdx","content":"---\ntitle: \"Amazon SageMaker\"\ndescription: \"Configure Amazon SageMaker with Continue to use deployed LLM endpoints for both chat and embedding models, supporting LMI and HuggingFace TEI deployments with AWS credentials\"\n---\n\nSageMaker can be used for both chat and embedding models. Chat models are supported for endpoints deployed with [LMI](https://docs.djl.ai/docs/serving/serving/docs/lmi/index.html), and embedding models are supported for endpoints deployed with [HuggingFace TEI](https://huggingface.co/blog/sagemaker-huggingface-embedding)\n\nHere is an example Sagemaker configuration setup:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: deepseek-6.7b-instruct\n      provider: sagemaker\n      model: lmi-model-deepseek-coder-xxxxxxx\n      region: us-west-2\n      roles:\n        - chat\n    - name: mxbai-embed\n      provider: sagemaker\n      model: mxbai-embed-large-v1-endpoint\n      roles:\n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"deepseek-6.7b-instruct\",\n        \"provider\": \"sagemaker\",\n        \"model\": \"lmi-model-deepseek-coder-xxxxxxx\",\n        \"region\": \"us-west-2\"\n      }\n    ],\n    \"embeddingsProvider\": {\n      \"provider\": \"sagemaker\",\n      \"model\": \"mxbai-embed-large-v1-endpoint\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\nThe value in model should be the SageMaker endpoint name you deployed.\n\nAuthentication will be through temporary or long-term credentials in\n~/.aws/credentials under a profile called \"sagemaker\".\n\n```title=\"~/.aws/credentials\n[sagemaker]\naws_access_key_id = abcdefg\naws_secret_access_key = hijklmno\naws_session_token = pqrstuvwxyz # Optional: means short term creds.\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/llamacpp.mdx","content":"---\ntitle: \"Llama.cpp\"\n---\n\n<Info>\n  Get started with [Llama.cpp](https://github.com/ggml-org/llama.cpp?tab=readme-ov-file#quick-start)\n</Info>\n\n## Configuration\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: <MODEL_NAME>\n        provider: llama.cpp\n        model: <MODEL_ID>\n        apiBase: http://localhost:8080\n    ```\n    </Tab>\n    <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"<MODEL_NAME>\",\n          \"provider\": \"llama.cpp\",\n          \"model\": \"<MODEL_ID>\"\n          \"apiBase\": \"http://localhost:8080\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/kindo.mdx","content":"---\ntitle: \"Kindo\"\ndescription: \"Configure Kindo with Continue to centralize control over your organization's AI operations with support for commercial and open-source models while ensuring data protection and policy compliance\"\n---\n\nKindo offers centralized control over your organization's AI operations, ensuring data protection and compliance with internal policies while supporting various commercial and open-source models. To get started, sign up [here](https://app.kindo.ai/), create an API key in [Settings > API > API Keys](https://app.kindo.ai/settings/api), and choose a model from the list of supported models in the \"Available Models\" tab or copy and paste the config in [Plugins > Your Configuration](https://app.kindo.ai/plugins).\n\n## Config Example\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Claude 3.5 Sonnet\n      provider: kindo\n      model: claude-3-5-sonnet\n      apiKey: <YOUR_KINDO_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Claude 3.5 Sonnet\",\n        \"provider\": \"kindo\",\n        \"model\": \"claude-3-5-sonnet\",\n        \"apiKey\": \"<YOUR_KINDO_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Tab Autocomplete Config Example\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: WhiteRabbitNeo\n      provider: kindo\n      model: /models/WhiteRabbitNeo-33B-DeepSeekCoder\n      apiKey: <YOUR_KINDO_API_KEY>\n      roles:\n        - autocomplete\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"tabAutocompleteModel\": [\n      {\n        \"title\": \"WhiteRabbitNeo\",\n        \"provider\": \"kindo\",\n        \"model\": \"/models/WhiteRabbitNeo-33B-DeepSeekCoder\",\n        \"apiKey\": \"<YOUR_KINDO_API_KEY>\",\n        \"template\": \"none\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Security\n\nTo update your organization's model access, adjust the controls in [security settings](https://app.kindo.ai/security-settings).\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/SambaNova.mdx","content":"---\ntitle: \"SambaNova\"\ndescription: \"Configure SambaCloud with Continue to access their high-performance platform for running large AI models, including Llama 4 Scout with world record open source model performance\"\n---\n\nThe SambaNova Cloud is a cloud platform for running large AI models with the world record open source models performance. You can follow the instructions in [this blog post](https://sambanova.ai/blog/accelerating-coding-with-sambanova-cloud?ref=blog.continue.dev) to configure your setup.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: GPT OSS 120B\n      provider: sambanova\n      model: gpt-oss-120b\n      apiKey: <YOUR_SAMBANOVA_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"GPT OSS 120B\",\n        \"provider\": \"sambanova\",\n        \"model\": \"gpt-oss-120b\",\n        \"apiKey\": \"<YOUR_SAMBANOVA_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/SambaNova.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/msty.mdx","content":"---\ntitle: \"Msty\"\ndescription: \"Configure Msty with Continue to easily run both online and local open-source models like Llama-2 and DeepSeek Coder through their user-friendly application for Windows, Mac, and Linux\"\n---\n\n[Msty](https://msty.app/) is an application for Windows, Mac, and Linux that makes it really easy to run online as well as local open-source models, including Llama-2, DeepSeek Coder, etc. No need to fidget with your terminal, run a command, or anything. Just download the app from the website, click a button, and you are up and running. Continue can then be configured to use the `Msty` LLM class:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Msty\n      provider: msty\n      model: deepseek-coder:6.7b\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Msty\",\n        \"provider\": \"msty\",\n        \"model\": \"deepseek-coder:6.7b\",\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Completion Options\n\nIn addition to the model type, you can also configure some of the parameters that Msty uses to run the model.\n\n- temperature: options.temperature - This is a parameter that controls the randomness of the generated text. Higher values result in more creative but potentially less coherent outputs, while lower values lead to more predictable and focused outputs.\n- top_p: options.topP - This sets a threshold (between 0 and 1) to control how diverse the predicted tokens should be. The model generates tokens that are likely according to their probability distribution, but also considers the top-k most probable tokens.\n- top_k: options.topK - This parameter limits the number of unique tokens to consider when generating the next token in the sequence. Higher values increase the variety of generated sequences, while lower values lead to more focused outputs.\n- num_predict: options.maxTokens - This determines the maximum number of tokens (words or characters) to generate for the given input prompt.\n- num_thread: options.numThreads - This is the multi-threading configuration option that controls how many threads the model uses for parallel processing. Higher values may lead to faster generation times but could also increase memory usage and complexity. Set this to one or two lower than the number of threads your CPU can handle to leave some for your GUI when running the model locally.\n- use_mmap: options.useMmap - For Ollama, this parameter allows the model to be mapped into memory. If disabled can enhance response time on low end devices but will slow down the stream.\n\n## Authentication\n\nIf you need to send custom headers for authentication, you may use the `requestOptions.headers` property like this:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Msty\n      provider: msty\n      model: deepseek-coder:6.7b\n      requestOptions:\n        headers:\n          Authorization: Bearer xxx\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Msty\",\n        \"provider\": \"msty\",\n        \"model\": \"deepseek-coder:6.7b\",\n        \"requestOptions\": {\n          \"headers\": {\n            \"Authorization\": \"Bearer xxx\"\n          }\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Msty.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/ovhcloud.mdx","content":"---\ntitle: \"OVHcloud\"\ndescription: \"Configure OVHcloud AI Endpoints with Continue to access their GDPR-compliant serverless inference API for models like Qwen, Llama, Mistral, and Deepseek, with strong security and data privacy features\"\n---\n\nOVHcloud AI Endpoints is a serverless inference API that provides access to a curated selection of models (e.g., Llama, Mistral, Qwen, Deepseek). It is designed with security and data privacy in mind and is compliant with GDPR.\n\n<Info>\n  To get started, create an API key on the OVHcloud [AI Endpoints\n  website](https://endpoints.ai.cloud.ovh.net/). For more information, including\n  pricing, visit the OVHcloud [AI Endpoints product\n  page](https://www.ovhcloud.com/en/public-cloud/ai-endpoints/).\n</Info>\n\n## Chat model\n\nWe recommend configuring **Qwen2.5-Coder-32B-Instruct** as your chat model.\nCheck our [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to see all of our models hsoted on AI Endpoints.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Qwen2.5-Coder-32B-Instruct\n        provider: ovhcloud\n        model: qwen2.5-coder-32b\n        apiKey: <YOUR_AIENDPOINTS_API_KEY>\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"Qwen2.5-Coder-32B-Instruct\",\n          \"provider\": \"ovhcloud\",\n          \"model\": \"qwen2.5-coder-32b\",\n          \"apiKey\": \"<YOUR_AIENDPOINTS_API_KEY>\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## Embeddings model\n\nWe recommend configuring **bge-multilingual-gemma2** as your embeddings model.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: BGE Multilingual Gemma2\n      provider: ovhcloud\n      model: bge-multilingual-gemma2\n      apiKey: <YOUR_AIENDPOINTS_API_KEY>\n      roles: \n        - embed\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"ovhcloud\",\n      \"model\": \"bge-multilingual-gemma2\",\n      \"apiKey\": \"<YOUR_AIENDPOINTS_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/more/llamafile.mdx","content":"---\ntitle: \"Llamafile\"\ndescription: \"Configure Llamafile with Continue to use self-contained binary files that can run open-source language models like Mistral without additional setup\"\n---\n\nA [llamafile](https://github.com/Mozilla-Ocho/llamafile#readme) is a self-contained binary that can run an open-source LLM. You can configure this provider in your config.json as follows:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Llamafile\n      provider: llamafile\n      model: mistral-7b\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"Llamafile\",\n        \"provider\": \"llamafile\",\n        \"model\": \"mistral-7b\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n[View the source](https://github.com/continuedev/continue/blob/main/core/llm/llms/Llamafile.ts)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/lmstudio.mdx","content":"---\ntitle: \"LM Studio\"\n---\n\n<Tip>\n  **Discover LM Studio models [here](https://hub.continue.dev/lmstudio)**\n</Tip>\n\n<Info>\n  Get started with [LM Studio](https://lmstudio.ai/download)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: lmstudio\n      model: <MODEL_ID>\n      apiBase: http://<MY_ENDPOINT>/v1 # if running a remote instance of LM Studio\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"lmstudio\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiBase\": \"http://<MY_ENDPOINT>/v1\" // if running a remote instance of LM Studio\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Note>\n  The default `apiBase` is `http://localhost:1234/v1`\n</Note>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/lmstudio/qwen-qwen3-coder-30b?view=config)**\n</Info>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/azure.mdx","content":"---\ntitle: \"How to Configure Azure AI Foundry with Continue\"\nslug: ../azure\nsidebarTitle: \"Azure AI Foundry\"\n---\n\n<Info>\n  Create an Azure AI Foundry resource in the [Azure Portal](https://portal.azure.com)\n</Info>\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <Model_NAME>\n      provider: azure\n      model: <MODEL_ID>\n      apiBase: <YOUR_DEPLOYMENT_BASE>\n      apiKey: <YOUR_AZURE_API_KEY> # If you use subscription key, try using Azure gateway to rename it apiKey\n      env:\n        deployment: <YOUR_DEPLOYMENT_NAME>\n        apiType: azure-foundry # Or \"azure-openai\" if using OpenAI models\n        apiVersion: 2023-07-01-preview # Azure API version\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [{\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"azure\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiBase\": \"<YOUR_DEPLOYMENT_BASE>\",\n        \"deployment\": \"<YOUR_DEPLOYMENT_NAME>\",\n        \"apiKey\": \"<YOUR_AZURE_API_KEY>\", // If you use subscription key, try using Azure gateway to rename it apiKey\n        \"apiType\": \"azure-foundry\" // Or \"azure-openai\" if using OpenAI models\n    }]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Azure OpenAI Service (Alternative)\n\n<Info>\n  Get access to the Azure OpenAI service [here](https://azure.microsoft.com/en-us/products/ai-services/openai-service)\n</Info>\n\nAzure OpenAI Service requires a handful of additional parameters to be configured, such as a deployment name and API base URL.\n\nTo find this information in _Azure AI Foundry_, first select the model that you would like to connect. Then visit _Endpoint_ > _Target URI_.\n\nFor example, a Target URI of `https://just-an-example.openai.azure.com/openai/deployments/gpt-4o-july/chat/completions?api-version=2023-03-15-preview` would map to the following:\n\n<Tabs>\n\t<Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: <MODEL_NAME>\n        model: <MODEL_ID>\n        provider: azure\n        apiBase: https://just-an-example.openai.azure.com\n        apiKey: <YOUR_AZURE_API_KEY>\n        env:\n          apiVersion: <API_VERSION>\n          deployment: <MODEL_DEPLOYMENT>\n          apiType: azure-openai\n    ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"title\": \"<MODEL_NAME>\",\n      \"model\": \"<MODEL_ID>\",\n      \"provider\": \"azure\",\n      \"apiBase\": \"https://just-an-example.openai.azure.com\",\n      \"deployment\": \"<MODEL_DEPLOYMENT>\",\n      \"apiVersion\": \"<API_VERSION>\",\n      \"apiKey\": \"<YOUR_AZURE_API_KEY>\",\n      \"apiType\": \"azure-openai\"\n    }\n    ```\n  </Tab>\n</Tabs>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/openrouter.mdx","content":"---\ntitle: \"How to Configure OpenRouter with Continue\"\nsidebarTitle: \"OpenRouter\"\n---\n\n<Tip>\n  **Discover OpenRouter models [here](https://hub.continue.dev/openrouter)**\n</Tip>\n\n<Info>\n  Get an API key from [OpenRouter](https://openrouter.ai/keys)\n</Info>\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: openrouter\n      model: <MODEL_ID>\n      apiBase: https://openrouter.ai/api/v1\n      apiKey: <YOUR_OPEN_ROUTER_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"openrouter\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiBase\": \"https://openrouter.ai/api/v1\",\n        \"apiKey\": \"<YOUR_OPEN_ROUTER_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/openrouter/qwen3-coder?view=config)**\n</Info>\n\n## Optional configuration\n\nOpenRouter allows you configure provider preferences, model routing configuration, and more. You can set these via `requestOptions`.\n\nFor example, to prevent extra long prompts from being compressed, you can explicitly turn off [Transforms](https://openrouter.ai/docs/features/message-transforms):)\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: openrouter\n      model: <MODEL_ID>\n      requestOptions:\n        extraBodyProperties:\n          transforms: []\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"openrouter\",\n        \"model\": \"<MODEL_ID>\",\n        \"requestOptions\": {\n          \"extraBodyProperties\": {\n            \"transforms\": []\n          }\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Model Capabilities\n\nOpenRouter models may require explicit capability configuration because the proxy doesn't always preserve the function calling support of the original model. \n\n<Info>\n  Continue automatically uses system message tools for models that don't support\n  native function calling, so Agent mode should work even without explicit\n  capability configuration. However, you can still override capabilities if\n  needed.\n</Info>\n\nIf you're experiencing issues with Agent mode or tools not working, you can add the capabilities field:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: openrouter\n      model: <MODEL_ID>\n      apiBase: https://openrouter.ai/api/v1\n      apiKey: <YOUR_OPEN_ROUTER_API_KEY>\n      capabilities:\n        - tool_use      # Enable function calling for Agent mode\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"openrouter\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiBase\": \"https://openrouter.ai/api/v1\",\n        \"apiKey\": \"<YOUR_OPEN_ROUTER_API_KEY>\",\n        \"capabilities\": {\n          \"tools\": true, // Enable function calling for Agent mode\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Note>\n  Not all models support function calling. Check the [OpenRouter models page](https://openrouter.ai/models) for specific model capabilities.\n</Note>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/huggingfaceinference.mdx","content":"---\ntitle: \"Hugging Face\"\n---\n\nHugging Face is the main platform for sharing open AI models. It provides inference in two ways. [Inference Providers](https://huggingface.co/docs/inference-providers/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\n\n## Inference Providers\n\nInference Providers is a serverless service powered by external inference providers and routed through Hugging Face and paid per token.\n\n<Info>\n\nYou can access your access token from [Hugging Face](https://huggingface.co/settings/tokens) and prioritize your [providers in settings](https://huggingface.co/settings/inference-providers/overview).\n\n</Info>\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"  \n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: deepseek\n      provider: huggingface-inference-providers\n      model: deepseek-ai/DeepSeek-V3.2-Exp\n      apiKey: <YOUR_HF_TOKEN>\n      apiBase: https://router.huggingface.co/v1\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"deepseek\",\n        \"provider\": \"huggingface-inference-providers\",\n        \"model\": \"deepseek-ai/DeepSeek-V3.2-Exp\",\n        \"apiKey\": \"<YOUR_HF_TOKEN>\",\n        \"apiBase\": \"https://router.huggingface.co/v1\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## Inference Endpoints\n\nInference Endpoints is a dedicated service that allows you to run your open models dedicated hardware. It is a more advanced way to get inference from Hugging Face models where you have more control over the whole process.\n\n<Info>\n\nBefore you can use Inference Endpoints, you need to create an endpoint. You can do this by going to [Inference Endpoints](https://endpoints.huggingface.co/burtenshaw/endpoints/dedicated) and clicking on \"Create Endpoint\".\n\n</Info>\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: deepseek\n      provider: huggingface-inference-endpoints\n      model: <ENDPOINT_ID>\n      apiKey: <YOUR_HF_TOKEN>\n      apiBase: https://<YOUR_ENDPOINT_ID>.aws.endpoints.huggingface.cloud\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"deepseek\",\n        \"provider\": \"huggingface-inference-endpoints\",\n        \"model\": \"<ENDPOINT_ID>\",\n        \"apiKey\": \"<YOUR_HF_TOKEN>\",\n        \"apiBase\": \"https://<YOUR_ENDPOINT_ID>.aws.endpoints.huggingface.cloud\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/gemini.mdx","content":"---\ntitle: \"How to Configure Gemini with Continue\"\nslug: ../gemini\nsidebarTitle: \"Gemini\"\n---\n\n<Tip>\n  **Discover Google models [here](https://hub.continue.dev/hub?q=Gemini)**\n</Tip>\n\n<Info>\n  Get an API key from [Google AI Studio](https://aistudio.google.com/)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: gemini\n      model: <MODEL_ID>\n      apiKey: <YOUR_GEMINI_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\", \n        \"provider\": \"gemini\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_GEMINI_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/google/gemini-2.5-pro?view=config)**\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/tetrate_agent_router_service.mdx","content":"---\ntitle: \"Tetrate Agent Router Service\"\nsidebarTitle: \"Tetrate Agent Router Service\"\nslug: ../tetrate-agent-router-service\n---\nThe **Tetrate Agent Router Service** provides a unified Gateway for accessing various AI models with fast inference capabilities.\n\nThis gateway acts as an intelligent router that can distribute requests across multiple model providers, offering enterprise-grade reliability and performance optimization.\n\n<Tip>\nWant to get started quickly? Sign up for the [Tetrate Agent Router Service](https://router.tetrate.ai/) to get an API key, then use [Tetrate on Continue Mission Control](https://hub.continue.dev/tetrate) to get started fast.\n</Tip>\n\n## Setup\n\n<Steps>\n<Step title=\"Sign in or sign up\">\nVisit the [Agent Router Service portal](https://router.tetrate.ai/) and create an account to get your API key\n<img src=\"../assets/tetrate-sign-in.png\" alt=\"Tetrate sign in page\" style={{width: '250px', maxWidth: '100%'}} />\n</Step>\n\n<Step title=\"Get your API key\">\nGo to the [API keys page](https://router.tetrate.ai/api-keys) to get your key\n<img src=\"../assets/tetrate-get-api-key.png\" alt=\"Tetrate get API key\" style={{width: '250px', maxWidth: '100%'}} />\n</Step>\n\n<Step title=\"Configure Continue\">\n- Choose a configuration method below.\n- If you use the Continue VS Code extension, install version `>=1.2.3`.\n</Step>\n\n</Steps>\n\n\n### Quickstart with Continue Mission Control\n\nFastest way: use preconfigured models from Tetrate on Continue Mission Control\n\n<Steps>\n<Step title=\"Browse models\">\nOpen [Tetrate on Continue Mission Control](https://hub.continue.dev/tetrate) and pick a model (e.g., [Claude Sonnet 4](https://hub.continue.dev/tetrate/claude-sonnet-4))\n</Step>\n\n<Step title=\"Use or remix\">\nClick \"Use Model\", or \"Remix\" to change the model ID if needed\n</Step>\n\n<Step title=\"Add your API key\">\nAdd your API key to Continue. See [adding secrets in Continue Mission Control](/hub/secrets/secret-types) and [managing local secrets in the FAQ](/faqs#managing-local-secrets-and-environment-variables)\n<Tip>\nAdd it as a Continue Mission Control secret named `TETRATE_API_KEY` to reuse across projects.\n</Tip>\n</Step>\n</Steps>\n\n![Remix a model from Tetrate on Continue Mission Control](../assets/tetrate-remix.png)\n\n<Info>\nIf a model is missing, remix a similar one and set the `model` field to the target ID.\n</Info>\n\n### Configuration Methods\n\n<CardGroup cols={1}>\n  <Card title=\"In Local Agent Configuration\" icon=\"folder\" vertical>\n    Use a Tetrate model block from the Hub or define it directly in your local agent configuration.\n  </Card>\n  <Card title=\"Continue Mission Control\" icon=\"cloud\" vertical>\n    Use a Tetrate model block from the Hub or define your own on the Hub.\n  </Card>\n  <Card title=\"Using Local Model Blocks\" icon=\"file-lines\" vertical>\n    Create a local model block for reuse across agents without publishing it to the Hub.\n  </Card>\n</CardGroup>\n\n\n### Configuration Examples\n<Info>\n**Click a tab** to see an example.\n</Info>\n<Tabs>\n<Tab title=\"In Local Agent Configuration\">\n\n**When to use**: Simple local setups (like using the VS Code extension) when you don't need shared or published blocks.\n\nUse a Tetrate model block from the Hub in your local agent configuration:\n\n```yaml title=\"~/.continue/config.yaml\"\nname: Local Agent\nversion: 1.0.0\nschema: v1\nmodels:\n  - uses: tetrate/claude-sonnet-4\n    with:\n      TETRATE_API_KEY: ${{ secrets.TETRATE_API_KEY }}\ncontext:\n  - provider: code\n  - provider: docs\n  - provider: diff\n  - provider: terminal\n  - provider: problems\n  - provider: folder\n  - provider: codebase\n```\n\nOr define the model directly:\n\n```yaml title=\"~/.continue/config.yaml\"\nname: Local Agent\nversion: 1.0.0\nschema: v1\nmodels:\n  - name: Claude Sonnet 4\n    provider: tars\n    model: claude-4-sonnet-20250514\n    apiKey: ${{ secrets.TETRATE_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    capabilities:\n      - tool_use\ncontext:\n  - provider: code\n  - provider: docs\n  - provider: diff\n  - provider: terminal\n  - provider: problems\n  - provider: folder\n  - provider: codebase\n```\n</Tab>\n<Tab title=\"Continue Mission Control\">\n**When to use**: Share configurations across teams or contribute to the community.\n\n**The Model Block:**\n\n```yaml title=\"tetrate/claude-sonnet-4\"\nname: Claude Sonnet 4\nversion: 1.0.14\nschema: v1\nmodels:\n  - name: Claude Sonnet 4 - Tetrate\n    provider: tars\n    model: claude-4-sonnet-20250514\n    apiKey: ${{ inputs.TETRATE_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    capabilities:\n      - tool_use\n```\nView the model block `tetrate/claude-sonnet-4` on the [Continue Mission Control](https://hub.continue.dev/tetrate/claude-sonnet-4).\n\n**Reference it in your Agent configuration:**\n\n```yaml title=\"my-agent\"\nname: My Agent\nversion: 1.0.0\nschema: v1\nmodels:\n  - uses: tetrate/claude-sonnet-4\n    with:\n      TETRATE_API_KEY: ${{ secrets.TETRATE_API_KEY }}\ncontext:\n  - uses: continuedev/diff-context\n  - uses: continuedev/terminal-context\n  - uses: continuedev/file-context\n```\n\n</Tab>\n\n<Tab title=\"Using Local Model Blocks\">\n**When to use**: Reuse configurations across agents and keep them local or in GitHub (not on Continue Mission Control).\n\n**The Local Model Block:**\n<Info>\nName the file `my-claude-4-model.yaml` so you can reference it in the Agent.\n</Info>\n\n```yaml title=\"~/.continue/models/my-claude-4-model.yaml\"\nname: Claude Sonnet 4\nversion: 1.0.1\nschema: v1\nmodels:\n  - name: Claude Sonnet 4\n    provider: tars\n    model: claude-4-sonnet-20250514\n    apiKey: ${{ inputs.TETRATE_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    capabilities:\n      - tool_use\n```\nReference it as `my-claude-4-model` in your Agent configuration:\n\n```yaml title=\"~/.continue/agents/simple-agent.yaml\"\nname: Simple Agent\nversion: 1.0.0\nschema: v1\nmodels:\n  - uses: my-claude-4-model\n    with:\n      TETRATE_API_KEY: ${{ secrets.TETRATE_API_KEY }}\ncontext:\n  - uses: continuedev/diff-context\n  - uses: continuedev/terminal-context\n  - uses: continuedev/file-context\n```\n\n<Tip>\nLearn more about [model blocks](/reference#models) and [local blocks](/reference#local-blocks).\n</Tip>\n\n</Tab>\n</Tabs>\n\n---\n\n## Troubleshooting Common Issues\n\n<CardGroup cols={2}>\n  <Card title=\"Invalid API key\" icon=\"key\" vertical>\n    Verify your API key is active and has no extra spaces.\n  </Card>\n  <Card title=\"Model not found\" icon=\"ban\" href=\"https://router.tetrate.ai/models\" vertical>\n    Confirm the model ID matches the Tetrate catalog.\n  </Card>\n  <Card title=\"Slow responses\" icon=\"code\" vertical>\n    Check your network or try a less-loaded model. Contact Tetrate support if issues persist.\n  </Card>\n  <Card title=\"Configuration not loading\" icon=\"file-lines\" vertical>\n    Validate YAML syntax and review error messages in Continue. If using Hub, ensure the block is published.\n  </Card>\n</CardGroup>\n\n---\n\n## Join the Community\nConnect with Tetrate and other builders for help and discussion.\n\n<CardGroup cols={1}>\n  <Card title=\"Join the Tetrate Community\" icon=\"slack\" href=\"https://join.slack.com/t/tetrate-agent-router/shared_invite/zt-399tv7hjm-6YMXztGywTOQrMIolvEBNg\">\nGet answers, insights, and best practices for secure, scalable infrastructure\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/vertexai.mdx","content":"---\ntitle: \"How to Configure Vertex AI with Continue\"\nslug: ../vertexai\nsidebarTitle: \"Vertex AI\"\n---\n\n<Info>\n  Enable the [Vertex AI API](https://console.cloud.google.com/marketplace/product/google/aiplatform.googleapis.com)\n  and set up [Google Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc)\n</Info>\n\n## Configuration\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: <MODEL_NAME>\n        provider: vertexai\n        model: <MODEL_ID>\n        env:\n          projectId: <PROJECT_ID>\n          region: us-east5\n    ```\n    </Tab>\n    <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"<MODEL_NAME>\",\n          \"provider\": \"vertexai\",\n          \"model\": \"<MODEL_ID>\",\n          \"projectId\": \"[PROJECT_ID]\",\n          \"region\": \"us-east5\"\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n## How to Enable Vertex AI Express Mode\n\nYou can use Vertex AI in [express mode](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview) by only providing an API Key. Only some Gemini models are supported in express mode for now.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: <MODEL_NAME>\n        provider: vertexai\n        model: <MODEL_ID>\n        apiKey: <YOUR_GOOGLE_CLOUD_API_KEY>\n    ```\n    </Tab>\n    <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"<MODEL_NAME>\",\n          \"provider\": \"vertexai\",\n          \"model\": \"<MODEL_ID>\",\n          \"apiKey\": \"<YOUR_GOOGLE_CLOUD_API_KEY>\",\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/bedrock.mdx","content":"---\ntitle: \"How to Configure Amazon Bedrock with Continue\"\nslug: ../bedrock\nsidebarTitle: \"Amazon Bedrock\"\n---\n\n<Tip>\n  **Discover Amazon Bedrock models [here](https://hub.continue.dev/amazon)**\n</Tip>\n\n<Info>\n  Get started with [Amazon Bedrock](https://aws.amazon.com/bedrock/)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: bedrock\n      model: <MODEL_ID>\n      env:\n        region: us-east-1\n        profile: bedrock\n      roles:\n        - chat\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"bedrock\",\n        \"model\": \"<MODEL_ID>\",\n        \"region\": \"us-east-1\",\n        \"profile\": \"bedrock\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/amazon/us-anthropic-claude-sonnet-4-20250514-v1?view=config)**\n</Info>\n\n## How to Enable Prompt Caching with Amazon Bedrock\n\nBedrock allows Claude models to cache tool payloads, system messages, and chat\nmessages between requests. Enable this behavior by adding\n`promptCaching: true` under `defaultCompletionOptions` in your model\nconfiguration.\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 0.0.1\nschema: v1\n\nmodels:\n  - name: <MODEL_NAME>\n    provider: bedrock\n    model: <MODEL_ID>\n    defaultCompletionOptions:\n      promptCaching: true\n```\n\nPrompt caching is not supported in JSON configuration files, so use the YAML syntax above to enable it.\n\n## How to Set Up Authentication for Amazon Bedrock\n\nAuthentication will be through temporary or long-term credentials in\n`~/.aws/credentials` under a configured profile (e.g. \"bedrock\").\n\n```title=\"~/.aws/credentials\n[bedrock]\naws_access_key_id = abcdefg\naws_secret_access_key = hijklmno\naws_session_token = pqrstuvwxyz # Optional: means short term creds.\n```\n\nYou can also use an AWS `accessKeyId` and `secretAccessKey` for authentication instead of a local credentials profile.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: bedrock\n      model: <MODEL_ID>\n      env:\n        region: us-east-1\n        accessKeyId: ${{ secrets.AWS_ACCESS_KEY_ID }} # can also enter key inline here for local configs\n        secretAccessKey: ${{ secrets.AWS_SECRET_ACCESS_KEY }} # can also enter key inline here for local configs\n      roles:\n        - chat\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"bedrock\",\n        \"model\": \"<MODEL_ID>\",\n        \"region\": \"us-east-1\",\n        \"accessKeyId\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"secretAccessKey\": \"<YOUR_SECRET_ACCESS_KEY>\" \n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## How to Configure Custom Imported Models with Amazon Bedrock\n\nTo setup Bedrock using custom imported models, add the following to your config file:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: bedrockimport\n      model: <MODEL_ID>\n      env:\n        region: us-west-2\n        profile: bedrock\n        modelArn: arn:aws:bedrock:us-west-2:XXXXX:imported-model/XXXXXX\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"bedrockimport\",\n        \"model\": \"<MODEL_ID>\",\n        \"modelArn\": \"arn:aws:bedrock:us-west-2:XXXXX:imported-model/XXXXXX\", \n        \"region\": \"us-west-2\",\n        \"profile\": \"bedrock\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\nAuthentication will be through temporary or long-term credentials in\n~/.aws/credentials under a configured profile (e.g. \"bedrock\").\n\n```title=\"~/.aws/credentials\n[bedrock]\naws_access_key_id = abcdefg\naws_secret_access_key = hijklmno\naws_session_token = pqrstuvwxyz # Optional: means short term creds.\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/anthropic.mdx","content":"---\ntitle: \"How to Configure Anthropic Claude Models with Continue\"\nslug: ../anthropic\nsidebarTitle: \"Anthropic\"\n---\n\n<Tip>\n  **Discover Anthropic models [here](https://hub.continue.dev/anthropic)**\n</Tip>\n\n<Info>\n  Get an API key from the [Anthropic Console](https://console.anthropic.com/account/keys)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: anthropic\n      model: <MODEL_ID>\n      apiKey: <YOUR_ANTHROPIC_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"anthropic\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_ANTHROPIC_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/anthropic/claude-sonnet-4-5?view=config)**\n</Info>\n\n## How to Enable Prompt Caching with Claude\n\nAnthropic supports [prompt caching with Claude](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching), which allows Claude models to cache system messages and conversation history between requests to improve performance and reduce costs.\n\nTo enable caching of the system message and the turn-by-turn conversation, update your model configuration as follows:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: anthropic\n      model: <MODEL_ID>\n      apiKey: <YOUR_ANTHROPIC_API_KEY>\n      roles:\n        - chat\n      defaultCompletionOptions:\n        promptCaching: true\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"cacheBehavior\": {\n          \"cacheSystemMessage\": true,\n          \"cacheConversation\": true\n        },\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"anthropic\",\n        \"model\": \"<MODEL_ID>\",\n        \"defaultCompletionOptions\": {\n          \"promptCaching\": true\n        },\n        \"apiKey\": \"<YOUR_ANTHROPIC_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/openai.mdx","content":"---\ntitle: \"How to Configure OpenAI Models with Continue\"\nslug: ../openai\nsidebarTitle: \"OpenAI\"\n---\n\n<Tip>\n  **Discover OpenAI models [here](https://hub.continue.dev/openai)**\n</Tip>\n\n<Info>\n  Get an API key from the [OpenAI Console](https://platform.openai.com/account/api-keys)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: openai\n      model: <MODEL_ID>\n      apiKey: <YOUR_OPENAI_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"openai\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<YOUR_OPENAI_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/openai/gpt-5?view=config)**\n</Info>\n\n## OpenAI API compatible providers\n\nOpenAI API compatible providers include\n\n- [KoboldCpp](https://github.com/lostruins/koboldcpp)\n- [text-gen-webui](https://github.com/oobabooga/text-generation-webui/tree/main/extensions/openai#setup--installation)\n- [FastChat](https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md)\n- [LocalAI](https://localai.io/basics/getting_started/)\n- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python#web-server)\n- [TensorRT-LLM](https://github.com/NVIDIA/trt-llm-as-openai-windows?tab=readme-ov-file#examples)\n- [vLLM](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n- [BerriAI/litellm](https://github.com/BerriAI/litellm)\n- [Tetrate Agent Router Service](https://router.tetrate.ai)\n\nIf you are using an OpenAI API compatible providers, you can change the `apiBase` like this:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <OPENAI_API_COMPATIBLE_PROVIDER_MODEL>\n      provider: openai\n      model: <MODEL_NAME>\n      apiBase: http://localhost:8000/v1\n      apiKey: <YOUR_CUSTOM_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<OPENAI_API_COMPATIBLE_PROVIDER_MODEL>\",\n        \"provider\": \"openai\",\n        \"model\": \"<MODEL_NAME>\",\n        \"apiKey\": \"<YOUR_CUSTOM_API_KEY>\",\n        \"apiBase\": \"http://localhost:8000/v1\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### How to Force Legacy Completions Endpoint Usage\n\nTo force usage of `completions` instead of `chat/completions` endpoint you can set:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <OPENAI_API_COMPATIBLE_PROVIDER_MODEL>\n      provider: openai\n      model: <MODEL_NAME>>\n      apiBase: http://localhost:8000/v1\n      useLegacyCompletionsEndpoint: true\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<OPENAI_API_COMPATIBLE_PROVIDER_MODEL>\",\n        \"provider\": \"openai\",\n        \"model\": \"<MODEL_NAME>\",\n        \"apiBase\": \"http://localhost:8000/v1\",\n        \"useLegacyCompletionsEndpoint\": true\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/ollama.mdx","content":"---\ntitle: \"How to Configure Ollama with Continue\"\nslug: ../ollama\nsidebarTitle: \"Ollama\"\n---\n\n<Tip>\n  **Discover Ollama models [here](https://hub.continue.dev/lmstudio)**\n</Tip>\n\n<Info>\n  Get started with [Ollama](https://ollama.com/download)\n</Info>\n\n## Configuration\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: <MODEL_NAME>\n        provider: ollama\n        model: <MODEL_ID>\n        apiBase: http://<my endpoint>:11434 # if running a remote instance of Ollama\n    ```\n    </Tab>\n    <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"<MODEL_NAME>\",\n          \"provider\": \"ollama\",\n          \"model\": \"<MODEL_ID>\"\n          \"apiBase\": \"http://<my endpoint>:11434\" // if running a remote instance of Ollama\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/ollama/qwen3-coder-30b?view=config)**\n</Info>\n\n## How to Configure Model Capabilities in Ollama\n\nOllama models usually have their capabilities auto-detected correctly. However, if you're using custom model names or experiencing issues with tools/images not working, you can explicitly set capabilities:\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: <CUSTOM_MODEL_NAME>\n        provider: ollama\n        model: <CUSTOM_MODEL_ID>\n        capabilities:\n          - tool_use      # Enable if your model supports function calling\n          - image_input   # Enable for vision models\n    ```\n    </Tab>\n    <Tab title=\"JSON (Deprecated)\">\n    ```json title=\"config.json\"\n    {\n      \"models\": [\n        {\n          \"title\": \"<CUSTOM_MODEL_NAME>\",\n          \"provider\": \"ollama\",\n          \"model\": \"<CUSTOM_MODEL_ID>\",\n          \"capabilities\": {\n            \"tools\": true, // Enable if your model supports function calling\n            \"uploadImage\": true // Enable for vision models\n          }\n        }\n      ]\n    }\n    ```\n    </Tab>\n</Tabs>\n\n<Note>\n  Many Ollama models support tool use by default. Vision models often also support image input\n</Note>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/top-level/inception.mdx","content":"---\ntitle: \"How to Configure Inception with Continue\"\nslug: ../inception\nsidebarTitle: \"Inception\"\n---\n\n<Tip>\n  **Discover Inception models [here](https://hub.continue.dev/inceptionlabs)**\n</Tip>\n\n<Info>\n  Get an API key from [Inception Platform](https://platform.inceptionlabs.ai/dashboard/api-keys)\n</Info>\n\n## Configuration\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: <MODEL_NAME>\n      provider: inception\n      model: <MODEL_ID>\n      apiKey: <INCEPTION_API_KEY>\n  ```\n  </Tab>\n  <Tab title=\"JSON (Deprecated)\">\n  ```json title=\"config.json\"\n  {\n    \"models\": [\n      {\n        \"title\": \"<MODEL_NAME>\",\n        \"provider\": \"inception\",\n        \"model\": \"<MODEL_ID>\",\n        \"apiKey\": \"<INCEPTION_API_KEY>\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  **Check out a more advanced configuration [here](https://hub.continue.dev/inceptionlabs/mercury-coder?view=config)**\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-providers/overview.mdx","content":"---\ntitle: \"Model Providers Overview\"\ndescription: \"Continue supports a wide range of AI model providers to power different features like chat, code editing, autocompletion, and embeddings. This overview helps you navigate through the available options and find the right provider for your needs.\"\n---\n\n## Popular Model Providers\n\nThese are the most commonly used model providers that offer a wide range of capabilities:\n\n| Provider                                                       | Description                                                                     | Capabilities                                |\n| :------------------------------------------------------------- | :------------------------------------------------------------------------------ | :------------------------------------------ |\n| [Anthropic](/customize/model-providers/top-level/anthropic)    | Providers of Claude models, known for long context windows and strong reasoning | Chat, Edit, Apply, Embeddings               |\n| [OpenAI](/customize/model-providers/top-level/openai)          | Creators of GPT models with strong coding capabilities                          | Chat, Edit, Apply, Embeddings               |\n| [Azure](/customize/model-providers/top-level/azure)            | Microsoft's cloud platform offering OpenAI models                               | Chat, Edit, Apply, Embeddings               |\n| [Amazon Bedrock](/customize/model-providers/top-level/bedrock) | AWS service offering access to various foundation models                        | Chat, Edit, Apply, Embeddings               |\n| [Ollama](/customize/model-providers/top-level/ollama)          | Run open-source models locally with a simple interface                          | Chat, Edit, Apply, Embeddings, Autocomplete |\n| [Google Gemini](/customize/model-providers/top-level/gemini)   | Google's multimodal AI models                                                   | Chat, Edit, Apply, Embeddings               |\n| [DeepSeek](/customize/model-providers/more/deepseek)           | Specialized code models with strong performance                                 | Chat, Edit, Apply                           |\n| [Mistral](/customize/model-providers/more/mistral)             | High-performance open models with commercial offerings                          | Chat, Edit, Apply, Embeddings               |\n| [xAI](/customize/model-providers/more/xAI)                     | Grok models from xAI                                                            | Chat, Edit, Apply                           |\n| [Vertex AI](/customize/model-providers/top-level/vertexai)     | Google Cloud's machine learning platform                                        | Chat, Edit, Apply, Embeddings               |\n| [Inception](/customize/model-providers/top-level/inception)    | On-premises open-source model runners                                           | Chat, Edit, Apply                           |\n| [HuggingFace](/customize/model-providers/top-level/huggingfaceinference) | Platform for open source models with inference providers and endpoints           | Chat, Edit, Apply, Embeddings               |\n\n## Additional Model Providers\n\nBeyond the top-level providers, Continue supports many other options:\n\n### Hosted Services\n\n| Provider                                                               | Description                                                |\n| :--------------------------------------------------------------------- | :--------------------------------------------------------- |\n| [Groq](/customize/model-providers/more/groq)                           | Ultra-fast inference for various open models               |\n| [Together AI](/customize/model-providers/more/together)                | Platform for running a variety of open models              |\n| [DeepInfra](/customize/model-providers/more/deepinfra)                 | Hosting for various open source models                     |\n| [OpenRouter](/customize/model-providers/top-level/openrouter)          | Gateway to multiple model providers                        |\n| [Tetrate Agent Router Service](/customize/model-providers/top-level/tetrate_agent_router_service) | Gateway with intelligent routing across multiple model providers |\n| [Cohere](/customize/model-providers/more/cohere)                       | Models specialized for semantic search and text generation |\n| [NVIDIA](/customize/model-providers/more/nvidia)                       | GPU-accelerated model hosting                              |\n| [Cloudflare](/customize/model-providers/more/cloudflare)               | Edge-based AI inference services                           |\n\n### Local Model Options\n\n| Provider                                                 | Description                                   |\n| :------------------------------------------------------- | :-------------------------------------------- |\n| [LM Studio](/customize/model-providers/top-level/lmstudio) | Desktop app for running models locally        |\n| [llama.cpp](/customize/model-providers/more/llamacpp)    | Optimized C++ implementation for running LLMs |\n| [LlamaStack](/customize/model-providers/more/llamastack) | Stack for running Llama models locally        |\n| [llamafile](/customize/model-providers/more/llamafile)   | Self-contained executable model files         |\n\n### Enterprise Solutions\n\n| Provider                                               | Description                           |\n| :----------------------------------------------------- | :------------------------------------ |\n| [SambaNova](/customize/model-providers/more/SambaNova) | Enterprise AI platform                |\n| [Watson x](/customize/model-providers/more/watsonx)    | IBM's enterprise AI platform          |\n| [Sagemaker](/customize/model-providers/more/sagemaker) | AWS machine learning platform         |\n| [Nebius](/customize/model-providers/more/nebius)       | Cloud-based machine learning platform |\n\n## How to Choose a Model Provider\n\nWhen selecting a model provider, consider:\n\n1. **Hosting preference**: Do you need local models for offline use or privacy, or are you comfortable with cloud services?\n2. **Performance requirements**: Different providers offer varying levels of speed, quality, and context length.\n3. **Specific capabilities**: Some models excel at code generation, others at embeddings or reasoning tasks.\n4. **Pricing**: Costs vary significantly between providers, from free local options to premium cloud services.\n5. **API key requirements**: Most cloud providers require API keys that you'll need to configure.\n\n## Configuration Format\n\nYou can add models to your `config.yaml` file like this:\n\n```yaml\nmodels:\n  - name: Claude 4 Sonnet\n    provider: anthropic # Choose provider from the lists above\n    model: claude-sonnet-4-20250514 # Specific model name\n    apiKey: ${{ secrets.OPENAI_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n```\nFor more detailed configuration, visit the specific provider pages linked above.\n\n## Change Your Model Provider\n\nContinue allows you to choose your favorite or even add multiple model providers. This allows you to use different models for different tasks, or to try another model if you‚Äôre not happy with the results from your current model. Continue supports all of the popular model providers, including OpenAI, Anthropic, Microsoft/Azure, Mistral, and more. You can even self host your own model provider if you‚Äôd like. [Learn more about model providers](./top-level/openai).\n"}
{"source":"github","repo":"continue","path":"docs/customize/prompts.mdx","content":"---\ntitle: \"Prompts\"\ndescription: \"These are the specialized instructions that shape how models and agents respond:\"\n---\n\n- **Define interaction patterns** for specific tasks or frameworks\n- **Encode domain expertise** for particular technologies\n- **Ensure consistent guidance** aligned with organizational practices\n- **Can be shared and reused** across multiple agents\n- **Act as automated code reviewers** that ensure consistency across teams\n\n\n## Learn More\n\n- [Explore prompts](https://hub.continue.dev/hub?type=prompts) on the Hub\n- Learn more in the [prompts deep dive](/customize/deep-dives/prompts)\n- View [`prompts`](/reference#prompts) in the YAML Reference for more details\n"}
{"source":"github","repo":"continue","path":"docs/customize/custom-providers.mdx","content":"---\ntitle: \"Context Providers\"\ndescription: \"Context Providers allow you to type '@' and see a dropdown of content that can all be fed to the LLM as context. Every context provider is a plugin, which means if you want to reference some source of information that you don't see here, you can request (or build!) a new context provider.\"\n---\n\nAs an example, say you are working on solving a new GitHub Issue. You type '@Issue' and select the one you are working on. Continue can now see the issue title and contents. You also know that the issue is related to the files 'readme.md' and 'helloNested.py', so you type '@readme' and '@hello' to find and select them. Now these 3 \"Context Items\" are displayed inline with the rest of your input.\n\n![Context Items](/images/customize/images/context-provider-example-0c96ff77286fa970b23dddfdc1fa986a.png)\n\n## Built-in Context Providers\n\nYou can add any built-in context-providers in your config file as shown below:\n\n### @File Context Provider ‚Äì Reference Files in Your Workspace\n\nReference any file in your current workspace.\n\nconfig.yaml\n\n```\ncontext:  - provider: file\n```\n\n### @Code Context Provider ‚Äì Reference Functions and Classes\n\nReference specific functions or classes from throughout your project.\n\nconfig.yaml\n\n```\ncontext:  - provider: code\n```\n\n### @Git Diff Context Provider ‚Äì Reference Branch Changes\n\nReference all of the changes you've made to your current branch. This is useful if you want to summarize what you've done or ask for a general review of your work before committing.\n\nconfig.yaml\n\n```\ncontext:  - provider: diff\n```\n\n### @Current File Context Provider ‚Äì Reference the Active File\n\nReference the currently open file.\n\nconfig.yaml\n\n```\ncontext:  - provider: currentFile\n```\n\n### @Terminal Context Provider ‚Äì Reference Last Terminal Command and Output\n\nReference the last command you ran in your IDE's terminal and its output.\n\nconfig.yaml\n\n```\ncontext:  - provider: terminal\n```\n\n### @Docs Context Provider ‚Äì Reference Documentation Site Content\n\nReference the contents from any documentation site.\n\nconfig.yaml\n\n```\ncontext:  - provider: docs\n```\n\nNote that this will only enable the `@Docs` context provider.\n\nTo use it, you need to add a documentation site to your config file. See the [docs](/customize/overview#documentation-context) page for more information.\n\n### @Open Context Provider ‚Äì Reference All Open Files or Pinned Files\n\nReference the contents of all of your open files. Set `onlyPinned` to `true` to only reference pinned files.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: open\n    params:\n      onlyPinned: true\n```\n\n### @Web Context Provider ‚Äì Reference Relevant Web Pages\n\nReference relevant pages from across the web, automatically determined from your input.\n\nOptionally, set `n` to limit the number of results returned (default 6).\n\nconfig.yaml\n\n```\ncontext:\n  - provider: web\n    params:\n      n: 5\n```\n\n### @Codebase Context Provider ‚Äì Reference Relevant Codebase Snippets\n\nReference the most relevant snippets from your codebase.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: codebase\n```\n\nRead more about indexing and retrieval [here](/customize/overview#codebase-context).\n\n### @Folder Context Provider ‚Äì Reference Code from a Specific Folder\n\nUses the same retrieval mechanism as `@Codebase`, but only on a single folder.\n\nconfig.yaml\n\n```\ncontext:  - provider: folder\n```\n\n### @Search Context Provider ‚Äì Reference Code Search Results\n\nReference the results of codebase search, just like the results you would get from VS Code search.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: search\n    params:\n      maxResults: 100 # optional, defaults to 200\n```\n\nThis context provider is powered by [ripgrep](https://github.com/BurntSushi/ripgrep).\n\n### @Url Context Provider ‚Äì Reference Content from a URL\n\nReference the markdown converted contents of a given URL.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: url\n```\n\n### @Clipboard Context Provider ‚Äì Reference Clipboard Items\n\nReference recent clipboard items\n\nconfig.yaml\n\n```\ncontext:\n  - provider: clipboard\n```\n\n### @Tree Context Provider ‚Äì Reference Workspace File Structure\n\nReference the structure of your current workspace.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: tree\n```\n\n### @Problems Context Provider ‚Äì Reference Problems in the Current File\n\nGet Problems from the current file.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: problems\n```\n\n### @Debugger Context Provider ‚Äì Reference Debugger Local Variables\n\nReference the contents of the local variables in the debugger. Currently only available in VS Code.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: debugger\n    params:\n      stackDepth: 3\n```\n\nUses the top _n_ levels (defaulting to 3) of the call stack for that thread.\n\n### @Repository Map Context Provider ‚Äì Reference Codebase Outline and Signatures\n\nReference the outline of your codebase. By default, signatures are included along with file in the repo map.\n\n`includeSignatures` params can be set to false to exclude signatures. This could be necessary for large codebases and/or to reduce context size significantly. Signatures will not be included if indexing is disabled.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: repo-map\n    params:\n      includeSignatures: false # default true\n```\n\nProvides a list of files and the call signatures of top-level classes, functions, and methods in those files. This helps the model better understand how a particular piece of code relates to the rest of the codebase.\n\nIn the submenu that appears, you can select either `Entire codebase`, or specify a subfolder to generate the repository map from.\n\nThis context provider is inspired by [Aider's repository map](https://aider.chat/2023/10/22/repomap.html).\n\n### @Operating System Context Provider ‚Äì Reference System Architecture and Platform\n\nReference the architecture and platform of your current operating system.\n\nconfig.yaml\n\n```\ncontext:\n  - provider: os\n```\n\n### Using the Model Context Protocol (MCP) in Continue\n\nThe [Model Context Protocol](https://modelcontextprotocol.io/introduction) is a standard proposed by Anthropic to unify prompts, context, and tool use. Continue supports any MCP server with the MCP context provider. Read their [quickstart](https://modelcontextprotocol.io/quickstart) to learn how to set up a local server and then set up your configuration like this:\n\nconfig.yaml\n\n```\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n```\n\nYou'll then be able to type \"@\" and see \"MCP\" in the context providers dropdown.\n\n### How to Request a New Context Provider in Continue\n\nNot seeing what you want? Create an issue [here](https://github.com/continuedev/continue/issues/new?assignees=TyDunn&labels=enhancement&projects=&template=feature-request-%F0%9F%92%AA.md&title=) to request a new Context Provider.\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/prompts.mdx","content":"---\ntitle: \"How to Create and Manage Prompts in Continue\"\ndescription: \"Prompts are used to kick off tasks for Agent mode, Plan mode, and Chat mode\"\nkeywords: [prompts, context, slash command]\nsidebarTitle: \"Prompts\"\n---\n\nPrompts are included as user messages and are especially useful as instructions for repetitive and/or complex tasks.\n\n## Slash commands\n\nBy setting `invokable` to `true`, you make the markdown file a prompt, which will be available when you type <kbd>/</kbd> in Chat, Plan, and Agent mode.\n\n```md title=\"explain-invokable.md\"\n---\nname: Explain invokable\ndescription: Explains what happens when you set invokable to true\ninvokable: true\n---\n\nExplain that when `invokable` is set to `true`, a slash command becomes available in the IDE extensions and CLI\n```\n\nThese slash commands can be combined with other instructions, including highlighted code, to provide additional context.\n\n## Example: `Create Supabase functions` prompt\n\nHere is a prompt that generates high-quality PostgreSQL functions that adhere to best practices:\n\n```md title=\"supabase-create-functions.md\"\n---\nname: Create Supabase functions\ndescription: Guidelines for writing Supabase database functions\ninvokable: true\n---\n\n# Database: Create functions\n\nYou're a Supabase Postgres expert in writing database functions. Generate **high-quality PostgreSQL functions** that adhere to the following best practices:\n\n## General Guidelines\n\n1. **Default to `SECURITY INVOKER`:**\n\n   - Functions should run with the permissions of the user invoking the function, ensuring safer access control.\n   - Use `SECURITY DEFINER` only when explicitly required and explain the rationale.\n\n2. **Set the `search_path` Configuration Parameter:**\n\n   - Always set `search_path` to an empty string (`set search_path = '';`).\n   - This avoids unexpected behavior and security risks caused by resolving object references in untrusted or unintended schemas.\n   - Use fully qualified names (e.g., `schema_name.table_name`) for all database objects referenced within the function.\n\n3. **Adhere to SQL Standards and Validation:**\n   - Ensure all queries within the function are valid PostgreSQL SQL queries and compatible with the specified context (ie. Supabase).\n\n...\n```\n\n<Info>You can read the rest of the `Create Supabase functions` prompt [here](http://hub.continue.dev/supabase/create-functions)</Info>\n\nIf you are using a local `config.yaml`, you can add it to your config like this:\n\n```md title=\"config.yaml\"\n...\n\nprompts:\n  - uses: supabase/create-functions\n\n...\n```\n\nIf you are using Continue Mission Control, you can add it to your config by selecting \"Use Rule\" [here](https://hub.continue.dev/supabase/create-functions)\n\nTo use this prompt, you can open Chat / Agent / Edit, type <kbd>/</kbd>, select the prompt, and type out any additional instructions you'd like to add.\n\n## Using a prompt with `cn (TUI mode)`\n\nYou can run this command to start [cn](../../guides/cli) with the [Create Supabase functions](http://hub.continue.dev/supabase/create-functions) prompt.\n\n```\ncn --prompt supabase/create-functions \"I need a function that checks for the health status\"\n```\nAlternatively, you can start [cn](../../guides/cli) and then type <kbd>/</kbd> to manually invoke the prompt yourself.\n\n## Using a prompt to kick off a Continuous AI workflow with `cn (Headless mode)\n\nYou can kick off Continuous AI workflows using a prompt with [cn](../../guides/cli) by adding the `-p` flag.\n\nFor example, say you are building a SaaS application and must repeatedly create custom Supabase validation functions for each new feature that accepts user input. \n\nThese functions require you to interpret business requirements, implement complex cross-table logic (like checking user permissions, tier limits, and time-based restrictions), and make judgment calls about edge cases. \n\nEach function is unique enough that it can't be templated or scripted. This is where kicking off a Continuous AI workflow to get you started can be quite helpful.\n\nHere is a command that you could run whenever you have a new feature:\n\n```\n cn -p --prompt supabase/create-functions \"I need a function for the new feature on my current branch similar to my existing database functions\"\n```\n\n<Info>You can see the entire `Create Supabase functions` prompt [here](http://hub.continue.dev/supabase/create-functions)</Info>\n\nWhen you run this workflow, [cn](../../guides/cli) will checkout your current branch, explore the new and existing code, and then draft a function for you.\n\nYou will then be able to review the implementation and improve it before you merge the new feature."}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/model-capabilities.mdx","content":"---\ntitle: \"How to Configure Model Capabilities in Continue\"\ndescription: Understanding and configuring model capabilities for tools and image support\nkeywords: [capabilities, tools, function calling, image input, config]\nsidebarTitle: \"Model Capabilities\"\n---\n\nContinue needs to know what features your models support to provide the best experience. This guide explains how model capabilities work and how to configure them.\n\n## What Are Model Capabilities?\n\nModel capabilities tell Continue what features a model supports:\n\n- **`tool_use`** - Whether the model can use tools and functions\n- **`image_input`** - Whether the model can process images\n\nWithout proper capability configuration, you may encounter issues like:\n\n- Agent mode being unavailable (requires tools)\n- Tools not working at all\n- Image uploads being disabled\n\n## How Continue Detects Model Capabilities\n\nContinue uses a two-tier system for determining model capabilities:\n\n### How Automatic Detection Works (Default)\n\nContinue automatically detects capabilities based on your provider and model name. For example:\n\n- **OpenAI**: GPT-4 and GPT-3.5 Turbo models support tools\n- **Anthropic**: Claude 3.5+ models support both tools and images\n- **Ollama**: Most models support tools, vision models support images\n- **Google**: All Gemini models support function calling\n\nThis works well for popular models, but may not cover custom deployments or newer models.\n\nFor implementation details, see:\n\n- [toolSupport.ts](https://github.com/continuedev/continue/blob/main/core/llm/toolSupport.ts) - Tool capability detection logic\n- [@continuedev/llm-info](https://www.npmjs.com/package/@continuedev/llm-info) - Image support detection\n\n### How to Configure Capabilities Manually\n\nYou can add capabilities to models that Continue doesn't automatically detect in your `config.yaml`.\n\n<Note>\n  You cannot override autodetection - you can only add capabilities. Continue\n  will always use its built-in knowledge about your model in addition to any\n  capabilities you specify.\n</Note>\n\n```yaml\nmodels:\n  - name: my-custom-gpt4\n    provider: openai\n    apiBase: https://my-deployment.com/v1\n    model: gpt-4-custom\n    capabilities:\n      - tool_use\n      - image_input\n```\n\n## When to Add Capabilities Manually\n\nAdd capabilities when:\n\n1. **Using custom deployments** - Your API endpoint serves a model with different capabilities than the standard version\n2. **Using newer models** - Continue doesn't yet recognize a newly released model\n3. **Experiencing issues** - Autodetection isn't working correctly for your setup\n4. **Using proxy services** - Some proxy services modify model capabilities\n\n## How to Configure Model Capabilities (Examples)\n\n### How to Add Basic Tool Support\n\nAdd tool support for a model that Continue doesn't recognize:\n\n```yaml\nmodels:\n  - name: custom-model\n    provider: openai\n    model: my-fine-tuned-gpt4\n    capabilities:\n      - tool_use\n```\n\n<Info>\n  The `tool_use` capability is for native tool/function calling support. The\n  model must actually support tools for this to work.\n</Info>\n\n<Warning>\n  **Experimental**: System message tools are available as an experimental\n  feature for models without native tool support. These are not automatically\n  used as a fallback and must be explicitly configured. Most models are trained\n  for native tools, so system message tools may not work as well.\n</Warning>\n\n### How to Handle Models with Limited Capabilities\n\nExplicitly set no capabilities (autodetection will still apply):\n\n```yaml\nmodels:\n  - name: limited-claude\n    provider: anthropic\n    model: claude-4.0-sonnet\n    capabilities: [] # Empty array doesn't disable autodetection\n```\n\n<Warning>\n  An empty capabilities array does not disable autodetection. Continue will\n  still detect and use the model's actual capabilities. To truly limit a model's\n  capabilities, you would need to use a model that doesn't support those\n  features.\n</Warning>\n\n### How to Enable Multiple Capabilities\n\nEnable both tools and image support:\n\n```yaml\nmodels:\n  - name: multimodal-gpt\n    provider: openai\n    model: gpt-4-vision-preview\n    capabilities:\n      - tool_use\n      - image_input\n```\n\n## Common Configuration Scenarios\n\nSome providers and custom deployments may require explicit capability configuration:\n\n- **OpenRouter**: May not preserve the original model's capabilities\n- **Custom API endpoints**: May have different capabilities than standard models\n- **Local models**: May need explicit capabilities if using non-standard model names\n\nExample configuration:\n\n```yaml\nmodels:\n  - name: custom-deployment\n    provider: openai\n    apiBase: https://custom-api.company.com/v1\n    model: custom-gpt\n    capabilities:\n      - tool_use # If supports function calling\n      - image_input # If supports vision\n```\n\n## How to Troubleshoot Capability Issues\n\nFor troubleshooting capability-related issues like Agent mode being unavailable or tools not working, see the [Troubleshooting guide](/troubleshooting#agent-mode-is-unavailable-or-tools-aren‚Äôt-working).\n\n## Best Practices for Model Capabilities\n\n1. **Start with autodetection** - Only override if you experience issues\n2. **Test after changes** - Verify tools and images work as expected\n3. **Keep Continue updated** - Newer versions improve autodetection\n\nRemember: Setting capabilities only adds to autodetection. Continue will still use its built-in knowledge about your model in addition to your specified capabilities.\n\n## Model Capability Support\n\nThis matrix shows which models support tool use and image input capabilities. Continue auto-detects these capabilities, but you can override them if needed.\n\n### OpenAI\n\n| Model         | Tool Use | Image Input | Context Window |\n| :------------ | -------- | ----------- | -------------- |\n| GPT-5.1       | Yes      | No          | 400k           |\n| GPT-5         | Yes      | No          | 400k           |\n| o3            | Yes      | No          | 128k           |\n| o3-mini       | Yes      | No          | 128k           |\n| GPT-4o        | Yes      | Yes         | 128k           |\n| GPT-4 Turbo   | Yes      | Yes         | 128k           |\n| GPT-4         | Yes      | No          | 8k             |\n| GPT-3.5 Turbo | Yes      | No          | 16k            |\n\n### Anthropic\n\n| Model             | Tool Use | Image Input | Context Window |\n| :---------------- | -------- | ----------- | -------------- |\n| Claude 4 Sonnet   | Yes      | Yes         | 200k           |\n| Claude 3.5 Sonnet | Yes      | Yes         | 200k           |\n| Claude 3.5 Haiku  | Yes      | Yes         | 200k           |\n\n### Cohere\n\n| Model               | Tool Use | Image Input | Context Window |\n| :------------------ | -------- | ----------- | -------------- |\n| Command A           | Yes      | No          | 256k           |\n| Command A Reasoning | Yes      | No          | 256k           |\n| Command A Translate | Yes      | No          | 8k             |\n| Command A Vision    | No       | Yes         | 128k           |\n\n### Google\n\n| Model            | Tool Use | Image Input | Context Window |\n| :--------------- | -------- | ----------- | -------------- |\n| Gemini 2.5 Pro   | Yes      | Yes         | 2M             |\n| Gemini 2.0 Flash | Yes      | Yes         | 1M             |\n\n### Mistral\n\n| Model           | Tool Use | Image Input | Context Window |\n| :-------------- | -------- | ----------- | -------------- |\n| Devstral Medium | Yes      | No          | 32k            |\n| Mistral         | Yes      | No          | 32k            |\n\n### DeepSeek\n\n| Model             | Tool Use | Image Input | Context Window |\n| :---------------- | -------- | ----------- | -------------- |\n| DeepSeek V3       | Yes      | No          | 128k           |\n| DeepSeek Coder V2 | Yes      | No          | 128k           |\n| DeepSeek Chat     | Yes      | No          | 64k            |\n\n### xAI\n\n| Model                     | Tool Use | Image Input | Context Window |\n| :------------------------ | -------- | ----------- | -------------- |\n| Grok Code Fast 1          | Yes      | Yes         | 256k           |\n| Grok 4 Fast Reasoning     | Yes      | Yes         | 2M             |\n| Grok 4 Fast Non-Reasoning | Yes      | Yes         | 2M             |\n| Grok 4                    | Yes      | Yes         | 256k           |\n| Grok 3                    | Yes      | Yes         | 131k           |\n| Grok 3 Mini               | Yes      | Yes         | 131k           |\n\n### Moonshot AI\n\n| Model   | Tool Use | Image Input | Context Window |\n| :------ | -------- | ----------- | -------------- |\n| Kimi K2 | Yes      | Yes         | 128k           |\n\n### Qwen\n\n| Model             | Tool Use | Image Input | Context Window |\n| :---------------- | -------- | ----------- | -------------- |\n| Qwen Coder 3 480B | Yes      | No          | 128k           |\n\n### Ollama (Local Models)\n\n| Model          | Tool Use | Image Input | Context Window |\n| :------------- | -------- | ----------- | -------------- |\n| Qwen 3 Coder   | Yes      | No          | 32k            |\n| Qwen 2.5 VL    | No       | Yes         | 128k           |\n| Devstral Small | Yes      | No          | 32k            |\n| Llama 3.1      | Yes      | No          | 128k           |\n| Llama 3        | Yes      | No          | 8k             |\n| Mistral        | Yes      | No          | 32k            |\n| Codestral      | Yes      | No          | 32k            |\n| Gemma 3 4B     | Yes      | Yes         | 128k           |\n\n### Notes\n\n- **Tool Use**: Function calling support (tools are required for Agent mode)\n- **Image Input**: Processing images\n- **Context Window**: Maximum number of tokens the model can process in a single request\n\n---\n\n**Is your model missing or incorrect?** Help improve this documentation! You can edit this page on GitHub using the link below.\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/mcp.mdx","content":"---\ntitle: \"How to Set Up Model Context Protocol (MCP) in Continue\"\ndescription: MCP use and customization\nkeywords: [tool, use, function calling, claude, automatic]\nsidebarTitle: \"Model Context Protocol (MCP)\"\n---\n\nAs AI systems get better, they're still held back by their training data and\ncan't access real-time information or specialized tools. The [Model Context\nProtocol](https://modelcontextprotocol.io/introduction) (MCP) fixes this by\nletting AI models connect with outside data sources, tools, and environments.\nThis allows smooth sharing of information and abilities between AI systems and\nthe wider digital world. This standard, created by Anthropic to bring together\nprompts, context, and tool use, is key for building truly useful AI experiences\nthat can be set up with custom tools.\n\n## How MCP Works in Continue\n\nCurrently custom tools can be configured using the Model Context\nProtocol standard to unify prompts, context, and tool use.\n\nMCP Servers can be added to hub configs using `mcpServers`. You can\nexplore available MCP servers\n[here](https://hub.continue.dev/explore/mcp).\n\n<Info>MCP can only be used in the **agent** mode.</Info>\n\n## Quick Start: How to Set Up Your First MCP Server\n\nBelow is a quick example of setting up a new MCP server for use in your config:\n\n1. Create a folder called `.continue/mcpServers` at the top level of your workspace\n2. Add a file called `playwright-mcp.yaml` to this folder\n3. Write the following contents and save\n\n```yaml title=\".continue/mcpServers/playwright-mcp.yaml\"\nname: Playwright mcpServer\nversion: 0.0.1\nschema: v1\nmcpServers:\n  - name: Browser search\n    command: npx\n    args:\n      - \"@playwright/mcp@latest\"\n```\n\nNow test your MCP server by prompting the following command:\n\n```\nOpen the browser and navigate Hacker News. Save the top 10 headlines in a hn.txt file.\n```\n\nThe result will be a generated file called `hn.txt` in the current working directory.\n\n![playwright mcp](/images/mcp-playwright.png)\n\n## How to Set Up Continue Documentation Search with MCP\n\nYou can set up an MCP server to search the Continue documentation directly from your config. This is particularly useful for getting help with Continue configuration and features.\n\nFor complete setup instructions, troubleshooting, and usage examples, see the [Continue MCP Reference](/reference/continue-mcp).\n\n## Using JSON MCP Format from Claude, Cursor, Cline, etc\n\n<Info>\nIf you're coming from another tool that uses JSON MCP format configuration files (like Claude Desktop, Cursor, or Cline), you can copy those JSON config files directly into your `.continue/mcpServers/` directory (note the plural \"Servers\") and Continue will automatically pick them up.\n\nFor example, place your JSON MCP config file at `.continue/mcpServers/mcp.json` in your workspace.\n</Info>\n\n## How to Configure MCP Servers\n\nTo set up your own MCP server, read the [MCP\nquickstart](https://modelcontextprotocol.io/quickstart) and then [create an\n`mcpServers`](https://hub.continue.dev/new?type=block&blockType=mcpServers) or add a local MCP\nserver block to your [config file](./configuration.md):\n\n```yaml title=\"config.yaml\"\n# ...\nmcpServers:\n  - name: SQLite MCP\n    command: npx\n    args:\n      - \"-y\"\n      - \"mcp-sqlite\"\n      - \"/path/to/your/database.db\"\n# ...\n```\n\n<Note>\nWhen creating a standalone block file in `.continue/mcpServers/`, remember to include the required metadata fields (`name`, `version`, `schema`) as shown in the Quick Start example above.\n</Note>\n\n### How to Configure MCP Server Properties\n\nMCP components include a few additional properties specific to MCP servers.\n\n- `name`: A display name for the MCP server.\n- `type`: The type of the MCP server: `sse`, `stdio`, `streamable-http`\n- `command`: The command to run to start the MCP server.\n- `args`: Arguments to pass to the command.\n- `env`: Secrets to be injected into the command as environment variables.\n\n### How to Choose MCP Transport Types\n\nMCP now supports remote server connections through HTTP-based transports, expanding beyond the traditional local stdio transport method. This enables integration with cloud-hosted MCP servers and distributed architectures.\n\n#### How to Use Server-Sent Events Transport (`sse`)\n\nFor real-time streaming communication, use the SSE transport:\n\n```yaml\n# ...\nmcpServers:\n  - name: Name\n    type: sse\n    url: https://....\n# ...\n```\n\n#### How to Use Standard Input/Output (`stdio`)\n\nFor local MCP servers that communicate via standard input and output:\n\n```yaml\n# ...\nmcpServers:\n  - name: Name\n    type: stdio\n    command: npx\n    args:\n      - \"@modelcontextprotocol/server-sqlite\"\n      - \"/path/to/your/database.db\"\n# ...\n```\n\n#### How to Use Streamable HTTP Transport\n\nFor standard HTTP-based communication with streaming capabilities:\n\n```yaml\n# ...\nmcpServers:\n  - name: Name\n    type: streamable-http\n    url: https://....\n# ...\n```\n\nThese remote transport options allow you to connect to MCP servers hosted on remote infrastructure, enabling more flexible deployment architectures and shared server resources across multiple clients.\n\nFor detailed information about transport mechanisms and their use cases, refer to the official MCP documentation on [transports](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse).\n\n### How to Work with Secrets in MCP Servers\n\nWith some MCP servers you will need to use API keys or other secrets. You can leverage locally stored environments secrets\nas well as access hosted secrets in the Continue Mission Control. To leverage Hub secrets, you can use the `inputs` property in your MCP env block instead of `secrets`.\n\n```yaml\n# ...\nmcpServers:\n  - name: Supabase MCP\n    command: npx\n    args:\n      - -y\n      - \"@supabase/mcp-server-supabase@latest\"\n      - --access-token\n      - ${{ secrets.SUPABASE_TOKEN }}\n    env:\n      SUPABASE_TOKEN: ${{ secrets.SUPABASE_TOKEN }}\n  - name: GitHub\n    command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-github\"\n    env:\n      GITHUB_PERSONAL_ACCESS_TOKEN: ${{ secrets.GITHUB_PERSONAL_ACCESS_TOKEN }}\n# ...\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/custom-providers.mdx","content":"---\ntitle: \"Context Providers\"\ndescription: \"Context Providers allow you to type '@' and see a dropdown of content that can all be provided to the model as context.\"\n---\n\nContext Providers allow you to type '@' and see a dropdown of content that can all be provided to the model as context.\n\n![Context Items](/images/customize/images/context-provider-example-0c96ff77286fa970b23dddfdc1fa986a.png)\n\n## Built-in Context Providers\n\nYou can add any built-in context-providers in your config file as shown below:\n\n### `@File`\n\nReference any file in your current workspace.\n\n```yaml config.yaml\ncontext:\n  - provider: file\n```\n\n### `@Code`\n\nReference specific functions or classes from throughout your project.\n\n```yaml config.yaml\ncontext:\n  - provider: code\n```\n\n### `@Git Diff`\n\nReference all of the changes you've made to your current branch. This is useful if you want to summarize what you've done or ask for a general review of your work before committing.\n\n```yaml config.yaml\ncontext:\n  - provider: diff\n```\n\n### `@Current File`\n\nReference the currently open file.\n\n```yaml config.yaml\ncontext:\n  - provider: currentFile\n```\n\n### `@Terminal`\n\nReference the last command you ran in your IDE's terminal and its output.\n\n```yaml config.yaml\ncontext:\n  - provider: terminal\n```\n\n### `@Open`\n\nReference the contents of all of your open files. Set `onlyPinned` to `true` to only reference pinned files.\n\n```yaml config.yaml\ncontext:\n  - provider: open\n    params:\n      onlyPinned: true\n```\n\n### `@Clipboard`\n\nReference recent clipboard items\n\n```yaml config.yaml\ncontext:\n  - provider: clipboard\n```\n\n### `@Tree`\n\nReference the structure of your current workspace.\n\n```yaml config.yaml\ncontext:\n  - provider: tree\n```\n\n### `@Problems`\n\nGet Problems from the current file.\n\n```yaml config.yaml\ncontext:\n  - provider: problems\n```\n\n### `@Debugger`\n\nReference the contents of the local variables in the debugger. Currently only available in VS Code.\n\n```yaml config.yaml\ncontext:\n  - provider: debugger\n    params:\n      stackDepth: 3\n```\n\nUses the top _n_ levels (defaulting to 3) of the call stack for that thread.\n\n### `@Repository Map`\n\nReference the outline of your codebase. By default, signatures are included along with file in the repo map.\n\n`includeSignatures` params can be set to false to exclude signatures. This could be necessary for large codebases and/or to reduce context size significantly. Signatures will not be included if indexing is disabled.\n\n```yaml config.yaml\ncontext:\n  - provider: repo-map\n    params:\n      includeSignatures: false # default true\n```\n\nProvides a list of files and the call signatures of top-level classes, functions, and methods in those files. This helps the model better understand how a particular piece of code relates to the rest of the codebase.\n\nIn the submenu that appears, you can select either `Entire codebase`, or specify a subfolder to generate the repostiory map from.\n\nThis context provider is inpsired by [Aider's repository map](https://aider.chat/2023/10/22/repomap.html).\n\n### `@Operating System`\n\nReference the architecture and platform of your current operating system.\n\n```yaml config.yaml\ncontext:\n  - provider: os\n```\n\n### `@HTTP`\n\nThe HttpContextProvider makes a POST request to the url passed in the configuration. The server must return 200 OK with a ContextItem object or an array of ContextItems.\n\n```yaml config.yaml\ncontext:\n  - provider: http\n    params:\n      url: \"https://api.example.com/v1/users\"\n      headers:\n        - Authorization: \"Bearer <token>\"\n```\n\nThe receiving URL should expect to receive the following parameters:\n\nPOST parameters\n\n```json\n{  query: string,  fullInput: string}\n```\n\nThe response 200 OK should be a JSON object with the following structure:\n\nResponse\n\n```\n[  {    \"name\": \"\",    \"description\": \"\",    \"content\": \"\"  }]// OR{  \"name\": \"\",  \"description\": \"\",  \"content\": \"\"}\n```\n\n### Model Context Protocol\n\nThe [Model Context Protocol](https://modelcontextprotocol.io/introduction) is a standard proposed by Anthropic to unify prompts, context, and tool use. Continue supports any MCP server with the MCP context provider. Read their [quickstart](https://modelcontextprotocol.io/quickstart) to learn how to set up a local server and then set up your configuration like this:\n\n```yaml config.yaml\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n```\n\nYou'll then be able to type \"@\" and see \"MCP\" in the context providers dropdown.\n\n## Deprecated Context Providers\n\n<Note>\n  To provide conext beyond the built-in context providers, we now recommend\n  using [MCP Servers](/customize/mcp-tools)\n</Note>\n\nView the [deprecated context providers](/reference/deprecated-context-providers) for documentation on:\n\n- [`@Codebase`](/reference/deprecated-codebase) - Use the [codebase awareness guide](/guides/codebase-documentation-awareness) instead\n- [`@Folder`](/reference/deprecated-codebase) - Use the [codebase awareness guide](/guides/codebase-documentation-awareness) instead\n- [`@Docs`](/reference/deprecated-docs) - Use the [documentation awareness guide](/guides/codebase-documentation-awareness) instead\n- `@Greptile` - Query Greptile index\n- `@Commits` - Reference git commits\n- `@Discord` - Reference Discord messages\n- `@Jira` - Reference Jira issues\n- `@Gitlab Merge Request` - Reference GitLab MRs\n- `@Google` - Google search results\n- `@Database` - Database schemas\n- `@Issue` - GitHub issues\n- `@Url` - URL content\n- `@Search` - Codebase search\n- `@Web` - Web search results\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/development-data.mdx","content":"---\ntitle: \"How to Collect and Manage Development Data in Continue\"\ndescription: Collecting data on how you build software\nkeywords: [development data, dev data, LLM-aided development]\nsidebarTitle: \"Development Data\"\n---\n\nWhen you use Continue, you automatically collect data on how you build software. By default, this development data is saved to `.continue/dev_data` on your local machine.\n\nYou can read more about how development data is generated as a byproduct of LLM-aided development and why we believe that you should start collecting it now: [It‚Äôs time to collect data on how you build software](https://blog.continue.dev/its-time-to-collect-data-on-how-you-build-software)\n\n## How to Configure Custom Data Destinations\n\nYou can also configure custom destinations for your data, including remote HTTP endpoints and local file directories.\n\nData destinations should be configured directly in the `data` section of your `config.yaml` file. See more details about adding `data` configuration in the [YAML specification](/reference#data).\n\nWhen sending development data to your own HTTP endpoint, it will receive an event JSON blob at the given `schema` version. You can view event names, schema versions, and fields [here in the source code](https://github.com/continuedev/continue/tree/main/packages/config-yaml/src/schemas/data).\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/rules.mdx","content":"---\ntitle: \"How to Create and Manage Rules in Continue\"\ndescription: \"Rules are used to provide system message instructions to the model for Agent mode, Chat mode, and Edit mode requests\"\nkeywords: [rules, system, prompt, message]\nsidebarTitle: \"Rules\"\n---\n\nRules provide instructions to the model for [Agent mode](../../ide-extensions/agent/quick-start), [Chat](../../ide-extensions/chat/quick-start), and [Edit](../../ide-extensions/edit/quick-start) requests.\n\n<Info>\n  Rules are not included in [autocomplete](./autocomplete) or\n  [apply](../model-roles/apply).\n</Info>\n\n## How Rules Work in Continue\n\nYou can view the current rules by clicking the pen icon above the main toolbar:\n\n![rules input toolbar section](/images/notch-rules.png)\n\nTo form the system message, rules are joined with new lines, in the order they appear in the toolbar. This includes the base chat system message ([see below](#how-to-customize-chat-system-message)).\n\n## Understanding Hub vs Local Rules Integration\n\n<Warning>\n**Important:** Rules created in different locations behave differently and have different synchronization patterns.\n</Warning>\n\nContinue supports two types of rules with different behaviors:\n\n<Columns cols={2}>\n<Card title=\"Local Rules\" icon=\"folder\">\n- **Location**: `.continue/rules` folder in your workspace\n- **Visibility**: Automatically visible when using Hub configs\n- **Creation**: Add rules button in VSCode or manual file creation\n- **File Management**: Creates actual `.md` files you can edit directly\n</Card>\n\n<Card title=\"Hub Rules\" icon=\"cloud\">\n- **Location**: Stored on Continue Mission Control, referenced in config.yaml\n- **Visibility**: Only appear when referenced in assistant configuration\n- **Creation**: Created directly on Hub or copied from local rules\n- **File Management**: No local files created, managed through Hub interface\n</Card>\n</Columns>\n\n### How Rules Are Applied\n\nWhen using Continue, rules are loaded in this order:\n1. **Hub assistant rules** (if using a Hub-based assistant)\n2. **Referenced Hub rules** (via `uses:` in config.yaml)\n3. **Local workspace rules** (from `.continue/rules` folder)\n4. **Global rules** (from `~/.continue/rules` folder)\n\n<Info>\n**TL;DR**: Local rules show up automatically when using Hub configs. Hub rules show up automatically when referenced in your config.\n</Info>\n\n## Quick Start: How to Create Your First Rule File\n\nBelow is a quick example of setting up a new rule file:\n\n1. Create a folder called `.continue/rules` at the top level of your workspace\n2. Add a file called `pirates-rule.md` to this folder.\n3. Write the following contents to `pirates-rule.md` and save.\n\n```md title=\".continue/rules/pirates-rule.md\"\n---\nname: Pirate rule\n---\n\n- Talk like a pirate.\n```\n\nNow test your rules by asking a question about a file in chat.\n\n![pirate rule test](/images/pirate-rule-test.png)\n\n## How to Create Rules\n\n### Creating Local Rules\n\nRules can be added locally using the \"Add Rules\" button.\n\n![add local rules button](/images/add-local-rules.png)\n\n<Info>\n**Automatically create local rules**: When in Agent mode, you can prompt the agent to create a rule for you using the `create_rule_block` tool if enabled.\n\nFor example, you can say \"Create a rule for this\", and a rule will be created for you in `.continue/rules` based on your conversation.\n</Info>\n\n### Creating Hub Rules\n\nRules can also be created and managed on the Continue Mission Control:\n\n1. **Browse existing rules**: [Explore available rules](https://hub.continue.dev/hub?type=rules)\n2. **Create new rules**: [Create your own](https://hub.continue.dev/new?type=block&blockType=rules) in the Hub\n3. **Copy from local rules**: Copy/paste content from your `.continue/rules` files to create Hub rules\n\n### Working Between Hub and Local Rules\n\n<Tabs>\n<Tab title=\"Hub to VSCode\">\nTo use Hub rules in your local setup:\n\n1. Reference them in your `config.yaml`:\n   ```yaml\n   rules:\n     - uses: username/my-hub-rule\n   ```\n2. The rule will automatically appear in your rules toolbar\n3. **Note**: No local file is created - the rule exists only on the Hub\n</Tab>\n\n<Tab title=\"VSCode to Hub\">\nTo move local rules to the Hub:\n\n1. Copy the content from your `.continue/rules/rule-name.md` file\n2. Go to [Create new rule](https://hub.continue.dev/new?type=block&blockType=rules)\n3. Paste the content and configure the rule\n4. Optionally, remove the local file and reference the Hub rule in your config\n\n<Warning>\n**Current Limitation**: There's no automatic sync from local to Hub. You must manually copy/paste rule content.\n</Warning>\n</Tab>\n</Tabs>\n\n### How to Configure Rule Properties and Syntax\n\n<Info>\n  Rules were originally defined in YAML format (demonstrated below), but we\n  introduced Markdown for easier editing. While both are still supported, we\n  recommend Markdown.\n</Info>\n\nRules can be simple text, written in YAML configuration files, or as Markdown (`.md`) files. They can have the following properties:\n\n- `name` (**required** for YAML): A display name/title for the rule\n- `globs` (optional): When files are provided as context that match this glob pattern, the rule will be included. This can be either a single pattern (e.g., `\"**/*.{ts,tsx}\"`) or an array of patterns (e.g., `[\"src/**/*.ts\", \"tests/**/*.ts\"]`).\n- `regex` (optional): When files are provided as context and their content matches this regex pattern, the rule will be included. This can be either a single pattern (e.g., `\"^import .* from '.*';$\"`) or an array of patterns (e.g., `[\"^import .* from '.*';$\", \"^export .* from '.*';$\"]`).\n- `description` (optional): A description for the rule. Agents may read this description when `alwaysApply` is false to determine whether the rule should be pulled into context.\n- `alwaysApply`: Determines whether the rule is always included. Behavior is described below:\n  - `true`: Always included, regardless of file context\n  - `false`: Included if globs exist AND match file context, or the agent decides to pull the rule into context based on its description\n  - `undefined` (default behavior): Included if no globs exist OR globs exist and match\n\n<Tabs>\n<Tab title=\"Markdown\">\n```md title=\"doc-standards.md\"\n---\nname: Documentation Standards\nglobs: docs/**/*.{md,mdx}\nalwaysApply: false\ndescription: Standards for writing and maintaining Continue Docs\n---\n\n# Continue Docs Standards\n\n- Follow Mintlify documentation standards\n- Include YAML frontmatter with title, description, and keywords\n- Use consistent heading hierarchy starting with h2 (##)\n- Include relevant Admonition components for tips, warnings, and info\n- Use descriptive alt text for images\n- Include cross-references to related documentation\n- Reference other docs with relative paths\n- Keep paragraphs concise and scannable\n- Use code blocks with appropriate language tags\n\n````\n</Tab>\n\n<Tab title=\"YAML\">\n```yaml title=\"doc-standards.yaml\"\nname: Documentation Standards\nversion: 1.0.0\nschema: v1\n\nrules:\n  - name: Documentation Standards\n    globs: docs/**/*.{md,mdx}\n    alwaysApply: false\n    rule: >\n      - Follow Mintlify documentation standards\n      - Include YAML frontmatter with title, description, and keywords\n      - Use consistent heading hierarchy starting with h2 (##)\n      - Include relevant Admonition components for tips, warnings, and info\n      - Use descriptive alt text for images\n      - Include cross-references to related documentation\n      - Reference other docs with relative paths\n      - Keep paragraphs concise and scannable\n      - Use code blocks with appropriate language tags\n````\n\n</Tab>\n</Tabs>\n\n### How to Set Up Project-Specific Rules\n\nYou can create project-specific rules by adding a `.continue/rules` folder to the root of your project and adding new rule files.\n\nRules files are loaded in lexicographical order, so you can prefix them with numbers to control the order in which they are applied. For example: `01-general.md`, `02-frontend.md`, `03-backend.md`.\n\n### Example: How to Create TypeScript-Specific Rules\n\n```md title=\".continue/rules/typescript.md\"\n---\nname: TypeScript Best Practices\nglobs: [\"**/*.ts\", \"**/*.tsx\"]\n---\n\n# TypeScript Rules\n\n- Always use TypeScript interfaces for object shapes\n- Use type aliases sparingly, prefer interfaces\n- Include proper JSDoc comments for public APIs\n- Use strict null checks\n- Prefer readonly arrays and properties where possible\n- modularize components into smaller, reusable pieces\n```\n\n## Troubleshooting Rules\n\n### Issue: Rules Created in Different Places Don't Sync\n\n**Problem**: You created rules in the Hub but don't see them in VSCode, or vice versa.\n\n**Solution**:\n- **Hub rules** only appear when referenced in your config.yaml using the `uses:` syntax\n- **Local rules** automatically appear when using Hub configs\n- There's currently no automatic bidirectional sync\n\n### Issue: \"Edit\" Links Point to Wrong Location\n\n**Problem**: When you click \"Edit\" on a rule in VSCode, it tries to open the Hub even though the rule is local, or shows an incorrect URL.\n\n**Root Cause**: This happens when you have a mix of local and Hub rules, and Continue can't properly determine where each rule originates.\n\n**Workaround**:\n1. **For local rules**: Navigate directly to `.continue/rules/` folder and edit the `.md` file\n2. **For Hub rules**: Go directly to your assistant page on [Continue Mission Control](https://hub.continue.dev) and edit from there\n3. Keep track of which rules are local vs Hub-based to avoid confusion\n\n<Info>\n**Known Issue**: This link accuracy problem is tracked in [Linear issue CON-3084](https://linear.app/continue/issue/CON-3084) and will be fixed in a future update.\n</Info>\n\n### Issue: Rules Don't Appear in Assistant\n\n**Problem**: Your rules exist but don't show up in the rules toolbar.\n\n**Check These**:\n1. **File location**: Ensure local rules are in `.continue/rules/` (not `.continue/rule/`)\n2. **File format**: Rules should be `.md` files with proper YAML frontmatter\n3. **Config reference**: Hub rules must be referenced in `config.yaml`\n4. **Assistant type**: Ensure you're using the correct assistant (local vs Hub)\n\n### How to Customize Chat System Message\n\nContinue includes a simple default system message for [Agent mode](../../ide-extensions/agent/quick-start) and [Chat](../../ide-extensions/chat/quick-start) requests, to help the model provide reliable codeblock formats in its output.\n\nThis can be viewed in the rules section of the toolbar (see above), or in the source code [here](https://github.com/continuedev/continue/blob/main/core/llm/constructMessages.ts#L4).\n\nAdvanced users can override this system message for a specific model if needed by using `chatOptions.baseSystemMessage`. See the [`config.yaml` reference](/reference#models).\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/autocomplete.mdx","content":"---\ntitle: \"Continue Autocomplete Setup and Configuration Guide\"\ndescription: \"Step-by-step guide to setting up and configuring autocomplete in Continue, including Codestral, Ollama, and IDE settings.\"\nkeywords: [autocomplete]\nsidebarTitle: Autocomplete\n---\n\nimport { ModelRecommendations } from \"/snippets/ModelRecommendations.jsx\";\n\n## Model Recommendations for Autocomplete\n\n<ModelRecommendations role=\"autocomplete\" />\n\n## How to Set Up Autocomplete in Continue with Codestral (Recommended)\n\nIf you want to have the best autocomplete experience, we recommend using Codestral, which is available through the [Mistral API](https://console.mistral.ai/). To do this, obtain an API key and add it to your config:\n\n<Tabs>\n  <Tab title=\"Hub\">\n  [Mistral Codestral model block](https://hub.continue.dev/mistral/codestral)\n  </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Codestral\n      provider: mistral\n      model: codestral-latest\n      apiKey: <YOUR_CODESTRAL_API_KEY>\n      roles:\n        - autocomplete\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"tabAutocompleteModel\": {\n      \"title\": \"Codestral\",\n      \"provider\": \"mistral\", \n      \"model\": \"codestral-latest\",\n      \"apiKey\": \"<YOUR_CODESTRAL_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  **Codestral API Key**: The API keys for Codestral and the general Mistral APIs\n  are different. If you are using Codestral, you probably want a Codestral API\n  key, but if you are sharing the key as a team or otherwise want to use\n  `api.mistral.ai`, then make sure to set `\"apiBase\":\n  \"https://api.mistral.ai/v1\"` in your `tabAutocompleteModel`.\n</Tip>\n\n## How to Set Up Autocomplete in Continue with Ollama (Local Model)\n\nIf you'd like to run your autocomplete model locally, we recommend using Ollama. To do this, first download the latest version of Ollama from [here](https://ollama.ai). Then, run the following command to download our recommended model:\n\n```bash\nollama run qwen2.5-coder:1.5b\n```\n\nThen, add the model to your configuration:\n\n<Tabs>\n  <Tab title=\"Hub\">\n  [Ollama Qwen 2.5 Coder 1.5B model block](https://hub.continue.dev/ollama/qwen2.5-coder-1.5b)\n  </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Qwen 1.5b Autocomplete Model\n      provider: ollama\n      model: qwen2.5-coder:1.5b\n      roles:\n        - autocomplete\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"tabAutocompleteModel\": {\n      \"title\": \"Qwen 1.5b Autocomplete Model\",\n      \"provider\": \"ollama\", \n      \"model\": \"qwen2.5-coder:1.5b\",\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\nOnce the model has been downloaded, you should begin to see completions in VS Code.\n\n<Note>\n  Typically, thinking-type models are not recommended as they generate more\n  slowly and are not suitable for scenarios that require speed.\n</Note>\n\nHowever, if you use any thinking-switchable models, you can configure these models for autocomplete functions by turning off the thinking mode.\n\nFor example:\n\n<Tabs>\n<Tab title=\"YAML\">\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 0.0.1\nschema: v1\n\nmodels:\n  - name: Qwen3 without Thinking for Autocomplete\n    provider: ollama\n    model: qwen3:4b # qwen3 is a thinking-switchable model\n    roles:\n      - autocomplete\n    requestOptions:\n      extraBodyProperties:\n        think: false # turning off the thinking\n```\n\n</Tab>\n</Tabs>\n\nThen, in the continue panel, select this model as the default model for autocomplete.\n\n## Autocomplete Configuration Options in Continue\n\n### Autocomplete Models Available on the Continue Mission Control\n\nExplore autocomplete model configurations on [the hub](https://hub.continue.dev/explore/models?roles=autocomplete)\n\n### Customize Autocomplete User Settings in the Continue Extension\n\n{/* - `Use autocomplete cache`: If on, caches completions */}\nThe following settings can be configured for autocompletion in the IDE extension User Settings Page:\n\n- `Multiline Autocompletions`: Controls multiline completions for autocomplete. Can be set to `always`, `never`, or `auto`. Defaults to `auto`\n- `Disable autocomplete in files`: List of comma-separated glob pattern to disable autocomplete in matching files. E.g., \"\\_/.md, \\*/.txt\"\n\n### How to Configure Autocomplete with `config.json` (Deprecated Format)\n\n#### YAML Configuration\n\nThe `config.yaml` format offers model-level configuration using the `autocompleteOptions` field. See the [YAML Reference](/reference#models) for more details.\n\n```yaml\nmodels:\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    roles:\n      - autocomplete\n    autocompleteOptions:\n      disable: false\n      maxPromptTokens: 1024\n      debounceDelay: 250\n      modelTimeout: 150\n      maxSuffixPercentage: 0.2\n      prefixPercentage: 0.3\n      onlyMyCode: true\n```\n\n#### JSON Configuration (Deprecated)\n\nThe `config.json` configuration format offers configuration options through `tabAutocompleteOptions`. See the [JSON Reference](/reference/json-reference#tabautocomplete-options) for more details.\n\n## Autocomplete FAQs and Troubleshooting in Continue\n\n### I want better completions, should I use GPT-5?\n\nPerhaps surprisingly, the answer is no. The models that we suggest for autocomplete are trained with a highly specific prompt format, which allows them to respond to requests for completing code (see examples of these prompts [here](https://github.com/continuedev/continue/blob/main/core/autocomplete/templating/AutocompleteTemplate.ts)). Some of the best commercial models like GPT-5 or Claude are not trained with this prompt format, which means that they won't generate useful completions. Luckily, a huge model is not required for great autocomplete. Most of the state-of-the-art autocomplete models are no more than 10b parameters, and increasing beyond this does not significantly improve performance.\n\n### Autocomplete Not Working ‚Äì How to Fix It\n\nFollow these steps to ensure that everything is set up correctly:\n\n1. Make sure you have the \"Enable Tab Autocomplete\" setting checked (in VS Code, you can toggle by clicking the \"Continue\" button in the status bar, and in JetBrains by going to Settings -> Tools -> Continue).\n2. Make sure you have downloaded Ollama.\n3. Run `ollama run qwen2.5-coder:1.5b` to verify that the model is downloaded.\n4. Make sure that any other completion providers are disabled (e.g. Copilot), as they may interfere.\n5. Check the output of the logs to find any potential errors: <kbd>cmd/ctrl</kbd> + <kbd>shift</kbd> + <kbd>P</kbd> -> \"Toggle Developer Tools\" -> \"Console\" tab in VS Code, ~/.continue/logs/core.log in JetBrains.\n6. Check VS Code settings to make sure that `\"editor.inlineSuggest.enabled\"` is set to `true` (use <kbd>cmd/ctrl</kbd> + <kbd>,</kbd> then search for this and check the box)\n7. If you are still having issues, please let us know in our [Discord](https://discord.gg/vapESyrFmJ) and we'll help as soon as possible.\n\n### Why Are My Completions Only Single-Line?\n\nTo ensure that you receive multi-line completions, you can set `\"multilineCompletions\": \"always\"` in `tabAutocompleteOptions`. By default, it is `\"auto\"`. If you still find that you are only seeing single-line completions, this may be because some models tend to produce shorter completions when starting in the middle of a file. You can try temporarily moving text below your cursor out of your active file, or switching to a larger model.\n\n### How to Set a Trigger Key for Autocomplete Suggestions\n\nIn VS Code, if you don't want to be shown suggestions automatically you can:\n\n1. Set `\"editor.inlineSuggest.enabled\": false` in VS Code settings to disable automatic suggestions\n2. Open \"Keyboard Shortcuts\" (cmd/ctrl+k, cmd/ctrl+s) and search for `editor.action.inlineSuggest.trigger`\n3. Click the \"+\" icon to add a new keybinding\n4. Press the key combination you want to use to trigger suggestions (e.g. <kbd>cmd/ctrl</kbd> + <kbd>space</kbd>)\n5. Now whenever you want to see a suggestion, you can press your key binding (e.g. <kbd>cmd/ctrl</kbd> + <kbd>space</kbd>) to trigger suggestions manually\n\n### Shortcut for Accepting One Line at a Time in Autocomplete\n\nThis is a built-in feature of VS Code, but it's just a bit hidden. Follow these settings to reassign the keyboard shortcuts in VS Code:\n\n1. Press <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>P</kbd>, type the command: `Preferences: Open Keyboard Shortcuts`, and enter the keyboard shortcuts settings page.\n2. Search for `editor.action.inlineSuggest.acceptNextLine`.\n3. Set the key binding to <kbd>Tab</kbd>.\n4. Set the trigger condition (when) to `inlineSuggestionVisible && !editorReadonly`.\n   This will make multi-line completion (including continue and from VS Code built-in or other plugin snippets) still work, and you will see multi-line completion. However, Tab will only fill in one line at a time. Any unnecessary code can be canceled with <kbd>Esc</kbd>.\n   If you need to apply all the code, just press <kbd>Tab</kbd> multiple times.\n\n### How to Turn Off Autocomplete in Continue (VS Code and JetBrains)\n\n#### VS Code\n\nClick the \"Continue\" button in the status panel at the bottom right of the screen. The checkmark will become a \"cancel\" symbol and you will no longer see completions. You can click again to turn it back on.\n\nAlternatively, open VS Code settings, search for \"Continue\" and uncheck the box for \"Enable Tab Autocomplete\".\n\nYou can also use the default shortcut to disable autocomplete directly using a chord: press and hold <kbd>ctrl/cmd</kbd> + <kbd>K</kbd> (continue holding <kbd>ctrl/cmd</kbd>) and press <kbd>ctrl/cmd</kbd> + <kbd>A</kbd>. This will turn off autocomplete without navigating through settings.\n\n#### JetBrains\n\nOpen Settings -> Tools -> Continue and uncheck the box for \"Enable Tab Autocomplete\".\n\n#### Feedback\n\nIf you're turning off autocomplete, we'd love to hear how we can improve! Please let us know in our [Discord](https://discord.gg/vapESyrFmJ) or file an issue on GitHub.\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/_apply.mdx","content":"---\ndescription: Learn how to use the Apply feature\nkeywords: [apply, speculative]\ntoc_max_heading_level: 5\n---\n\n{/* This is a work in progress and excluded by prefixing the file name with an underscore */}\n\n## Instant Diff Apply\n\nIf you have large files (exeeding the output token of your model) the edit apply will delete all lines that exceed the model window. To mitigate you can instead of applying the changes directly, ask the chat LLM to \"generate a unified diff\".\nThe resulting diff (if the LLM has not made a mistake) can be instantly applied, changing only the lines that need to change.\n\nExplainer Video: https://youtu.be/b7Xxsot4gyw\n"}
{"source":"github","repo":"continue","path":"docs/customize/deep-dives/configuration.mdx","content":"---\ntitle: \"How to Configure Continue\"\ndescription: Learn how to access and manage Continue configurations through Hub or local YAML files\nkeywords: [config, settings, customize]\nsidebarTitle: \"Configuration\"\n---\n\nYou can easily access your configuration from the Continue Chat sidebar. Open the sidebar by pressing <kbd>cmd/ctrl</kbd> + <kbd>L</kbd> (VS Code) or <kbd>cmd/ctrl</kbd> + <kbd>J</kbd> (JetBrains) and click the Agent selector above the main chat input. Then, you can hover over an agent and click the `new window` (hub agents) or `gear` (local agents) icon.\n\n![configure](/images/configure-continue.png)\n\n## How to Manage Hub Configs\n\n  Hub Configs can be managed in [the Hub](https://hub.continue.dev). See [Editing a config](/hub/configs/edit-a-config)\n\n## How to Configure Local Configs with YAML\n\nLocal user-level configuration is stored and can be edited in your home directory in `config.yaml`:\n\n- `~/.continue/config.yaml` (MacOS / Linux)\n- `%USERPROFILE%\\.continue\\config.yaml` (Windows)\n\nTo open this `config.yaml`, you need to open the configs dropdown in the top-right portion of the chat input. On that dropdown beside the \"Local Config\" option, select the cog icon. It will open the local `config.yaml`.\n\n![local-config-open-steps](/images/local-config-open-steps.png)\n\nWhen editing this file, you can see the available options suggested as you type, or check the reference below. When you save a config file from the IDE, Continue will automatically refresh to take into account your changes. A config file is automatically created the first time you use Continue, and always automatically generated with default values if it doesn't exist.\n\nSee the full reference for `config.yaml` [here](/reference).\n\n## Legacy Configuration Methods (Deprecated)\n\n<Info>\n  View the `config.json` migration guide [here](/reference/yaml-migration)\n</Info>\n\n- [`config.json`](/reference) - The original configuration format which is stored in a file at the same location as `config.yaml`\n- [`.continuerc.json`](#how-to-use-continuercjson-for-workspace-configuration) - Workspace-level configuration\n- [`config.ts`](#how-to-use-configts-for-advanced-configuration) - Advanced configuration (probably unnecessary) - a TypeScript file in your home directory that can be used to programmatically modify (_merged_) the `config.json` schema:\n  - `~/.continue/config.ts` (MacOS / Linux)\n  - `%USERPROFILE%\\.continue\\config.ts` (Windows)\n\n### How to Use `.continuerc.json` for Workspace Configuration\n\nThe format of `.continuerc.json` is the same as `config.json`, plus one _additional_ property `mergeBehavior`, which can be set to either \"merge\" or \"overwrite\". If set to \"merge\" (the default), `.continuerc.json` will be applied on top of `config.json` (arrays and objects are merged). If set to \"overwrite\", then every top-level property of `.continuerc.json` will overwrite that property from `config.json`.\n\nExample\n\n```json title=\".continuerc.json\"\n{\n  \"tabAutocompleteOptions\": {\n    \"disable\": true\n  },\n  \"mergeBehavior\": \"overwrite\"\n}\n```\n\n### How to Use `config.ts` for Advanced Configuration\n\n`config.yaml` or `config.json` can handle the vast majority of necessary configuration, so we recommend using it whenever possible. However, if you need to programmatically extend Continue configuration, you can use a `config.ts` file, placed at `~/.continue/config.ts` (MacOS / Linux) or `%USERPROFILE%\\.continue\\config.ts` (Windows).\n\n`config.ts` must export a `modifyConfig` function, like:\n\n<Warning>\n  The `slashCommands` array shown below is deprecated. For creating custom slash\n  commands, use [prompt files](./prompts) instead.\n</Warning>\n\n```ts title=\"config.ts\"\nexport function modifyConfig(config: Config): Config {\n  config.slashCommands?.push({\n    name: \"commit\",\n    description: \"Write a commit message\",\n    run: async function* (sdk) {\n      // The getDiff function takes a boolean parameter that indicates whether\n      // to include unstaged changes in the diff or not.\n      const diff = await sdk.ide.getDiff(false); // Pass false to exclude unstaged changes\n      for await (const message of sdk.llm.streamComplete(\n        `${diff}\\n\\nWrite a commit message for the above changes. Use no more than 20 tokens to give a brief description in the imperative mood (e.g. 'Add feature' not 'Added feature'):`,\n        new AbortController().signal,\n        {\n          maxTokens: 20,\n        },\n      )) {\n        yield message;\n      }\n    },\n  });\n  return config;\n}\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/telemetry.mdx","content":"---\ntitle: \"Telemetry\"\ndescription: \"Learn about Continue's anonymous telemetry collection practices, what usage data is collected, and how to opt out if you prefer not to share your usage information\"\n---\n\n## Overview\n\nThe open-source Continue Extensions collect and report **anonymous** usage information to help us improve our product. This data enables us to understand user interactions and optimize the user experience effectively. You can opt out of telemetry collection at any time if you prefer not to share your usage information.\n\nWe utilize [Posthog](https://posthog.com/), an open-source platform for product analytics, to gather and store this data. For transparency, you can review the implementation code [here](https://github.com/continuedev/continue/blob/main/gui/src/hooks/CustomPostHogProvider.tsx) or read our [official privacy policy](https://continue.dev/privacy).\n\n## Tracking Policy\n\nAll data collected by the open-source Continue extensions is anonymized and stripped of personally identifiable information (PII) before being sent to PostHog. We are committed to maintaining the privacy and security of your data.\n\n## What We Track\n\nThe following usage information is collected and reported:\n\n- **Suggestion Interactions:** Whether you accept or reject suggestions (excluding the actual code or prompts involved).\n- **Model and Command Information:** The name of the model and command used.\n- **Token Metrics:** The number of tokens generated.\n- **System Information:** The name of your operating system (OS) and integrated development environment (IDE).\n- **Pageviews:** General pageview statistics.\n\n## How to Opt Out\n\nYou can disable anonymous telemetry by toggling \"Allow Anonymous Telemetry\" off in the user settings.\n\nAlternatively in VS Code, you can disable telemetry through your VS Code settings by unchecking the \"Continue: Telemetry Enabled\" box (this will override the Settings Page settings). VS Code settings can be accessed with `File` > `Preferences` > `Settings` (or use the keyboard shortcut `ctrl` + `,` on Windows/Linux or `cmd` + `,` on macOS).\n"}
{"source":"github","repo":"continue","path":"docs/customize/models.mdx","content":"---\ntitle: \"Models\"\ndescription: \"Models form the foundation of the entire agent experience, offering different specialized capabilities:\"\n---\n\nimport { ModelRecommendations } from \"/snippets/ModelRecommendations.jsx\";\n\n- **[Chat](/customize/model-roles/chat)**: Power conversational interactions about code and provide detailed guidance\n- **[Edit](/customize/model-roles/edit)**: Handle complex code transformations and refactoring tasks\n- **[Apply](/customize/model-roles/apply)**: Execute targeted code modifications with high accuracy\n- **[Autocomplete](/customize/model-roles/autocomplete)**: Provide real-time suggestions as developers type\n- **[Embedding](/customize/model-roles/embeddings)**: Transform code into vector representations for semantic search\n- **[Reranker](/customize/model-roles/reranking)**: Improve search relevance by ordering results based on semantic meaning\n\n![Models Overview](/images/customize/images/model-blocks-overview-36c30e7e01928d7a9b5b26ff1639c34b.png)\n\n## Recommended Models\n\n### Best Models by Role\n\n<ModelRecommendations role=\"all\" />\n\n## Learn More About Models\n\nContinue supports [many model providers](/customize/model-providers/top-level/openai), including Anthropic, OpenAI, Gemini, Ollama, Amazon Bedrock, Azure, xAI, and more. Models can have various roles like `chat`, `edit`, `apply`, `autocomplete`, `embed`, and `rerank`.\n\nRead more about [model roles](/customize/model-roles), [model capabilities](/customize/deep-dives/model-capabilities) and view [`models`](/reference#models) in the YAML Reference.\n\n### Example Model Setup Instructions\n\n# Frontier Models\n\n[Claude Sonnet 4.5](https://hub.continue.dev/anthropic/claude-sonnet-4-5) from Anthropic\n\n1. Get your API key from [Anthropic](https://console.anthropic.com/)\n2. Add[Claude Sonnet 4.5](https://hub.continue.dev/anthropic/claude-sonnet-4-5) to a config on Continue Mission Control\n3. Add `ANTHROPIC_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Qwen Coder 3 480B](https://hub.continue.dev/openrouter/qwen3-coder) from Qwen\n\n1. Get your API key from [OpenRouter](https://openrouter.ai/settings/keys)\n2. Add [Qwen Coder 3 480B](https://hub.continue.dev/openrouter/qwen3-coder)  a config on Continue Mission Control\n3. Add `OPENROUTER_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[GPT-5](https://hub.continue.dev/openai/gpt-5) from OpenAI\n\n1. Get your API key from [OpenAI](https://platform.openai.com)\n2. Add [GPT-5](https://hub.continue.dev/openai/gpt-5)  a config on Continue Mission Control\n3. Add `OPENAI_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Kimi K2](https://hub.continue.dev/openrouter/kimi-k2) from Moonshot AI\n\n1. Get your API key from [OpenRouter](https://openrouter.ai/settings/keys)\n2. Add [Kimi K2](https://hub.continue.dev/openrouter/kimi-k2)  a config on Continue Mission Control\n3. Add `OPENROUTER_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Gemini 2.5 Pro](https://hub.continue.dev/google/gemini-2.5-pro) from Google\n\n1. Get your API key from [Google AI Studio](https://aistudio.google.com)\n2. Add [Gemini 2.5 Pro](https://hub.continue.dev/google/gemini-2.5-pro)  a config on Continue Mission Control\n3. Add `GEMINI_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Grok Code Fast 1](https://hub.continue.dev/xai/grok-code-fast-1) from xAI\n\n1. Get your API key from [xAI](https://console.x.ai/)\n2. Add [Grok Code Fast 1](https://hub.continue.dev/xai/grok-code-fast-1)  a config on Continue Mission Control\n3. Add `XAI_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Devstral Medium](https://hub.continue.dev/mistral/devstral-medium) from Mistral AI\n\n1. Get your API key from [Mistral AI](https://console.mistral.ai/)\n2. Add [Devstral Medium](https://hub.continue.dev/mistral/devstral-medium)  a config on Continue Mission Control\n3. Add `MISTRAL_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n[gpt-oss-120b](https://hub.continue.dev/openrouter/gpt-oss-120b) from OpenAI\n\n1. Get your API key from [OpenRouter](https://openrouter.ai/settings/keys)\n2. Add [gpt-oss-120b](https://hub.continue.dev/openrouter/gpt-oss-120b)  a config on Continue Mission Control\n3. Add `OPENROUTER_API_KEY` as a [User Secret](https://docs.continue.dev/mission-control/secrets/secret-types#user-secrets) on Continue Mission Control [here](https://hub.continue.dev/settings/secrets)\n4. Click `Reload config` in the config selector in the Continue IDE extension\n\n### Local Models\n\n<Info>\nNeed a quick setup walkthrough? Check out [Using Ollama with Continue: A Developer's Guide](https://docs.continue.dev/guides/ollama-guide).\n</Info>\n\nThese models can be run on your computer if you have enough VRAM.\n\nTheir limited tool calling and reasoning capabilities will make it challenging to use agent mode.\n\n[Qwen3 Coder 30B](https://hub.continue.dev/ollama/qwen3-coder-30b)\n\n1. Add [Qwen3 Coder 30B](https://hub.continue.dev/ollama/qwen3-coder-30b)  a config on Continue Mission Control\n2. Run the model with [Ollama](https://docs.continue.dev/guides/ollama-guide#using-ollama-with-continue-a-developers-guide)\n3. Click `Reload config` in the config selector in the Continue IDE extension\n\n[gpt-oss-20b](https://hub.continue.dev/ollama/gpt-oss-20b)\n\n1. Add [gpt-oss-20b](https://hub.continue.dev/ollama/gpt-oss-20b)  a config on Continue Mission Control\n2. Run the model with [Ollama](https://docs.continue.dev/guides/ollama-guide#using-ollama-with-continue-a-developers-guide)\n3. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Devstral Small 27B](https://hub.continue.dev/ollama/devstral)\n\n1. Add [Devstral Small](https://hub.continue.dev/ollama/devstral)  a config on Continue Mission Control\n2. Run the model with [Ollama](https://docs.continue.dev/guides/ollama-guide#using-ollama-with-continue-a-developers-guide)\n3. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Qwen2.5-Coder 7B](https://hub.continue.dev/ollama/qwen2.5-coder-7b) from Qwen\n\n1. Add [Qwen2.5-Coder 7B](https://hub.continue.dev/ollama/qwen2.5-coder-7b)  a config on Continue Mission Control\n2. Run the model with [Ollama](https://docs.continue.dev/guides/ollama-guide#using-ollama-with-continue-a-developers-guide)\n3. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Gemma 3 4B](https://hub.continue.dev/ollama/gemma3-4b) from Google\n\n1. Add [Gemma 3 4B](https://hub.continue.dev/ollama/gemma3-4b)  a config on Continue Mission Control\n2. Run the model with [Ollama](https://docs.continue.dev/guides/ollama-guide#using-ollama-with-continue-a-developers-guide)\n3. Click `Reload config` in the config selector in the Continue IDE extension\n\n[Qwen2.5-Coder 1.5B](https://hub.continue.dev/ollama/qwen2.5-coder-1.5b) from Qwen\n\n1. Add [Qwen2.5-Coder 1.5B](https://hub.continue.dev/ollama/qwen2.5-coder-1.5b)  a config on Continue Mission Control\n2. Run the model with [Ollama](https://docs.continue.dev/guides/ollama-guide#using-ollama-with-continue-a-developers-guide)\n3. Click `Reload config` in the config selector in the Continue IDE extension\n"}
{"source":"github","repo":"continue","path":"docs/customize/rules.mdx","content":"---\ntitle: \"Rules\"\ndescription: \"Rules allow you to provide specific instructions that guide how the AI agent behaves when working with your code. Instead of the AI making assumptions about your coding standards, architecture patterns, or project-specific requirements, you can explicitly define guidelines that ensure consistent, contextually appropriate responses.\"\n---\n\nThink of these as the guardrails for your AI coding agents:\n\n- **Enforce company-specific coding standards** and security practices\n- **Implement quality checks** that match your engineering culture\n- **Create paved paths** for developers to follow organizational best practices\n\nBy implementing rules, you transform the AI from a generic coding agent into a knowledgeable team member that understands your project's unique requirements and constraints.\n\n## How Rules Work\n\nYour agent detects rules and applies the specified rules while in [Agent](/ide-extensions/agent/quick-start), [Chat](/ide-extensions/chat/quick-start), and [Edit](/ide-extensions/edit/quick-start) modes.\n\n## Where to Manage Rules\n\n<Columns cols={2}>\n<Card title=\"Local Rules (.continue/rules)\" icon=\"folder\">\n- Create files in `.continue/rules` folder\n- Automatically appear with Hub configs\n- Edit directly in your file system\n- Version controlled alongside your code\n- Best for project-specific rules (e.g., \"remember to generate migrations after modifying the db\")\n</Card>\n\n<Card title=\"Hub Rules\" icon=\"cloud\">\n- Manage on [Continue Mission Control](https://hub.continue.dev)\n- Reference in config.yaml with `uses:`\n- Share with team and community\n- Easy to include in multiple agents\n- Best for organization-wide rules (e.g., \"always use X library for Y task\")\n</Card>\n</Columns>\n\n<Info>\n**Quick Setup**: Start with local rules for immediate use, then promote commonly used rules to the Hub for sharing and reuse.\n</Info>\n\nLearn more in the [rules deep dive](/customize/deep-dives/rules), and view [`rules`](/reference#rules) in the YAML Reference for more details."}
{"source":"github","repo":"continue","path":"docs/customize/mcp-tools.mdx","content":"---\ntitle: \"MCP servers\"\ndescription: \"Learn how to use Model Context Protocol (MCP) blocks in Continue to integrate external tools, connect databases, and extend your development environment.\"\n---\n\nModel Context Protocol (MCP) servers let Continue connect to external tools, systems, and databases by running MCP servers.\n\nThese servers make it possible to:\n\n- **Enable integration** with external tools and systems\n- **Create extensible interfaces** for custom capabilities\n- **Support complex interactions** with your development environment\n- **Allow partners** to contribute specialized functionality\n- **Connect to databases** to understand schema and data models during development\n\n![MCP servers overview](/images/customize/images/mcp-blocks-overview-c9a104f9b586779c156f9cf34da197c2.png)\n\n## Learn More About MCP servers\n\nLearn more in the [MCP deep dive](/customize/deep-dives/mcp), and view [`mcpServers`](/reference#mcpservers) in the YAML Reference for more details.\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/apply.mdx","content":"---\ntitle: Apply Role\ndescription: Apply model role\nkeywords: [apply, model, role]\nsidebar_position: 4\n---\n\nWhen editing code, Chat and Edit model output often doesn't clearly align with existing code. A model with the `apply` role is used to generate a more precise diff to apply changes to a file.\n\n## Recommended Apply models\n\n<Info>\n  For the latest Apply model recommendations, see our [comprehensive model recommendations](/customize/models#recommended-models).\n</Info>\n\nWe recommend [Morph Fast Apply](https://morphllm.com) or [Relace's Instant Apply model](https://hub.continue.dev/relace/instant-apply) for the fastest Apply experience. You can sign up for Morph's free tier [here](https://morphllm.com/dashboard) or get a Relace API key [here](https://app.relace.ai/settings/api-keys).\n\nHowever, most Chat models can also be used for applying code changes. We recommend smaller/cheaper models for the task, such as Claude 3.5 Haiku.\n\n<Info>\n  Explore all apply models in [the\n  Hub](https://hub.continue.dev/explore/models?roles=apply)\n</Info>\n\n## Prompt templating\n\nYou can customize the prompt template used for applying code changes by setting the `promptTemplates.apply` property in your model configuration. Continue uses [Handlebars syntax](https://handlebarsjs.com/guide/) for templating.\n\nAvailable variables for the apply template:\n\n- `{{{original_code}}}` - The original code before changes\n- `{{{new_code}}}` - The new code after changes\n\nExample:\n\n```yaml\nmodels:\n  - name: My Custom Apply Template\n    provider: anthropic\n    model: claude-3-5-sonnet-latest\n    promptTemplates:\n      apply: |\n        Original: {{{original_code}}}\n        New: {{{new_code}}}\n\n        Please generate the final code without any markers or explanations.\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/reranking.mdx","content":"---\ntitle: Rerank Role\ndescription: Rerank model role\nkeywords: [rerank, reranking, model, role]\nsidebar_position: 6\n---\n\nA \"reranking model\" is trained to take two pieces of text (often a user question and a document) and return a relevancy score between 0 and 1, estimating how useful the document will be in answering the question. Rerankers are typically much smaller than LLMs, and will be extremely fast and cheap in comparison.\n\nIn Continue, rerankers are designated using the `rerank` role and used by [codebase awareness](/guides/codebase-documentation-awareness) in order to select the most relevant code snippets after vector search.\n\n## Recommended reranking models\n\n<Info>\n  For a comparison of all reranking models including open and closed options, see our [comprehensive model recommendations](/customize/models#recommended-models).\n</Info>\n\nIf you have the ability to use any model, we recommend `rerank-2` by Voyage AI, which is listed below along with the rest of the options for rerankers.\n\n### Voyage AI\n\nVoyage AI offers the best reranking model for code with their `rerank-2` model. After obtaining an API key from [here](https://www.voyageai.com/), you can configure a reranker as follows:\n\n<Tabs>\n  <Tab title=\"Hub\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - uses: voyageai/rerank-2\n  ```\n  </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: My Voyage Reranker\n      provider: voyage\n      apiKey: <YOUR_VOYAGE_API_KEY>\n      model: rerank-2\n      roles:\n        - rerank\n  ```\n  </Tab>\n</Tabs>\n\n### Cohere\n\nSee Cohere's documentation for rerankers [here](https://docs.cohere.com/docs/rerank-2).\n\n<Tabs>\n  {/* HUB_TODO block doesn't exist */}\n  {/* <Tab title=\"Hub\">\n  [Cohere Reranker English v3](https://hub.continue.dev/)\n  </Tab> */}\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Cohere Reranker\n      provider: cohere\n      model: rerank-english-v3.0\n      apiKey: <YOUR_COHERE_API_KEY>\n      roles:\n        - rerank\n  ```\n  </Tab>\n</Tabs>\n\n### LLM\n\nIf you only have access to a single LLM, then you can use it as a reranker. This is discouraged unless truly necessary, because it will be much more expensive and still less accurate than any of the above models trained specifically for the task. Note that this will not work if you are using a local model, for example with Ollama, because too many parallel requests need to be made.\n\n<Tabs>\n  {/* HUB_TODO block doesn't exist */}\n  {/* <Tab title=\"Hub\">\n  [GPT-4o LLM Reranker Block](https://hub.continue.dev/)\n  </Tab> */}\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: LLM Reranker\n      provider: openai\n      model: gpt-4o\n      roles:\n        - rerank\n  ```\n  </Tab>\n</Tabs>\n\n\n\n### Text Embeddings Inference\n\n[Hugging Face Text Embeddings Inference](https://huggingface.co/docs/text-embeddings-inference/en/index) enables you to host your own [reranker endpoint](https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/rerank). You can configure your reranker as follows:\n\n<Tabs>\n  {/* HUB_TODO */}\n  {/* <Tab title=\"Hub\">\n  [HuggingFace TEI Reranker block](https://hub.continue.dev/)\n  </Tab> */}\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Huggingface-tei Reranker\n        provider: huggingface-tei\n        model: tei\n        apiBase: http://localhost:8080\n        apiKey: <YOUR_TEI_API_KEY>\n        roles:\n          - rerank\n    ```\n    </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/autocomplete.mdx","content":"---\ntitle: \"Autocomplete Role in Continue Models\"\nsidebarTitle: \"Autocomplete Role\"\ndescription: \"Learn how the autocomplete role works in Continue, which models to use, and how to customize prompt templates for inline code suggestions.\"\nkeywords: [autocomplete, model, role]\nsidebar_position: 2\n---\n\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx'\n\n\nAn \"autocomplete model\" is an LLM that is trained on a special format called fill-in-the-middle (FIM). This format is designed to be given the prefix and suffix of a code file and predict what goes between. This task is very specific, which on one hand means that the models can be smaller (even a 3B parameter model can perform well). On the other hand, this means that Chat models, though larger, will often perform poorly even with extensive prompting.\n\nIn Continue, autocomplete models are used to display inline [Autocomplete](../../ide-extensions/autocomplete/quick-start) suggestions as you type. Autocomplete models are designated by adding the `autocomplete` to the model's `roles` in `config.yaml`.\n\n## Recommended Autocomplete models\n\n<ModelRecommendations role=\"autocomplete\" />\n\nVisit the [Autocomplete Deep Dive](../deep-dives/autocomplete) for detailed setup instructions and configuration options.\n\n## Prompt templating\n\nYou can customize the prompt template used when autocomplete happens by setting the `promptTemplates.autocomplete` property in your model configuration. Continue uses [Handlebars syntax](https://handlebarsjs.com/guide/) for templating.\n\nAvailable variables for the apply template:\n\n- `{{{prefix}}}` - the code before your cursor\n- `{{{suffix}}}` - the code after your cursor\n- `{{{filename}}}` - the name of the file your cursor currently is\n- `{{{reponame}}}` - the name of the folder where the codebase is\n- `{{{language}}}` - the name of the programming language in full (ex. Typescript)\n\nExample:\n\n```yaml\nmodels:\n  - name: My Custom Autocomplete Template\n    provider: ollama\n    model: qwen2.5-coder:1.5b\n    promptTemplates:\n      autocomplete: |\n        `\n        globalThis.importantFunc = importantFunc\n        <|fim_prefix|>{{{prefix}}}<|fim_suffix|>{{{suffix}}}<|fim_middle|>\n        `\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/edit.mdx","content":"---\ntitle: Edit Role\ndescription: Edit model role\nkeywords: [edit, editing, model, role]\nsidebar_position: 3\n---\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx'\n\nIt's often useful to select a different model to respond to Edit instructions than for Chat instructions, as Edits are often more code-specific and may require less conversational readability.\n\nIn Continue, you can add `edit` to a model's roles to specify that it can be used for Edit requests. If no edit models are specified, the selected `chat` model is used.\n\n```yaml title=\"config.yaml\"\nname: My Config\nversion: 0.0.1\nschema: v1\n\nmodels:\n  - name: Claude 4 Sonnet\n    provider: anthropic\n    model: claude-3-5-sonnet-latest\n    apiKey: <YOUR_ANTHROPIC_API_KEY>\n    roles:\n      - edit\n```\n\nExplore edit models in [the hub](https://hub.continue.dev/explore/models?roles=edit). Generally, our recommendations for Edit overlap with recommendations for Chat.\n\n## Model Recommendations\n\n<ModelRecommendations role=\"chat_edit\" />\n\n## Prompt templating\n\nYou can customize the prompt template used for editing code by setting the `promptTemplates.edit` property in your model configuration. Continue uses [Handlebars syntax](https://handlebarsjs.com/guide/) for templating.\n\nAvailable variables for the edit template:\n\n- `{{{userInput}}}` - The user's edit request/instruction\n- `{{{language}}}` - The programming language of the code\n- `{{{codeToEdit}}}` - The code that's being edited\n- `{{{prefix}}}` - Content before the edit area\n- `{{{suffix}}}` - Content after the edit area\n- `{{{supportsCompletions}}}` - Whether the model supports completions API\n- `{{{supportsPrefill}}}` - Whether the model supports prefill capability\n\nExample:\n\n```yaml\nmodels:\n  - name: My Custom Edit Template\n    provider: openai\n    model: gpt-4o\n    promptTemplates:\n      edit: |\n        `Here is the code before editing:\n        \\`\\`\\`{{{language}}}\n        {{{codeToEdit}}}\n        \\`\\`\\`\n\n        Here is the edit requested:\n        \"{{{userInput}}}\"\n\n        Here is the code after editing:`\n```\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/chat.mdx","content":"---\ntitle: Chat Role\ndescription: Chat model role\nkeywords: [chat, model, role]\nsidebar_position: 1\n---\n\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx'\n\n\nA \"chat model\" is an LLM that is trained to respond in a conversational format. Because they should be able to answer general questions and generate complex code, the best chat models are typically large, often 405B+ parameters.\n\nIn Continue, these models are used for normal [Chat](../../ide-extensions/chat/quick-start). The selected chat model will also be used for [Edit](../../ide-extensions/edit/quick-start) and [Apply](./apply.mdx) if no `edit` or `apply` models are specified, respectively.\n\n## Recommended Chat models\n\n<ModelRecommendations role=\"chat_edit\" />\n## Best overall experience\n\nFor the best overall Chat experience, you will want to use a 400B+ parameter model or one of the frontier models.\n\n### Claude Opus 4.5 and Claude Sonnet 4 from Anthropic\n\nOur current top recommendations are Claude Opus 4.5 and Claude Sonnet 4 from [Anthropic](../model-providers/top-level/anthropic).\n\n<Tabs>\n  <Tab title=\"Hub\">\n  View the [Claude Opus 4.5 model block](https://hub.continue.dev/anthropic/claude-4-5-opus) or [Claude Sonnet 4 model block](https://hub.continue.dev/anthropic/claude-4-sonnet) on the hub.\n  </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Claude Opus 4.5\n      provider: anthropic\n      model: claude-opus-4-5\n      apiKey: <YOUR_ANTHROPIC_API_KEY>\n  ```\n  </Tab>\n</Tabs>\n\n### Gemma from Google DeepMind\n\nIf you prefer to use an open-weight model, then the Gemma family of Models from Google DeepMind is a good choice. You will need to decide if you use it through a SaaS model provider, e.g. [Together](../model-providers/more/together), or self-host it, e.g. [Ollama](../model-providers/top-level/ollama).\n\n<Tabs>\n  <Tab title=\"Hub\">\n    <Tabs>\n        <Tab title=\"Ollama\">\n        Add the [Ollama Gemma 3 27B block](https://hub.continue.dev/ollama/gemma3-27b) from the hub\n        </Tab>\n        <Tab title=\"Together\">\n        Add the [Together Gemma 2 27B Instruct block](https://hub.continue.dev/togetherai/gemma-2-instruct-27b) from the hub\n        </Tab>\n    </Tabs>\n  </Tab>\n  <Tab title=\"YAML\">\n    <Tabs>\n        <Tab title=\"Ollama\">\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        models:\n          - name: \"Gemma 3 27B\"\n            provider: \"ollama\"\n            model: \"gemma3:27b\"\n        ```\n        </Tab>\n        <Tab title=\"Together\">\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        models:\n          - name: \"Gemma 3 27B\"\n            provider: \"together\"\n            model: \"google/gemma-2-27b-it\"\n            apiKey: <YOUR_TOGETHER_API_KEY>\n        ```\n        </Tab>\n    </Tabs>\n  </Tab>\n</Tabs>\n\n### GPT-5.1 from OpenAI\n\nIf you prefer to use a model from [OpenAI](../model-providers/top-level/openai), then we recommend GPT-5.1.\n\n<Tabs>\n    <Tab title=\"Hub\">\n    Add the [OpenAI GPT-5.1 block](https://hub.continue.dev/openai/gpt-5.1) from the hub\n    </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: GPT-5.1\n      provider: openai\n      model: gpt-5.1\n      apiKey: <YOUR_OPENAI_API_KEY>\n  ```\n  </Tab>\n</Tabs>\n\n### Grok-4 from xAI\n\nIf you prefer to use a model from [xAI](../model-providers/more/xAI), then we recommend Grok-4.\n\n<Tabs>\n    <Tab title=\"Hub\">\n    Add the [xAI Grok-4.1 block](https://hub.continue.dev/xai/grok-4-1-fast-non-reasoning) from the hub\n    </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Grok-4.1\n      provider: xAI\n      model: grok-4-1-fast-non-reasoning\n      apiKey: <YOUR_XAI_API_KEY>\n  ```\n  </Tab>\n</Tabs>\n\n### Gemini 3 Pro from Google\n\nIf you prefer to use a model from [Google](../model-providers/top-level/gemini), then we recommend Gemini 3 Pro.\n\n<Tabs>\n    <Tab title=\"Hub\">\n    Add the [Gemini 3 Pro block](https://hub.continue.dev/google/gemini-3-pro-preview) from the hub\n    </Tab>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Gemini 3 Pro\n      provider: gemini\n      model: gemini-3-pro-preview\n      apiKey: <YOUR_GEMINI_API_KEY>\n  ```\n  </Tab>\n</Tabs>\n\n## Local, offline experience\n\nFor the best local, offline Chat experience, you will want to use a model that is large but fast enough on your machine.\n\n### Llama 3.1 8B\n\nIf your local machine can run an 8B parameter model, then we recommend running Llama 3.1 8B on your machine (e.g. using [Ollama](../model-providers/top-level/ollama) or [LM Studio](../model-providers/top-level/lmstudio)).\n\n<Tabs>\n  <Tab title=\"Hub\">\n    <Tabs>\n        <Tab title=\"Ollama\">\n    Add the [Ollama Llama 3.1 8b block](https://hub.continue.dev/ollama/llama3.1-8b) from the hub\n    </Tab>\n    {/* HUB_TODO nonexistent block */}\n    {/* <Tab title=\"LM Studio\">\n    Add the [LM Studio Llama 3.1 8b block](https://hub.continue.dev/explore/models) from the hub\n    </Tab> */}\n    </Tabs>\n  </Tab>\n  <Tab title=\"YAML\">\n    <Tabs>\n      <Tab title=\"Ollama\">\n      ```yaml title=\"config.yaml\"\n      name: My Config\n      version: 0.0.1\n      schema: v1\n\n      models:\n        - name: Llama 3.1 8B\n          provider: ollama\n          model: llama3.1:8b\n      ```\n      </Tab>\n      <Tab title=\"LM Studio\">\n      ```yaml title=\"config.yaml\"\n      name: My Config\n      version: 0.0.1\n      schema: v1\n\n      models:\n        - name: Llama 3.1 8B\n          provider: lmstudio\n          model: llama3.1:8b\n      ```\n      </Tab>\n      <Tab title=\"Msty\">\n      ```yaml title=\"config.yaml\"\n      name: My Config\n      version: 0.0.1\n      schema: v1\n\n      models:\n        - name: Llama 3.1 8B\n          provider: msty\n          model: llama3.1:8b\n      ```\n      </Tab>\n    </Tabs>\n  </Tab>\n</Tabs>\n\n### DeepSeek Coder 2 16B\n\nIf your local machine can run a 16B parameter model, then we recommend running DeepSeek Coder 2 16B (e.g. using [Ollama](../model-providers/top-level/ollama) or [LM Studio](../model-providers/top-level/lmstudio)).\n\n<Tabs>\n  {/* HUB_TODO nonexistent blocks */}\n  {/* <Tab title=\"Hub\">\n    <Tabs>\n    <Tab title=\"Ollama\">\n    Add the [Ollama Deepseek Coder 2 16B block](https://hub.continue.dev/explore/models) from the hub\n    </Tab>\n    <Tab title=\"LM Studio\">\n    Add the [LM Studio Deepseek Coder 2 16B block](https://hub.continue.dev/explore/models) from the hub\n    </Tab>\n    </Tabs>\n  </Tab> */}\n  <Tab title=\"YAML\">\n    <Tabs>\n        <Tab title=\"Ollama\">\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        models:\n          - name: DeepSeek Coder 2 16B\n            provider: ollama\n            model: deepseek-coder-v2:16b\n        ```\n        </Tab>\n        <Tab title=\"LM Studio\">\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        models:\n          - name: DeepSeek Coder 2 16B\n            provider: lmstudio\n            model: deepseek-coder-v2:16b\n        ```\n        </Tab>\n        <Tab title=\"Msty\">\n        ```yaml title=\"config.yaml\"\n        name: My Config\n        version: 0.0.1\n        schema: v1\n\n        models:\n          - name: DeepSeek Coder 2 16B\n            provider: msty\n            model: deepseek-coder-v2:16b\n        ```\n        </Tab>\n    </Tabs>\n  </Tab>\n</Tabs>\n\n## Other experiences\n\nThere are many more models and providers you can use with Chat beyond those mentioned above. Read more [here](../model-roles/chat.mdx)\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/embeddings.mdx","content":"---\ntitle: Embed Role\ndescription: Embed model role\nkeywords: [embedding, model, role, embeddings]\nsidebar_position: 5\n---\n\nAn \"embeddings model\" is trained to convert a piece of text into a vector, which can later be rapidly compared to other vectors to determine similarity between the pieces of text. Embeddings models are typically much smaller than LLMs, and will be extremely fast and cheap in comparison.\n\nIn Continue, embeddings are generated during indexing and then used by [codebase awareness](/guides/codebase-documentation-awareness) to perform similarity search over your codebase.\n\nYou can add `embed` to a model's `roles` to specify that it can be used to embed.\n\n<Info>\n  [Built-in model (VS Code only)] `transformers.js` is used as a built-in\n  embeddings model in VS Code. In JetBrains, there currently is no built-in\n  embedder.\n</Info>\n\n## Recommended embedding models\n\n<Info>\n  See our [comprehensive model recommendations](/customize/models#recommended-models) for the best embedding models comparison.\n</Info>\n\nIf you have the ability to use any model, we recommend `voyage-code-3`, which is listed below along with the rest of the options for embeddings models.\n\nIf you want to generate embeddings locally, we recommend using `nomic-embed-text` with [Ollama](../model-providers/top-level/ollama#embeddings-model).\n\n### Voyage AI\n\nAfter obtaining an API key from [here](https://www.voyageai.com/), you can configure like this:\n\n<Tabs>\n  <Tab title=\"Hub\">\n  [Voyage Code 3 Embedder Block](https://hub.continue.dev/voyageai/voyage-code-3)\n  </Tab>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: Voyage Code 3\n        provider: voyage\n        model: voyage-code-3\n        apiKey: <YOUR_VOYAGE_API_KEY>\n        roles: \n          - embed\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"embeddingsProvider\": {\n        \"provider\": \"voyage\",\n        \"model\": \"voyage-code-3\",\n        \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\"\n      }\n    }\n    ```\n    </Tab>\n</Tabs>\n\n### Ollama\n\nSee [here](../model-providers/top-level/ollama#embeddings-model) for instructions on how to use Ollama for embeddings.\n\n### Transformers.js (currently VS Code only)\n\n[Transformers.js](https://huggingface.co/docs/transformers.js/index) is a JavaScript port of the popular [Transformers](https://huggingface.co/transformers/) library. It allows embeddings to be calculated entirely locally. The model used is `all-MiniLM-L6-v2`, which is shipped alongside the Continue extension.\n\n<Tabs>\n    <Tab title=\"YAML\">\n    ```yaml title=\"config.yaml\"\n    name: My Config\n    version: 0.0.1\n    schema: v1\n\n    models:\n      - name: default-transformers\n        provider: transformers.js\n        roles:\n          - embed\n    ```\n    </Tab>\n    <Tab title=\"JSON\">\n    ```json title=\"config.json\"\n    {\n      \"embeddingsProvider\": {\n        \"provider\": \"transformers.js\"\n      }\n    }\n    ```\n    </Tab>\n</Tabs>\n\n### Text Embeddings Inference\n\n[Hugging Face Text Embeddings Inference](https://huggingface.co/docs/text-embeddings-inference/en/index) enables you to host your own embeddings endpoint. You can configure embeddings to use your endpoint as follows:\n\n<Tabs>\n  {/* HUB_TODO nonexistent block */}\n  {/* <Tab title=\"Hub\">\n  [HuggingFace Text Embedder Block](https://hub.continue.dev/)\n  </Tab> */}\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: Huggingface TEI Embedder\n      provider: huggingface-tei\n      apiBase: http://localhost:8080\n      apiKey: <YOUR_TEI_API_KEY>\n      roles: [embed]\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"embeddingsProvider\": {\n      \"provider\": \"huggingface-tei\",\n      \"apiBase\": \"http://localhost:8080\",\n      \"apiKey\": \"<YOUR_TEI_API_KEY>\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### OpenAI\n\nSee [here](../model-providers/top-level/openai#how-to-configure-openai-embeddings-models) for instructions on how to use OpenAI for embeddings.\n\n### Cohere\n\nSee [here](../model-providers/more/cohere#embeddings-model) for instructions on how to use Cohere for embeddings.\n\n### Gemini\n\nSee [here](../model-providers/top-level/gemini#how-to-configure-gemini-embeddings-models) for instructions on how to use Gemini for embeddings.\n\n### Vertex\n\nSee [here](../model-providers/top-level/vertexai#how-to-configure-vertex-ai-embeddings-models) for instructions on how to use Vertex for embeddings.\n\n### Mistral\n\nSee [here](../model-providers/more/mistral#how-to-configure-mistral-embeddings-models) for instructions on how to use Mistral for embeddings.\n\n### NVIDIA\n\nSee [here](../model-providers/more/nvidia#embeddings-model) for instructions on how to use NVIDIA for embeddings.\n\n### Bedrock\n\nSee [here](../model-providers/top-level/bedrock#how-to-configure-amazon-bedrock-embeddings-models) for instructions on how to use Bedrock for embeddings.\n\n### WatsonX\n\nSee [here](../model-providers/more/watsonx#embeddings-model) for instructions on how to use WatsonX for embeddings.\n\n### LMStudio\n\nSee [here](../model-providers/top-level/lmstudio#embeddings-model) for instructions on how to use LMStudio for embeddings.\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/00-intro.mdx","content":"---\ntitle: Intro to Roles\ndescription: Apply model role\nkeywords: [apply, model, role]\nsidebar_position: 0\nsidebar_label: Introduction\n---\n\nModels in Continue can be configured to be used for various roles in the extension.\n\n- [`chat`](./chat.mdx): Used for chat conversations in the extension sidebar\n- [`autocomplete`](./autocomplete): Used for autocomplete code suggestions in the editor\n- [`edit`](./edit.mdx): Used to generate code based on edit prompts\n- [`apply`](./apply.mdx): Used to decide how to apply edits to a file\n- [`embed`](./embeddings.mdx): Used to generate embeddings used for vector search (@Codebase and @Docs context providers)\n- [`rerank`](./reranking.mdx): Used to rerank results from vector search\n\nThese roles can be specified for a `config.yaml` model block using `roles`. See the [YAML Specification](/reference#models) for more details.\n\n<Info>\n  For recommendations on which models work best for each role, see our [comprehensive model recommendations](/customize/models#recommended-models).\n</Info>\n\n## Selecting model roles\n\nYou can control which of the models in your config for a given role will be currently used for that role. Above the main input, click the 3 dots and then the cube icon to expand the `Models` section. Then you can use the dropdowns to select an active model for each role.\n\n![Settings Active Models Section](/images/settings-model-roles.png)\n\n<Info>\n  `roles` are not explicitly defined within `config.json` (deprecated) - they\n  are infered by the top level keys like `embeddingsProvider`\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles/intro.mdx","content":"---\ntitle: \"Intro to Roles\"\ndescription: \"Models in Continue can be configured to be used for various roles in the extension.\"\nsidebarTitle: \"Introduction\"\nicon: \"book-open\"\n---\n\n- [`chat`](/customize/model-roles/chat): Used for chat conversations in the extension sidebar\n- [`autocomplete`](/customize/model-roles/autocomplete): Used for autocomplete code suggestions in the editor\n- [`edit`](/customize/model-roles/edit): Used to generate code based on edit instructions\n- [`apply`](/customize/model-roles/apply): Used to decide how to apply edits to a file\n- [`embed`](/customize/model-roles/embeddings): Used to generate embeddings used for vector search (@Codebase and @Docs context providers)\n- [`rerank`](/customize/model-roles/reranking): Used to rerank results from vector search\n\nThese roles can be specified for a `config.yaml` model block using `roles`. See the [YAML Specification](/reference#models) for more details.\n\n## Selecting model roles\n\nYou can control which of the models in your config for a given role will be currently used for that role. Above the main input, click the 3 dots and then the cube icon to expand the `Models` section. Then you can use the dropdowns to select an active model for each role.\n\n<Frame>\n  <img src=\"/images/settings-model-roles-5e5f8a6bd9137b70cf94178a7e45847c.png\" />\n</Frame>\n\n<Note>\n  ### INFO\n\n`roles` are not explicitly defined within `config.json` (deprecated) - they are infered by the top level keys like `embeddingsProvider`\n\n</Note>\n"}
{"source":"github","repo":"continue","path":"docs/customize/overview.mdx","content":"---\ntitle: \"Customization Overview\"\ndescription: \"Learn how to customize Continue with model providers, rules, prompts, and tools\"\n---\n\nContinue can be deeply customized to fit your specific development workflow and preferences. This guide covers the main ways you can customize Continue to enhance your coding experience.\n\n## Change Your Model Provider\n\nContinue allows you to choose your favorite or even add multiple model providers. This allows you to use different models for different tasks, or to try another model if you're not happy with the results from your current model. Continue supports all of the popular model providers, including OpenAI, Anthropic, Microsoft/Azure, Mistral, and more. You can even self host your own model provider if you'd like.\n\n[Learn more about model providers ‚Üí](/customize/model-providers/overview)\n\n## Select Different Models for Specific Tasks\n\nDifferent Continue features can use different models. We call these _model roles_. For example, you can use a different model for Chat mode than you do for Autocomplete.\n\n[Learn more about model roles ‚Üí](/customize/model-roles)\n\n## Create Custom Prompts\n\nCreate custom slash commands and prompt templates to streamline your workflow.\n\n[Learn more about prompts ‚Üí](/customize/prompts)\n\n## Configure Rules\n\nSet up rules to guide AI responses and ensure consistent behavior across your codebase.\n\n[Learn more about rules ‚Üí](/customize/rules)\n\n## Call External Tools and Functions\n\nGive your agent the power of tools using [Agent mode in the extensions](/ide-extensions/agent/quick-start) or add custom tools to your agent using [MCP Servers](/customize/mcp-tools).\n\n[Learn more about MCP tools ‚Üí](/customize/mcp-tools)\n\n## Customize VS Code Settings\n\nAdjust IDE-specific settings to optimize your Continue experience.\n\n[Learn more about settings ‚Üí](/ide-extensions/settings)\n\n## Deep Dives\n\nDetailed technical explanations of Continue's internal workings and advanced configuration options.\n\n[Read Deep Dives ‚Üí](/customize/deep-dives/configuration)\n\n## Reference\n\nComplete configuration reference and API documentation.\n\n[View Reference ‚Üí](/reference)\n\n\n---\n\nWhatever you choose, you'll probably start by editing your configuration.\n\n## Edit Your Configuration\n\nYou can easily access your configuration from the Continue Chat sidebar. Open the sidebar by pressing `cmd/ctrl` + `L` (VS Code) or `cmd/ctrl` + `J` (JetBrains) and click the Agent selector above the main chat input. Then, you can hover over an agent and click the `new window` (hub agents) or `gear` (local agents) icon.\n\n![configure](/images/customize/images/configure-continue-a5c8c79f3304c08353f3fc727aa5da7e.png)\n\n## Manage Your Configuration\n\n- See [Editing Hub Configurations](/hub/configs/edit-a-config) for more details on managing your hub configuration\n- See the [Config Deep Dive](/reference) for more details on local configurations\n"}
{"source":"github","repo":"continue","path":"docs/customize/model-roles.mdx","content":"---\ntitle: \"Model roles\"\nsidebarTitle: Overview\nicon: \"circle-info\"\ndescription: \"Learn about the different model roles in Continue including chat, autocomplete, edit, apply, embeddings, and reranking for customizing your AI coding agent's capabilities\"\n---\n\n<CardGroup>\n  <Card title=\"Introduction\" href=\"/customize/model-roles\" icon=\"file-lines\" horizontal >\n    Apply model role\n  </Card>\n\n<Card\n  title=\"Chat Role\"\n  href=\"/customize/model-roles/chat\"\n  icon=\"file-lines\"\n  horizontal\n>\n  Chat model role\n</Card>\n\n<Card\n  title=\"Autocomplete Role\"\n  href=\"/customize/model-roles/autocomplete\"\n  icon=\"file-lines\"\n  horizontal\n>\n  Autocomplete model role\n</Card>\n\n<Card\n  title=\"Edit Role\"\n  href=\"/customize/model-roles/edit\"\n  icon=\"file-lines\"\n  horizontal\n>\n  Edit model role\n</Card>\n\n<Card\n  title=\"Apply Role\"\n  href=\"/customize/model-roles/apply\"\n  icon=\"file-lines\"\n  horizontal\n>\n  Apply model role\n</Card>\n\n<Card\n  title=\"Embeddings Role\"\n  href=\"/customize/model-roles/embeddings\"\n  icon=\"file-lines\"\n  horizontal\n>\n  Embeddings model role\n</Card>\n\n  <Card title=\"Rerank Role\" href=\"/customize/model-roles/reranking\" icon=\"file-lines\" horizontal >\n    Rerank model role\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/getting-started/extensions.mdx","content":"---\ntitle: \"Understanding Configs\"\ndescription: \"Continue offers two ways to configure your AI agents\"\n---\n\nThis content has moved to our comprehensive guide: [Understanding Hub vs Local Configuration](/guides/understanding-configs)\n"}
{"source":"github","repo":"continue","path":"docs/agents/create-and-edit.mdx","content":"---\ntitle: \"Create and Edit Agents\"\ndescription: \"Build custom AI workflows with prompts, rules, and tools through the Continue Mission Control interface\"\nsidebarTitle: \"Create & Edit\"\n---\n\n<Card title=\"Creating Custom Agents\" icon=\"wrench\">\n  You can only create and edit Agents through Mission Control in the Continue Mission Control web interface. \n  This ensures proper validation, versioning, and team collaboration features.\n</Card>\n\n## Creating an Agent\n\n  ![Create an agent gif](/images/hub/create-an-agent.gif)\n\n\n<Steps>\n  <Step title=\"üß≠ Navigate to Create Agent\">\n    From the Continue Mission Control top navigation bar, select **\"+\"** ‚Üí **\"New Agent\"**.\n    \n    This opens the **Create an Agent (beta)** form with all required fields.\n\n  </Step>\n\n  <Step title=\"üß© Fill Out the Agent Form\">\n    Configure your agent with the following fields:\n\n    | Field | What to Enter | Example |\n    |-------|---------------|---------|\n    | **Name** | Display name shown in Mission Control | `GitHub PR Agent` |\n    | **Prompt** | First instruction the agent receives | `Open a GitHub PR to fix the specified issue.` |\n    | **Description** | What the agent does | `Creates a pull request and includes AI-generated summaries.` |\n    | **Tools (MCPs)** | Select built-in or custom MCPs | `GitHub, PostHog, Supabase` |\n    | **Rules** | Add any organizational rules | `continuedev/gh-pr-commit-workflow, continuedev/summarization` |\n    | **Model** | Choose a default LLM | `Claude Sonnet 4.5` |\n    | **Owner + Slug** | Determines namespace | `my-org/github-pr-agent` |\n    | **Visibility** | Access level | `Public, Organization, or Private` |\n\n    <Info>\n      **Tip**: Start with a simple prompt and add complexity through rules and tools. The prompt should be a clear, single instruction that defines the agent's primary goal.\n    </Info>\n  </Step>\n\n  <Step title=\"üí° Preview the Configuration\">\n    As you fill in the fields, the right-hand panel shows a live preview of your agent's YAML configuration:\n\n    ```yaml\n    name: Open PR with Fix\n    description: Open a GitHub PR to fix the specified issue\n    tools: built_in, anthropic/github-mcp\n    rules:\n      - continuedev/gh-pr-commit-workflow\n      - continuedev/summarization\n    model: Claude Sonnet 4.5\n    visibility: public\n    ```\n\n    This preview helps you verify the configuration before creating the agent.\n  </Step>\n\n  <Step title=\"‚úÖ Create and Test\">\n    Click **\"Create Agent\"** to save and publish it.\n    \n    Your agent is immediately available to run in:\n    - Mission Control web interface\n    - TUI mode: `cn --agent your-org/your-agent-name`\n    - Headless mode: `cn --agent -p your-org/your-agent-name \"prompt\" --auto`\n  </Step>\n</Steps>\n\n## Editing an Agent\n\nYou can edit any agent you own or that belongs to your organization.\n\n<Steps>\n  <Step title=\"üß≠ Access the Agent\">\n    From the **Agents** page or Mission Control view, click your agent's name, then select **\"Edit Agent\"**.\n  </Step>\n\n  <Step title=\"üîß Update Configuration\">\n    You can modify any of these components:\n\n    <Tabs>\n      <Tab title=\"Prompt\">\n        **Refine task behavior**\n        \n        ```\n        # Before\n        Fix the bug in the authentication system\n        \n        # After  \n        Fix the authentication bug by:\n        1. Identifying the root cause\n        2. Implementing a secure solution\n        3. Adding appropriate tests\n        4. Opening a PR with detailed explanation\n        ```\n      </Tab>\n\n      <Tab title=\"Rules\">\n        **Enforce team standards**\n        \n        Add or remove organizational rules:\n        - `continuedev/security-first` - Always scan for vulnerabilities\n        - `continuedev/test-coverage` - Require tests for new code\n        - `my-org/style-guide` - Follow company coding standards\n      </Tab>\n\n      <Tab title=\"Tools (MCPs)\">\n        **Connect additional systems**\n        \n        - Add GitHub MCP for repository operations\n        - Include Sentry MCP for error monitoring  \n        - Connect Supabase MCP for database operations\n        - Use custom MCPs for internal tools\n      </Tab>\n\n      <Tab title=\"Model & Visibility\">\n        **Adjust behavior and access**\n        \n        - **Model**: Switch between Claude, GPT-4, or other supported LLMs\n        - **Visibility**: Control who can see and use the agent\n          - Public: Anyone can discover and use\n          - Organization: Only your team members\n          - Private: Only you can access\n      </Tab>\n    </Tabs>\n  </Step>\n\n  <Step title=\"üíæ Save Changes\">\n    Click **\"Update Agent\"** when finished.\n    \n    The updated version is instantly available to your team with automatic versioning for change tracking.\n  </Step>\n</Steps>\n\n<Warning>\n  Test your agent thoroughly to ensure they interact as expected.\n</Warning>\n\n## Example Agent Configurations\n\nHere are proven agent configurations you can create or use as inspiration:\n\n<CardGroup cols={2}>\n  <Card title=\"Security Scanner Agent\" icon=\"shield\" href=\"https://hub.continue.dev/continuedev/snyk-continuous-ai-agent\">\n    **Snyk Continuous AI Agent** - Comprehensive security scanning with Snyk MCP integration.\n    \n    Automates dependency analysis, vulnerability scanning, and creates remediation PRs with AI-powered fix suggestions.\n  </Card>\n\n  <Card title=\"Performance Monitor\" icon=\"gauge\" href=\"https://hub.continue.dev/continuedev/netlify-continuous-ai-agent\">\n    **Netlify Continuous AI Agent** - Performance optimization with A/B testing and monitoring.\n    \n    Tracks Core Web Vitals, identifies regressions, and provides optimization recommendations.\n  </Card>\n\n  <Card title=\"Error Monitoring Agent\" icon=\"bug\" href=\"https://hub.continue.dev/continuedev/sentry-continuous-ai-agent\">\n    **Sentry Continuous AI Agent** - Automated error analysis and issue creation.\n    \n    Monitors production errors, provides root cause analysis, and creates actionable GitHub issues.\n  </Card>\n\n  <Card title=\"GitHub Management\" icon=\"github\" href=\"https://hub.continue.dev/continuedev/github-manager-ai-agent\">\n    **GitHub Manager AI Agent** - Comprehensive GitHub workflow automation.\n    \n    Handles issue triage, PR reviews, and release note generation with natural language prompts.\n  </Card>\n\n  <Card title=\"Database Management\" icon=\"database\" href=\" https://hub.continue.dev/continuedev/supabase-agent\">\n    **Supabase Continuous AI Agent** - Database security and management workflows.\n    \n    Audits Row Level Security, identifies vulnerabilities, and generates fixes automatically.\n  </Card>\n</CardGroup>\n\n\n## Troubleshooting\n\nCommon issues and solutions:\n\n| Problem | Solution |\n|---------|----------|\n| **Agent doesn't complete tasks** | Simplify the prompt, add more specific instructions, verify tool permissions |\n| **Tools aren't working** | Check MCP configuration, verify API keys and permissions in Hub settings |\n| **Inconsistent behavior** | Add rules to enforce consistent patterns, test with various input scenarios |\n| **Performance issues** | Consider model choice, simplify complex multi-step workflows, optimize tool usage |\n"}
{"source":"github","repo":"continue","path":"docs/agents/overview.mdx","content":"---\ntitle: \"Agents Overview\"\ndescription: \"Standardized AI workflows that combine prompts, rules, and tools to complete specific, repeatable tasks\"\nsidebarTitle: \"Overview\"\n---\n\n<Card title=\"Agents in Continue\" icon=\"robot\">\n  Agents are custom AI workflows that combine a prompt, rules, and tools (MCPs and more) to complete specific, repeatable tasks.\n  They live in the Continue Mission Control and can be executed from Mission Control (web interface), TUI mode, or headless automation.\n</Card>\n\n## What Are Agents?\n\nAgents let your team standardize common workflows like opening GitHub PRs, summarizing analytics, or auditing security. Each agent includes:\n\n### Agent Components\n\n| **Component** | **Description** | **Example** |\n|----------------|-----------------|--------------|\n| **Prompt** | The agent‚Äôs main instruction that defines what it does and how it should behave. Additional user input is appended when invoked. | Example prompt: ‚ÄúSummarize ongoing work with status, blockers, and next steps. Use markdown formatting and a concise, professional tone.‚Äù |\n| **Rules** | Define consistent standards or behaviors that guide how the agent responds. Rules help ensure reliability across team usage. | ‚ÄúAlways include related issue or PR links when summarizing work.‚Äù |\n| **Tools / MCPs** | External systems or integrations the agent can call to complete tasks. MCPs (Model Context Protocols) extend functionality through APIs. | GitHub, PostHog, Supabase |\n| **Model** | The large language model that powers the agent‚Äôs reasoning and output generation. | Claude Sonnet 4.5 |\n| **Visibility** | Determines who can view or use the agent. Controls access at creation time. | Public, Organization, or Private |\n\n<Info>\n  Mission Control is in beta. Please share any feedback with us in [GitHub discussions](https://github.com/continuedev/continue/discussions/8051).\n</Info>\n\n## Ways to Run Agents\n\nYou can run Agents in three main ways:\n\n<Tabs>\n  <Tab title=\"üåê Mission Control\">\n    **Interactive web interface**\n    \n    Trigger from the Continue Mission Control and review results in real-time.\n    \n    ```bash\n    # Navigate to hub.continue.dev/agents\n    # Click \"Run Agent\" on any agent\n    # Monitor progress and review outputs\n    ```\n    \n    Perfect for: Interactive debugging, reviewing agent outputs, team collaboration\n  </Tab>\n\n  <Tab title=\"üíª TUI Mode\">\n    **Interactive terminal mode**\n    \n    Launch from terminal for live interaction and testing.\n    \n    ```bash\n    cn --agent my-org/github-pr-agent\n    # Interactive chat interface opens\n    # Type your specific request\n    # Review and approve actions\n    ```\n    \n    Perfect for: Local development, testing prompts, quick one-off tasks\n  </Tab>\n\n  <Tab title=\"ü§ñ Headless Mode\">\n    **Automated execution**\n    \n    Run one-off or scheduled tasks automatically without interaction.\n    \n    ```bash\n    cn -p --agent my-org/snyk-agent \"Run weekly security scan\" --auto\n    ```\n    \n    Perfect for: CI/CD pipelines, scheduled tasks, webhook integrations\n  </Tab>\n</Tabs>\n\n## Pre-configured Agents\n\nSkip the setup and use battle-tested agents from our cookbook collection:\n\n<CardGroup cols={2}>\n  <Card title=\"üîí Security Scanning\" icon=\"shield-check\" href=\"/guides/snyk-mcp-continue-cookbook\">\n    `continuedev/snyk-continuous-ai-agent`\n    Finds vulnerabilities and opens PRs with fixes\n  </Card>\n  \n  <Card title=\"üêõ GitHub Management\" icon=\"github\" href=\"/guides/github-mcp-continue-cookbook\">\n    `continuedev/github-project-manager-agent`\n    Triages issues and manages project workflows\n  </Card>\n  \n  <Card title=\"üìä Analytics Insights\" icon=\"chart-bar\" href=\"/guides/posthog-github-continuous-ai\">\n    `continuedev/posthog-continuous-ai-agent`\n    Analyzes user data and creates actionable tasks\n  </Card>\n  \n  <Card title=\"‚ö° Performance Optimization\" icon=\"gauge-high\" href=\"/guides/netlify-mcp-continuous-deployment\">\n    `continuedev/netlify-continuous-ai-agent`\n    Monitors Core Web Vitals and optimizes deployments\n  </Card>\n  \n  <Card title=\"üóÑÔ∏è Database Auditing\" icon=\"database\" href=\"/guides/supabase-mcp-database-workflow\">\n    `continuedev/supabase-agent`\n    Audits RLS security and generates migrations\n  </Card>\n  \n  <Card title=\"üîÑ Data Pipelines\" icon=\"arrows-rotate\" href=\"/guides/dlt-mcp-continue-cookbook\">\n    `continuedev/dlt-agent`\n    Inspects pipelines and debugs load errors\n  </Card>\n</CardGroup>\n\n<Info>\n  **Explore More Agents**: Browse the complete collection of pre-configured agents and MCP integration cookbooks in our [Guides Overview](/guides/overview#mcp-integration-cookbooks).\n</Info>\n\n{/* ## Webhook Integrations\n\nTrigger Agents automatically through secure webhooks for CI/CD integration:\n\n```bash\nPOST /webhooks/ingest/:webhookId\n{\n  \"payload\": { \"trigger\": \"nightly-security-scan\" }\n}\n```\n\nPerfect for connecting with:\n- GitHub Actions workflows\n- Monitoring alerts (Sentry, DataDog)\n- Deployment pipelines\n- Scheduled maintenance tasks */}\n\n## Collaboration\n\nAgents are organizational assets ‚Äî once created, everyone in your org can use them:\n\n| Role | Permissions |\n|------|-------------|\n| **Public** | Use, create, remix, and delete Agents |\n| **Organization** | Share agents with your team |\n| **Private** | Create, edit, and delete Agents for your personal use |\n\n## Getting Started\n\n<Steps>\n  <Step title=\"Try a Pre-configured Agent\">\n    Start with a cookbook agent like `continuedev/github-project-manager-agent`:\n    \n    ```bash\n    cn --agent continuedev/github-project-manager-agent \"List open issues labeled bug\"\n    ```\n  </Step>\n  \n  <Step title=\"Practice with TUI Mode\">\n    Test agents locally before automation:\n    \n    ```bash\n    cn --agent continuedev/snyk-continuous-ai-agent\n    # Interactive mode - perfect for learning how to use the agent for your use case\n    ```\n  </Step>\n  \n  <Step title=\"Create Your First Agent\">\n    Ready to build custom workflows? Learn how in our [Create and Edit guide](/hub/agents/create-and-edit).\n  </Step>\n  \n  <Step title=\"Set Up Automation\">\n    Integrate with CI/CD using headless mode and webhooks for continuous workflows.\n  </Step>\n</Steps>\n\n## Monitoring\n\nYou can monitor all activity for your Agents directly in **Mission Control**.\n\n<AccordionGroup>\n\n  <Accordion title=\"Inbox Overview\">\n    The **Inbox** shows every Task or Workflow run, and you can filter by Agent.\n  </Accordion>\n\n  <Accordion title=\"Detailed Session View\">\n    Click any session in the Inbox to open the Detail page.  \n\n    Each session includes:\n    - **Summary** ‚Äî what the agent did and why  \n    - **Diff** ‚Äî generated code changes  \n    - **Logs** ‚Äî full execution trace, tool calls, and reasoning  \n\n    This is the best way to review output, debug issues, or confirm correctness.\n  </Accordion>\n\n</AccordionGroup>\n\n<Info>\nAll monitoring is scoped to your current workspace. \n</Info>\n\n## Best Practices\n\nThe practice of using cloud agents, which we call Continuous AI, requires forethought to set up the right guardrails:\n\n- **Start Small**: Begin with tasks you're confident Continue can accomplish\n- **Use Thorough Prompts**: Agents can run for extended periods, so invest in detailed instructions  \n- **Test Locally First**: Practice with TUI mode before deploying to production\n- **Team Alignment**: Discuss agent usage and adjust code review habits for higher PR volume\n- **Iterate and Improve**: Be willing to refine prompts based on results\n\n## Next Steps\n\n<CardGroup cols={2}>\n  <Card title=\"Create an Agent\" icon=\"plus\" href=\"/hub/agents/create-and-edit\">\n    Build your first custom agent with prompts, rules, and tools\n  </Card>\n  \n  <Card title=\"Explore Cookbooks\" icon=\"book-open\" href=\"/guides/overview#mcp-integration-cookbooks\">\n    Browse pre-built agents for security, analytics, and more\n  </Card>\n  \n  <Card title=\"CLI Guide\" icon=\"terminal\" href=\"/cli/overview\">\n    Learn to run agents from the command line\n  </Card>\n  \n  <Card title=\"Mission Control\" icon=\"desktop\" href=\"https://hub.continue.dev/agents\">\n    Access the web interface to manage agents\n  </Card>\n</CardGroup>"}
{"source":"github","repo":"continue","path":"docs/agents/intro.mdx","content":"---\ntitle: \"Getting Started with Agents\"\ndescription: \"Quick start guide for running AI workflows in Mission Control\"\nsidebarTitle: \"Getting Started\"\n---\n\n<Card title=\"üöÄ Quick Start with Mission Control\" icon=\"rocket\">\n  Mission Control is your web interface for running and managing AI agents. Get started in minutes with pre-built workflows for security scanning, GitHub management, analytics insights, and more.\n</Card>\n\n<Info>\n  Mission Control is in beta. Please share any feedback with us in [GitHub discussions](https://github.com/continuedev/continue/discussions/8051).\n</Info>\n\nUse Mission Control to kick off agents for:\n- **Bug fixes & code quality** - Address nitpicks and technical debt\n- **Feature development** - Build boilerplate-heavy features  \n- **Security & compliance** - [Automated vulnerability scanning](../guides/snyk-mcp-continue-cookbook)\n- **Investigation & analysis** - Research issues to kickstart your work\n- **Custom workflows** - Run repeatable tasks with your own rules and prompts\n\n## Quick Start\n\n<Steps>\n  <Step title=\"Access Mission Control\">\n    Go to [hub.continue.dev/agents](https://hub.continue.dev/agents) and connect with your GitHub account.\n    \n    ![Mission Control Setup](/images/hub/workflows/images/workflows-setup.png)\n  </Step>\n  \n  <Step title=\"Choose an Agent\">\n    Select from pre-configured agents or create your own:\n    \n    <CardGroup cols={2}>\n      <Card title=\"Security Scanner\" icon=\"shield-check\">\n        `continuedev/snyk-continuous-ai-agent`\n        Finds vulnerabilities and creates fix PRs\n      </Card>\n      \n      <Card title=\"GitHub Manager\" icon=\"github\">\n        `continuedev/github-project-manager-agent`  \n        Triages issues and manages workflows\n      </Card>\n    </CardGroup>\n  </Step>\n  \n  <Step title=\"Run Your First Agent\">\n    Enter a prompt and watch your agent work:\n    \n    ```\n    \"Fix the TypeError in api/users.ts and open a PR with tests\"\n    ```\n  </Step>\n</Steps>\n\n<Info>\n  **New to agents?** Check out our [pre-configured agents](/hub/agents/overview#pre-configured-agents) to test out a workflow immediately.\n</Info>\n\n## Try an Agent First: Your 60-Second Challenge\n\nBefore creating your own agent, let's see one in action! The fastest way to experience the power of Continue agents is with our demo repository.\n\n![Agent Mission Control](/images/hub/assets/images/agent-mission-control.gif)\n\n### See an Agent Create a Pull Request in Under 60 Seconds\n\n<Steps>\n  <Step title=\"üç¥ Fork Our Demo Repository\">\n    Get a safe sandbox to experiment with:\n    \n    **Option 1: GitHub Web Interface**\n    Visit [github.com/continuedev/demo-project](https://github.com/continuedev/demo-project) and click **Fork**\n    \n    **Option 2: GitHub CLI**\n    ```bash\n    gh repo fork continuedev/demo-project\n    ```\n  </Step>\n\n  <Step title=\"üéØ Access Mission Control\">\n    Go to [hub.continue.dev/agents](https://hub.continue.dev/agents) and:\n    - **Connect GitHub** and authorize Continue when prompted\n    - This gives agents access to create PRs in your repositories\n  </Step>\n\n  <Step title=\"üöÄ Run Your First Agent\">\n    **Target your forked demo repo** and try one of these commands:\n    \n    <Tabs>\n      <Tab title=\"Add New Feature\">\n        ```\n        Create a new function that calculates fibonacci numbers.\n        ```\n      </Tab>\n      \n      <Tab title=\"Fix Existing Bug\">\n        ```\n        Fix the TypeError in api/users.ts\n        ```\n      </Tab>\n      \n      <Tab title=\"Add Documentation\">\n        ```\n        Add comprehensive README documentation with setup instructions.\n        ```\n      </Tab>\n    </Tabs>\n  </Step>\n\n  <Step title=\"‚ú® Watch the Magic\">\n    **Result:** A fully-formed Pull Request opens in your demo repo with:\n    - Complete implementation or fix\n    - Proper commit messages\n    - Detailed PR description\n    - Ready for review and merge\n  </Step>\n</Steps>\n\n<Info>\n  **Why Use the Demo Repo?**\n  - **Safe testing environment** - No risk to your production code\n  - **Pre-configured issues** - Common bugs and features to practice with\n  - **Immediate results** - See agents in action without setup complexity\n  - **Learn by example** - Study the generated code and PR descriptions\n</Info>\n\n<Tip>\n  Once you see how agents work with the demo repo, you'll understand exactly how to create and customize your own for real projects!\n</Tip>\n\n## Example Workflow Tasks\n\nHere are some example tasks you can try with your agents:\n\n<AccordionGroup>\n  <Accordion title=\"Bug Fixes & Code Quality\">\n    \n    - \"Fix the TypeError in api/users.ts where the user object might be undefined\"\n    - \"Add null checks to all database query results in the services/ directory\"\n    - \"Fix all ESLint warnings in the components folder and open a PR\"\n    - \"Update deprecated React lifecycle methods to hooks in legacy components\"\n    \n  </Accordion>\n\n  <Accordion title=\"Feature Development\">\n    \n    - \"Create a new REST endpoint for user profile updates with validation and error handling\"\n    - \"Add pagination to the products list page with previous/next buttons\"\n    - \"Implement dark mode toggle using Tailwind CSS classes across all pages\"\n    - \"Add unit tests for the authentication service using Jest\"\n    \n  </Accordion>\n\n  <Accordion title=\"Security & Compliance\">\n    \n    - \"Run a complete security scan and create GitHub issues for critical vulnerabilities\"\n    - \"Scan the codebase for hardcoded API keys and move them to environment variables\"\n    - \"Add input sanitization to all user-facing form fields\"\n    - \"Update npm packages with security vulnerabilities and test compatibility\"\n    \n  </Accordion>\n\n  <Accordion title=\"Documentation & Boilerplate\">\n    \n    - \"Add JSDoc comments to all exported functions in the utils/ directory\"\n    - \"Create a README.md for the new payment-processing module with setup instructions\"\n    - \"Generate TypeScript interfaces for all API response schemas\"\n    - \"Add error handling boilerplate to all async functions missing try-catch blocks\"\n    \n  </Accordion>\n\n  <Accordion title=\"Investigation & Analysis\">\n    \n    - \"Investigate why the login API is returning 500 errors intermittently and suggest fixes\"\n    - \"Analyze the performance bottleneck in the data processing pipeline\"\n    - \"Review the database schema for the orders table and suggest optimizations\"\n    - \"Find all TODO comments related to authentication and create a summary\"\n    \n  </Accordion>\n\n  <Accordion title=\"Refactoring\">\n    \n    - \"Extract the repeated validation logic in controllers into a shared utility function\"\n    - \"Convert all class components in src/legacy to functional components with hooks\"\n    - \"Rename all instances of 'userId' to 'accountId' across the codebase\"\n    - \"Split the 500-line UserService.ts into smaller, single-responsibility services\"\n    \n  </Accordion>\n</AccordionGroup>\n\n## Ways to Run Agents\n\nChoose the method that fits your workflow:\n\n<Tabs>\n  <Tab title=\"üåê Mission Control\">\n    **Web interface for interactive use**\n    \n    Perfect for:\n    - Exploring agent capabilities\n    - Reviewing outputs before action\n    - Team collaboration\n    - One-time tasks\n    \n    Access at [hub.continue.dev/agents](https://hub.continue.dev/agents)\n  </Tab>\n  \n  <Tab title=\"üíª CLI (TUI Mode)\">\n    **Terminal interface for development**\n    \n    ```bash\n    cn --agent continuedev/github-project-manager-agent\n    ```\n    \n    Perfect for:\n    - Local development workflows\n    - Testing agent behavior\n    - Interactive debugging\n    - Quick iterations\n  </Tab>\n  \n  <Tab title=\"ü§ñ Headless Mode\">\n    **Automated execution for CI/CD**\n    \n    ```bash\n    cn --agent continuedev/snyk-continuous-ai-agent -p \"Run security scan\" --auto\n    ```\n    \n    Perfect for:\n    - Scheduled workflows\n    - CI/CD integration\n    - Webhook triggers\n    - Background processing\n  </Tab>\n</Tabs>\n\n## Best Practices for Success\n\nThe practice of using agents (Continuous AI) requires thoughtful setup of guardrails and habits:\n\n<CardGroup cols={2}>\n  <Card title=\"Start Small\" icon=\"seedling\">\n    Begin with tasks you're confident Continue can handle, like fixing known bugs with simple solutions.\n  </Card>\n  \n  <Card title=\"Practice Locally\" icon=\"terminal\">\n    Test with [Continue CLI](../guides/cli) in TUI mode before deploying automation.\n  </Card>\n  \n  <Card title=\"Be Thorough\" icon=\"clipboard-list\">\n    Agents can run for extended periods - invest time in detailed prompts with all important context.\n  </Card>\n  \n  <Card title=\"Team Alignment\" icon=\"users\">\n    Discuss agent usage and adjust code review habits for higher PR volumes.\n  </Card>\n</CardGroup>\n\n<Info>\n  **Learn More**: Explore advanced patterns and case studies on the [Continuous AI Blog](https://blog.continue.dev).\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/reference/deprecated-codebase.mdx","content":"---\ntitle: \"@Codebase (Deprecated)\"\ndescription: Deprecated codebase context provider - use the new codebase awareness approach instead\nkeywords: [deprecated, codebase, embeddings, context, legacy]\nnoindex: true\n---\n\n<Warning>\n  **This feature is deprecated.** The `@Codebase` context provider has been deprecated in favor of a more integrated approach to codebase awareness. Please refer to our [Guide on Making Agent Mode Aware of Codebases and Documentation](/guides/codebase-documentation-awareness) for the recommended approach.\n</Warning>\n\n## Migration Guide\n\nIf you're currently using `@Codebase` or `@Folder` context providers, please migrate to the new approach outlined in our [codebase and documentation awareness guide](/guides/codebase-documentation-awareness). The new approach provides:\n\n- Better integration with Continue's Agent mode features\n- More intelligent context selection\n- Improved performance and accuracy\n\n## Legacy Documentation\n\n*Below is the original documentation for the `@Codebase` context provider, preserved for reference*\n\nContinue indexes your codebase so that it can later automatically pull in the most relevant context from throughout your workspace. This is done via a combination of embeddings-based retrieval and keyword search. By default, all embeddings are calculated locally using `transformers.js` and stored locally in `~/.continue/index`.\n\n<Info>\n  **Note:** `transformers.js` cannot be used in JetBrains IDEs. However, you can\n  select a different embeddings model from [the list\n  here](../customize/model-roles/embeddings).\n</Info>\n\n## How to Use @Codebase and @Folder Context Providers\n\nCurrently, the codebase retrieval feature is available as the \"codebase\" and \"folder\" context providers. You can use them by typing `@Codebase` or `@Folder` in the input box, and then asking a question. The contents of the input box will be compared with the embeddings from the rest of the codebase (or folder) to determine relevant files.\n\n### When @Codebase Context Provider Is Useful\n\nHere are some common use cases where it can be useful:\n\n- **Asking high-level questions about your codebase**\n  - \"How do I add a new endpoint to the server?\"\n  - \"Do we use VS Code's CodeLens feature anywhere?\"\n  - \"Is there any code written already to convert HTML to markdown?\"\n- **Generate code using existing samples as reference**\n  - \"Generate a new React component with a date picker, using the same patterns as existing components\"\n  - \"Write a draft of a CLI application for this project using Python's argparse\"\n  - \"Implement the `foo` method in the `bar` class, following the patterns seen in other subclasses of `baz`.\n- **Use `@Folder` to ask questions about a specific folder, increasing the likelihood of relevant results**\n  - \"What is the main purpose of this folder?\"\n  - \"How do we use VS Code's CodeLens API?\"\n  - Or any of the above examples, but with `@Folder` instead of `@Codebase`\n\n### When @Codebase Context Provider Is Not Useful\n\nHere are use cases where it is not useful:\n\n- **When you need the LLM to see _literally every_ file in your codebase**\n  - \"Find everywhere where the `foo` function is called\"\n  - \"Review our codebase and find any spelling mistakes\"\n- **Refactoring tasks**\n  - \"Add a new parameter to the `bar` function and update usages\"\n\n## How to Configure @Codebase Context Provider Settings\n\nThere are a few options that let you configure the behavior of the `@codebase` context provider, which are the same for the `@folder` context provider:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\" \n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  context:\n    - provider: codebase\n      params:\n        nRetrieve: 25\n        nFinal: 5\n        useReranking: true\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"codebase\",\n        \"params\": {\n          \"nRetrieve\": 25,\n          \"nFinal\": 5,\n          \"useReranking\": true\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### `nRetrieve`\n\nNumber of results to initially retrieve from vector database (default: 25)\n\n### `nFinal`\n\nFinal number of results to use after re-ranking (default: 5)\n\n### `useReranking`\n\nWhether to use re-ranking, which will allow initial selection of `nRetrieve` results, then will use an LLM to select the top `nFinal` results (default: true)\n\n## How to Ignore Files During Indexing\n\nContinue respects `.gitignore` files in order to determine which files should not be indexed. If you'd like to exclude additional files, you can add them to a `.continueignore` file, which follows the exact same rules as `.gitignore`.\n\nContinue also supports a **global** `.continueignore` file that will be respected for all workspaces, which can be created at `~/.continue/.continueignore`.\n\nIf you want to see exactly what files Continue has indexed, the metadata is stored in `~/.continue/index/index.sqlite`. You can use a tool like [DB Browser for SQLite](https://sqlitebrowser.org/) to view the `tag_catalog` table within this file.\n\nIf you need to force a refresh of the index, reload the VS Code window with <kbd>cmd/ctrl</kbd> + <kbd>shift</kbd> + <kbd>p</kbd> + \"Reload Window\"."}
{"source":"github","repo":"continue","path":"docs/reference/yaml-migration.mdx","content":"---\ntitle: \"Migrating Config to YAML\"\ndescription: \"Continue's YAML configuration format provides more readable, maintainable, consistent configuration files, as well as new configuration options and removal of some old configuration options. YAML is the preferred format and will be used to integrate with future Continue products. Below is a brief guide for migration from config.json to config.yaml.\"\n---\n\nSee also\n\n- [Intro to YAML](https://yaml.org/)\n- [YAML Continue Config Reference](/reference)\n\n## Create YAML file\n\nCreate a `config.yaml` file in your Continue Global Directory (`~/.continue` on Mac, `%USERPROFILE%\\.continue`) alongside your current config.json file. If a `config.yaml` file is present, it will be loaded instead of config.json.\n\nGive your configuration a `name` and a `version`:\n\nconfig.yaml\n\n```\nname: my-configurationversion: 0.0.1schema: v1\n```\n\n### Models\n\nAdd all model configurations in `config.json`, including models in `models`, `tabAutocompleteModel`, `embeddingsProvider`, and `reranker`, to the `models` section of your new YAML config file. A new `roles` YAML field specifies which roles a model can be used for, with possible values `chat`, `autocomplete`, `embed`, `rerank`, `edit`, `apply`, `summarize`.\n\n- `models` in config should have `roles: [chat]`\n\n- `tabAutocompleteModel`(s) in config should have `roles: [autocomplete]`\n\n- `embeddingsProvider` in config should have `roles: [embed]`\n\n- `reranker` in config should have `roles: [rerank]`\n\n- `experimental.modelRoles` is replaced by simply adding roles to the model\n  - `inlineEdit` -> e.g. `roles: [chat, edit]`\n  - `applyCodeBlock` -> e.g. `roles: [chat, apply]`\n\nModel-level `requestOptions` remain, with minor changes. See [YAML Continue Config Reference](/reference#models)\n\nModel-level `completionOptions` are replaced by `defaultCompletionOptions`, with minor changes. See [YAML Continue Config Reference](/reference#models)\n\n**Before**\n\nconfig.json\n\n```json\n{\n  \"models\": [\n    {\n      \"title\": \"GPT-4\",\n      \"provider\": \"openai\",\n      \"model\": \"gpt-4\",\n      \"apiKey\": \"<YOUR_OPENAI_API_KEY>\",\n      \"completionOptions\": { \"temperature\": 0.5, \"maxTokens\": 2000 }\n    },\n    { \"title\": \"Ollama\", \"provider\": \"ollama\", \"model\": \"AUTODETECT\" },\n    {\n      \"title\": \"My Open AI Compatible Model\",\n      \"provider\": \"openai\",\n      \"apiBase\": \"http://3.3.3.3/v1\",\n      \"model\": \"my-openai-compatible-model\",\n      \"requestOptions\": { \"headers\": { \"X-Auth-Token\": \"<API_KEY>\" } }\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"My Starcoder\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:3b\"\n  },\n  \"embeddingsProvider\": {\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-ada-002\",\n    \"apiKey\": \"<YOUR_OPENAI_API_KEY>\",\n    \"maxEmbeddingChunkSize\": 256,\n    \"maxEmbeddingBatchSize\": 5\n  },\n  \"reranker\": {\n    \"name\": \"voyage\",\n    \"params\": { \"model\": \"rerank-2\", \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\" }\n  }\n}\n```\n\n**After**\n\nconfig.yaml\n\n```yaml\nmodels:\n  - name: GPT-4\n    provider: openai\n    model: gpt-4\n    apiKey: <YOUR_OPENAI_API_KEY>\n    defaultCompletionOptions:\n      temperature: 0.5\n      maxTokens: 2000\n    roles:\n      - chat\n      - edit\n  - name: My Voyage Reranker\n    provider: voyage\n    model: rerank-2\n    apiKey: <YOUR_VOYAGE_API_KEY>\n    roles:\n      - rerank\n  - name: My Starcoder\n    provider: ollama\n    model: starcoder2:3b\n    roles:\n      - autocomplete\n  - name: My Ada Embedder\n    provider: openai\n    model: text-embedding-ada-002\n    apiKey: <YOUR_OPENAI_API_KEY>\n    roles:\n      - embed\n    embedOptions:\n      maxChunkSize: 256\n      maxBatchSize: 5\n  - name: Ollama Autodetect\n    provider: ollama\n    model: AUTODETECT\n    roles:\n      - chat\n  - name: My Open AI Compatible Model\n    provider: openai\n    model: my-openai-compatible-model\n    apiBase: http://3.3.3.3/v1\n    requestOptions:\n      headers:\n        X-Auth-Token: <API_KEY>\n    roles:\n      - chat\n      - apply\n```\n\nNote that the `repoMapFileSelection` experimental model role has been deprecated and is only available in `config.json`.\n\n### Context Providers\n\nThe JSON `contextProviders` field is replaced by the YAML `context` array.\n\n- JSON `name` maps to `provider`\n- JSON `params` map to `params`\n\n**Before**\n\nconfig.json\n\n```json\n{\n  \"contextProviders\": [\n    { \"name\": \"docs\" },\n    { \"name\": \"codebase\", \"params\": { \"nRetrieve\": 30, \"nFinal\": 3 } },\n    { \"name\": \"diff\", \"params\": {} }\n  ]\n}\n```\n\n**After**\n\nconfig.yaml\n\n```yaml\ncontext:\n  - provider: docs\n  - provider: codebase\n    params:\n      nRetrieve: 30\n      nFinal: 3\n  - provider: diff\n```\n\n### System Message\n\nThe `systemMessage` property has been replaced with a `rules` property that takes an array of strings.\n\n**Before**\n\nconfig.json\n\n```json\n{\n  \"systemMessage\": \"Always give concise responses\"\n}\n```\n\n**After**\n\nconfig.yaml\n\n```yaml\nrules:\n  - Always give concise responses\n```\n\n### Prompts\n\nRather than with `customCommands`, you can now use the `prompts` field to define custom prompts.\n\n**Before**\n\nconfig.json\n\n```json\n{\n  \"customCommands\": [\n    {\n      \"name\": \"check\",\n      \"description\": \"Check for mistakes in my code\",\n      \"prompt\": \"{{{ input }}}\\n\\nPlease read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\\n- Syntax errors\\n- Logic errors\\n- Security vulnerabilities\\n- Performance issues\\n- Anything else that looks wrong\\n\\nOnce you find an error, please explain it as clearly as possible, but without using extra words. For example, instead of saying 'I think there is a syntax error on line 5', you should say 'Syntax error on line 5'. Give your answer as one bullet point per mistake found.\"\n    }\n  ]\n}\n```\n\n**After**\n\nconfig.yaml\n\n```yaml\nprompts:\n  - name: check\n    description: Check for mistakes in my code\n    prompt: |\n      Please read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\n        - Syntax errors\n        - Logic errors\n        - Security vulnerabilities\n        - Performance issues\n        - Anything else that looks wrong\n      Once you find an error, please explain it as clearly as possible, but without using extra words. For example, instead of saying 'I think there is a syntax error on line 5', you should say 'Syntax error on line 5'. Give your answer as one bullet point per mistake found.\n```\n\n### Documentation\n\nDocumentation is largely the same, but the `title` property has been replaced with `name`. The `startUrl`, `rootUrl`, and `faviconUrl` properties remain.\n\n**Before**\n\nconfig.json\n\n```json\n{\n  \"docs\": [\n    {\n      \"startUrl\": \"https://docs.nestjs.com/\",\n      \"title\": \"nest.js\"\n    },\n    {\n      \"startUrl\": \"https://mysite.com/docs/\",\n      \"title\": \"My site\"\n    }\n  ]\n}\n```\n\n**After**\n\nconfig.yaml\n\n```yaml\ndocs:\n  - name: nest.js\n    startUrl: https://docs.nestjs.com/\n  - name: My site\n    startUrl: https://mysite.com/docs/\n```\n\n### MCP Servers\n\n**Properties:**\n\n- `name` (**required**): The name of the MCP server.\n- `command` (**required**): The command used to start the server.\n- `args`: An optional array of arguments for the command.\n- `env`: An optional map of environment variables for the server process.\n- `cwd`: An optional working directory to run the command in. Can be absolute or relative path.\n\n**Before**\n\nconfig.json\n\n```json\n{\n  \"experimental\": {\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\n            \"mcp-server-sqlite\",\n            \"--db-path\",\n            \"/Users/NAME/test.db\"\n          ],\n          \"env\": {\n            \"KEY\": \"<VALUE>\"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n**After**\n\nconfig.yaml\n\n```yaml\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n    env:\n      KEY: <VALUE>\n```\n\n---\n\n## Deprecated configuration options\n\nSome deprecated config.json settings are no longer stored in config and have been moved to be editable through the user settings (Gear Icon). If found in config.json, they will be auto-migrated to User Settings and removed from config.json.\n\nThe following top-level fields from config.json have been deprecated and don't have a config.yaml equivalent:\n\n- Slash commands (`slashCommands`)\n\n- top-level `requestOptions`\n\n- top-level `completionOptions`\n\n- `tabAutocompleteOptions`\n\n  - `disable`\n  - `maxPromptTokens`\n  - `debounceDelay`\n  - `maxSuffixPercentage`\n  - `prefixPercentage`\n  - `template`\n  - `onlyMyCode`\n\n- `analytics`\n\nThe following top-level fields from config.json have been deprecated. Most UI-related and user-specific options will move into a settings page in the UI\n\n- `customCommands`\n- `experimental`\n- `userToken`\n\n## New Configuration options\n\nThe YAML configuration format offers new configuration options not available in the JSON format. See the [YAML Config Reference](/reference) for more information.\n"}
{"source":"github","repo":"continue","path":"docs/reference/deprecated-context-providers.mdx","content":"---\ntitle: \"Context Providers (Deprecated)\"\ndescription: \"These context providers have been deprecated and are no longer actively maintained. They remain documented here for reference purposes only.\"\nnoindex: true\n---\n\n<Warning>\nThe context providers documented on this page have been deprecated and are no longer actively maintained. We recommend using alternative solutions or the Model Context Protocol (MCP) for similar functionality. These providers may be removed in future versions.\n</Warning>\n\n## Deprecated Providers\n\nThe following context providers have been deprecated but are kept here for reference. If you're currently using any of these providers, consider migrating to alternative solutions.\n\n### `@Greptile`\n\nQuery a [Greptile](https://www.greptile.com/) index of the current repo/branch.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: greptile\n        params:\n          greptileToken: \"...\"\n          githubToken: \"...\"\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"greptile\",\n          \"params\": { \"GreptileToken\": \"...\", \"GithubToken\": \"...\" }\n        }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\n### `@Commits`\n\nReference specific git commit metadata and diff or all of the recent commits.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: commit\n        params:\n          Depth: 50\n          LastXCommitsDepth: 10\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        { \"name\": \"commit\", \"params\": { \"Depth\": 50, \"LastXCommitsDepth\": 10 } }\n      ]\n    }\n    ```\n  </Tab>\n</Tabs>\n\nThe depth is how many commits will be loaded into the submenu, defaults to 50. The LastXCommitsDepth is how many recent commits will be included, defaults to 10.\n\n### `@Discord`\n\nReference the messages in a Discord channel.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: discord\n        params:\n          discordKey: \"bot token\"\n          guildId: \"1234567890\"\n          channels:\n            - id: \"123456\"\n              name: \"example-channel\"\n            - id: \"678901\"\n              name: \"example-channel-2\"\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"discord\",\n          \"params\": {\n            \"discordKey\": \"bot token\",\n            \"guildId\": \"1234567890\",\n            \"channels\": [\n              { \"id\": \"123456\", \"name\": \"example-channel\" },\n              { \"id\": \"678901\", \"name\": \"example-channel-2\" }\n            ]\n          }\n        }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\nMake sure to include your own [Bot Token](https://discord.com/developers/applications), and join it to your related server . If you want more granular control over which channels are searched, you can specify a list of channel IDs to search in. If you don't want to specify any channels, just include the guild id(Server ID) and all channels will be included. The provider only reads text channels.\n\n### `@Jira`\n\nReference the conversation in a Jira issue.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: jira\n        params:\n          domain: company.atlassian.net\n          token: ATATT...\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"jira\",\n          \"params\": { \"domain\": \"company.atlassian.net\", \"token\": \"ATATT...\" }\n        }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\nMake sure to include your own [Atlassian API Token](https://id.atlassian.com/manage-profile/security/api-tokens), or use your `email` and `token`, with token set to your password for basic authentication. If you use your own Atlassian API Token, don't configure your email.\n\n#### Jira Datacenter Support\n\nThis context provider supports both Jira API version 2 and 3. It will use version 3 by default since that's what the cloud version uses, but if you have the datacenter version of Jira, you'll need to set the API Version to 2 using the `apiVersion` property.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: jira\n        params:\n          apiVersion: \"2\"\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    { \"contextProviders\": [{ \"name\": \"jira\", \"params\": { \"apiVersion\": \"2\" } }] }\n    ```\n  </Tab>\n</Tabs>\n\n#### Issue Query\n\nBy default, the following query will be used to find issues:\n\n```\nassignee = currentUser() AND resolution = Unresolved order by updated DESC\n```\n\nYou can override this query by setting the `issueQuery` parameter.\n\n#### Max results\n\nYou can set the `maxResults` parameter to limit the number of results returned. The default is `50`.\n\n### `@Gitlab Merge Request`\n\nReference an open MR for this branch on GitLab.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: gitlab-mr\n        params:\n          token: \"...\"\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    { \"contextProviders\": [{ \"name\": \"gitlab-mr\", \"params\": { \"token\": \"...\" } }] }\n    ```\n  </Tab>\n</Tabs>\n\nYou will need to create a [personal access token](https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html) with the `read_api` scope.\n\n#### Using Self-Hosted GitLab\n\nYou can specify the domain to communicate with by setting the `domain` parameter in your configurtion. By default this is set to `gitlab.com`.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: gitlab-mr\n        params:\n          token: \"...\"\n          domain: \"gitlab.example.com\"\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"gitlab-mr\",\n          \"params\": { \"token\": \"...\", \"domain\": \"gitlab.example.com\" }\n        }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\n#### Filtering Comments\n\nIf you select some code to be edited, you can have the context provider filter out comments for other files. To enable this feature, set `filterComments` to `true`.\n\n### `@Google`\n\nReference the results of a Google search.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: google\n        params:\n          serperApiKey: <YOUR_SERPER.DEV_API_KEY>\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"google\",\n          \"params\": { \"serperApiKey\": \"<YOUR_SERPER.DEV_API_KEY>\" }\n        }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\nFor example, type \"@Google python tutorial\" if you want to search and discuss ways of learning Python.\n\nNote: You can get an API key for free at [serper.dev](https://serper.dev).\n\n### @Database Context Provider ‚Äì Reference Database Table Schemas\n\nReference table schemas from Sqlite, Postgres, MSSQL, and MySQL databases.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: database\n        params:\n          connections:\n            - name: examplePostgres\n              connection_type: postgres\n              connection:\n                user: username\n                host: localhost\n                database: exampleDB\n                password: yourPassword\n                port: 5432\n            - name: exampleMssql\n              connection_type: mssql\n              connection:\n                user: username\n                server: localhost\n                database: exampleDB\n                password: yourPassword\n            - name: exampleSqlite\n              connection_type: sqlite\n              connection:\n                filename: /path/to/your/sqlite/database.db\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"database\",\n          \"params\": {\n            \"connections\": [\n              {\n                \"name\": \"examplePostgres\",\n                \"connection_type\": \"postgres\",\n                \"connection\": {\n                  \"user\": \"username\",\n                  \"host\": \"localhost\",\n                  \"database\": \"exampleDB\",\n                  \"password\": \"yourPassword\",\n                  \"port\": 5432\n                }\n              },\n              {\n                \"name\": \"exampleMssql\",\n                \"connection_type\": \"mssql\",\n                \"connection\": {\n                  \"user\": \"username\",\n                  \"server\": \"localhost\",\n                  \"database\": \"exampleDB\",\n                  \"password\": \"yourPassword\"\n                }\n              },\n              {\n                \"name\": \"exampleSqlite\",\n                \"connection_type\": \"sqlite\",\n                \"connection\": { \"filename\": \"/path/to/your/sqlite/database.db\" }\n              }\n            ]\n          }\n      }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\nEach connection should include a unique name, the `connection_type`, and the necessary connection parameters specific to each database type.\n\nAvailable connection types:\n\n- `postgres`\n- `mysql`\n- `sqlite`\n\n### `@Issue`\n\nReference the conversation in a GitHub issue.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: issue\n        params:\n          repos:\n            - owner: continuedev\n              repo: continue\n          githubToken: ghp_xxx\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"issue\",\n          \"params\": {\n            \"repos\": [{ \"owner\": \"continuedev\", \"repo\": \"continue\" }],\n            \"githubToken\": \"ghp_xxx\"\n          }\n        }\n      ]\n    }\n```\n  </Tab>\n</Tabs>\n\nMake sure to include your own [GitHub personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token) to avoid being rate-limited.\n\n### `@Url`\n\nReference the markdown converted contents of a given URL.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: url\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    { \"contextProviders\": [{ \"name\": \"url\" }] }\n    ```\n  </Tab>\n</Tabs>\n\n### `@Search`\n\nReference the results of codebase search, just like the results you would get from VS Code search.\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: search\n        params:\n          maxResults: 100 # optional, defaults to 200\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    {\n      \"contextProviders\": [\n        {\n          \"name\": \"search\",\n          \"params\": {\n            \"maxResults\": 100 // optional, defaults to 200\n          }\n        }\n      ]\n    }\n    ```\n  </Tab>\n</Tabs>\n\nThis context provider is powered by [ripgrep](https://github.com/BurntSushi/ripgrep).\n\n### `@Web`\n\nReference relevant pages from across the web, automatically determined from your input.\n\nOptionally, set `n` to limit the number of results returned (default 6).\n\n<Tabs>\n  <Tab title=\"YAML\">\n    ```yaml config.yaml\n    context:\n      - provider: web\n        params:\n          n: 5\n    ```\n  </Tab>\n  <Tab title=\"JSON\">\n    ```json config.json\n    { \"contextProviders\": [{ \"name\": \"web\", \"params\": { \"n\": 5 } }] }\n    ```\n  </Tab>\n</Tabs>\n"}
{"source":"github","repo":"continue","path":"docs/reference/json-reference.mdx","content":"---\ntitle: \"config.json Reference (Deprecated)\"\ndescription: \"Legacy configuration reference for config.json - use config.yaml instead\"\nnoindex: true\n---\n\n<Warning>\n  **This page documents deprecated functionality.** For creating slash commands and customizing Continue, please use:\n  - [Prompt blocks and prompt files](/customize/deep-dives/prompts) for slash commands\n  - [config.yaml](/reference) for configuration\n\nSee the `config.yaml` reference and migration guide\n[here](/reference).\n\n</Warning>\n\nBelow are details for each property that can be set in `config.json`. The config schema code is found in [`extensions/vscode/config_schema.json`](https://github.com/continuedev/continue/blob/main/extensions/vscode/config_schema.json).\n\n**All properties at all levels are optional unless explicitly marked required**\n\n## `models`\n\nYour **chat** models are defined here, which are used for [Chat](/ide-extensions/chat/how-it-works) and [Edit](/ide-extensions/edit/how-it-works).\n\nEach model has specific configuration options tailored to its provider and functionality, which can be seen as suggestions while editing the json.\n\n**Properties:**\n\n- `title` (**required**): The title to assign to your model, shown in dropdowns, etc.\n\n- `provider` (**required**): The provider of the model, which determines the type and interaction method. Options inclued `openai`, `ollama`, `xAI`, etc., see IntelliJ suggestions.\n\n- `model` (**required**): The name of the model, used for prompt template auto-detection. Use `AUTODETECT` special name to get all available models.\n\n- `apiKey`: API key required by providers like OpenAI, Anthropic, Cohere, and xAI.\n\n- `apiBase`: The base URL of the LLM API.\n\n- `contextLength`: Maximum context length of the model, typically in tokens (default: 2048).\n\n- `maxStopWords`: Maximum number of stop words allowed, to avoid API errors with extensive lists.\n\n- `template`: Chat template to format messages. Auto-detected for most models but can be overridden. See intelliJ suggestions.\n\n- `promptTemplates`: A mapping of prompt template names (e.g., `edit`) to template strings. See the [Deep Dives section](/customize/deep-dives/prompts) for customization details.\n\n- `completionOptions`: Model-specific completion options, same format as top-level [`completionOptions`](#completionoptions), which they override.\n\n- `systemMessage`: A system message that will precede responses from the LLM.\n\n- `requestOptions`: Model-specific HTTP request options, same format as top-level [`requestOptions`](#requestoptions), which they override.\n\n- `apiType`: Specifies the type of API (`openai` or `azure`).\n\n- `apiVersion`: Azure API version (e.g., `2023-07-01-preview`).\n\n- `engine`: Engine for Azure OpenAI requests.\n\n- `capabilities`: Override auto-detected capabilities:\n  - `uploadImage`: Boolean indicating if the model supports image uploads.\n  - `tools`: Boolean indicating if the model supports tool use.\n\n_(AWS Only)_\n\n- `profile`: AWS security profile for authorization.\n- `modelArn`: AWS ARN for imported models (e.g., for `bedrockimport` provider).\n- `region`: Region where the model is hosted (e.g., `us-east-1`, `eu-central-1`).\n\nExample:\n\nconfig.json\n\n```json\n{\n  \"models\": [\n    { \"title\": \"Ollama\", \"provider\": \"ollama\", \"model\": \"AUTODETECT\" },\n    {\n      \"model\": \"gpt-4o\",\n      \"contextLength\": 128000,\n      \"title\": \"GPT-4o\",\n      \"provider\": \"openai\",\n      \"apiKey\": \"YOUR_API_KEY\"\n    }\n  ]\n}\n```\n\n## `tabAutocompleteModel`\n\nSpecifies the model or models for tab autocompletion, defaulting to an Ollama instance. This property uses the same format as `models`. Can be an array of models or an object for one model.\n\nExample\n\nconfig.json\n\n```json\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"My Starcoder\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:3b\"\n  }\n}\n```\n\n### `tabAutocompleteOptions`\n\nSpecifies options for tab autocompletion behavior.\n\n<Info>\n  **Note**: Many of these parameters can also be configured per-model using the\n  `autocompleteOptions` field in `config.yaml`. See the [YAML\n  Reference](/reference#models) for more details.\n</Info>\n\n**Properties:**\n\n- `disable`: If `true`, disables tab autocomplete (default: `false`).\n- `maxPromptTokens`: Maximum number of tokens for the prompt (default: `1024`).\n- `debounceDelay`: Delay (in ms) before triggering autocomplete (default: `350`).\n- `maxSuffixPercentage`: Maximum percentage of prompt for suffix (default: `0.2`).\n- `prefixPercentage`: Percentage of input for prefix (default: `0.3`).\n- `template`: Template string for autocomplete, using Mustache templating. You can use the `{{{prefix}}}`, `{{{suffix}}}`, `{{{filename}}}`, `{{{reponame}}}`, and `{{{language}}}` variables.\n- `onlyMyCode`: If `true`, only includes code within the repository (default: `true`).\n\nExample\n\nconfig.json\n\n```json\n{\n  \"tabAutocompleteOptions\": {\n    \"debounceDelay\": 500,\n    \"maxPromptTokens\": 1500,\n    \"disableInFiles\": [\"*.md\"]\n  }\n}\n```\n\n## `embeddingsProvider`\n\nEmbeddings model settings - the model used for @Codebase and @docs.\n\n**Properties:**\n\n- `provider` (**required**): Specifies the embeddings provider, with options including `transformers.js`, `ollama`, `openai`, `cohere`, `gemini`, etc\n- `model`: Model name for embeddings.\n- `apiKey`: API key for the provider.\n- `apiBase`: Base URL for API requests.\n- `requestOptions`: Additional HTTP request settings specific to the embeddings provider.\n- `maxEmbeddingChunkSize`: Maximum tokens per document chunk. Minimum is 128 tokens.\n- `maxEmbeddingBatchSize`: Maximum number of chunks per request. Minimum is 1 chunk.\n\n(AWS ONLY)\n\n- `region`: Specifies the region hosting the model.\n- `profile`: AWS security profile.\n\nExample:\n\nconfig.json\n\n```json\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-ada-002\",\n    \"apiKey\": \"<API_KEY>\",\n    \"maxEmbeddingChunkSize\": 256,\n    \"maxEmbeddingBatchSize\": 5\n  }\n}\n```\n\n## `completionOptions`\n\nParameters that control the behavior of text generation and completion settings. Top-level `completionOptions` apply to all models, _unless overridden at the model level_.\n\n**Properties:**\n\n- `stream`: Whether to stream the LLM response. Currently only respected by the `anthropic` and `ollama` providers; other providers will always stream (default: `true`).\n- `temperature`: Controls the randomness of the completion. Higher values result in more diverse outputs.\n- `topP`: The cumulative probability for nucleus sampling. Lower values limit responses to tokens within the top probability mass.\n- `topK`: The maximum number of tokens considered at each step. Limits the generated text to tokens within this probability.\n- `presencePenalty`: Discourages the model from generating tokens that have already appeared in the output.\n- `frequencyPenalty`: Penalizes tokens based on their frequency in the text, reducing repetition.\n- `mirostat`: Enables Mirostat sampling, which controls the perplexity during text generation. Supported by Ollama, LM Studio, and llama.cpp providers (default: `0`, where `0` = disabled, `1` = Mirostat, and `2` = Mirostat 2.0).\n- `stop`: An array of stop tokens that, when encountered, will terminate the completion. Allows specifying multiple end conditions.\n- `maxTokens`: The maximum number of tokens to generate in a completion (default: `2048`).\n- `numThreads`: The number of threads used during the generation process. Available only for Ollama as `num_thread`.\n- `keepAlive`: For Ollama, this parameter sets the number of seconds to keep the model loaded after the last request, unloading it from memory if inactive (default: `1800` seconds, or 30 minutes).\n- `numGpu`: For Ollama, this parameter overrides the number of gpu layers that will be used to load the model into VRAM.\n- `useMmap`: For Ollama, this parameter allows the model to be mapped into memory. If disabled can enhance response time on low end devices but will slow down the stream.\n- `reasoning`: Enables thinking/reasoning for Anthropic Claude 3.7+ and some Ollama models.\n- `reasoningBudgetTokens`: Sets budget tokens for thinking/reasoning in Anthropic Claude 3.7+ models.\n\nExample\n\nconfig.json\n\n```json\n{ \"completionOptions\": { \"stream\": false, \"temperature\": 0.5 } }\n```\n\n## `requestOptions`\n\nDefault HTTP request options that apply to all models and context providers, unless overridden at the model level.\n\n**Properties:**\n\n- `timeout`: Timeout for each request to the LLM (default: 7200 seconds).\n\n- `verifySsl`: Whether to verify SSL certificates for requests.\n\n- `caBundlePath`: Path to a custom CA bundle for HTTP requests - path to `.pem` file (or array of paths)\n\n- `proxy`: Proxy URL to use for HTTP requests.\n\n- `headers`: Custom headers for HTTP requests.\n\n- `extraBodyProperties`: Additional properties to merge with the HTTP request body.\n\n- `noProxy`: List of hostnames that should bypass the specified proxy.\n\n- `clientCertificate`: Client certificate for HTTP requests.\n  - `cert`: Path to the client certificate file.\n  - `key`: Path to the client certificate key file.\n  - `passphrase`: Optional passphrase for the client certificate key file.\n\nExample\n\nconfig.json\n\n```json\n{ \"requestOptions\": { \"headers\": { \"X-Auth-Token\": \"xxx\" } } }\n```\n\n### `reranker`\n\nConfiguration for the reranker model used in response ranking.\n\n**Properties:**\n\n- `name` (**required**): Reranker name, e.g., `cohere`, `voyage`, `llm`, `huggingface-tei`, `bedrock`\n\n- `params`:\n  - `model`: Model name\n  - `apiKey`: Api key\n  - `region`: Region (for Bedrock only)\n\nExample\n\nconfig.json\n\n```json\n{\n  \"reranker\": {\n    \"name\": \"voyage\",\n    \"params\": { \"model\": \"rerank-2\", \"apiKey\": \"<VOYAGE_API_KEY>\" }\n  }\n}\n```\n\n## `docs`\n\nList of documentation sites to index.\n\n**Properties:**\n\n- `title` (**required**): Title of the documentation site, displayed in dropdowns, etc.\n- `startUrl` (**required**): Start page for crawling - usually root or intro page for docs\n\n- `maxDepth`: Maximum link depth for crawling. Default `4`\n- `favicon`: URL for site favicon (default is `/favicon.ico` from `startUrl`).\n- `useLocalCrawling`: Skip the default crawler and only crawl using a local crawler.\n\nExample\n\nconfig.json\n\n```\n\"docs\": [    {    \"title\": \"Continue\",    \"startUrl\": \"https://docs.continue.dev/intro\",    \"faviconUrl\": \"https://docs.continue.dev/favicon.ico\",  }]\n```\n\n## `slashCommands`\n\nCustom commands initiated by typing \"/\" in the sidebar. Commands include predefined functionality or may be user-defined.\n\n**Properties:**\n\n- `name`: The command name. Options include \"issue\", \"share\", \"cmd\", \"http\", \"commit\", and \"review\".\n- `description`: Brief description of the command.\n- `step`: (Deprecated) Used for built-in commands; set the name for pre-configured options.\n- `params`: Additional parameters to configure command behavior (command-specific - see code for command)\n\nThe following commands are built-in and can be added to `config.json` to make them visible:\n\n### `/share`\n\nGenerate a shareable markdown transcript of your current chat history.\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"share\",\n      \"description\": \"Export the current chat session to markdown\",\n      \"params\": { \"outputDir\": \"~/.continue/session-transcripts\" }\n    }\n  ]\n}\n```\n\nUse the `outputDir` parameter to specify where you want to the markdown file to be saved.\n\n### `/cmd`\n\nGenerate a shell command from natural language and (only in VS Code) automatically paste it into the terminal.\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    { \"name\": \"cmd\", \"description\": \"Generate a shell command\" }\n  ]\n}\n```\n\n### `/commit`\n\nShows the LLM your current git diff and asks it to generate a commit message.\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"commit\",\n      \"description\": \"Generate a commit message for the current changes\"\n    }\n  ]\n}\n```\n\n### `/http`\n\nWrite a custom slash command at your own HTTP endpoint. Set 'url' in the params object for the endpoint you have setup. The endpoint should return a sequence of string updates, which will be streamed to the Continue sidebar. See our basic [FastAPI example](https://github.com/continuedev/continue/blob/74002369a5e435735b83278fb965e004ae38a97d/core/context/providers/context_provider_server.py#L34-L45) for reference.\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"http\",\n      \"description\": \"Does something custom\",\n      \"params\": { \"url\": \"<my server endpoint>\" }\n    }\n  ]\n}\n```\n\n### `/issue`\n\nDescribe the issue you'd like to generate, and Continue will turn into a well-formatted title and body, then give you a link to the draft so you can submit. Make sure to set the URL of the repository you want to generate issues for.\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"issue\",\n      \"description\": \"Generate a link to a drafted GitHub issue\",\n      \"params\": { \"repositoryUrl\": \"https://github.com/continuedev/continue\" }\n    }\n  ]\n}\n```\n\n### `/onboard`\n\nThe onboard slash command helps to familiarize yourself with a new project by analyzing the project structure, READMEs, and dependency files. It identifies key folders, explains their purpose, and highlights popular packages used. Additionally, it offers insights into the project's architecture.\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"onboard\",\n      \"description\": \"Familiarize yourself with the codebase\"\n    }\n  ]\n}\n```\n\nExample:\n\nconfig.json\n\n```json\n{\n  \"slashCommands\": [\n    { \"name\": \"commit\", \"description\": \"Generate a commit message\" },\n    { \"name\": \"share\", \"description\": \"Export this session as markdown\" },\n    { \"name\": \"cmd\", \"description\": \"Generate a shell command\" }\n  ]\n}\n```\n\n### `customCommands`\n\nUser-defined commands for prompt shortcuts in the sidebar, allowing quick access to common actions.\n\n**Properties:**\n\n- `name`: The name of the custom command.\n- `prompt`: Text prompt for the command.\n- `description`: Brief description explaining the command's function.\n\nExample:\n\nconfig.json\n\n```json\n{\n  \"customCommands\": [\n    {\n      \"name\": \"test\",\n      \"prompt\": \"Write a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don't edit any file.\",\n      \"description\": \"Write unit tests for highlighted code\"\n    }\n  ]\n}\n```\n\n### `contextProviders`\n\nList of the pre-defined context providers that will show up as options while typing in the chat, and their customization with `params`.\n\n**Properties:**\n\n- `name`: Name of the context provider, e.g. `docs` or `web`\n- `params`: A context-provider-specific record of params to configure the context behavior\n\nExample\n\nconfig.json\n\n```json\n{\n  \"contextProviders\": [\n    { \"name\": \"code\", \"params\": {} },\n    { \"name\": \"docs\", \"params\": {} },\n    { \"name\": \"diff\", \"params\": {} },\n    { \"name\": \"open\", \"params\": {} }\n  ]\n}\n```\n\n## `userToken`\n\nAn optional token that identifies the user, primarily for authenticated services.\n\n## `systemMessage`\n\nDefines a system message that appears before every response from the language model, providing guidance or context.\n\n## `experimental`\n\nSeveral experimental config parameters are available, as described below:\n\n`experimental`:\n\n- `defaultContext`: Defines the default context for the LLM. Uses the same format as `contextProviders` but includes an additional `query` property to specify custom query parameters.=\n\n- `modelRoles`:\n\n  - `inlineEdit`: Model title for inline edits.\n  - `applyCodeBlock`: Model title for applying code blocks.\n  - `repoMapFileSelection`: Model title for repo map selections.\n\n- `modelContextProtocolServers`: See [Model Context Protocol](/customize/deep-dives/mcp)\n\nExample\n\nconfig.json\n\n```json\n{\n  \"experimental\": {\n    \"modelRoles\": { \"inlineEdit\": \"Edit Model\" },\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\"mcp-server-sqlite\", \"--db-path\", \"/Users/NAME/test.db\"]\n        }\n      }\n    ]\n  }\n}\n```\n\n## Fully deprecated settings\n\nSome deprecated `config.json` settings are no longer stored in config and have been moved to be editable through the user settings. If found in `config.json`, they will be auto-migrated to User Settings and removed from `config.json`.\n\n- `allowAnonymousTelemetry`: This value will be migrated to the safest merged value (`false` if either are `false`).\n\n- `promptPath`: This value will override during migration.\n\n- `disableIndexing`: This value will be migrated to the safest merged value (`true` if either are `true`).\n\n- `disableSessionTitles`/`ui.getChatTitles`: This value will be migrated to the safest merged value (`true` if either are `true`). `getChatTitles` takes precedence if set to false\n\n- `tabAutocompleteOptions`\n\n  - `useCache`: This value will override during migration.\n  - `disableInFiles`: This value will be migrated to the safest merged value (arrays of file matches merged/deduplicated)\n  - `multilineCompletions`: This value will override during migration.\n\n- `experimental`\n\n  - `useChromiumForDocsCrawling`: This value will override during migration.\n  - `readResponseTTS`: This value will override during migration.\n\n- `ui` - all will override during migration\n\n  - `codeBlockToolbarPosition`\n  - `fontSize`\n  - `codeWrap`\n  - `displayRawMarkdown`\n  - `showChatScrollbar`\n"}
{"source":"github","repo":"continue","path":"docs/reference/deprecated-docs.mdx","content":"---\ntitle: \"@Docs (Deprecated)\"\ndescription: Deprecated documentation context provider - use the new documentation awareness approach instead\nkeywords: [deprecated, documentation, docs, context, embeddings, legacy]\nnoindex: true\n---\n\n<Warning>\n  **This feature is deprecated.** The `@Docs` context provider has been deprecated in favor of a more integrated approach to documentation awareness. Please refer to our [Guide on Making Agent Mode Aware of Codebases and Documentation](/guides/codebase-documentation-awareness) for the recommended approach.\n</Warning>\n\n## Migration Guide\n\nIf you're currently using the `@Docs` context provider, please migrate to the new approach outlined in our [codebase and documentation awareness guide](/guides/codebase-documentation-awareness). The new approach provides:\n\n- Better integration with Continue's Agent mode features\n- More intelligent context selection\n- Improved performance and accuracy\n\n## Legacy Documentation\n\n**Below is the original documentation for the `@Docs` context provider, preserved for reference**\n\nThe `@Docs` context provider allows you to efficiently reference documentation directly within Continue.\n\n## How to Enable the @Docs Context Provider\n\nTo enable the `@Docs` context provider, add it to the list of context providers in your `config.json` file.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  context:\n    - provider: docs\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"docs\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n## How the @Docs Context Provider Works\n\nThe `@Docs` context provider works by\n\n1. Crawling specified documentation sites\n2. Generating embeddings for the chunked content\n3. Storing the embeddings locally on your machine\n4. Embedding chat input to include similar documentation chunks as context\n\n## How to Index Your Own Documentation\n\n**Note:** Documentation configuration should now be done directly in your `config.yaml` file. The previous `docs` blocks functionality has been deprecated.\n\n### How to Add Documentation Through the @Docs Context Provider\n\nTo add a single documentation site, we recommend using the **Add Documentation** Form within the GUI. This can be accessed\n\n- from the `@Docs` context provider - type `@Docs` in the chat, hit `Enter`, and search for `Add Docs`\n- from the `More` page (three dots icon) in the `@docs indexes` section\n  the `@Docs` context provider.\n\nIn the **Add Documentation** Form, enter a `Title` and `Start URL` for the site.\n\n- `Title`: The name of the documentation site, used for identification in the UI.\n- `Start URL`: The URL where the indexing process should begin.\n\nIndexing will begin upon submission. Progress can be viewed in the form or later in the `@docs indexes` section of the `More` page.\n\nDocumentation sources may be suggested based on package files in your repo. This currently works for Python `requirements.txt` files and Node.js (Javascript/Typescript) `package.json` files.\n\n- Packages with a valid documentation URL (with a `+` icon) can be clicked to immediately kick off indexing\n- Packages with partial information (with a pen icon) can be clicked to fill the form with the available information\n- Note that you can hover over the information icon to see where the package suggestion was found.\n\n![Add documentation form](/images/docs-form.png)\n\n### How to Add Documentation in Configuration Files\n\nFor bulk documentation site adds or edits, we recommend editing your global configuration file directly. Documentation sites are stored in an array within `docs` in your global configuration, as follows:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  docs:\n    - title: Nest.js\n      startUrl: https://docs.nestjs.com/\n      faviconUrl: https://docs.nestjs.com/favicon.ico\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"docs\": [\n      {\n        \"title\": \"Nest.js\",\n        \"startUrl\": \"https://docs.nestjs.com/\",\n        \"faviconUrl\": \"https://docs.nestjs.com/favicon.ico\"\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\nSee [the config reference](/reference) for all documentation site configuration options.\n\nIndexing will re-sync upon saving the configuration file.\n\n## How to Configure @Docs Advanced Settings\n\n### How to Use Your Custom Embeddings Provider\n\nIf you have set up an [embeddings provider](../customize/model-roles/embeddings.mdx), @docs will use your embeddings provider. Switching embeddings providers will trigger a re-index of all documentation sites in your configuration.\n\n### How to Configure Reranking for Better Results\n\nAs with [@Codebase context provider configuration](./deprecated-codebase#configuration), you can adjust the reranking behavior of the `@Docs` context provider with the `nRetrieve`, `nFinal`, and `useReranking`.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  context:\n    - provider: docs\n      params:\n        nRetrieve: 25 # The number of docs to retrieve from the embeddings query\n        nFinal: 5 # The number of docs chunks to return IF reranking\n        useReranking: true # use reranking if a reranker is configured (defaults to true)\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"docs\",\n        \"params\": {\n          \"nRetrieve\": 25, // The number of docs to retrieve from the embeddings query\n          \"nFinal\": 5, // The number of docs chunks to return IF reranking\n          \"useReranking\": true // use reranking if a reranker is configured (defaults to true)\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### How to Set Up GitHub Token for GitHub Documentation\n\nThe GitHub API rate limits public requests to 60 per hour. If you want to reliably index GitHub repos, you can add a github token to your config file:\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  context:\n    - provider: docs\n      params:\n        githubToken: <GITHUB_TOKEN>\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"docs\",\n        \"params\": {\n          \"githubToken\": \"github_...\"\n        }\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### How to Enable Local Crawling for Private Documentation\n\nBy default, Continue crawls documentation sites using a specialized crawling service that provides the best experience for most users and documentation sites.\n\nIf your documentation is private, you can skip the default crawler and use a local crawler instead by setting `useLocalCrawling` to `true`.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  docs:\n    - title: My Private Docs\n      startUrl: http://10.2.1.2/docs\n      faviconUrl: http://10.2.1.2/docs/assets/favicon.ico,\n      useLocalCrawling: true\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"docs\": [\n      {\n        \"title\": \"My Private Docs\",\n        \"startUrl\": \"http://10.2.1.2/docs\",\n        \"faviconUrl\": \"http://10.2.1.2/docs/assets/favicon.ico\",\n        \"useLocalCrawling\": true\n      }\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n<Info>\n  The default local crawler is a lightweight tool that cannot render sites that\n  are dynamically generated using JavaScript. If your sites need to be rendered,\n  you can enable the experimental `Use Chromium for Docs Crawling` feature from\n  your User Settings Page. This will download and install Chromium\n  to `~/.continue/.utils`, and use it as the local crawler. **Note:** Chromium\n  crawling support is being deprecated and may be removed in future versions.\n</Info>\n\nFurther notes:\n\n- If the site is only locally accessible, the default crawler will fail anyways and fall back to the local crawler. `useLocalCrawling` is especially useful if the URL itself is confidential.\n- For GitHub Repos this has no effect because only the GitHub Crawler will be used, and if the repo is private it can only be accessed with a priveleged GitHub token anyways.\n\n## How to Manage Your Documentation Indexes\n\nYou can view indexing statuses and manage your documentation sites from the `@docs indexes` section of the `More` page (three dots)\n\n- Continue does not automatically re-index your docs. Use `Click to re-index` to trigger a reindex for a specific source\n- While a site is indexing, click `Cancel indexing` to cancel the process\n- Failed indexing attempts will show an error status bar and icon\n- Delete a documentation site from your configuration using the trash icon\n\n![More Page @docs indexes section](/images/docs-indexes.png)\n\nYou can also view the overall status of currently indexing docs from a hideable progress bar at the bottom of the chat page\n\n![Documentation indexing peek](/images/docs-indexing-peek.png)\n\nYou can also use the following IDE command to force a re-index of all docs: `Continue: Docs Force Re-Index`.\n\n## Configuration Examples for Different Setups\n\n### How to Set Up @Docs in VS Code (Minimal Configuration)\n\nThe following configuration example works out of the box for VS Code. This uses the built-in embeddings provider with no reranking.\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  context:\n    - provider: docs\n    \n  docs:\n    - title: Nest.js\n      startUrl: https://docs.nestjs.com/\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"docs\",\n      }\n    ],\n    \"docs\": [\n      {\n        \"title\": \"Nest.js\",\n        \"startUrl\": \"https://docs.nestjs.com/\",\n      },\n    ]\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### How to Set Up @Docs in JetBrains IDEs (Minimal Configuration)\n\nHere is the equivalent minimal example for Jetbrains, which requires setting up an [embeddings provider](../customize/model-roles/embeddings.mdx).\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: LMStudio embedder\n      provider: lmstudio\n      model: nomic-ai/nomic-embed-text-v1.5-GGUF\n      roles:\n        - embed\n  context:\n    - provider: docs\n  docs:\n    - title: Nest.js\n      startUrl: https://docs.nestjs.com/\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"docs\",\n      }\n    ],\n    \"docs\": [\n      {\n        \"title\": \"Nest.js\",\n        \"startUrl\": \"https://docs.nestjs.com/\",\n      },\n    ],\n    \"embeddingsProvider\": {\n      \"provider\": \"lmstudio\",\n      \"model\": \"nomic-ai/nomic-embed-text-v1.5-GGUF\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\n### How to Set Up @Docs with Advanced Features (VS Code or JetBrains)\n\nThe following configuration example includes:\n\n- Examples of both public and private documentation sources\n- A custom embeddings provider\n- A reranker model available, with reranking parameters customized\n- A GitHub token to enable GitHub crawling\n\n<Tabs>\n  <Tab title=\"YAML\">\n  ```yaml title=\"config.yaml\"\n  name: My Config\n  version: 0.0.1\n  schema: v1\n\n  models:\n    - name: LMStudio Nomic Text\n      provider: lmstudio\n      model: nomic-ai/nomic-embed-text-v1.5-GGUF\n      roles:\n        - embed\n    - name: Voyage Rerank-2\n      provider: voyage\n      apiKey: <VOYAGE_API_KEY>\n      model: rerank-2\n      roles:\n        - rerank\n  context:\n    - provider: docs\n      params:\n         githubToken: <GITHUB_TOKEN>\n         nRetrieve: 25\n         nFinal: 5\n         useReranking: true\n  docs:\n    - title: Nest.js\n      startUrl: https://docs.nestjs.com/\n    - title: My Private Docs\n      startUrl: http://10.2.1.2/docs\n      faviconUrl: http://10.2.1.2/docs/assets/favicon.ico\n      useLocalCrawling: true\n  ```\n  </Tab>\n  <Tab title=\"JSON\">\n  ```json title=\"config.json\"\n  {\n    \"contextProviders\": [\n      {\n        \"name\": \"docs\",\n        \"params\": {\n          \"githubToken\": \"github_...\",\n          \"nRetrieve\": 25,\n          \"nFinal\": 5,\n          \"useReranking\": true\n        }\n      }\n    ],\n    \"docs\": [\n      {\n        \"title\": \"Nest.js\",\n        \"startUrl\": \"https://docs.nestjs.com/\"\n      },\n      {\n        \"title\": \"My Private Docs\",\n        \"startUrl\": \"http://10.2.1.2/docs\",\n        \"faviconUrl\": \"http://10.2.1.2/docs/assets/favicon.ico\",\n        \"maxDepth\": 4,\n        \"useLocalCrawling\": true\n      }\n    ],\n    \"reranker\": {\n      \"name\": \"voyage\",\n      \"params\": {\n        \"model\": \"rerank-2\",\n        \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\"\n      }\n    },\n    \"embeddingsProvider\": {\n      \"provider\": \"lmstudio\",\n      \"model\": \"nomic-ai/nomic-embed-text-v1.5-GGUF\"\n    }\n  }\n  ```\n  </Tab>\n</Tabs>\n\nThis setup could also involve enabling Chromium as a backup for local documentation through the User Settings Page (Gear Icon)."}
{"source":"github","repo":"continue","path":"docs/reference/continue-mcp.mdx","content":"---\ntitle: Continue Documentation MCP Server\ndescription: Set up an MCP server to search Continue documentation\nkeywords: [mcp, documentation, search, mintlify, reference]\n---\n\nThe Continue Documentation MCP Server allows you to search and retrieve information from the Continue documentation directly within your agent conversations.\n\n## Set up\n\n### Configure Continue\n\n1. Create a folder called `.continue/mcpServers` at the top level of your workspace\n2. Add a file called `continue-docs-mcp.yaml` to this folder\n3. Write the following contents and save:\n\n```yaml title=\".continue/mcpServers/continue-docs-mcp.yaml\"\nname: Continue Documentation MCP\nversion: 0.0.1\nschema: v1\nmcpServers:\n  - uses: continuedev/continue-docs-mcp\n```\n\n### Enable Agent Mode\n\nMCP servers only work in agent mode. Make sure to switch to agent mode in Continue before testing.\n\n## Usage Examples\n\nOnce configured, you can use the MCP server to search Continue documentation:\n\n### Model Configuration Help\n\n```\nHow do I add Claude 4 Sonnet as a model from Bedrock in Continue?\n```\n\n### Context Providers\n\n```\nWhat context providers are available in Continue?\n```\n\n### Customization\n\n```\nHow do I add custom rules to my configuration in Continue?\n```\n\n## Troubleshooting\n\n### MCP Server Not Loading\n\n1. **Check configuration**: Ensure your YAML configuration uses the correct `uses` field with `continuedev/continue-docs-mcp`\n2. **Check agent mode**: MCP servers only work in agent mode\n3. **Restart Continue**: Try restarting the Continue extension\n\n### No Search Results\n\n1. **Verify connection**: The MCP server needs internet access to search the documentation\n2. **Check query format**: Try rephrasing your search query\n3. **Test with known topics**: Search for well-documented features like \"model configuration\"\n\n## Related Documentation\n\n- [MCP Overview](/customize/deep-dives/mcp)\n- [Agent Mode](/ide-extensions/agent/quick-start)\n- [Configuration](/customize/overview)\n"}
{"source":"github","repo":"continue","path":"docs/README.md","content":"## Mintlify Starter Kit\n\nClick on `Use this template` to copy the Mintlify starter kit. The starter kit contains examples including\n\n- Guide pages\n- Navigation\n- Customizations\n- API Reference pages\n- Use of popular components\n\n### Development\n\nInstall the [Mintlify CLI](https://www.npmjs.com/package/mint) to preview the documentation changes locally. To install, use the following command\n\n```\nnpm i -g mint\n```\n\nRun the following command at the root of your documentation (where docs.json is)\n\n```\nmint dev\n```\n\n### Publishing Changes\n\nInstall our Github App to auto propagate changes from your repo to your deployment. Changes will be deployed to production automatically after pushing to the default branch. Find the link to install on your dashboard.\n\n#### Troubleshooting\n\n- If the dev environment isn't running - Run `mint update` to ensure you have the most recent version of the CLI.\n- Page loads as a 404 - Make sure you are running in a folder with `docs.json`\n"}
{"source":"github","repo":"continue","path":"docs/home.mdx","content":"---\ntitle: \"Introduction\"\ndescription: \"Learn how Continue enables developers to embrace continuous AI, enabling them to build custom AI code agents with open source VS Code and JetBrains extensions featuring chat, autocomplete, edit, and agent capabilities\"\n---\n\n## What is Continue?\n\n**Continue enables developers to create, share, and use custom AI code agents with our open source [VS Code](https://marketplace.visualstudio.com/items?itemName=Continue.continue) and [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension) extensions and [hub of models, rules, prompts, docs, and other building blocks](https://hub.continue.dev)**\n\n## Key Features\n\n**You can use Continue to build and run custom agents across your IDE, terminal, and CI.**\n\n1. Get started with Continue in [VS Code](https://marketplace.visualstudio.com/items?itemName=Continue.continue) or [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension) extensions:\n- [Agent mode](/ide-extensions/agent/quick-start) to work on development tasks together with AI\n- [Chat mode](/ide-extensions/chat/quick-start) to ask general questions and clarify code sections\n- [Edit mode](/ide-extensions/edit/quick-start) to modify code section without leaving your current file\n- [Autocomplete](/ide-extensions/autocomplete/quick-start) to receive inline code suggestions as you type\n\n2. Try out [Continue CLI (cn)](https://docs.continue.dev/guides/cli) and give us feedback\n\n3. Discover the models, prompts, rules, MCP tools, and agents you need to automate your workflows with AI on [Continue Mission Control](https://hub.continue.dev/)\n"}
{"source":"github","repo":"continue","path":"docs/CONTRIBUTING.mdx","content":"---\ntitle: \"Contributing to Continue Documentation\"\nsidebarTitle: \"Docs Contributions\"\ndescription: \"Welcome to the Continue documentation! We're excited that you want to contribute. This guide will help you get started with our documentation workflow and tools.\"\n---\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n1. **Node.js** (v20 or higher)\n2. **Git** for version control\n3. **GitHub CLI** - Install from [cli.github.com](https://cli.github.com/)\n\n### Setup Steps\n\n1. **Fork and clone the repository**\n\n   ```bash\n   # Fork the repository (this will also clone it locally)\n   gh repo fork continuedev/continue\n   cd continue/docs\n   ```\n\n2. **Install dependencies**\n\n   ```bash\n   npm install\n   ```\n\n3. **Start the local development server**\n   ```bash\n   npm run dev\n   ```\n   Your docs will be available at `http://localhost:3000`\n\n## üí¨ Creating Discussions and Issues\n\nBefore creating any issues, we ask that you start with a GitHub Discussion. This helps us organize feedback and determine the best path forward.\n\n### Starting a Discussion\n\n<Steps>\n  <Step title=\"Go to GitHub Discussions\">\n    Visit the [Continue GitHub Discussions](https://github.com/continuedev/continue/discussions) page.\n  </Step>\n  \n  <Step title=\"Choose the right category\">\n    Select the **Docs** category for documentation improvements, corrections, or suggestions.\n   \n  </Step>\n  \n  <Step title=\"Provide detailed information\">\n    Include the following in your discussion:\n    - Clear description of the issue or suggestion\n    - Steps to reproduce (if applicable)\n    - Expected vs. actual behavior\n    - Screenshots or code examples when helpful\n    - Your environment (OS, IDE, Continue version)\n  </Step>\n</Steps>\n\n### Issue Escalation Process\n\n<Info>\n  \n  **Important**: All issues should start as discussions. The Continue team will determine if and when a discussion should be escalated to a GitHub issue.\n  \n</Info>\n\nThe Continue team will review discussions and may:\n- Provide a solution or clarification directly in the discussion\n- Ask for additional information or testing\n- Convert the discussion to an issue if it requires code changes or is a confirmed bug\n- Close the discussion if it's resolved or not actionable\n\n### When Discussions Become Issues\n\nA discussion will typically be converted to an issue when:\n- A bug in the documentation is confirmed\n- A new feature or significant documentation change is approved\n- Community consensus supports the proposed change\n- Technical implementation is required\n\n## ü§ñ AI-Powered Documentation with Continue\n\nWe strongly encourage using Continue's AI assistance to maintain consistency and quality in our documentation. Here are three ways to set this up:\n\n### Option 1: Use the Pre-Built Documentation Agent (Recommended)\n\nThe easiest way to get started is using our pre-configured documentation agent:\n\n<Steps>\n  <Step title=\"Install the agent from Continue Mission Control\">\n    Visit [the Docs Assistant - Mintlify in Mission Control](https://hub.continue.dev/continuedev/docs-mintlify) and click \"Install\" to add it to your Continue setup. This agent comes pre-configured with all our documentation standards.\n\n    <Info>\n    Learn more about Continue Configs in our [config documentation](/guides/understanding-configs).\n    </Info>\n  </Step>\n  \n  <Step title=\"Use the agent\">\n    <Tabs>\n      <Tab title=\"Command Line\">\n        ```bash\n        # Install the Continue CLI if you haven't already\n        npm install -g @continuedev/cli\n        \n        # Use the agent from the command line\n        cn \"Create a new guide for using the Continue CLI with Linear\" --config continuedev/docs-mintlify\n        ```\n      </Tab>\n      \n      <Tab title=\"IDE\">\n        1. Open Continue in your IDE\n        2. Select the \"Docs Assistant - Mintlify\" agent from the model dropdown\n        3. Ask it to help you create or edit documentation\n        \n        Example prompts:\n        - \"Create a new guide for using the Continue CLI with Linear\"\n        - \"Update the getting-started guide with the new installation process\"\n        - \"Format this documentation according to Mintlify standards\"\n      </Tab>\n    </Tabs>\n  </Step>\n</Steps>\n\n<Tip>\n  You can also remix this config to customize it for your specific needs. Learn how to create your own remix in our [remix config documentation](/hub/configs/create-a-config#how-to-remix-a-config).\n</Tip>\n\n### Option 2: Create Your Own Custom Agent\n\nIf you want more control or customization, you can create your own documentation agent:\n\n<Steps>\n      <Step title=\"Create a new config\">\n          Follow our [config creation guide](/hub/configs/create-a-config) to set up your own config.\n  </Step>\n  \n  <Step title=\"Add the Continue Docs MCP\">\n    Install from Continue Mission Control: https://hub.continue.dev/continuedev/continue-docs-mcp\n    \n    This MCP provides context about Continue's documentation structure and standards.\n  </Step>\n  \n  <Step title=\"Add Mintlify formatting rules\">\n    Install from Continue Mission Control: https://hub.continue.dev/mintlify/technical-writing-rule\n    \n    This rule ensures proper Mintlify component formatting.\n  </Step>\n  \n  <Step title=\"Use your custom agent\">\n    <Tabs>\n      <Tab title=\"Command Line\">\n        ```bash\n        # Install the Continue CLI if you haven't already\n        npm install -g @continuedev/cli\n        \n        # Use your agent from the command line\n        cn --config your-org/your-agent-name \"Create a new guide for API authentication\"\n        ```\n      </Tab>\n      \n      <Tab title=\"IDE\">\n        1. Open Continue in your IDE\n        2. Select your custom agent from the model dropdown\n        3. Ask it to help you create or edit documentation\n        \n        Example prompts:\n        - \"Help me format this documentation according to Mintlify standards\"\n        - \"Create a troubleshooting section for this feature\"\n      </Tab>\n    </Tabs>\n  </Step>\n</Steps>\n\n## üìù Documentation Standards\n\n### Mintlify Component Guidelines\n\nWhen using Mintlify components, follow these formatting rules:\n\n#### Cards and Info Boxes\n\n```mdx\n<Card title=\"Best Practice\" icon=\"lightbulb\">\n\nAlways include blank lines and proper indentation:\n\n- Use 2-space indentation\n- Add blank lines after opening tags\n- Format lists as bullet points\n\n</Card>\n```\n\n#### Warning and Note Components\n\n```mdx\n<Warning>\n\nImportant information should be formatted clearly:\n\n- Each point on its own line\n- Consistent indentation\n- Clear, concise language\n\n</Warning>\n```\n\n### Writing Style\n\n1. **Be concise**: Get to the point quickly\n2. **Use examples**: Show, don't just tell\n3. **Include code blocks**: Provide working examples\n4. **Add visuals**: Screenshots and diagrams help understanding\n5. **Test your changes**: Ensure all links and code examples work\n\n<Warning>\nIf you are creating a new guide, please test the steps yourself to ensure accuracy.\n</Warning>\n\n## üîß Common Tasks\n\n### Adding a New Guide\n\n1. Create a new `.mdx` file in the guides directory\n2. Add frontmatter:\n   ```mdx\n   ---\n   title: \"Your Guide Title\"\n   description: \"Brief description of what this guide covers\"\n   ---\n   ```\n3. Use the Continue agent to help format your content\n4. Update `docs.json` to include your new page in the navigation\n\n<Tip>\nWe have both guides and cookbooks. Use guides for step-by-step instructions and cookbooks for creating agents for the CLI.\n</Tip>\n\n### Updating Existing Documentation\n\n1. Use the Continue agent with prompts like:\n   - \"Update the installation guide with the new npm package\"\n   - \"Add a troubleshooting section for connection issues\"\n2. The agent will maintain consistent formatting automatically\n\n### Adding Code Examples\n\nUse language-specific code blocks:\n\n````mdx\n```typescript\n// Your TypeScript code here\nconst example = \"Hello, Continue!\";\n```\n````\n\n## üêõ Testing Your Changes\n\n1. **Local preview**: In the docs directory, run `npm run dev` and check your changes\n2. **Link validation**: Ensure all internal and external links work\n3. **Format check**: Use the Continue agent to validate Mintlify formatting\n4. **Build test**: Run `npm run build` to ensure no build errors\n\n## üì§ Submitting Your Contribution\n\n1. **Create a feature branch**\n\n   ```bash\n   git checkout -b docs/your-feature-name\n   ```\n\n2. **Commit your changes**\n\n   ```bash\n   git add .\n   git commit -m \"docs: describe your changes\"\n   ```\n\n3. **Push and create a Pull Request**\n\n   ```bash\n   # Push to your fork and create a PR\n   git push origin docs/your-feature-name\n   gh pr create --web\n   ```\n\n<Tip>\n\n  The `gh pr create` command will automatically:\n  - Push your branch to your fork\n  - Create a pull request to the main repository\n  - Allow you to add a title, description, and reference issues\n  - Open the PR in your browser if using `--web`\n\n</Tip>\n\n### Linking Your PR to Issues\n\n<Info>\n\n**Important**: When your PR addresses a specific issue, make sure to link it using GitHub keywords.\n\n</Info>\n\nTo automatically link your PR to an issue and close it when the PR is merged, use one of these keywords in your PR description:\n\n- `closes #issue_number`\n- `fixes #issue_number`\n- `resolves #issue_number`\n\n**Example PR description:**\n```markdown\nImproved the installation guide with clearer steps for Windows users.\n\nCloses #1234\n```\n\nThis helps us track which issues are being worked on and automatically closes them when your PR is merged.\n\n<Tip>\n\nLearn more about linking PRs to issues in the [GitHub documentation](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/using-keywords-in-issues-and-pull-requests).\n\n</Tip>\n\n## üí° Tips for Success\n\n- **Use the Continue agent**: It knows our documentation standards and will save you time\n- **Preview frequently**: Check your changes in the local dev server\n- **Ask questions**: Open an issue or discussion if you need clarification\n- **Small PRs are better**: Focus on one topic or fix per PR\n- **Update examples**: Ensure code examples reflect the latest API\n\n\n## üÜò Getting Help\n\n- **Start a Discussion**: Use [GitHub Discussions](https://github.com/continuedev/continue/discussions) for documentation issues, suggestions, or questions\n- **Continue agent questions**: Check the [Continue Mission Control page](https://hub.continue.dev/continuedev/docs-mintlify)\n- **Discord community**: Join our Discord for real-time help\n- **Existing docs**: Review similar pages for formatting examples\n---\n\nThank you for contributing to Continue! Your efforts help make our documentation better for everyone. To learn more about contibuting to other parts of the project, check out our [main CONTRIBUTING guide](https://github.com/continuedev/continue/blob/main/CONTRIBUTING.md) üéâ\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/quick-start.mdx","content":"---\ntitle: \"Quick Start Tutorial\"\ndescription: \"Learn Continue's core features through hands-on exercises. Get started with Autocomplete, Edit, Chat, and Agent mode in minutes.\"\nsidebarTitle: \"Quick Start\"\n---\n\nWelcome to Continue! This interactive tutorial will guide you through all four core features using practical examples. Follow along step-by-step to learn more about Continue's capabilities.\n\n<Info>\n  **Prerequisites**: Make sure you have [installed\n  Continue](/ide-extensions/install) and [signed\n  in](https://auth.continue.dev/) to get started.\n</Info>\n\n---\n\n## üîÑ Autocomplete\n\n**What it does**: Provides intelligent inline code suggestions as you type, powered by AI.\n\n<Steps>\n<Step title=\"Create a Test File\">\nCreate a new file called `tutorial.js` (or use any language you prefer) in your IDE\n</Step>\n\n<Step title=\"Try Autocomplete\">\nCopy this starter code and place your cursor at the end of the comment:\n\n```javascript\n// TODO: Implement a sorting algorithm function\nfunction sortingAlgorithm(arr) {\n  // Place cursor here and press Enter\n}\n```\n\nPress **Enter** and watch Continue suggest code completions. Press **Tab** to accept suggestions.\n\n</Step>\n\n<Step title=\"See the Magic\">\nContinue will intelligently suggest function implementations based on the context and comment.\n</Step>\n</Steps>\n\n<Tip>\n  Autocomplete works best when you provide clear function names, comments, or\n  type annotations that give context about your intent.\n</Tip>\n\n---\n\n![autocomplete-demo](../images/autocomplete-quick-start.gif)\n\n## ‚úèÔ∏è Edit\n\n**What it does**: Make quick, targeted changes to specific code sections using natural language instructions.\n\n<Steps>\n<Step title=\"Add Sample Code\">\nPaste this bubble sort implementation in your file:\n\n```javascript\nfunction sortingAlgorithm(x) {\n  for (let i = 0; i < x.length; i++) {\n    for (let j = 0; j < x.length - 1; j++) {\n      if (x[j] > x[j + 1]) {\n        let temp = x[j];\n        x[j] = x[j + 1];\n        x[j + 1] = temp;\n      }\n    }\n  }\n  return x;\n}\n```\n\n</Step><Step title=\"Highlight the Function\">\n**Highlight** the entire function in your editor\n</Step>\n\n<Step title=\"Open Edit Mode\">Press **Cmd/Ctrl + I** to open Edit mode</Step>\n\n<Step title=\"Give Instructions\">\n  Type: `\"make this more readable and add TypeScript types\"`\n</Step>\n\n<Step title=\"Watch the Magic\">\n  Watch Continue refactor your code automatically!\n</Step>\n\n<Step title=\"Review Changes\">\nContinue will show you a diff of the proposed changes. Accept or reject individual changes as needed.\n</Step>\n</Steps>\n\n<Note>\n  Edit is perfect for refactoring, adding documentation, fixing bugs, or\n  converting between languages/frameworks.\n</Note>\n\n![edit-demo](../images/edit-quick-start.gif)\n\n## üí¨ Chat Mode\n\n**What it does**: Interactive AI assistant that can analyze code, answer questions, and provide guidance without leaving your IDE.\n\n<Steps>\n<Step title=\"Add Another Function\">\nAdd this second sorting function to your file:\n\n```javascript\nfunction sortingAlgorithm2(x) {\n  for (let i = 0; i < x.length; i++) {\n    for (let j = 0; j < x.length - 1; j++) {\n      if (x[j] > x[j + 1]) {\n        let temp = x[j];\n        x[j] = x[j + 1];\n        x[j + 1] = temp;\n      }\n    }\n  }\n  return x;\n}\n```\n\n</Step>\n<Step title=\"Start a Conversation\">\n1. **Highlight** the function \n2. Use the keyboard shortcuts below to add it to Chat.\n3. Ask: `\"What sorting algorithm is this and how can I optimize it?\"`\n</Step>\n\n<Step title=\"Explore Further\">\nTry these follow-up questions:\n- `\"Show me how to implement quicksort instead\"`\n- `\"What's the time complexity of this algorithm?\"`\n- `\"Can you write unit tests for this function?\"`\n</Step>\n</Steps>\n\n### Chat Mode Keyboard Shortcuts\n\n<Tabs>\n<Tab title=\"VS Code\">\n\n**Cmd/Ctrl + L**  \nNew Chat / New Chat With Selected Code / Close Continue Sidebar If Chat Already In Focus\n\n**Cmd/Ctrl + Shift + L**  \nFocus Current Chat / Add Selected Code To Current Chat / Close Continue Sidebar If Chat Already In Focus\n\n</Tab>\n<Tab title=\"JetBrains IDEs\">\n\n**Cmd/Ctrl + J**  \nNew Chat / New Chat With Selected Code / Close Continue Sidebar If Chat Already In Focus\n\n**Cmd/Ctrl + Shift + J**  \nFocus Current Chat / Add Selected Code To Current Chat / Close Continue Sidebar If Chat Already In Focus\n\n</Tab>\n</Tabs>\n\n<Tip>\n  Use Chat for code reviews, debugging help, learning new concepts, or\n  brainstorming solutions to complex problems.\n</Tip>\n\n![chat-demo](../images/chat-quick-start.gif)\n\n\n---\n\n## ü§ñ Agent Mode\n\n**What it does**: An autonomous coding assistant that can read files, make changes, run commands, and handle complex multi-step tasks.\n\n<Steps>\n<Step title=\"Switch to Agent Mode\">\n1. Open the Continue panel\n2. Click the **dropdown** in the bottom left of the input box\n3. Select **\"Agent\"** mode\n</Step>\n\n<Step title=\"Give Agent Mode a Complex Task\">\n  Try this prompt: ``` \"Write comprehensive unit tests for the sorting functions\n  in this file. Create the tests in a new file using Jest, and make sure to test\n  edge cases like empty arrays and single elements.\" ```\n</Step>\n\n<Step title=\"Watch Agent Mode Work\">\nAgent mode will:\n- ‚úÖ Analyze your existing code\n- ‚úÖ Create a new test file\n- ‚úÖ Write comprehensive tests\n- ‚úÖ Handle setup and imports\n- ‚úÖ Explain what it's doing at each step\n</Step>\n</Steps>\n\n<Warning>\n  Agent mode has powerful capabilities including file creation and modification.\n  Always review Agent mode's changes before accepting them.\n</Warning>\n\n![agent-demo](../images/agent-quick-start.gif)\n\n---\n## Explore More Extension Examples\n\nReady to explore more? Continue offers five powerful features to enhance your coding workflow:\n<Tabs>\n<Tab title=\"Agent Mode\">\n[Agent Mode](/ide-extensions/agent/quick-start) equips the Chat model with the tools needed to handle a wide range of coding tasks\n\n![agent mode](/images/agent-9ef792cfc196a3b5faa984fb072c4400.gif)\n\n<Info>\n  Learn more about [Agent Mode](/ide-extensions/agent/quick-start)\n</Info>\n</Tab>\n\n<Tab title=\"Chat Mode\">\n[Chat](/ide-extensions/chat/quick-start) makes it easy to ask for help from an LLM without needing to leave the IDE\n\n![chat](/images/chat-489b68d156be2aafe09ee7cedf233fba.gif)\n\n<Info>\n  Learn more about [Chat Mode](/ide-extensions/chat/quick-start)\n</Info>\n</Tab>\n\n<Tab title=\"Plan\">\n[Plan](/ide-extensions/agent/plan-mode) provides a safe environment with read-only tools for exploring code and planning changes\n\n![plan](/images/plan-mode.gif)\n\n<Info>\n  Learn more about [Plan Mode](/ide-extensions/agent/plan-mode)\n</Info>\n</Tab>\n\n<Tab title=\"Edit\">\n[Edit](/ide-extensions/edit/quick-start) is a convenient way to modify code without leaving your current file\n\n![edit](/images/edit.gif)\n\n<Info>\n  Learn more about [Edit](/ide-extensions/edit/quick-start)\n</Info>\n</Tab>\n\n<Tab title=\"Autocomplete\">\n[Autocomplete](/ide-extensions/autocomplete/quick-start) provides inline code suggestions as you type\n\n![autocomplete](/images/autocomplete-9d4e3f7658d3e65b8e8b20f2de939675.gif)\n\n<Info>\n  Learn more about [Autocomplete](/ide-extensions/autocomplete/quick-start)\n</Info>\n</Tab>\n</Tabs>\n\n## üöÄ Next Steps\n\nCongratulations! You've experienced all four core Continue features. Here's what to explore next:\n\n<CardGroup cols={2}>\n  <Card title=\"Customize Your Setup\" icon=\"gear\" href=\"/customize/overview\">\n    Configure models, add context providers, and personalize your workflow\n  </Card>\n  <Card\n    title=\"Advanced Features\"\n    icon=\"rocket\"\n    href=\"/ide-extensions/agent/quick-start\"\n  >\n    Dive deeper into Agent mode capabilities and advanced use cases\n  </Card>\n  <Card\n    title=\"Model Providers\"\n    icon=\"brain\"\n    href=\"/customize/model-providers/overview\"\n  >\n    Connect your preferred AI models and providers\n  </Card>\n  <Card\n    title=\"Join Community\"\n    icon=\"users\"\n    href=\"https://discord.gg/NWtdYexhMs\"\n  >\n    Get help and share experiences with other Continue users\n  </Card>\n</CardGroup>\n\n---\n\n## üìö Feature Deep Dives\n\nReady to master specific features? Check out these detailed guides:\n\n<AccordionGroup>\n  <Accordion title=\"Autocomplete Deep Dive\">\n    Learn about [configuring autocomplete\n    models](/customize/model-roles/autocomplete), [fine-tuning\n    suggestions](/customize/deep-dives/autocomplete), and [troubleshooting\n    common issues](/troubleshooting).\n  </Accordion>\n\n<Accordion title=\"Chat & Agent Mode Best Practices\">\n  Discover [effective prompting techniques](/customize/deep-dives/prompts), [hub\n  v. local configuration](/guides/understanding-configs), and [custom slash\n  commands](/customize/deep-dives/prompts).\n</Accordion>\n\n  <Accordion title=\"Enterprise & Teams\">\n    Explore [Hub configurations](/hub/configs/intro), [organization\n    management](/hub/governance/creating-an-org), and [sharing\n    configurations](/hub/sharing).\n  </Accordion>\n</AccordionGroup>\n\n<Info>\n  **Need help?** Check our [troubleshooting guide](/troubleshooting) or ask a\n  question in our [community\n  discussions](https://github.com/continuedev/continue/discussions).\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/agent/context-selection.mdx","content":"---\ntitle: \"Context Selection\"\ndescription: \"Learn how Continue's agent mode selects relevant code context using file content, language server definitions, imports, and recent file history.\"\n---\n\nYou can use the same methods to manually add context as [Chat](/ide-extensions/chat/context-selection).\n\nTool call responses are automatically included as context items. This enables Agent mode to see the result of the previous action and decide what to do next.\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/agent/quick-start.mdx","content":"---\ntitle: \"Quick Start\"\ndescription: \"Get started with Continue's Agent mode to automatically implement code changes, fix bugs, and run commands using AI-powered tools that can modify your codebase based on natural language instructions\"\n---\n\nAgent mode equips the Chat model with the tools needed to handle a wide range of coding tasks, allowing the model to make decisions and save you the work of manually finding context and performing actions.\n\n<Accordion title=\"Learn how to choose the right mode\" icon=\"lightbulb\">\n  <Columns cols={3}>\n    <Card title=\"Chat Mode\" icon=\"comments\">\n      *Learn and discuss without changing code.*\n\n      **Mental Model:** Talking to a knowledgeable colleague\n\n      **Best For:** Explaining concepts, comparing approaches, code review discussions.\n\n    </Card>\n\n    <Card title=\"Plan Mode\" icon=\"binoculars\">\n        *Safely explore and plan with read-only tools.*\n\n      **Mental Model:** Architect surveying before renovation\n\n      **Best For:** Understanding a codebase, bug investigation, planning implementations.\n\n    </Card>\n\n    <Card title=\"Agent Mode\" icon=\"hammer\">\n    *Make actual changes with full tool access.*\n\n      **Mental Model:** Contractor executing approved blueprints\n\n      **Best For:** Implementing features, fixing bugs, running tests and commands.\n\n    </Card>\n\n  </Columns>  \n</Accordion>\n\n### How to Use Agent Mode\n\nYou can switch to `Agent` in the mode selector below the chat input box. The mode selector offers three options:\n\n- **Chat mode**: No tools available, pure conversation\n- **Plan mode**: Read-only tools for safe exploration and planning\n- **Agent mode**: All tools available for making changes\n\n![How to select agent mode](/images/mode-select-agent.png)\n\n<Info>\n  If Agent mode or Plan mode is disabled with a `Not Supported` message, the selected\n  model or provider doesn't support tools, or Continue doesn't yet support tools\n  with it. See [Model Blocks](/customize/models) for more information.\n</Info>\n\n<Tip>\n  Use the keyboard shortcut `Cmd/Ctrl + .` to quickly cycle between modes.\n</Tip>\n\n### How to Chat with Agent mode\n\nAgent mode lives within the same interface as [Chat](/ide-extensions/chat/how-it-works) mode, so the same [input](/ide-extensions/chat/quick-start#how-to-start-a-conversation) is used to send messages and you can still use the same manual methods of providing context, such as [`@` context providers](/ide-extensions/chat/quick-start#how-to-use--for-additional-context) or adding [highlighted code from the editor](/ide-extensions/chat/quick-start#how-to-include-code-context).\n\n#### How to Use Natural Language with Agent mode\n\nWith Agent mode, you can provide natural language instruction and let the model do the work. As an example, you might say\n\n> Set the @typescript-eslint/naming-convention rule to \"off\" for all eslint configurations in this project\n\nAgent mode will then decide which tools to use to get the job done.\n\n## How to Give Agent Mode Permission\n\nBy default, Agent mode will ask permission when it wants to use a tool. Click `Continue` to allow Agent mode to proceed with the tool call or `Cancel` to reject it.\n\n![agent requesting permission](/images/ide-extensions/agent/images/agent-permission-c150919a5c43eb4f55d9d4a46ef8b2d6.png)\n\nYou can use tool policies to exclude or make usage automatic for specific tools. See [MCP Tools](/customize/mcp-tools) for more background.\n\n## How to View Tool Responses\n\nAny data returned from a tool call is automatically fed back into the model as a context item. Most errors are also caught and returned, so that Agent mode can decide how to proceed.\n\n![agent response](/images/ide-extensions/agent/images/agent-response-c7287c82aac93fb4376f9d85b352b2d7.png)\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/agent/model-setup.mdx","content":"---\ntitle: \"Model Setup for Agent Mode\"\ndescription: \"Learn how to set up models for Agent Mode in Continue, including recommended models and configuration options for optimal performance\"\nsidebarTitle: \"Model Setup\"\n---\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx'\n\nThe models you set up for Chat mode will be used with Agent mode if the model supports tool calling. The recommended models and how to set them up can be found [here](/ide-extensions/chat/model-setup).\n\n## How System Message Tools Work\n\nContinue implements an innovative approach called **system message tools** that ensures consistent tool functionality across all models, regardless of their native capabilities. This allows Agent mode to work seamlessly with a wider range of models and providers.\n\n### How System Message Tools Function\n\nInstead of relying solely on native tool calling APIs (which vary between providers), Continue converts tools into XML format and includes them in the system message. The model generates tool calls as structured XML within its response, which Continue then parses and executes. This approach provides:\n\n- **Universal compatibility** - Any model capable of following instructions can use tools, not just those with native tool support\n- **Consistent behavior** - Tool calls work identically across OpenAI, Anthropic, local models, and others\n- **Better reliability** - Models that struggle with native tools often perform better with system message tools\n- **Seamless switching** - Change between providers without modifying your workflow\n\n## Recommended Agent Models\n\n<ModelRecommendations role=\"all\" />\n\n### How to Configure Agent Mode\n\nAgent mode automatically determines whether to use native or system message tools based on the model's capabilities. No additional configuration is required - simply select your model and Continue handles the rest.\n\n## How to Check Model Compatibility\n\nTo see which models support specific features like tool use (for Agent mode) and image input, check out our [Model Capabilities guide](/customize/deep-dives/model-capabilities)."}
{"source":"github","repo":"continue","path":"docs/ide-extensions/agent/plan-mode.mdx","content":"---\ntitle: \"Plan Mode in Continue ‚Äì Safe, Read-Only Code Exploration\"\nsidebarTitle: \"Plan Mode\"\ndescription: \"Learn how to use Plan Mode in Continue to explore and understand codebases safely with read-only tools, search, and analysis before making changes\"\n---\n\n## What is Plan mode?\n\nPlan mode is a restricted environment that provides read-only access to your codebase. It's designed for safe exploration, understanding code, and planning changes without making any modifications.\n\n![Plan mode in action](/images/plan-mode.gif)\n\n### What Are the Key Features of Plan Mode?\n\n- **Read-only tools**: Access files, search, and analyze without risk\n- **Safe exploration**: Perfect for understanding unfamiliar codebases\n- **Planning focus**: Develop implementation strategies before execution\n- **MCP support**: Works with all MCP tools alongside built-in read-only tools\n\n### How Plan Mode Works\n\nPlan mode filters the available tools to only include read-only operations. This means you can:\n\n- Read any file in your project\n- Search through code with grep and glob patterns\n- View repository structure and diffs\n- Fetch web content for additional context\n- Use all MCP tools\n\nBut you cannot:\n\n- Create, edit, or delete files\n- Run terminal commands\n- Make any system changes\n\n### How to Get Started with Plan Mode\n\nSelect \"Plan\" from the mode selector below the chat input, or use `Cmd/Ctrl + .` to cycle through modes.\n\n![Plan mode selector](/images/plan-mode-selector.png)\n\nFor detailed information about tools and usage, see the [Agent documentation](/ide-extensions/agent/how-it-works), which covers both Agent and Plan modes.\n\n### What Is the Common Workflow for Plan Mode?\n\n1. **Start in Plan mode** to explore and understand\n2. **Develop your approach** with the model's help\n3. **Switch to Agent mode** when ready to implement\n\n<Tip>\n  Plan mode shares the same interface and context features as Chat and Agent\n  modes. You can use `@` context providers and highlight code just like in other\n  modes.\n</Tip>\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/agent/how-to-customize.mdx","content":"---\ntitle: \"How to Customize Agent Mode\"\ndescription: \"Learn how to customize Agent Mode in Continue to better fit your workflow and coding style.\"\nsidebarTitle: \"Customize Agent Mode\"\n---\n\n## How to Add Rules Blocks\n\nAdding Rules can be done in your configuration locally or in Mission Control. You can explore Rules on the Continue Mission Control and refer to the [Rules deep dive](/customize/deep-dives/rules) for more details.\n\n## How to Customize System Messages\n\nYou can customize the system messages for Chat, Agent, and Plan modes using model-level configuration:\n\n```yaml\nmodels:\n  - name: GPT-4o\n    provider: openai\n    model: gpt-4o\n    chatOptions:\n      baseSystemMessage: \"You are a helpful coding agent.\"\n      baseAgentSystemMessage: \"You are a systematic coding agent. Break down problems methodically.\"\n      basePlanSystemMessage: \"You are a planning agent. Create clear, actionable steps.\"\n```\n\n## How to Add MCP Tools\n\nYou can add MCP servers to your configuration to give Agent mode access to more tools. Explore [MCP Servers on Mission Control](https://hub.continue.dev/hub?type=mcpServers) and consult the [MCP guide](/customize/deep-dives/mcp) for more details.\n\n## How to Configure Tool Policies\n\nYou can adjust the Agent mode's tool usage behavior to three options:\n\n- **Ask First (default)**: Request user permission with \"Cancel\" and \"Continue\" buttons\n- **Automatic**: Automatically call the tool without requesting permission\n- **Excluded**: Do not send the tool to the model\n\n:::warning\nBe careful setting tools to \"automatic\" if their behavior is not read-only.\n:::\n\nTo manage tool policies:\n\n1. Click the tools icon in the input toolbar\n2. View and change policies by clicking on the policy text\n3. You can also toggle groups of tools on/off\n\nTool policies are stored locally per user.\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/agent/how-it-works.mdx","content":"---\ntitle: \"How Agent Mode Works\"\ndescription: \"Agent mode offers the same functionality as Chat mode, while also including tools in the request to the model and an interface for handling tool calls and responses.\"\n---\n\n## How the Tool Handshake Works\n\nTools provide a flexible, powerful way for models to interface with the external world. They are provided to the model as a JSON object with a name and an arguments schema. For example, a `read_file` tool with a `filepath` argument will give the model the ability to request the contents of a specific file.\n\nThe following handshake describes how Agent mode uses tools:\n\n1. In Agent mode, available tools are sent along with `user` chat requests\n2. The model can choose to include a tool call in its response\n3. The user gives permission. This step is skipped if the policy for that tool is set to `Automatic`\n4. Continue calls the tool using built-in functionality or the MCP server that offers that particular tool\n5. Continue sends the result back to the model\n6. The model responds, potentially with another tool call and step 2 begins again\n\n<Info>\n  Tool availability varies by mode: - **Chat mode**: No tools included - **Plan\n  mode**: Only read-only tools included - **Agent mode**: All tools included\n</Info>\n\n## What Built-in Tools Are Available\n\nContinue includes several built-in tools which provide the model access to IDE functionality.\n\n### What Tools Are Available in Plan Mode (Read-Only)\n\nIn Plan mode, only these read-only tools are available:\n\n- **Read file** (`read_file`)\n- **Read currently open file** (`read_currently_open_file`)\n- **List directory** (`ls`)\n- **Glob search** (`glob_search`)\n- **Grep search** (`grep_search`)\n- **Fetch URL content** (`fetch_url_content`)\n- **Search web** (`search_web`)\n- **View diff** (`view_diff`)\n- **View repo map** (`view_repo_map`)\n- **View subdirectory** (`view_subdirectory`)\n- **Codebase tool** (`codebase_tool`)\n\n### What Tools Are Available in Agent Mode (All Tools)\n\nIn Agent mode, all tools are available including the read-only tools above plus:\n\n- **Create new file** (`create_new_file`): Create a new file within the project\n- **Edit file** (`edit_existing_file`): Make changes to existing files\n- **Run terminal command** (`run_terminal_command`): Run commands from the workspace root\n- **Create Rule Block** (`create_rule_block`): Create a new rule block in `.continue/rules`\n- All other write/execute tools for modifying the codebase\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/plan/quick-start.mdx","content":"---\ntitle: \"Quick Start\"\ndescription: \"Get started with Continue's Plan mode to safely explore codebases, plan refactors, debug issues, and develop implementation strategies without modifying code before switching to execution\"\n---\n\n## How to use it\n\nPlan mode provides a safe environment for understanding and constructing plans without making changes. It equips the Chat model with read-only tools, allowing you to explore, analyze, and plan modifications before executing them.\n\n### Use Plan\n\nYou can switch to `Plan` in the mode selector below the chat input box.\n\n![How to select plan mode](/images/plan-mode-selector.png)\n\n<Info>\n  If Plan is disabled with a `Not Supported` message, the selected model or\n  provider doesn't support tools. See [Model Setup](/ide-extensions/agent/model-setup)\n  for more information.\n</Info>\n\n### Chat with Plan\n\nPlan mode lives within the same interface as [Chat mode](/ide-extensions/chat/how-it-works) and [Agent mode](/ide-extensions/agent/how-it-works), so the same [input](/ide-extensions/chat/quick-start#how-to-start-a-conversation) is used to send messages and you can still use the same manual methods of providing context, such as [`@` context providers](/ide-extensions/chat/quick-start#how-to-use--for-additional-context) or adding [highlighted code from the editor](/ide-extensions/chat/quick-start#how-to-include-code-context).\n\n#### What makes Plan different\n\nUnlike Agent mode, Plan mode:\n\n- Only uses read-only tools (file reading, searching, analysis)\n- Cannot modify, create, or delete files\n- Focuses on understanding and planning rather than execution\n- Provides a safe environment for exploration\n\n### Common use cases\n\nPlan mode is ideal for:\n\n- **Code exploration**: Understanding how a codebase works\n- **Planning refactors**: Analyzing impact before making changes\n- **Debugging**: Investigating issues without modifying code\n- **Architecture review**: Understanding system design and dependencies\n- **Pre-implementation planning**: Thinking through changes before executing\n\n### Example workflow\n\n1. **Start in Plan mode** to explore and understand the task\n2. **Develop a plan** with the model's help\n3. **Switch to Agent mode** when ready to implement changes\n\nFor example, you might say:\n\n> Help me understand how the authentication system works and plan improvements to add OAuth support\n\nPlan mode will analyze the existing code, understand the current implementation, and help you create a detailed plan‚Äîall without making any changes.\n\n## Switching to execution\n\nWhen you're ready to implement your plan, simply switch to Agent mode using the mode selector or keyboard shortcut (`Cmd/Ctrl + .`). The conversation context carries over, so Agent mode can immediately start implementing the plan you developed.\n\n<Tip>\n  Use the keyboard shortcut `Cmd/Ctrl + .` to quickly cycle between Agent, Chat,\n  and Plan modes.\n</Tip>\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/plan/how-it-works.mdx","content":"---\ntitle: \"How Plan Works\"\ndescription: \"Plan mode provides a restricted environment with read-only tools, enabling safe exploration and planning without making changes to your codebase.\"\n---\n\n## Understanding Plan mode\n\nPlan mode is designed to help you understand code and construct plans before making changes. It sits between Chat mode (no tools) and Agent mode (all tools), providing a middle ground where you can explore and analyze without risk.\n\n### The key difference: Read-only tools\n\nWhile Agent mode has access to all tools including those that modify files, Plan mode restricts access to only read-only tools. This ensures that:\n\n- No files are created, modified, or deleted\n- No commands are executed that could change system state\n- All exploration is safe and non-destructive\n- You can confidently investigate without unintended consequences\n\n## How Plan mode works\n\nWhen you select Plan mode:\n\n1. Continue sends a special system message instructing the model to focus on understanding and planning\n2. The tool list is filtered to include only read-only tools\n3. The model can use these tools to explore and analyze your codebase\n4. When you're ready to implement changes, you switch to Agent mode\n\n### Available tools in Plan mode\n\nPlan mode includes these read-only built-in tools:\n\n- **Read file** (`read_file`): Read the contents of any file in the project\n- **Read currently open file** (`read_currently_open_file`): Read the contents of the currently open file\n- **List directory** (`ls`): List files and directories\n- **Glob search** (`glob_search`): Search for files matching a pattern\n- **Grep search** (`grep_search`): Search file contents using regex patterns\n- **Fetch URL content** (`fetch_url_content`): Retrieve content from web URLs\n- **Search web** (`search_web`): Perform web searches for additional context\n- **View diff** (`view_diff`): View the current git diff\n- **View repo map** (`view_repo_map`): Get an overview of the repository structure\n- **View subdirectory** (`view_subdirectory`): Get a detailed view of a specific directory\n- **Codebase tool** (`codebase_tool`): Advanced codebase analysis capabilities\n\n### MCP tools support\n\nIn addition to built-in read-only tools, Plan mode also supports all MCP (Model Context Protocol) tools. This allows integration with external services that provide additional context or analysis capabilities without modifying your local environment.\n\n<Info>\n  While Plan mode restricts built-in tools to read-only operations, MCP tools\n  are not filtered by Continue. Some MCP tools may have write capabilities that\n  could modify your system or external services. Always verify what actions your\n  MCP tools can perform before using them in Plan mode.\n</Info>\n\n## The planning workflow\n\nA typical Plan mode workflow follows these steps:\n\n1. **Exploration**: Use read-only tools to understand the current state\n2. **Analysis**: Identify what needs to change and potential impacts\n3. **Planning**: Develop a detailed plan for implementation\n4. **Verification**: Review the plan and ensure it addresses all requirements\n5. **Execution**: Switch to Agent mode to implement the plan\n\n### Example: Planning a refactor\n\n```\nUser: Help me plan a refactor to extract the authentication logic into a separate module\n\nPlan mode:\n1. Reads relevant files to understand current structure\n2. Analyzes dependencies and usage patterns\n3. Identifies all places that need updates\n4. Creates a step-by-step refactoring plan\n5. Suggests: \"Switch to Agent mode to implement this plan\"\n```\n\n## System message and behavior\n\nPlan mode uses a dedicated system message that:\n\n- Instructs the model to focus on understanding and planning\n- Emphasizes using only read-only tools\n- Reminds the model to suggest switching to Agent mode for implementation\n- Encourages thorough analysis before suggesting changes\n\n<Info>\n  The system message can be customized per model. See it [directly in\n  the\n  code](https://github.com/continuedev/continue/blob/main/core/llm/defaultSystemMessages.ts).\n</Info>\n\n## When to use Plan mode vs other modes\n\n### Use Plan mode when:\n\n- Exploring unfamiliar codebases\n- Planning complex refactors or features\n- Debugging and understanding issues\n- Reviewing code architecture\n- Creating implementation strategies\n\n### Use Chat mode when:\n\n- Having discussions without needing file access\n- Asking general programming questions\n- Getting explanations without exploring code\n\n### Use Agent mode when:\n\n- Ready to implement changes\n- Need to create or modify files\n- Want to run commands or tests\n- Implementing a plan developed in Plan mode\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/install.mdx","content":"---\ntitle: \"Install\"\ndescription: \"Get Continue installed in your favorite IDE in just a few steps.\"\n---\n\n<iframe\n  width=\"100%\"\n  height=\"400\"\n  src=\"https://www.youtube.com/embed/gjekTru4Cg4\"\n  title=\"Install Continue VS Code Extension\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\n<Tabs>\n<Tab title=\"VS Code\">\n\n<Steps>\n<Step title=\"Install from Marketplace\">\nClick `Install` on the [Continue extension page in the Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=Continue.continue)\n</Step>\n\n<Step title=\"Install in VS Code\">\n  This will open the Continue extension page in VS Code, where you will need to\n  click `Install` again\n</Step>\n\n<Step title=\"Move to Right Sidebar\">\nThe Continue logo will appear on the left sidebar. For a better experience, move Continue to the right sidebar\n\n![move-to-right-sidebar](/images/move-to-right-sidebar-b2d315296198e41046fc174d8178f30a.gif)\n\n</Step>\n\n<Step title=\"Sign In\">\n[Sign in to Mission Control](https://auth.continue.dev/) to get started\n</Step>\n</Steps>\n\n<Info>\n  If you have any problems, see the [troubleshooting guide](/troubleshooting) or\n  ask for help in [our Discord](https://discord.gg/NWtdYexhMs)\n</Info>\n\n</Tab>\n\n<Tab title=\"JetBrains\">\n\n<Steps>\n<Step title=\"Open Settings\">\nOpen your JetBrains IDE and open **Settings** using `Ctrl` + `Alt` + `S`\n</Step>\n\n<Step title=\"Find Continue Plugin\">\n  Select **Plugins** on the sidebar and search for \"Continue\" in the marketplace\n</Step>\n\n<Step title=\"Install Plugin\">\nClick `Install`, which will cause the Continue logo to show up on the right toolbar\n\n![jetbrains-getting-started.png](/images/getting-started/images/jetbrains-getting-started-d62b7edee1cdd58508c5075faf285955.png)\n\n</Step>\n\n<Step title=\"Sign In\">\n[Sign in to Mission Control](https://auth.continue.dev/) to get started\n</Step>\n</Steps>\n\n<Info>\n  If you have any problems, see the [troubleshooting guide](/troubleshooting) or\n  ask for help in [our Discord](https://discord.com/invite/EfJEfdFnDQ)\n</Info>\n\n</Tab>\n</Tabs>\n\n## Signing in\n\nClick \"Get started\" to sign in to Mission Control and get started.\n\n![Hub Onboarding in the Extension](../images/getting-started/images/hub-onboarding-card-81abd457b6d131c4b0aa89a5a6d647d3.png)\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/settings.mdx","content":"---\ntitle: Settings\ndescription: Configure Continue through VS Code's streamlined settings interface\n---\n\n\n\n## Quick Access\n\n<CardGroup cols={3}>\n  <Card title=\"Settings Icon\" icon=\"gear\">\n    Click the gear in the Continue sidebar\n  </Card>\n  <Card title=\"VS Code Settings\" icon=\"sliders\">\n    File ‚Üí Preferences ‚Üí Settings ‚Üí Extensions ‚Üí Continue\n  </Card>\n  <Card title=\"Config File\" icon=\"code\">\n    Edit `config.yml` directly for advanced options\n  </Card>\n</CardGroup>\n\n<Tip>\n  Use the toolbar buttons for quick access to specific settings: - **Rules**\n  (pencil icon) - Custom coding preferences - **Tools** (wrench icon) - Manage\n  integrations - **Models** (cube icon) - Configure AI providers\n</Tip>\n<img\n  src=\"/images/continue-settings-card-layout.png\"\n  alt=\"Continue Settings Panel - Card-based Layout\"\n/>\n## Core Settings\n\n<Tabs>\n  <Tab title=\"Interface\">\n    <AccordionGroup>\n      <Accordion title=\"Display Options\" icon=\"display\">\n        | Setting | Description |\n        |---------|-------------|\n        | Session Tabs | Manage multiple chat sessions |\n        | Code Wrapping | Auto-wrap long code lines |\n        | Markdown Display | Show raw markdown vs rendered |\n        | Chat Scrollbar | Toggle scrollbar visibility |\n      </Accordion>\n      \n      <Accordion title=\"Behavior\" icon=\"wand-magic-sparkles\">\n        | Setting | Description |\n        |---------|-------------|\n        | Auto-accept Diffs | Apply code changes automatically |\n        | Tool Rejection | Continue after tool rejection |\n        | Auto-naming | Generate session titles automatically |\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"Autocomplete\">\n    Configure code completions:\n\n    <img\n      src=\"/images/autocomplete-settings.png\"\n      alt=\"Continue Autocomplete Settings\"\n    />\n\n    <Note>\n      Autocomplete models need to be added to your config to enable selecting an autocomplete model. If none is available, you will be linked to the docs showing recommended models. See our [model recommendations](/customize/model-roles/autocomplete) for the best autocomplete models.\n    </Note>\n\n    <img\n      src=\"/images/autocomplete-model-selection.png\"\n      alt=\"Autocomplete Model Selection\"\n    />\n\n    <Info>\n      To better understand how to set up configs and models, see our [Understanding Configs guide](/guides/understanding-configs).\n    </Info>\n\n  </Tab>\n\n  <Tab title=\"Indexing\">\n    Control how Continue understands your codebase:\n\n    <img\n      src=\"/images/indexing-settings.png\"\n      alt=\"Continue Indexing Settings\"\n    />\n\n    <Steps>\n      <Step title=\"Enable Indexing\">\n        Toggle codebase indexing in the settings panel\n      </Step>\n      <Step title=\"Monitor Progress\">\n        Watch real-time status in the UI\n      </Step>\n      <Step title=\"Verify Coverage\">\n        Check which files are indexed via the status indicator\n      </Step>\n    </Steps>\n\n    <Note>\n      Indexing enables Continue to understand your entire codebase structure, significantly improving context awareness and suggestions.\n    </Note>\n\n  </Tab>\n\n    <Tab title=\"Experimental\">\n    <Warning>\n      These features are in beta and may change or have stability issues.\n    </Warning>\n\n    | Feature | Purpose |\n    |---------|---------|  \n    | Add Current File by Default | The currently open file is added as context in every new conversation |\n    | Enable experimental tools | Enables access to experimental tools that are still in development |\n    | Only use system message tools | Continue will not attempt to use native tool calling and will only use system message tools |\n    | @Codebase: use tool calling only | @codebase context provider will only use tool calling for code retrieval |\n    | Stream after tool rejection | Streaming will continue after the tool call is rejected |\n\n  </Tab>\n</Tabs>\n\n## Model & Assistant Selection\n\n<img\n  src=\"/images/assistant-selector-dropdown.png\"\n  alt=\"New assistant dropdown with cleaner icons and improved organization display\"\n/>\n\nThe refined assistant selector features:\n\n- **Organization badges** for easy provider identification\n- **Smart error handling** that sorts problematic configurations while keeping them selectable\n- **Keyboard navigation** for quick model switching\n\n## Privacy & Data\n\n<CardGroup cols={3}>\n  <Card title=\"Local Processing\" icon=\"lock\">\n    All code analysis happens locally unless explicitly shared\n  </Card>\n  <Card title=\"Telemetry Control\" icon=\"chart-line\">\n    Opt in/out of anonymous usage statistics\n  </Card>\n  <Card title=\"Session Management\" icon=\"database\">\n    Sessions auto-save and restore between IDE restarts\n  </Card>\n</CardGroup>\n\n## Troubleshooting\n\n<AccordionGroup>\n  <Accordion title=\"Model Connection Issues\" icon=\"plug\">\n    - Verify API keys in `config.yml` - Check network connectivity - Confirm\n    endpoint URLs are correct\n  </Accordion>\n\n<Accordion title=\"Tool Loading Failures\" icon=\"wrench\">\n  - Review MCP server configurations - Check tool permissions - Verify tool\n  dependencies are installed\n</Accordion>\n\n    <Accordion title=\"Indexing Problems\" icon=\"magnifying-glass\">\n    - Check file permissions\n    - Review `.gitignore` patterns\n    - Verify sufficient disk space\n\n  </Accordion>\n</AccordionGroup>\n\n<Info>\n  Still having issues? Check our comprehensive [troubleshooting\n  guide](/troubleshooting) or visit the [FAQs](/faqs) for more solutions.\n</Info>\n\n## Next Steps\n\n<CardGroup cols={2}>\n  <Card title=\"Configure Models\" icon=\"robot\" href=\"/customize/models\">\n    Set up your AI providers\n  </Card>\n  <Card title=\"Add Tools\" icon=\"wrench\" href=\"/customize/mcp-tools\">\n    Extend Continue with MCP tools\n  </Card>\n  <Card title=\"Custom Rules\" icon=\"pencil\" href=\"/customize/rules\">\n    Define coding preferences\n  </Card>\n  <Card title=\"Prompts\" icon=\"message\" href=\"/customize/prompts\">\n    Customize AI behavior\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/autocomplete/context-selection.mdx","content":"---\ntitle: \"Context Selection\"\ndescription: \"Learn how Continue's autocomplete selects relevant code context using file content, language server definitions, imports, and recent file history.\"\n---\n\nAutocomplete will automatically determine context based on the current cursor position. We use the following techniques to determine what to include in the prompt:\n\n## File Prefix and Suffix Context\n\nWe will always include the code from your file prior to and after the cursor position.\n\n## Language Server Protocol (LSP) Definitions\n\nSimilar to how you can use cmd/ctrl + click in your editor, we use the same tool (the LSP) to power \"go to definition\". For example, if you are typing out a function call, we will include the function definition. Or, if you are writing code inside of a method, we will include the type definitions for any parameters or the return type.\n\n## Imported File Context\n\nBecause there are often many imports, we can't include all of them. Instead, we look for symbols around your cursor that have matching imports and use that as context.\n\n## Recent File Context\n\nWe automatically consider recently opened or edited files and include snippets that are relevant to the current completion.\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/autocomplete/quick-start.mdx","content":"---\ntitle: \"Quick Start with Continue Autocomplete\"\ndescription: \"Learn how to quickly start using Continue's AI autocomplete in your IDE, including enabling inline code suggestions and keyboard shortcuts for accepting, rejecting, or partially accepting completions.\"\nsidebarTitle: \"Autocomplete Quick Start\"\n---\n\n## How to Enable and Use Continue Autocomplete\n\nAutocomplete provides inline code suggestions as you type. To enable it, simply click the \"Continue\" button in the status bar at the bottom right of your IDE or ensure the \"Enable Tab Autocomplete\" option is checked in your IDE settings.\n\n## Keyboard Shortcuts for Autocomplete\n\n### Accept a Full Suggestion\n\nAccept a full suggestion by pressing `Tab`\n\n### Reject a Full Suggestion\n\nReject a full suggestion with `Esc`\n\n### Partially Accept a Suggestion\n\nFor more granular control, use `cmd/ctrl` + `‚Üí` to accept parts of the suggestion word-by-word.\n\n### Force a Suggestion (VS Code)\n\nIf you want to trigger a suggestion immediately without waiting, or if you've dismissed a suggestion and want a new one, you can force it by using the keyboard shortcut **`cmd/ctrl` + `alt` + `space`**.\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/autocomplete/next-edit.mdx","content":"---\ntitle: \"Next Edit\"\ndescription: \"Learn how Continue's Next Edit feature predicts and suggests your next code changes using AI, going beyond traditional autocomplete to anticipate entire code modifications\"\n---\n\n<Warning>\n  Next Edit is currently an experimental feature. It requires\n  [Instinct](https://hub.continue.dev/continuedev/instinct) or [Mercury Coder\n  model](https://hub.continue.dev/inception/mercury-coder) configured in\n  your Continue autocomplete settings and is not yet available for JetBrains\n  use.\n</Warning>\n\n## What is Next Edit?\n\nNext Edit is an advanced AI-powered code prediction feature that anticipates what changes you'll make next in your code. Unlike traditional autocomplete that reacts to your typing, Next Edit proactively analyzes your recent edits and coding patterns to suggest entire code modifications before you even start typing.\n\nThink of it as having an AI pair programmer that understands your coding flow and suggests the logical next step in your development process.\n\n## How Next Edit Works\n\n### The Prediction Process\n\n1. **Context Analysis**: Captures your current cursor position and recent edit history\n2. **Pattern Recognition**: Analyzes your coding patterns and the surrounding code context\n3. **Next Step Prediction**: Uses specialized AI models to predict what you'll likely change next\n4. **Visual Presentation**: Shows predictions as diff overlays rather than simple text completions\n5. **Interactive Review**: Lets you accept (Tab) or reject (Esc) the suggested changes\n6. **Jump to Next Edit Location**: Lets you jump (Tab) to the next edit location or reject (Esc)\n\n### Intelligent Triggering\n\nNext Edit activates automatically when:\n\n- You finish making a code change\n- The AI detects a logical continuation point\n- Specialized next-edit models are available\n- The current context suggests a predictable next step\n\n## Next Edit vs Traditional Autocomplete\n\n<AccordionGroup>\n  <Accordion title=\"Prediction Scope\">\n    **Autocomplete**: Completes the current line or statement you're typing\n    \n    **Next Edit**: Predicts entire code modifications across multiple lines\n  </Accordion>\n\n  <Accordion title=\"User Interaction\">\n    **Autocomplete**: Shows ghost text at your cursor position\n\n    **Next Edit**: Displays diff overlays showing before/after changes, while also displaying ghost text if the change is purely additive.\n\n   </Accordion>\n\n    <Accordion title=\"Timing\">\n     **Autocomplete**: Reactive - responds to what you're currently typing\n\n     **Next Edit**: Proactive - anticipates what you'll do next\n    </Accordion>\n\n  <Accordion title=\"Context Awareness\">\n    **Autocomplete**: Focuses on immediate code completion\n    \n    **Next Edit**: Analyzes recent edit patterns and broader code context\n  </Accordion>\n</AccordionGroup>\n\n## How to Use Next Edit\n\n### Prerequisites\n\nNext Edit requires:\n\n- Compatible AI models (Mercury Coder or Instinct)\n- VS Code (JetBrains support coming soon)\n- **API Keys**: Organizations can add API keys to [org secrets](/hub/secrets/secret-types) for team-wide access. Individual users need to provide their own API keys in their [model configuration](/customize/model-providers/overview)\n\n<Note>\n  To use Next Edit, you must have the Instinct or Mercury Coder model configured\n  in your Continue autocomplete model settings. This model is specifically\n  designed for next edit predictions. Once it's been loaded, you must reload VS\n  Code to activate it.\n  <img\n    src=\"/images/ide-extensions/autocomplete/images/instinct-autocomplete.png\"\n    alt=\"Next Edit Model Setup\"\n  />\n</Note>\n\n### Using Next Edit Predictions\n\n<Steps>\n  <Step title=\"Make a Code Change\">\n    Edit your code normally. Next Edit will analyze your change patterns.\n  </Step>\n\n<Step title=\"Review Predictions\">\n  When Next Edit activates, you'll see a diff overlay showing predicted changes\n  in an editable region.\n</Step>\n\n<Step title=\"Accept or Reject\">\n  - **Tab**: Accept the prediction and apply changes - **Esc**: Reject the\n  prediction and continue coding normally\n</Step>\n\n  <Step title=\"Continue Coding\">\n    If accepted, your cursor moves to the last changed line. If rejected, your\n    workflow continues uninterrupted.\n  </Step>\n</Steps>\n\n## What are the Model Requirements for Next Edit?\n\n### Specialized Models\n\nNext Edit requires AI models specifically trained for code prediction:\n\n- **Mercury Coder**: Primary model optimized for next edit prediction\n- **Instinct**: The leading open Next Edit model, trained by Continue\n\n### Automatic Detection\n\nContinue automatically enables Next Edit when:\n\n1. Your configured autocomplete model supports next edit capabilities\n2. You have development team access permissions\n3. The current code context suggests predictable next steps\n\n## Best Practices\n\n<CardGroup cols={2}>\n  <Card title=\"Trust the Process\" icon=\"robot\">\n    Let Next Edit observe your coding patterns for a few editing sessions before expecting highly accurate predictions.\n  </Card>\n\n<Card title=\"Review Carefully\" icon=\"magnifying-glass\">\n  Always review suggested changes before accepting, especially for complex logic\n  modifications.\n</Card>\n\n<Card title=\"Provide Feedback\" icon=\"message\">\n  Have feedback? We want to hear it. __[Add your thoughts to our feedback\n  discussions](https://github.com/continuedev/continue/discussions/categories/feedback)__\n  to help us improve.\n</Card>\n\n  <Card title=\"Combine with Other Features\" icon=\"puzzle-piece\">\n    Use Next Edit alongside Continue's Chat and Agent modes for comprehensive AI-assisted development.\n  </Card>\n</CardGroup>\n\n---\n\n_Next Edit represents Continue's vision for proactive AI coding assistance that anticipates developer needs rather than just reacting to input. As this feature evolves, it will become a powerful tool for accelerating development workflows and reducing repetitive coding tasks._\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/autocomplete/model-setup.mdx","content":"---\ntitle: \"Recommended Models for Autocomplete in Continue\"\ndescription: \"Choose the best autocomplete model for Continue, including hosted high-performance options, fast speed/quality tradeoffs, and local privacy-first models.\"\nsidebarTitle: \"Recommended Autocomplete Models\"\n---\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx';\n\nSetting up the right model for autocomplete is important for a smooth coding experience. Here are our top recommendations:\n\n## Model Recommendations\n\n<ModelRecommendations role=\"autocomplete\" />\n\n<Info>\n  For a complete comparison of all models, see our [comprehensive model recommendations](/customize/models#recommended-models).\n</Info>\n\n## Next Edit Model\n\nFor proactive code prediction that anticipates your next edit, Continue supports specialized [Next Edit](/ide-extensions/autocomplete/next-edit) models:\n\n**Supported Next Edit model:**\n\n- `mercury-coder-nextedit`: Primary model optimized for next edit prediction\n\nNext Edit automatically activates when you have a compatible model configured for autocomplete and the appropriate access permissions.\n\n## Need Help?\n\nIf you're not seeing any completions or need more detailed configuration options, check out our comprehensive [autocomplete deep dive guide](/customize/deep-dives/autocomplete).\n\n## Model Compatibility\n\nTo see a complete list of models and their capabilities, visit our [Model Capabilities guide](/customize/deep-dives/model-capabilities).\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/autocomplete/how-to-customize.mdx","content":"---\ntitle: ‚ÄúCustomize Autocomplete Settings in Continue‚Äù\ndescription: ‚ÄúLearn how to customize autocomplete behavior in Continue, including user settings, configuration options, and adjustments to improve AI code suggestions in your IDE.‚Äù\n---\n\nContinue offers a handful of settings to customize autocomplete behavior. Visit the User Settings Page (Gear Icon) to manage these settings.\n\nFor a comprehensive guide on all configuration options and their impacts, see the [Autocomplete deep dive](/customize/deep-dives/autocomplete).\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/autocomplete/how-it-works.mdx","content":"---\ntitle: \"How Autocomplete Works in Continue\"\nsidebarTitle: \"How Autocomplete Works\"\ndescription: \"Understand how Continue's autocomplete works, including timing optimization, context retrieval from your codebase, and filtering to improve AI code suggestions.\"\n---\n\n## Timing Optimization for Autocomplete\n\nIn order to display suggestions quickly, without sending too many requests, we do the following:\n\n- Debouncing: If you are typing quickly, we won't make a request on each keystroke. Instead, we wait until you have finished.\n- Caching: If your cursor is in a position that we've already generated a completion for, this completion is reused. For example, if you backspace, we'll be able to immediately show the suggestion you saw before.\n\n## Context Retrieval from Your Codebase\n\nContinue uses a number of retrieval methods to find relevant snippets from your codebase to include in the prompt.\n\n## Filtering and Post-Processing AI Suggestions\n\nLanguage models aren't perfect, but can be made much closer by adjusting their output. We do extensive post-processing on responses before displaying a suggestion, including:\n\n- Removing special tokens\n- Stopping early when regenerating code to avoid long, irrelevant output\n- Fixing indentation for proper formatting\n- Occasionally discarding low-quality responses, such as those with excessive repetition\n\nYou can learn more about how it works in the [Autocomplete deep dive](/customize/models#autocomplete).\n\n<Info>\n  **Looking for AI that predicts your next changes or additions?** Check out\n  [Next Edit](/ide-extensions/autocomplete/next-edit), an experimental feature that\n  proactively suggests code changes before you even start typing, going beyond\n  traditional autocomplete to anticipate entire code modifications.\n</Info>\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/edit/context-selection.mdx","content":"---\ntitle: \"Context Selection in Edit Mode\"\nsidebarTitle: \"Context Selection\"\ndescription: \"Learn how Continue's Edit mode selects relevant code context using file content, language server\"\n---\n\n## How to Use Text Input\n\nTyping a question or instructions into the input box is the only required context.\n\n## What Context is Included in Edit Mode\n\nThe **entire contents** of the current file are included in the prompt for context. The model will only attempt to edit the highlighted/specified ranges.\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/edit/quick-start.mdx","content":"---\ntitle: \"Quick Start with Continue Edit\"\nsideBarTitle: \"Quick Start\"\ndescription: \"Get started with Continue's Edit feature for making quick, targeted code changes directly in your file using AI suggestions, with keyboard shortcuts for accepting or rejecting modifications\"\n---\n\n## How to Continue Edit\n\nEdit is a convenient way to make quick changes to specific code and files. Select code, describe your code changes, and a diff will be streamed inline to your file which you can accept or reject.\n\nEdit is recommended for small, targeted changes, such as\n\n- Writing comments\n- Generating unit tests\n- Refactoring functions or methods\n\n## How to Activate Edit\n\nHighlight the block of code you would like to modify and press `Cmd+I` (Mac) or `Ctrl+I` (Windows/Linux) to activate Edit mode. You can also press `Cmd/Ctrl+I` with no code highlighted, which will default to inserting code at the current cursor location.\n\nOnce you've activated Edit, you're ready to provide instructions.\n\n### How to Provide Instructions\n\nDescribe the changes you would like the model to make to your highlighted code. For edits, a good prompt should be relatively short and concise. For longer, more complex tasks, we recommend using [Chat](/ide-extensions/chat/quick-start).\n\n### How to Accept or Reject Changes\n\nProposed changes appear as inline diffs within your highlighted text.\n\nYou can navigate through each proposed change, accepting or rejecting them using `Cmd+Opt+Y` (Mac) or `Ctrl+Alt+Y` (Windows/Linux) to accept, or `Cmd+Opt+N` (Mac) or `Ctrl+Alt+N` (Windows/Linux) to reject.\n\nYou can also accept or reject all changes at once using `Cmd+Shift+Enter` (Mac) or `Ctrl+Shift+Enter` (Windows/Linux) to accept, or `Cmd+Shift+Delete` (Mac) or `Ctrl+Shift+Backspace` (Windows/Linux) to reject.\n\nIf you want to request a new suggestion for the same highlighted code section, you can use `Cmd+I` (Mac) or `Ctrl+I` (Windows/Linux) to re-prompt the model.\n\n## How to Use Edit in Jetbrains\n\nIn Jetbrains, Edit is implemented as an inline popup. See the header GIF example.\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/edit/model-setup.mdx","content":"---\ntitle: \"How to Set Up Edit Models\"\nsidebarTitle: \"Recommended Edit Models\"\ndescription: \"Learn how to set up and customize models for Edit functionality in Continue.\"\n---\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx';\n\nThe model you set up for Chat mode will be used for Edit mode by default.\n\n## Recommendations\n\n<ModelRecommendations role=\"chat_edit\" />\n\n<Info>\n  See our [comprehensive model recommendations](/customize/models#recommended-models) for the best models for each role, including Edit and Apply.\n</Info>\n\n## How to Set Up an Apply Model\n\nWe also recommend setting up an Apply model for the best Edit experience.\n\nFor recommended Apply models, please refer to our [Model Recommendations page](/customize/models).\n\n## How to Determine Model Compatibility\n\nFor a complete overview of which models support various features, see our [Model Capabilities guide](/customize/deep-dives/model-capabilities).\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/edit/how-to-customize.mdx","content":"---\ntitle: \"How to Customize Edit Functionality\"\ndescription: \"Learn how to customize the Edit functionality in Continue to better suit your workflow.\"\nsidebarTitle: \"Customize Edit\"\n---\n\n## How to Set Active Edit/Apply Model\n\nYou can configure particular models to be used for Edit and Apply requests.\n\n1. Click the 3 dots above the main input\n2. Click the cube icon to expand the \"Models\" section\n3. Use the dropdowns to select models for Edit and Apply\n\n![Edit model selection](/images/edit-model.png)\n\nLearn more about the [Edit role](/customize/model-roles/edit) and [Apply role](/customize/model-roles/apply).\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/edit/how-it-works.mdx","content":"---\ntitle: \"How Edit Works\"\ndescription: \"Using the highlighted code, the contents of the current file containing your highlight, and your input instructions, we prompt the model to edit the code according to your instructions. No other additional context is provided to the model.\"\n---\n\n## How Edit Functionality Works\n\nWhen you start an edit session, Continue:\n\n1. **Gathers Context**: Uses the highlighted code and the current file contents\n2. **Prompts the Model**: Sends the gathered context and your input instructions to the model\n3. **Applies Changes**: The model response is then streamed directly back to the highlighted range in your code, where we apply a diff formatting to show the proposed changes.\n\nIf you accept the diff, we remove the previously highlighted lines, and if you reject the diff, we remove the proposed changes.\n\n<Info>\n**Looking for AI that predicts your next edit?** Check out [Next Edit](/ide-extensions/autocomplete/next-edit), an experimental feature that proactively suggests code changes before you even start typing, going beyond traditional autocomplete to anticipate entire code modifications.\n</Info>\n\nIf you would like to view the exact prompt that is sent to the model during an edit, you can [find it in the prompt logs](/troubleshooting#check-the-logs).\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/chat/context-selection.mdx","content":"---\ntitle: \"Chat Mode Context Selection\"\nsidebarTitle: \"Context Selection\"\ndescription: \"Learn how Continue selects relevant context for your chat requests, including text input, highlighted code, active files.\"\n---\n\n## How to Use Text Input\n\nTyping a question or instructions into the input box is the only required context.\n\n## How to Include Highlighted Code\n\nThe highlighted code you've selected by pressing cmd/ctrl + L (VS Code) or cmd/ctrl + J (JetBrains) will be included in your prompt.\n\n## How to Include the Active File\n\nYou can include the currently open file as context by pressing opt/alt + enter when you send your request.\n\n## How to Include a Specific File\n\nYou can include a specific file in your current workspace by typing '@Files' and selecting the file.\n\n## Codebase Search\n\nFor better codebase awareness, see our [guide on making agent mode aware of codebases and documentation](/guides/codebase-documentation-awareness).\n\n## How to Include Documentation Sites\n\nFor better documentation awareness, see our [guide on making agent mode aware of codebases and documentation](/guides/codebase-documentation-awareness).\n\n## How to Include Terminal Contents\n\nYou can include the contents of the terminal in your IDE by typing '@Terminal'.\n\n## How to Include Git Diff\n\nYou can include all of the changes you've made to your current branch by typing '@Git Diff'.\n\n## How to Use Other Context Providers\n\nYou can see a full list of built-in context providers [here](/customize/deep-dives/custom-providers).\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/chat/quick-start.mdx","content":"---\ntitle: \"Chat Mode Quick Start\"\nsidebarTitle: \"Quick Start\"\ndescription: \"Get started with Continue's AI chat assistant to solve coding problems directly in your IDE, with features for code context sharing, codebase search, and applying generated solutions to your files\"\n---\n\nChat makes it easy to ask for help from an AI without leaving your IDE. Get explanations, generate code, and iterate on solutions conversationally.\n\n## How to Use Chat - Basic Usage\n\n### How to Start a Conversation\n\nType your question or request in the chat input and press Enter.\n\n**Examples:**\n\n- \"Explain this function\"\n- \"How do I handle errors in this code?\"\n- \"Generate a test for this component\"\n\n### How to Include Code Context\n\nSelect code in your editor, then use the keyboard shortcut to include it in your chat:\n\n- VS Code\n- JetBrains\n\nPress `Cmd/Ctrl + L` to send selected code to chat\n\nPress `Cmd/Ctrl + J` to send selected code to chat\n\n### How to Use @ for Additional Context\n\nType `@` to include specific context:\n\n- `@Files` - Reference specific files\n- `@Terminal` - Include terminal output\n\n## How to Work with Responses\n\nWhen the AI provides code in its response, you'll see action buttons:\n\n- **Apply to current file** - Replace your selected code\n- **Insert at cursor** - Add code at your cursor position\n- **Copy** - Copy code to clipboard\n\n## What Are the Pro Tips for Chat\n\n### Start Fresh\n\nPress `Cmd/Ctrl + L` (VS Code) or `Cmd/Ctrl + J` (JetBrains) in an empty chat to start a new session.\n\n### Be Specific\n\nInclude details about:\n\n- What you're trying to accomplish\n- Any constraints or requirements\n- Your preferred coding style or patterns\n\n### Iterate\n\nIf the first response isn't perfect:\n\n- Ask follow-up questions\n- Request modifications\n- Provide additional context\n\n## What Are Common Use Cases for Chat\n\n### Code Explanation\n\nSelect confusing code and ask \"What does this code do?\"\n\n### Bug Fixing\n\nInclude error messages and ask \"How do I fix this error?\"\n\n### Code Generation\n\nDescribe what you want: \"Create a React component that displays a user profile\"\n\n### Refactoring\n\nSelect code and ask \"How can I make this more efficient?\"\n\n---\n\n_Chat is designed for quick interactions and iterative problem-solving. Don't hesitate to ask follow-up questions!_\n"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/chat/model-setup.mdx","content":"---\ntitle: \"Recommended Models for Chat in Continue\"\ndescription: \"Choose the best chat model for Continue, including hosted high-performance options, fast speed/quality tradeoffs, and local privacy-first models.\"\nsidebarTitle: \"Recommended Chat Models\"\n---\nimport { ModelRecommendations } from '/snippets/ModelRecommendations.jsx';\n\nThe model you use for Chat mode will be:\n\n- used with Edit mode by default but can be switched\n- always used with Agent mode if the model supports tool calling\n\n## Model Recommendations\n\n<ModelRecommendations role=\"chat_edit\" />\n\n<Info>\n  For a comprehensive comparison of all available models by role, see our [model recommendations table](/customize/models#recommended-models).\n</Info>\n\nFor model recommendations, please refer to our [Model Recommendations page](/customize/models)."}
{"source":"github","repo":"continue","path":"docs/ide-extensions/chat/how-to-customize.mdx","content":"---\ntitle: \"How to Customize Chat\"\ndescription: \"Learn how to customize the Chat feature in Continue to better suit your workflow.\"\n---\n\n## How to Customize Chat\n\nThere are a number of different ways to customize Chat:\n\n- Add [rules](/customize/rules) to give the model persistent instructions through the system prompt\n- Create [prompts](/customize/prompts) to kickoff workflows with instructions you repeat often"}
{"source":"github","repo":"continue","path":"docs/ide-extensions/chat/how-it-works.mdx","content":"---\ntitle: \"How Chat Works\"\ndescription: \"Continue's Chat feature provides a conversational interface with AI models directly in your IDE sidebar.\"\n---\n\n## How Chat Core Functionality Works\n\nWhen you start a chat conversation, Continue:\n\n1. **Gathers Context**: Uses any selected code sections and @-mentioned context\n2. **Constructs Prompt**: Combines your input with relevant context\n3. **Sends to Model**: Prompts the configured AI model for a response\n4. **Streams Response**: Returns the AI response in real-time to the sidebar\n\n## How Context Management Works\n\n### What Context Is Automatically Included\n\n- Selected code in your editor\n- Current file context when relevant\n- Previous conversation history in the session\n\n### How to Add Manual Context\n\n- `@Files` - Reference specific files\n\n## How Response Handling Works\n\nEach code section in the AI response includes action buttons:\n\n- **Apply to current file** - Replace selected code\n- **Insert at cursor** - Add code at cursor position\n- **Copy** - Copy code to clipboard\n\n## How Session Management Works\n\n- Use `Cmd/Ctrl + L` (VS Code) or `Cmd/Ctrl + J` (JetBrains) to start a new session\n- Clears all previous context for a fresh start\n- Helpful for switching between different tasks\n\n## What Advanced Features Are Available\n\n### How to Use Prompt Inspection\n\nView the exact prompt sent to the AI model in the [prompt logs](/troubleshooting) for debugging and optimization.\n\n### Context\n\nLearn more about how you can bring in context:\n\n- [Codebase Context](/guides/codebase-documentation-awareness)\n- [Documentation Context](/guides/codebase-documentation-awareness)\n- [Built-in Context Providers](/customize/deep-dives/custom-providers)\n\n---\n\n_Chat is designed to feel like a natural conversation while maintaining full transparency about what context is being used._\n"}
{"source":"github","repo":"continue","path":"docs/index.mdx","content":"---\ntitle: \"Welcome to Continue\"\nicon: book-open\ndescription: \"Create, run, and automate AI agents across Mission Control, terminal, and CI/CD\"\n---\n\n<Frame>\n  <img src=\"/images/intro-0c302b9c15b890c251b1ad04586c880f.png\" />\n</Frame>\n\n## Mission Control\n<Note>\n  Central dashboard for managing Agents, Tasks, Workflows, and Integrations.\n</Note>\n\n<CardGroup cols={2}>\n  <Card title=\"Create a Task\" icon=\"plus\" href=\"/mission-control/tasks\">\n    Trigger your Agent with a prompt, tools, and rules to complete a specific job.\n\n    <sub>‚Ä¢ Run manually or from your Inbox</sub><br />\n    <sub>‚Ä¢ Reuse and share with teammates</sub>\n  </Card>\n  <Card title=\"Set up a Workflow\" icon=\"clock\" href=\"/mission-control/workflows\">\n    Schedule recurring Tasks or connect webhooks to trigger Agents automatically.\n\n    <sub>‚Ä¢ Cron or webhook-based triggers</sub><br />\n    <sub>‚Ä¢ Ideal for daily audits or status checks</sub>\n  </Card>\n</CardGroup>\n\n## Integrations\n<Note>\n  Connect Continue to your existing developer tools to power Tasks and Workflows.\n</Note>\n\n<CardGroup cols={2}>\n  <Card title=\"GitHub\" icon=\"github\" href=\"/mission-control/integrations/github\">\n    Enable repository access for Agents to read and create PRs.\n  </Card>\n  <Card title=\"Slack\" icon=\"slack\" href=\"/mission-control/integrations/slack-agent\">\n    Mention <code>@Continue</code> in Slack to trigger Agents and receive updates.\n  </Card>\n  <Card title=\"Sentry\" icon=\"alert-triangle\" href=\"/mission-control/integrations/sentry\">\n    Automatically generate PRs to fix new issues from Sentry alerts.\n  </Card>\n  <Card title=\"Snyk\" icon=\"shield-check\" href=\"/mission-control/integrations/snyk\">\n    Detect and fix security vulnerabilities automatically.\n  </Card>\n</CardGroup>\n\n\n## Continue CLI\n\n<Note>Terminal-native AI coding assistance with TUI and headless modes.</Note>\n\n<CodeGroup>\n\n    ```bash npm\n    npm i -g @continuedev/cli\n    ```\n\n    ```bash yarn\n    yarn global add @continuedev/cli\n    ```\n\n    ```bash pnpm\n    pnpm add -g @continuedev/cli\n    ```\n\n</CodeGroup>\n\n<CardGroup cols={2}>\n  <Card\n    title=\"TUI Mode\"\n    icon=\"terminal\"\n    href=\"/cli/overview#tui-mode%3A-interactive-development\"\n  >\n    Interactive terminal interface for development workflows\n\n    <sub>‚Ä¢ Automate builds & refactoring</sub>\n    <br />\n    <sub>‚Ä¢ Pre-commit hooks & scripted fixes</sub>\n    <br />\n    <sub>‚Ä¢ Terminal-first development</sub>\n  </Card>\n  <Card\n    title=\"Headless Mode\"\n    icon=\"server\"\n    href=\"/cli/overview#headless-mode%3A-production-automation\"\n  >\n    Automated AI coding for CI/CD and server environments\n\n    <sub>‚Ä¢ Run in CI/CD pipelines</sub>\n    <br />\n    <sub>‚Ä¢ Batch processing & bulk operations</sub>\n    <br />\n    <sub>‚Ä¢ Server & container deployments</sub>\n  </Card>\n</CardGroup>\n\n## IDE Extensions\n\n<Note>\n  Optional IDE extensions for real-time code editing and assistance.\n</Note>\n\n\n<CardGroup cols={2}>\n  <Card\n    title=\"VS Code\"\n    icon=\"code\"\n    href=\"https://marketplace.visualstudio.com/items?itemName=Continue.continue\"\n  >\n    Install from VS Code Marketplace\n    <sub>Real-time coding assistance and refactoring</sub>\n  </Card>\n  <Card\n    title=\"JetBrains\"\n    icon=\"jet-fighter\"\n    href=\"https://plugins.jetbrains.com/plugin/22707-continue-extension\"\n  >\n    Install from JetBrains Plugin Repository\n    <sub>Community supported for autocomplete and multi-file edits</sub>\n  </Card>\n</CardGroup>\n\n## Resources\n\n<CardGroup cols={3}>\n  <Card title=\"Continue Mission Control\" icon=\"globe\" href=\"https://hub.continue.dev/\">\n    Create your agents, run tasks, add workflows and integrations\n  </Card>\n  <Card\n    title=\"GitHub Discussions\"\n    icon=\"comments\"\n    href=\"https://github.com/continuedev/continue/discussions\"\n  >\n    Get help from the community\n  </Card>\n  <Card\n    title=\"Integrations\"\n    icon=\"plug\"\n    href=\"/mission-control/integrations\"\n  >\n    Connect GitHub, Slack, Sentry, and Snyk to power your automations.\n  </Card>\n</CardGroup>\n"}
{"source":"github","repo":"continue","path":"docs/autocomplete/how-to-use-it.mdx","content":"---\ntitle: \"Autocomplete\"\nsidebarTitle: \"How To Use AI Autocomplete\"\nicon: \"circle-question\"\ndescription: \"Learn how to use Continue's AI-powered code autocomplete feature with keyboard shortcuts for accepting, rejecting, or partially accepting inline suggestions as you type\"\n---\n\n<Frame>\n  <img src=\"/images/autocomplete-9d4e3f7658d3e65b8e8b20f2de939675.gif\" />\n</Frame>\n\n## How to Use AI Code Autocomplete in Continue\n\nAutocomplete provides inline code suggestions as you type. To enable it, simply click the \"Continue\" button in the status bar at the bottom right of your IDE or ensure the \"Enable Tab Autocomplete\" option is checked in your IDE settings.\n\n### Accepting a full suggestion\n\nAccept a full suggestion by pressing `Tab`\n\n### Rejecting a full suggestion\n\nReject a full suggestion with `Esc`\n\n### Partially accepting a suggestion\n\nFor more granular control, use `cmd/ctrl` + `‚Üí` to accept parts of the suggestion word-by-word.\n\n### Forcing a suggestion (VS Code)\n\nIf you want to trigger a suggestion immediately without waiting, or if you've dismissed a suggestion and want a new one, you can force it by using the keyboard shortcut **`cmd/ctrl` + `alt` + `space`**.\n"}
{"source":"github","repo":"continue","path":"docs/chat/how-to-use-it.mdx","content":"---\ntitle: \"Chat\"\nsidebarTitle: \"How To Use Chat Mode\"\nicon: \"circle-question\"\ndescription: \"Learn how to use Continue's Chat mode to solve coding problems without leaving your IDE, including code context sharing, applying generated solutions, and switching between models\"\n---\n\n<Frame>\n  <img src=\"/images/chat-489b68d156be2aafe09ee7cedf233fba.gif\" />\n</Frame>\n\n## How to Use AI Chat in Continue for Coding Help\n\nChat makes it easy to ask for help from an LLM without needing to leave the IDE. You send it a task, including any relevant information, and it replies with the text / code most likely to complete the task. If it does not give you what you want, then you can send follow up messages to clarify and adjust its approach until the task is completed.\n\nChat is best used to understand and iterate on code or as a replacement for search engine queries.\n\n## Send a Coding Question or Task to AI Chat\n\nTo send a question, add it to the input box in the extention and press enter. You send it a question, and it replies with an answer. You tell it to solve a problem, and it provides you a solution. You ask for some code, and it generates it.\n\n## Add Code Context to AI Chat by Highlighting Code\n\nYou select a code section with your mouse, press `cmd/ctrl` + `L` (VS Code) or `cmd/ctrl` + `J` (JetBrains) to send it to the LLM, and then ask for it to be explained to you or request it to be refactored in some way.\n\n## Use @ to Include Project Context in AI Chat Responses\n\nIf there is information from the codebase, documentation, IDE, or other tools that you want to include as context, you can type @ to select and include it as context. You can learn more about how to use this in [Chat context selection](/ide-extensions/chat/quick-start#how-to-use--for-additional-context).\n\n## Insert AI-Generated Code Changes Directly into Your File\n\nWhen the LLM replies with edits to a file, you can click the ‚ÄúApply‚Äù button. This will update the existing code in the editor to reflect the suggested changes.\n\n## How to Begin a New AI Chat Session for a Different Coding Task\n\nOnce you complete a task and want to start a new one, press `cmd/ctrl` + `L` (VS Code) or `cmd/ctrl` + `J` (JetBrains) to begin a new session, ensuring only relevant context for the next task is provided to the LLM.\n\n## Change AI Models in Continue Chat for Different Coding Needs\n\nIf you have configured multiple models, you can switch between models using the dropdown or by pressing `cmd/ctrl` + `‚Äô`"}
{"source":"github","repo":"continue","path":"actions/README.md","content":"# Continue PR Review Actions\n\nGitHub Actions that provide automated code reviews for pull requests using Continue CLI.\n\n## Available Actions\n\nThis repository provides a GitHub Action for automated PR reviews:\n\n### General Review Action\n\nProvides high-level PR assessment with overall feedback and recommendations.\n\n- **Path:** `continuedev/continue/actions/general-review@main`\n- **Trigger:** `@continue-review`\n- **Output:** Summary comment with strengths, issues, and recommendations\n\n## Quick Start\n\n### Setting up General Review\n\n```yaml\nname: PR General Review\non:\n  pull_request:\n    types: [opened, ready_for_review]\n  issue_comment:\n    types: [created]\n\npermissions:\n  contents: read\n  pull-requests: write\n  issues: write\n\njobs:\n  review:\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    steps:\n      - uses: continuedev/continue/actions/general-review@main\n        with:\n          continue-api-key: ${{ secrets.CONTINUE_API_KEY }}\n          continue-org: \"your-org-name\"\n          continue-config: \"your-org-name/review-bot\"\n```\n\n## Inputs\n\nThe action accepts the following inputs:\n\n| Input              | Description                            | Required |\n| ------------------ | -------------------------------------- | -------- |\n| `continue-api-key` | API key for Continue service           | Yes      |\n| `continue-org`     | Organization for Continue config       | Yes      |\n| `continue-config`  | Config path (e.g., \"myorg/review-bot\") | Yes      |\n\n## Setup Requirements\n\n### 1. Continue API Key\n\nAdd your Continue API key as a secret named `CONTINUE_API_KEY` in your repository:\n\n1. Go to your repository's Settings\n2. Navigate to Secrets and variables ‚Üí Actions\n3. Click \"New repository secret\"\n4. Name: `CONTINUE_API_KEY`\n5. Value: Your Continue API key\n\n### 2. Continue Configuration\n\nSet up your review bot configuration in Continue:\n\n1. Create a configuration for your organization\n2. Configure the review bot settings\n3. Note your organization name and config path\n\n### 3. Workflow Permissions\n\nThe workflow requires these permissions:\n\n- `contents: read` - To checkout and read repository code\n- `pull-requests: write` - To post review comments on PRs\n- `issues: write` - To respond to comment triggers\n\n## Triggering Reviews\n\nThe action can be triggered in two ways:\n\n### Automatic Triggers\n\n- When a PR is opened by a team member (OWNER, MEMBER, or COLLABORATOR)\n- When a PR is marked as \"ready for review\" by a team member\n\n### Manual Triggers\n\nTeam members can trigger reviews by commenting on any pull request:\n\n- `@continue-review` - Triggers a review\n\n## Review Output\n\nThe general review provides a structured comment that includes:\n\n- **Strengths**: What was done well in the PR\n- **Issues Found**: Categorized by severity (Critical, High, Medium, Low)\n- **Suggestions**: Improvement recommendations\n- **Overall Assessment**: Final recommendation (APPROVE, REQUEST_CHANGES, or COMMENT)\n\n## How It Works\n\n1. Checks out repository code\n2. Fetches PR diff using GitHub CLI\n3. Generates a comprehensive review prompt\n4. Runs Continue CLI with specified configuration\n5. Posts review as a PR comment\n\n## Versioning\n\nWe recommend using the main branch:\n\n- `@main` - Uses the latest code from the main branch\n\nExample:\n\n```yaml\nuses: continuedev/continue/actions/general-review@main\n```\n\n## Troubleshooting\n\n### Review not triggering\n\n- Ensure the PR author or commenter has appropriate permissions (OWNER, MEMBER, or COLLABORATOR)\n- Check that the workflow file is in the default branch\n- Verify the Continue API key is correctly set as a repository secret\n\n### No review output generated\n\n- Check the action logs for any errors\n- Verify your Continue configuration is correct\n- Ensure your Continue API key is valid\n\n## Support\n\nFor issues or questions:\n\n- [Continue Documentation](https://docs.continue.dev)\n- [GitHub Issues](https://github.com/continuedev/continue/issues)\n- [Discord Community](https://discord.gg/vapESyrFmJ)\n"}
{"source":"github","repo":"continue","path":"extensions/cli/CHANGELOG.md","content":"## [1.4.2](https://github.com/continuedev/cli/compare/v1.4.1...v1.4.2) (2025-07-17)\n\n### Bug Fixes\n\n- remove warning ([31b3f67](https://github.com/continuedev/cli/commit/31b3f6707b0f42180c8ae56faf9dfb2f566be5c1))\n\n## [1.4.1](https://github.com/continuedev/cli/compare/v1.4.0...v1.4.1) (2025-07-17)\n\n### Bug Fixes\n\n- update readme ([0d45e14](https://github.com/continuedev/cli/commit/0d45e14b1be6557ace027cfeadfbf418061a28b0))\n\n# [1.4.0](https://github.com/continuedev/cli/compare/v1.3.6...v1.4.0) (2025-07-17)\n\n### Features\n\n- dim user msg text color ([b22429b](https://github.com/continuedev/cli/commit/b22429ba0a101f99b14313345a8a8894caff3081))\n\n## [1.3.6](https://github.com/continuedev/cli/compare/v1.3.5...v1.3.6) (2025-07-16)\n\n### Bug Fixes\n\n- update readme ([01329e4](https://github.com/continuedev/cli/commit/01329e4b4f66d97c93c98e0342f100792f626126))\n\n## [1.3.5](https://github.com/continuedev/cli/compare/v1.3.4...v1.3.5) (2025-07-16)\n\n### Bug Fixes\n\n- package.json issues page ([91e1e2e](https://github.com/continuedev/cli/commit/91e1e2e004e86d6fb1df3bbfbbf94f6d46a4adb7))\n- update review.yaml config ([b49ab0d](https://github.com/continuedev/cli/commit/b49ab0d9e54edf8374b113af4752e1c6fba15738))\n\n## [1.3.4](https://github.com/continuedev/cli/compare/v1.3.3...v1.3.4) (2025-07-16)\n\n### Bug Fixes\n\n- add /clear command to clear chat history ([3782732](https://github.com/continuedev/cli/commit/378273239b32fce735266cb4835512f5c547a86d))\n\n## [1.3.3](https://github.com/continuedev/cli/compare/v1.3.2...v1.3.3) (2025-07-16)\n\n### Bug Fixes\n\n- update chat history state after streaming and save complete conversation ([936786a](https://github.com/continuedev/cli/commit/936786aee08fcbdde2484896730a0c67f1ebafee))\n\n## [1.3.2](https://github.com/continuedev/cli/compare/v1.3.1...v1.3.2) (2025-07-16)\n\n### Bug Fixes\n\n- remove .js extensions from highlight.js language imports ([6426777](https://github.com/continuedev/cli/commit/6426777d659a2747681d22d4c78574d6e0686ba6))\n\n## [1.3.1](https://github.com/continuedev/cli/compare/v1.3.0...v1.3.1) (2025-07-16)\n\n### Bug Fixes\n\n- type err ([f5f1bca](https://github.com/continuedev/cli/commit/f5f1bca7c6a9f2c4f2842605847d55b765fd3499))\n\n# [1.3.0](https://github.com/continuedev/cli/compare/v1.2.0...v1.3.0) (2025-07-15)\n\n### Features\n\n- add session management with --resume flag and cn alias ([0096fbc](https://github.com/continuedev/cli/commit/0096fbcc4934d7264d9a7a134908e016e037df1a))\n\n# [1.2.0](https://github.com/continuedev/cli/compare/v1.1.0...v1.2.0) (2025-07-14)\n\n### Bug Fixes\n\n- bg color for diff ([63c9ecf](https://github.com/continuedev/cli/commit/63c9ecf24fcf74f43d07244c963cef04f91baee8))\n- display diff ([fde6098](https://github.com/continuedev/cli/commit/fde6098ef2c4d99a9b9b8d6ffc1299436e0f52c1))\n- improve streaming message handling and display buffering ([d1372b3](https://github.com/continuedev/cli/commit/d1372b3aa2f79221957468fd1decbae2258becc0))\n- remove part of the system message ([daae972](https://github.com/continuedev/cli/commit/daae97242f1fca92f42f8c979df8d9c3f912f69d))\n- truncate diff at 16 lines ([cbb77e3](https://github.com/continuedev/cli/commit/cbb77e35ff19809c1c05760fbdf7c995d85dc72c))\n\n### Features\n\n- add /clear command to clear chat history ([c9c000c](https://github.com/continuedev/cli/commit/c9c000cfab6aa6efe156126f1eafc99349466413))\n- convert absolute paths to relative paths in tool display ([2c722c4](https://github.com/continuedev/cli/commit/2c722c485eea4c3c283fb643193a4b1ffe14bf7f))\n\n# [1.1.0](https://github.com/continuedev/cli/compare/v1.0.1...v1.1.0) (2025-07-14)\n\n### Bug Fixes\n\n- improve streaming message handling and API endpoint ([b822b93](https://github.com/continuedev/cli/commit/b822b938f110af2de2c040f45cd3fca770d9a4a7))\n- use ignore-walk for sys msg ([eb59dcc](https://github.com/continuedev/cli/commit/eb59dcc76a5bf16d4da2e4f2c7dc87765e8382b3))\n\n### Features\n\n- base system message ([25a67cf](https://github.com/continuedev/cli/commit/25a67cf947f786d4cdc36c60e36e303a4e466c87))\n\n## [1.0.1](https://github.com/continuedev/cli/compare/v1.0.0...v1.0.1) (2025-07-14)\n\n### Bug Fixes\n\n- release index.js ([091cfa7](https://github.com/continuedev/cli/commit/091cfa7b64c1591f371e444e602d04fdcbca48d4))\n\n# 1.0.0 (2025-07-14)\n\n### Bug Fixes\n\n- adjust placeholder ([94e6f12](https://github.com/continuedev/cli/commit/94e6f12924ff5ee429a43c53d1810d5519f026bb))\n- adjust waiting message ([454df15](https://github.com/continuedev/cli/commit/454df1554500a530645ef5e7a6a80fea6a27f207))\n- allow multiple consecutive new lines ([423dbdd](https://github.com/continuedev/cli/commit/423dbdd457683260f66eda9024f35dee9c49d5db))\n- backspace on mac ([aa5606f](https://github.com/continuedev/cli/commit/aa5606f65f3284c9ff4d0a02cb1e4611b5cd080c))\n- cursor position ([388eeeb](https://github.com/continuedev/cli/commit/388eeeb37afa7625b954e4dd44cbcd7cf0ab22bb))\n- delete hallucinated tools ([2d4cf55](https://github.com/continuedev/cli/commit/2d4cf55c393121570ff2c4459c6942e124bc88dd))\n- dot shade of green ([8f6ec39](https://github.com/continuedev/cli/commit/8f6ec398ff23ec1d59567cb5b2de92dabd58ab4b))\n- empty assistant message ([5e773f5](https://github.com/continuedev/cli/commit/5e773f52684dc8b267fa33a94b0b707d292a6111))\n- even nicer tool UI ([3716ebc](https://github.com/continuedev/cli/commit/3716ebcdd73a23e021e59a9d162d61915ce31184))\n- more slashcommand UI ([daa4538](https://github.com/continuedev/cli/commit/daa45387725f6b0b9b247ebde63f1f3018052bc2))\n- nicer tool display ([6749e10](https://github.com/continuedev/cli/commit/6749e10daad3fc9238dce6bf5c7309dd610cad2a))\n- option + left/rght ([efa2c43](https://github.com/continuedev/cli/commit/efa2c434d0a5942e9aaf5171c2d1f03c5df3ca51))\n- refactor display name for tools ([837ac2d](https://github.com/continuedev/cli/commit/837ac2d5c33ad0ef6482f8216dd7b3a8ec852fe3))\n- round corners ([a08e217](https://github.com/continuedev/cli/commit/a08e2176a70f5a216f6e03657e9f6522eade99b8))\n- shift+enter for newline ([fa30094](https://github.com/continuedev/cli/commit/fa30094e61dbb2b11f7940b875fc719d06ed3eec))\n- slash command prefix matching ([0a40201](https://github.com/continuedev/cli/commit/0a402010d9ae98a6913b7ceb68b97dfd9772754e))\n- slash command tab ([ee40d8b](https://github.com/continuedev/cli/commit/ee40d8bbb48d502055cad1e811e973598b0bf9bd))\n- stream above input box ([c8945ad](https://github.com/continuedev/cli/commit/c8945adb85c99379039afa02ac4d63f114a6de6b))\n- tool call custom UI ([fa4b98f](https://github.com/continuedev/cli/commit/fa4b98f455db6fe48b4c5810327d0222fade7902))\n- tool name matching ([9792b14](https://github.com/continuedev/cli/commit/9792b1411f713c2615c0d3b82df2c7032d434205))\n- tool ui again ([9c18d28](https://github.com/continuedev/cli/commit/9c18d28cbefed79f316a0b9d219f7ec585455eca))\n- tool UI again ([9d4ea20](https://github.com/continuedev/cli/commit/9d4ea20deb278a7c8d427647b5c1d5cf4d333352))\n- tui no longer completely broken ([8c0fd5e](https://github.com/continuedev/cli/commit/8c0fd5e87e7e8ce59b4b022669d4aba2ba1b8bcf))\n\n### Features\n\n- basic ink tui ([ed536d3](https://github.com/continuedev/cli/commit/ed536d3505167cd15e46ec4dee49847eb87b63a8))\n- better slash command UI ([c62cbdc](https://github.com/continuedev/cli/commit/c62cbdc21e6e93788d8e01c0a97c17ef0c036261))\n- disable PR description ([a5722ba](https://github.com/continuedev/cli/commit/a5722bab8bd7fb925806a767e9f76a4165f5f6b3))\n- esc to interrupt ([5643a53](https://github.com/continuedev/cli/commit/5643a53b5a3440d1e3e2ee41df08524289ad94b2))\n- integrate SDK into CLI ([#1](https://github.com/continuedev/cli/issues/1)) ([597e35b](https://github.com/continuedev/cli/commit/597e35b5be0ab1d1f62b6d9e2ac71e168d06e39c))\n- more spinners ([dccb311](https://github.com/continuedev/cli/commit/dccb31106a43f2c295da3f8a4225a44969cb3ef7))\n- new TextBuffer class for improved keystroke handling ([e7ab301](https://github.com/continuedev/cli/commit/e7ab301dbe3b7e27bd6bb15992f3cf1a3f9694d7))\n- paste ([65f10a1](https://github.com/continuedev/cli/commit/65f10a1a4f2862bf1166d187e49fc732a503f9da))\n- slash command UI and ASCII spinner ([7eefce2](https://github.com/continuedev/cli/commit/7eefce2de39fca9ab51b2d2fb1d0b24286c09404))\n- suppress `console.info` in headless ([8177eef](https://github.com/continuedev/cli/commit/8177eef6a00a06f5c80d4b0ee4c84f00fd156cd5))\n- TUI mode as default ([e73098b](https://github.com/continuedev/cli/commit/e73098b82ed5ea73f6d8d33eab24fdf6c67bb8ec))\n"}
{"source":"github","repo":"continue","path":"extensions/cli/src/test-helpers/README.md","content":"# Test Helpers\n\nThis directory contains helper functions for testing the Continue CLI.\n\n## Mock LLM Server\n\nThe `mock-llm-server.ts` module provides utilities for creating mock LLM servers in tests. This allows you to test the CLI's behavior without making real API calls.\n\n### Basic Usage\n\n```typescript\nimport {\n  setupMockLLMTest,\n  cleanupMockLLMServer,\n} from \"../test-helpers/mock-llm-server.js\";\n\ndescribe(\"My Test\", () => {\n  let context: any;\n  let mockServer: MockLLMServer;\n\n  beforeEach(async () => {\n    context = await createTestContext();\n    mockServer = await setupMockLLMTest(context, {\n      response: \"Hello from mock LLM!\",\n    });\n  });\n\n  afterEach(async () => {\n    await cleanupMockLLMServer(mockServer);\n    await cleanupTestContext(context);\n  });\n\n  it(\"should get response from mock\", async () => {\n    const result = await runCLI(context, {\n      args: [\"-p\", \"Test prompt\", \"--config\", context.configPath],\n    });\n    expect(result.stdout).toContain(\"Hello from mock LLM!\");\n  });\n});\n```\n\n### Advanced Usage\n\n#### Dynamic Responses\n\n```typescript\nmockServer = await setupMockLLMTest(context, {\n  response: (prompt: string) => {\n    if (prompt.includes(\"weather\")) {\n      return \"It's sunny!\";\n    }\n    return \"Default response\";\n  },\n});\n```\n\n#### Non-Streaming Responses\n\n```typescript\nmockServer = await setupMockLLMTest(context, {\n  response: \"Non-streaming response\",\n  streaming: false, // Sends entire response at once instead of word-by-word\n});\n```\n\nNote: The CLI always expects SSE format, so responses are always sent as streaming events. The `streaming` option only controls whether the response is chunked word-by-word or sent all at once.\n\n#### Custom Request Handling\n\n```typescript\nconst mockServer = await createMockLLMServer({\n  customHandler: (req, res) => {\n    // Custom logic here\n    res.writeHead(200);\n    res.end(\"Custom response\");\n  },\n});\n```\n\n#### Request Tracking\n\nThe mock server tracks all requests it receives:\n\n```typescript\n// After making requests...\nexpect(mockServer.requests).toHaveLength(1);\nexpect(mockServer.requests[0].body.messages[0].content).toBe(\"User prompt\");\n```\n\n### API Reference\n\n#### `setupMockLLMTest(context, options)`\n\nSets up a complete test environment with mock LLM server, config file, and skips onboarding.\n\n#### `createMockLLMServer(options)`\n\nCreates just the mock server without test environment setup.\n\n#### `createMockLLMConfig(mockServer, modelName)`\n\nGenerates a config YAML string for the mock server.\n\n#### `cleanupMockLLMServer(mockServer)`\n\nProperly shuts down the mock server.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/src/e2e/spec.md","content":"# End-to-End Testing for Continue CLI\n\nThis document describes the principles, practices, and patterns for writing end-to-end (E2E) tests for the Continue CLI.\n\n## Overview\n\nEnd-to-end tests verify that the CLI functions correctly as a whole by executing the actual CLI binary and validating its behavior from the user's perspective. These tests help ensure that all components work together properly in a real-world environment.\n\n## Key Principles\n\n1. **Isolation**: Each test runs in a completely isolated environment with its own temporary directory\n2. **Independence**: Tests should not depend on each other and can be run in any order\n3. **Realistic Usage**: Tests should simulate how users actually interact with the CLI\n4. **Minimal Mocking**: Mock only external dependencies (like LLM APIs) when necessary\n5. **Thorough Cleanup**: Tests should clean up after themselves to avoid leaking resources\n\n## Test Structure\n\nA typical E2E test follows this structure:\n\n```typescript\ndescribe(\"E2E: Feature\", () => {\n  let context: any;\n\n  beforeEach(async () => {\n    context = await createTestContext();\n  });\n\n  afterEach(async () => {\n    await cleanupTestContext(context);\n  });\n\n  it(\"should do something\", async () => {\n    const result = await runCLI(context, {\n      args: [\"command\", \"--flag\"],\n    });\n\n    expect(result.exitCode).toBe(0);\n    expect(result.stdout).toContain(\"Expected output\");\n  });\n});\n```\n\n## Test Context\n\nThe test context (`createTestContext()`) provides:\n\n- Temporary test directory (`context.testDir`)\n- Path to the CLI binary (`context.cliPath`)\n- Optional paths for config and session files\n\n## Core Testing Utilities\n\n### Basic CLI Execution\n\n```typescript\n// Run CLI with arguments\nconst result = await runCLI(context, {\n  args: [\"--help\"],\n});\n\n// Verify output and exit code\nexpect(result.exitCode).toBe(0);\nexpect(result.stdout).toContain(\"Continue CLI\");\n```\n\n### Testing with Input\n\n```typescript\n// Run CLI with input\nconst result = await runCLI(context, {\n  args: [\"-p\", \"Prompt with input\"],\n  input: \"User input\\n\",\n});\n```\n\n### Interactive Testing\n\n```typescript\n// For complex interactive flows\nconst result = await withInteractiveInput(\n  context,\n  [\"chat\"],\n  [\"First input\", \"Second input\"],\n);\n```\n\n### Testing Error Cases\n\n```typescript\n// Test error scenarios\nconst result = await runCLI(context, {\n  args: [\"--invalid-flag\"],\n  expectError: true,\n});\n\nexpect(result.exitCode).not.toBe(0);\nexpect(result.stderr).toContain(\"error\");\n```\n\n## Testing with Mock LLM\n\nFor tests involving AI models, we use a mock LLM server:\n\n```typescript\nlet mockServer: MockLLMServer;\n\nbeforeEach(async () => {\n  context = await createTestContext();\n  mockServer = await setupMockLLMTest(context, {\n    response: \"Hello World!\",\n  });\n});\n\nafterEach(async () => {\n  await cleanupMockLLMServer(mockServer);\n  await cleanupTestContext(context);\n});\n\nit(\"should get response from mock LLM\", async () => {\n  const result = await runCLI(context, {\n    args: [\"-p\", \"--config\", context.configPath, \"Hi\"],\n  });\n\n  expect(result.stdout).toContain(\"Hello World!\");\n});\n```\n\n### Advanced Mock LLM Features\n\n- **Dynamic responses** based on prompts:\n\n  ```typescript\n  mockServer = await setupMockLLMTest(context, {\n    response: (prompt) =>\n      prompt.includes(\"weather\") ? \"It's sunny!\" : \"I don't know\",\n  });\n  ```\n\n- **Non-streaming responses**:\n\n  ```typescript\n  mockServer = await setupMockLLMTest(context, {\n    response: \"All at once response\",\n    streaming: false,\n  });\n  ```\n\n- **Request tracking**:\n  ```typescript\n  expect(mockServer.requests).toHaveLength(1);\n  expect(mockServer.requests[0].body.messages[0].content).toBe(\"User prompt\");\n  ```\n\n## Testing Sessions and Configuration\n\n### Creating Test Configuration\n\n```typescript\nconst configPath = await createTestConfig(context, {\n  // Config object\n});\n```\n\n### Creating and Reading Sessions\n\n```typescript\n// Create a mock session\nawait createMockSession(context, [\n  { role: \"user\", content: \"Hello\" },\n  { role: \"assistant\", content: \"Hi there!\" },\n]);\n\n// Read the session\nconst session = await readSession(context);\n```\n\n## Best Practices\n\n1. **Use descriptive test names** that explain what functionality is being tested\n2. **Keep tests focused** on specific functionality or user flows\n3. **Set appropriate timeouts** for tests that may take longer (especially with mock LLM)\n4. **Validate both happy path and error cases**\n5. **Test CLI flags and commands** thoroughly\n6. **Clean up resources** even if tests fail (use try/finally if needed)\n7. **Use realistic inputs** that match how users would interact with the CLI\n\n## Testing Specific Features\n\n### Testing Headless Mode\n\n```typescript\nconst result = await runCLI(context, {\n  args: [\"-p\", \"Prompt in headless mode\"],\n});\n```\n\n### Testing Interactive Mode\n\n```typescript\nconst result = await withInteractiveInput(\n  context,\n  [\"chat\"],\n  [\"User input\", \"/exit\"],\n);\n```\n\n### Testing Subcommands\n\n```typescript\nconst result = await runCLI(context, {\n  args: [\"login\", \"--help\"],\n});\n```\n\n## Debugging Tests\n\n- Set longer timeouts for complex tests: `}, 30000);` (30 seconds)\n- Examine the full output: `console.log(result.stdout, result.stderr)`\n- Check the test directory: `console.log(context.testDir)`\n- For intermittent failures, try running the test in isolation\n"}
{"source":"github","repo":"continue","path":"extensions/cli/src/ui/UI_TEST_CHECKLIST.md","content":"# TUIChat UI Test Checklist\n\n## ‚úÖ Simple Tests (Priority 1)\n\n### Message Display Tests\n\n- [ ] Empty chat displays correctly (no messages)\n- [ ] Single user message displays with correct formatting (‚óè indicator)\n- [ ] Single assistant message displays with correct formatting (‚óè indicator)\n- [ ] Multiple messages display in correct order\n- [ ] System messages display with correct styling (gray, italic)\n\n### User Input Tests\n\n- [ ] Input field shows typed text\n- [ ] Input field clears after pressing Enter\n- [ ] Input field handles special characters\n- [ ] Input prompt shows correctly (\"cn>\" or custom)\n\n### Loading State Tests\n\n- [ ] Loading spinner appears when `isLoading` is true\n- [ ] Loading spinner disappears when `isLoading` is false\n- [ ] Loading text displays correctly\n\n## üìù Medium Complexity Tests (Priority 2)\n\n### Tool Execution Display Tests\n\n- [ ] Tool start indicator (‚óã) displays correctly\n- [ ] Tool name displays next to indicator\n- [ ] Tool arguments display correctly\n- [ ] Tool success indicator (‚úì) displays correctly\n- [ ] Tool error indicator (‚úó) displays correctly\n- [ ] Tool result summary displays collapsed by default\n- [ ] Tool result expands when selected\n\n### Markdown Rendering Tests\n\n- [ ] Bold text renders correctly\n- [ ] Italic text renders correctly\n- [ ] Code blocks render with syntax highlighting\n- [ ] Inline code renders with correct styling\n- [ ] Headers render as bold\n- [ ] Lists render correctly\n\n### Component Visibility Tests\n\n- [ ] Config selector shows when multiple configs available\n- [ ] Organization selector shows when multiple orgs available\n- [ ] Scroll indicators appear for long content\n- [ ] Box borders render correctly\n\n## üîß Complex Tests (Priority 3)\n\n### State Transition Tests\n\n- [ ] Message transitions from streaming to complete\n- [ ] Tool execution transitions through states (start ‚Üí result)\n- [ ] Error states display and recover correctly\n- [ ] Focus changes between input and selectors\n\n### Scrolling and Navigation Tests\n\n- [ ] Chat scrolls to bottom on new messages\n- [ ] PageUp/PageDown scroll through history\n- [ ] Home/End jump to top/bottom\n- [ ] Scroll position maintains during updates\n\n### Error Handling Tests\n\n- [ ] API errors display user-friendly messages\n- [ ] Tool errors display correctly\n- [ ] Network errors handle gracefully\n- [ ] Invalid input errors display\n\n### Integration Tests\n\n- [ ] Full conversation flow works correctly\n- [ ] Tool execution integrates with message display\n- [ ] Config switching updates UI correctly\n- [ ] Session resume displays previous messages\n\n## üéØ Test Implementation Strategy\n\n1. **Start with the simplest tests** - Message display and input handling\n2. **Use minimal mocking** - Only mock what's necessary (API calls)\n3. **Test user-visible behavior** - Not implementation details\n4. **Keep tests focused** - One behavior per test\n5. **Use descriptive test names** - Should explain what's being tested\n"}
{"source":"github","repo":"continue","path":"extensions/cli/src/ui/__tests__/README.md","content":"# TUI Testing Framework\n\nThis directory contains a comprehensive testing framework for the Continue CLI's Terminal User Interface (TUI) that supports running tests in both normal (local) and remote modes.\n\n## Overview\n\nThe testing framework ensures that the TUI behaves consistently whether running locally or connected to a remote `cn serve` instance. Most tests run in both modes automatically, verifying feature parity between local and remote operation.\n\n## Key Components\n\n### 1. Mock Remote Server (`mockRemoteServer.ts`)\n\nA mock Express server that simulates the `cn serve` endpoints:\n\n- `GET /state` - Returns current chat state\n- `POST /message` - Receives user messages and can simulate responses\n\n### 2. Test Helper (`TUIChat.testHelper.ts`)\n\nProvides utilities for writing tests that work in both modes:\n\n- `runTest()` - Run a single test in specified mode(s)\n- `runTestSuite()` - Run a test suite in specified mode(s)\n- Helper functions for common operations\n\n### 3. Test Files\n\n- `TUIChat.basic.test.tsx` - Basic UI and layout tests\n- `TUIChat.messages.test.tsx` - Message handling tests\n- `TUIChat.remote.test.tsx` - Remote-specific behavior tests\n- Additional test files for other features...\n\n## Writing Tests\n\n### Basic Test (runs in both modes)\n\n```typescript\nimport { runTest } from \"./TUIChat.testHelper.js\";\n\nrunTest(\"my test name\", async (ctx) => {\n  const { renderResult, mode, server } = ctx;\n\n  // Your test code here\n  const frame = renderResult.lastFrame();\n  expect(frame).toContain(\"Expected content\");\n\n  // Mode-specific logic if needed\n  if (mode === \"remote\" && server) {\n    // Remote-specific assertions\n  }\n});\n```\n\n### Test Suite (runs all tests in both modes)\n\n```typescript\nimport { runTestSuite } from \"./TUIChat.testHelper.js\";\n\nrunTestSuite(\"My Test Suite\", () => {\n  runTest(\"test 1\", async (ctx) => {\n    // Test code\n  });\n\n  runTest(\"test 2\", async (ctx) => {\n    // Test code\n  });\n});\n```\n\n### Mode-Specific Tests\n\n```typescript\n// Remote-only test\nrunTest(\n  \"remote-specific behavior\",\n  async (ctx) => {\n    // This only runs in remote mode\n  },\n  { mode: \"remote\" },\n);\n\n// Normal-only test\nrunTest(\n  \"local-specific behavior\",\n  async (ctx) => {\n    // This only runs in normal mode\n  },\n  { mode: \"normal\" },\n);\n```\n\n## Helper Functions\n\n### `sendMessage(ctx, message, waitTime?)`\n\nSends a message and waits for it to be processed:\n\n```typescript\nawait sendMessage(ctx, \"Hello world\");\n```\n\n### `waitForServerState(server, predicate, timeout?)`\n\nWaits for server state to match a condition (remote mode only):\n\n```typescript\nawait waitForServerState(server, (state) => state.messages.length > 0, 5000);\n```\n\n### `expectRemoteMode(frame)` / `expectNormalMode(frame)`\n\nVerify mode-specific UI indicators:\n\n```typescript\nif (mode === \"remote\") {\n  expectRemoteMode(frame);\n} else {\n  expectNormalMode(frame);\n}\n```\n\n## Server Setup\n\nFor tests that need specific server behavior:\n\n```typescript\nrunTest(\n  \"test with custom server\",\n  async (ctx) => {\n    // Test code\n  },\n  {\n    serverSetup: (server) => {\n      server.onMessage((msg) => {\n        // Custom response logic\n        server.simulateResponse(`Echo: ${msg}`);\n      });\n    },\n  },\n);\n```\n\n## Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run specific test file\nnpm test TUIChat.basic.test.tsx\n\n# Run with coverage\nnpm test -- --coverage\n```\n\n## Best Practices\n\n1. **Write mode-agnostic tests by default** - Most functionality should work the same in both modes\n2. **Use helper functions** - They handle mode-specific details automatically\n3. **Test server state in remote mode** - Verify both UI and server state for comprehensive coverage\n4. **Handle async operations properly** - Use appropriate wait times and state checks\n5. **Clean up resources** - The framework handles server lifecycle automatically\n\n## Debugging\n\n- Tests output the current mode in brackets: `[NORMAL MODE]` or `[REMOTE MODE]`\n- Server state can be inspected with `server.getState()`\n- Use `console.log(renderResult.lastFrame())` to see the current UI state\n- Mock server runs on a random port to avoid conflicts\n"}
{"source":"github","repo":"continue","path":"extensions/cli/src/commands/devbox-entrypoint.md","content":"# Devbox entrypoint behavior (cn serve)\n\nContext: runloop resumes a devbox by re-running the same entrypoint script, which invokes `cn serve --id <agentId> ...`. Because the entrypoint always replays, the CLI must avoid duplicating state on restart.\n\n- **Session reuse:** `serve` now calls `loadOrCreateSessionById` when `--id` is provided so the same session file is reused instead of generating a new UUID. This keeps chat history intact across suspend/resume.\n- **Skip replaying the initial prompt:** `shouldQueueInitialPrompt` checks existing history and only queues the initial prompt when there are no non-system messages. This prevents the first prompt from being resent when a suspended devbox restarts.\n- **Environment persistence:** The devbox entrypoint (control-plane) writes all env vars to `~/.continue/devbox-env` and sources it before `cn serve`, so keys survive suspend/resume. The CLI assumes env is already present.\n\nOperational notes:\n\n- Changing the entrypoint is expensive; prefer adapting CLI/session behavior as above.\n- When testing suspend/resume, confirm a single session file under `~/.continue/sessions` for the agent id and that follow-up messages append normally without replaying the first prompt.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/src/permissions/README.md","content":"# Tool Permissions System\n\nThe tool permissions system allows you to control which tools the AI can use and how it can use them. There are three permission levels:\n\n- **allow**: The tool will be executed automatically without asking for permission\n- **ask**: The tool will prompt the user for permission before execution\n- **exclude**: The tool will be filtered out entirely and won't be available to the AI\n\n## Default Policies\n\nThe system comes with sensible default policies:\n\n- Read-only tools (`readFile`, `listFiles`, `searchCode`, `fetch`) are **allowed** by default\n- Write operations (`writeFile`) require **confirmation** (ask)\n- Terminal commands (`runTerminalCommand`) require **confirmation** (ask)\n- MCP tools with IDE prefix are **allowed** by default\n- Other MCP tools require **confirmation** (ask)\n- Any unmatched tools default to **ask**\n\n## How It Works\n\n### 1. Tool Filtering (Exclude Policy)\n\nTools with \"exclude\" permission are filtered out before being sent to the AI model. This means the AI won't even know these tools exist.\n\n### 2. Permission Checking (Ask Policy)\n\nWhen the AI tries to use a tool with \"ask\" permission, the system will:\n\n1. Display a permission request in the UI\n2. Wait for user approval (y/n)\n3. Execute the tool if approved, or return an error if denied\n\n### 3. Automatic Execution (Allow Policy)\n\nTools with \"allow\" permission are executed immediately without user intervention.\n\n## Architecture\n\n### Core Components\n\n- **`types.ts`**: Defines the permission policy types and interfaces\n- **`defaultPolicies.ts`**: Contains the hardcoded default permission policies\n- **`permissionChecker.ts`**: Implements the permission checking logic\n- **`permissionManager.ts`**: Manages permission requests and user responses\n- **UI Components**: Handle displaying permission requests and collecting user input\n\n### Data Flow\n\n1. **Tool Loading**: `getAllowedTools()` in `streamChatResponse.ts` filters out excluded tools\n2. **Tool Execution**: Before executing each tool call, permissions are checked\n3. **User Interaction**: For \"ask\" policies, UI displays permission request\n4. **Execution**: Tool is executed or denied based on permission result\n\n## UI Integration\n\nThe permission system integrates with the existing chat UI:\n\n- Permission requests appear as special message types\n- Users can approve/deny with y/n keys\n- The decision is shown in the chat history\n- The system handles the async nature of permission requests\n\n## Future Enhancements\n\nThe system is designed to be extensible for future features like:\n\n- Custom permission policies loaded from configuration\n- \"Remember this decision\" functionality\n- Argument-based permission matching (partially implemented)\n- Per-session or per-project permission overrides\n\n## Example Usage\n\n```typescript\n// Check permission for a tool call\nconst result = checkToolPermission({\n  name: \"writeFile\",\n  arguments: { path: \"/important.txt\", content: \"data\" },\n});\n\nif (result.permission === \"ask\") {\n  // Request user permission\n}\n```\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/telemtry.md","content":"# Continue anonymous Posthog telemetry\n\n## Behavior\n\n- Used by Continue for product metrics (not used by customers)\n- uses public posthog key in repo\n- `CONTINUE_TELEMETRY_ENABLED=0` disables telemetry\n- non-anonymous and private data like code is never sent to posthog\n- Event user ids are the Continue user id is signed in, or a unique machine id if not\n- Current events are slash command usage and chat calls\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/onboarding.md","content":"# Onboarding\n\nWhen a user first runs `cn` in interactive mode, they will be taken through \"onboarding\". After they have completed onboarding once, they will follow a normal config loading flow.\n\n## Onboarding flow\n\n**The onboarding flow runs when the user hasn't completed onboarding before, regardless of whether they have a valid config.yaml file.**\n\n1. If the --config flag is provided, load this config\n2. If the CONTINUE_USE_BEDROCK environment variable is set to \"1\", automatically use AWS Bedrock configuration and skip interactive prompts\n3. Present the user with available options:\n\n   - Log in with Continue: log them in, which will automatically create their assistant and then we can load the first assistant from the first org\n   - Enter your Anthropic API key: let them enter the key, and then either create a ~/.continue/config.yaml with the following contents OR update the existing config.yaml to add the model\n\n   ```yaml\n   name: Local Config\n   version: 1.0.0\n   schema: v1\n\n   models:\n     - uses: anthropic/claude-4-sonnet\n       with:\n         ANTHROPIC_API_KEY: <THEIR_ANTHROPIC_API_KEY>\n   ```\n\n   When CONTINUE_USE_BEDROCK=1 is detected, it will use AWS Bedrock configuration. The user must have AWS credentials configured through the standard AWS credential chain (AWS CLI, environment variables, IAM roles, etc.).\n\nWhen something in the onboarding flow is done automatically, we should tell the user what happened. For example, when CONTINUE_USE_BEDROCK=1 is detected, the CLI displays: \"‚úì Using AWS Bedrock (CONTINUE_USE_BEDROCK detected)\"\n\n### AWS Bedrock Environment Variable\n\nUsers can bypass the interactive onboarding menu by setting the `CONTINUE_USE_BEDROCK` environment variable to \"1\":\n\n```bash\nexport CONTINUE_USE_BEDROCK=1\ncn <command>\n```\n\nThis will:\n\n- Skip the interactive onboarding prompts\n- Automatically configure the CLI to use AWS Bedrock\n- Require that AWS credentials are already configured through the standard AWS credential chain\n- Display a confirmation message to the user\n- Mark onboarding as completed\n\n## Normal flow\n\n**The normal flow runs when the user has already completed onboarding.**\n\n1. If the --config flag is provided, load this config\n2. If the user is logged in, look for the first assistant in the selected org\n3. If there are no assistants in that org, then look for a local ~/.continue/config.yaml\n4. If there is no config.yaml, look for an ANTHROPIC_API_KEY in the environment and manually construct the config object to include just the claude-4-sonnet model with that API key\n5. If none of the above, then bring the user to step 3 of the onboarding flow\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/testing-strategies.md","content":"# Testing Strategies\n\nThis document provides an overview of the different testing strategies used in this repository, when to use each type, and links to examples.\n\n## Test Types\n\n### 1. Unit Tests\n\n**Purpose**: Test individual functions, utilities, and modules in isolation.\n\n**When to use**:\n\n- Testing pure functions with predictable inputs/outputs\n- Testing utility functions and helpers\n- Testing business logic components\n- Testing error handling and edge cases\n\n**Framework**: Vitest with TypeScript support\n\n**Examples**:\n\n- [`src/util/formatError.test.ts`](../src/util/formatError.test.ts) - Tests error formatting utility\n- [`src/util/exponentialBackoff.test.ts`](../src/util/exponentialBackoff.test.ts) - Tests backoff logic\n- [`src/logging.test.ts`](../src/logging.test.ts) - Tests logging functionality\n- [`src/args.test.ts`](../src/args.test.ts) - Tests argument parsing\n\n**How to run**: `npm test` (includes linting)\n\n### 2. Service Tests\n\n**Purpose**: Test service classes and dependency injection system.\n\n**When to use**:\n\n- Testing service initialization and lifecycle\n- Testing service dependencies and injection\n- Testing service container behavior\n- Testing service configuration loading\n\n**Examples**:\n\n- [`src/services/ServiceContainer.test.ts`](../src/services/ServiceContainer.test.ts) - Tests dependency injection\n- [`src/services/ConfigService.test.ts`](../src/services/ConfigService.test.ts) - Tests configuration loading\n- [`src/services/circular-dependencies.test.ts`](../src/services/circular-dependencies.test.ts) - Tests circular dependency detection\n\n### 3. UI Component Tests\n\n**Purpose**: Test React components using Ink testing library for terminal UI.\n\n**When to use**:\n\n- Testing component rendering and display\n- Testing user interactions and input handling\n- Testing component state changes\n- Testing message display and formatting\n\n**Framework**: Vitest + Ink Testing Library + React Testing utilities\n\n**Examples**:\n\n- [`src/ui/__tests__/TUIChat.basic.test.tsx`](../src/ui/__tests__/TUIChat.basic.test.tsx) - Basic rendering tests\n- [`src/ui/__tests__/TUIChat.messages.test.tsx`](../src/ui/__tests__/TUIChat.messages.test.tsx) - Message display tests\n- [`src/ui/__tests__/TUIChat.input.test.tsx`](../src/ui/__tests__/TUIChat.input.test.tsx) - Input handling tests\n- [`src/ui/MarkdownRenderer.test.tsx`](../src/ui/MarkdownRenderer.test.tsx) - Markdown rendering tests\n\n**Test Checklist**: See [`src/ui/UI_TEST_CHECKLIST.md`](../src/ui/UI_TEST_CHECKLIST.md) for comprehensive UI testing guidelines.\n\n### 4. E2E (End-to-End) Tests\n\n**Purpose**: Test complete user workflows and CLI behavior in realistic scenarios.\n\n**When to use**:\n\n- Testing full CLI command execution\n- Testing authentication flows\n- Testing configuration loading and switching\n- Testing tool execution and responses\n- Testing headless and interactive modes\n\n**Examples**:\n\n- [`src/e2e/auth.test.ts`](../src/e2e/auth.test.ts) - Authentication workflows\n- [`src/e2e/basic-commands.test.ts`](../src/e2e/basic-commands.test.ts) - Basic CLI commands\n- [`src/e2e/headless-tool-calls.test.ts`](../src/e2e/headless-tool-calls.test.ts) - Tool execution in headless mode\n- [`src/e2e/config-switching.test.tsx`](../src/e2e/config-switching.test.tsx) - Configuration switching\n\n## Testing Infrastructure\n\n### Test Helpers\n\n**Service Container Testing**:\n\n- [`src/test-helpers/testServiceContainer.ts`](../src/test-helpers/testServiceContainer.ts) - Mock service container setup\n- [`src/test-helpers/ui-test-context.ts`](../src/test-helpers/ui-test-context.ts) - UI test context setup\n\n**CLI Testing**:\n\n- Test helpers in [`src/test-helpers/`](../src/test-helpers/) for CLI command execution\n\n### Mocks\n\n**UI Mocks**: [`src/ui/__mocks__/`](../src/ui/__mocks__/)\n\n- Syntax highlighter mocks\n- Component mocks for testing\n\n**Service Mocks**: [`src/__mocks__/`](../src/__mocks__/)\n\n- Authentication mocks\n- Service mocks\n- Logging mocks\n\n## Configuration\n\n**Vitest Configuration**: [`vitest.config.ts`](../vitest.config.ts)\n\n- TypeScript support with ES modules\n- React/Ink component testing setup\n- Mock configurations\n- Test timeout and environment settings\n\n## Running Tests\n\n```bash\n# Run all tests with linting\nnpm test\n\n# Run tests in watch mode\nnpx vitest --watch\n\n# Run specific test file\nnpx vitest src/util/formatError.test.ts\n\n# Run tests with coverage\nnpx vitest --coverage\n```\n\n## Best Practices\n\n1. **Unit Tests**: Focus on pure functions and isolated logic\n2. **Service Tests**: Test dependency injection and service lifecycle\n3. **UI Tests**: Test user-visible behavior, not implementation details\n4. **E2E Tests**: Test complete user workflows and CLI behavior\n\n5. **Keep tests focused**: One behavior per test\n6. **Use descriptive names**: Test names should explain what's being tested\n7. **Minimal mocking**: Only mock external dependencies and APIs\n8. **Test error cases**: Include negative test cases and edge conditions\n9. **Maintain test isolation**: Each test should be independent\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/permissions.md","content":"# Permissions\n\n## Permission types\n\nTo make sure that users can oversee the actions of the LLM, we implement a permissions system. Every tool has one of the following permissions:\n\n- `allow`: The tool will be automatically called without asking\n- `ask`: We will ask the user before calling the tool, giving them the options to accept or reject\n- `exclude`: This tool will be completely excluded, so the model won't even know it exists\n\n## Rule precedence\n\nThere is a default set of permissions for the builtin tools in [`src/permissions/defaultPolicies.ts`](../src/permissions/defaultPolicies.ts). But these policies can be overriden by multiple layers. The order of precedence is as follows, which the earlier items taking precedence:\n\n1. **Mode policies** (highest priority - see [modes.md](./modes.md))\n2. Command line flags (`--allow`, `--ask`, `--exclude`)\n3. Permissions in `config.yaml` / configuration\n4. Permissions in `~/.continue/permissions.yaml`\n5. Default policies\n\n**Note:** Mode policies **completely override all other permission settings** in plan and auto modes. Available modes:\n\n- `normal`: No mode policies (uses existing configuration)\n- `plan`: **Absolute override** - excludes all write tools, allows only read tools (ignores user config)\n- `auto`: **Absolute override** - allows all tools without asking (ignores user config)\n\n## Tool matching patterns\n\nWe use a tool matching pattern to match tools to permissions. This format looks like the following:\n\n- `Read` matches any call to the `Read` tool\n- `Read(*)` also matches any call to the `Read` tool\n- `Read(**/*.ts)` matches any call to the `Read` tool where the primary parameter matches the glob pattern `**/*.ts`.\n\nNote that for an `exclude` policy, it doesn't make sense to have argument matching.\n\n## Command line flags\n\nEach of the `--allow`, `--ask`, and `--exclude` flags allow you to set the permission for a tool. Usage must be a \"tool matching pattern\" as described above for each flag, with each providing a policy that will be added to the list of policies in order.\n\n```bash\n# Allow Read, Ask Write, and Exclude Bash\ncn --allow Read --ask Write --exclude Bash\n\n# Start in plan mode (read-only tools only)\ncn --readonly \"Help me understand this codebase\"\n\n# Use mode switching during chat\ncn \"Let me work on this feature\"  # Starts in normal mode\n# Then use Shift+Tab to cycle through modes\n```\n\n## `config.yaml` / Configuration (implement later)\n\n::: info\nThis should not be implemented yet.\n:::\n\nTo let users define their permissions as a part of their custom assistant, they can do so in the permissions section of `config.yaml` or their configuration:\n\n```yaml\npermissions:\n  allow:\n    - Read(*)\n\n  ask:\n    - Write(**/*.py)\n\n  exclude:\n    - Write\n```\n\n## `~/.continue/permissions.yaml` (personal settings)\n\nIt would be frustrating for users to have to set the same permissions across all of their assistants, so we provide them a file for personal settings. It should be basically equivalent to the `permissions` section of `config.yaml`:\n\n```yaml title=\"~/.continue/permissions.yaml\"\nallow:\n  - Read(*)\n\nask:\n  - Write(**/*.py)\n\nexclude:\n  - Write\n```\n\nExcept that it's important to understand that this file is _not_ intended to be edited by the user. It is only for persistence, and users should interact with their permissions by using the TUI.\n\nThis file should be created the first time that the CLI starts.\n\n## Headless mode permissions\n\nWhen running in headless mode (using the `-p` or `--print` flag), the CLI uses the same default policies as normal mode, but with different behavior for tools that require confirmation:\n\n- **Normal mode**: Write operations and terminal commands require confirmation (`ask`) - user is prompted\n- **Headless mode**: Same default policies, but tools requiring confirmation (`ask`) will cause the process to exit with an error message\n\nTo use tools that normally require confirmation in headless mode, you must explicitly allow them:\n\n```bash\n# Headless mode with explicit permissions for write operations\ncn -p --allow write_file \"Write a hello world script\"\n\n# Headless mode with wildcard permission (allow all tools)\ncn -p --allow \"*\" \"Write and run a script\"\n\n# Headless mode with specific restrictions\ncn -p --exclude run_terminal_command \"Clean up the codebase\"\n```\n\nThis approach ensures that headless mode is secure by default while providing clear guidance on how to enable the needed permissions.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/tui.md","content":"# Continue CLI Terminal UI spec\n\nThis spec is incomplete.\n\n## Stack\n\nThe Continue CLI uses Ink as a react TUI library.\n\n## cwd/git display\n\nThe lower left corner of the TUI should display\n\n- the current git branch if in a git repo\n- also the current owner/repo if enough columns are available to display and it's a gh repo\n- if not in git, the cwd\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/config-loading.md","content":"# Configuration Loading Behavior Specification\n\n## Overview\n\nThis document specifies the behavior of the CLI's configuration loading system, including precedence rules, authentication interactions, and error handling.\n\n## Authentication Precedence\n\n**Authentication Source Priority:**\n\n1. **Environment Variable**: `CONTINUE_API_KEY` environment variable\n2. **File-Based Auth**: `~/.continue/auth.json` file\n3. **No Authentication**: Unauthenticated mode\n\n**Authentication Effects:**\n\n- **Environment Auth**: Always uses personal organization, no config URI persistence\n- **File Auth**: Supports organizations, persists config URIs, token refresh\n- **No Auth**: Limited to default assistant, no personalization\n\n## Configuration Source Precedence\n\n**When CLI is invoked, config source is determined in this order:**\n\n1. **CLI `--config` Flag** (highest priority)\n\n   - File path (starts with `.`, `/`, `~`): Loads local YAML file\n   - Assistant slug (`owner/package`): Fetches from Continue platform\n   - Overrides any saved preferences\n\n2. **Saved Config URI** (if no `--config` flag)\n\n   - Retrieved from authentication config\n   - Converts `file://` URIs back to file paths\n   - Converts `slug://` URIs back to assistant slugs\n\n3. **Default Resolution** (if no flag and no saved URI)\n   - **Authenticated**: First user assistant from `listAssistants()`\n   - **config.yaml**: The saved config file at `~/.continue/config.yaml`\n   - **Unauthenticated**: Falls back to `continuedev/default-cli-config`\n\n## Authentication State Interactions\n\n### Authenticated Users\n\n**Available Options:**\n\n- Personal assistants\n- Organization assistants (if organization selected)\n- Local YAML files\n- Public assistants\n\n**Behavior:**\n\n- `listAssistants()` returns personalized results\n- Config selections are saved as URIs in auth config\n- Organization context affects available assistants\n\n### Unauthenticated Users\n\n**Available Options:**\n\n- Local YAML files only\n- Default assistant (`continuedev/default-cli-config`)\n\n**Behavior:**\n\n- No access to personal or organization assistants\n- No config URI persistence\n- Direct fallback to default when no config specified\n\n### Environment Variable Auth (`CONTINUE_API_KEY`)\n\n**Behavior:**\n\n- Treated as authenticated for API access\n- Always uses personal organization context\n- No persistence of config URIs\n- Cannot switch organizations\n\n## Organization Context\n\n**Organization Selection:**\n\n- **Interactive Mode**: Auto-selects first available organization\n- **Headless Mode**: Defaults to personal organization\n- **Environment Auth**: Always personal organization\n\n**Effects on Config Loading:**\n\n- Organization ID passed to all API calls\n- Affects which assistants appear in `listAssistants()`\n- Organization changes trigger complete config reload\n\n## Error Handling Behavior\n\n### Config Loading Errors\n\n**File Not Found:**\n\n- Local file specified but doesn't exist\n- **Result**: Error thrown, CLI exits\n\n**Invalid YAML:**\n\n- Local file exists but has syntax errors\n- **Result**: Parsing error thrown, CLI exits\n\n**Network Failures:**\n\n- API calls fail for assistant slugs\n- **Result**: Network error bubbled up, CLI exits\n\n**Assistant Not Found:**\n\n- Valid slug format but assistant doesn't exist\n- **Result**: 404 error from API, CLI exits\n\n### Fallback Scenarios\n\n**No User Assistants:**\n\n- Authenticated user has no personal assistants\n- **Result**: Falls back to `continuedev/default-cli-config`\n\n**Default Agent Unavailable:**\n\n- Fallback to default agent fails\n- **Result**: Error thrown, CLI cannot start\n\n**Token Expired:**\n\n- Saved auth token is expired\n- **Result**: Automatic refresh attempted, re-auth required if refresh fails\n\n## Config URI Persistence\n\n**When URIs are Saved:**\n\n- Any successful config load via service layer\n- File paths converted to `file://path/to/config.yaml`\n- Assistant slugs converted to `slug://owner/package`\n\n**When URIs are NOT Saved:**\n\n- Environment variable authentication in use\n- Config loading failures\n- Unauthenticated sessions\n\n## Rule Integration\n\n**Rule Processing:**\n\n- `--rule` flags processed independently of config loading\n- Multiple rules supported, injected into system message\n- Rule failures are warnings, don't prevent config loading\n- Rule sources: file paths, hub slugs, direct strings\n\n## Session Continuity\n\n**Next Session Behavior:**\n\n- **With Saved URI**: Uses saved config automatically\n- **CLI Override**: `--config` flag overrides saved URI and updates it\n- **Config Switching**: UI actions update saved URI for future sessions\n\n**Cross-Session State:**\n\n- Authentication persists until logout\n- Organization selection persists\n- Config URI persists (except for environment auth)\n\n## Complete Decision Flow\n\n```\n1. Parse CLI arguments\n2. Load authentication state (env var > file > none)\n3. Determine config source:\n   - CLI --config flag provided? Use it\n   - Saved config URI exists? Use it\n   - Default resolution based on auth state\n4. Load configuration:\n   - File path? Parse YAML locally\n   - Assistant slug? Fetch from API\n   - Default resolution? List assistants or use default\n5. Process and inject rules\n6. Save config URI (if authenticated via file)\n7. Initialize services with loaded config\n```\n\nThis behavior ensures users get predictable config loading with clear precedence rules while maintaining session continuity and graceful fallbacks.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/index.md","content":"# `cn`\n\n## Overall development goals\n\nFight to keep `cn` simple. This is an active process. The default is that `cn` will become disastrously complicated.\n\n### Maintainability\n\nMaintain minimal code (avoid new features unless there is overwhelming demand or they remove a clear blocker for many users). If someone offers up a feature you should seek to understand what they actually want and then decide as a maintainer of `cn` whether this can fit into some larger concept in a simple way. If someone asks to make something configurable, you should probably consider just changing the default.\n\n### Don't write code by hand\n\nUse `cn` for everything. You should very rarely need to write code by hand. If you find yourself doing this outside of small tweaks, you should ask what about the codebase is causing `cn` to be unable to make the change.\n\nIf we build a system that allows us to entrust all changes to `cn` then it will be more reliable than us writing the code. Not because we can't write better code, but because `cn` is consistently thorough + it will force us to build the right systems around the code.\n\n### Do not test by hand\n\nIf you find yourself using `npm run start` to test that existing functionality is still working, this means that you are missing a test. A green check mark from `npm test` should be full proof that the code can be shipped. See our testing strategies [here](./testing-strategies.md).\n\nFor new features you obviously need to run them yourself to apply your taste. `cn` can't match that right now.\n\n### Minimize the number of concepts\n\nThe spec for `cn` (if it were to be fully written out) should be relatively short. This helps keep the code / spec consistent. It also allows us to keep the docs short and makes it easier for users to understand.\n\n### Low tolerance\n\nTolerance for the following should be very close to zero:\n\n- Un-addressed code reviews\n- Failing tests\n- Flaky tests\n- Sentry issues\n\nIf we are inundated then we should build a system to solve that. For example, if it is tiresome to address all AI code reviews, then set up a trigger to have `cn` automatically address them.\n\n### Start with a spec\n\nHigh level concepts / features should have a spec when possible. Good examples of this are [modes](./modes.md), [permissions](./permissions.md), [OTLP metrics](./otlp-metrics.md), and [wire format](./wire-format.md). Like code, the spec can and should be largely auto-generated.\n\n## Codebase Design\n\n- We intentionally write different implementations of the \"loop\" instead of trying to build a single engine that can be used across TUI, headless mode, `cn serve`, and more. So far, it would be more work than it is worth to add this abstraction.\n- \"Everything is a service.\" Nobody (human or AI) should need to re-invent a polling mechanism or way to store / update state. This decision has been made already so we can focus on the business logic. The service setup lets us easily keep reactive state and avoid React anti-patterns like `useEffect`.\n- We use `ink` for UI, following the same usual React principles that we do elsewhere. Keep [components](../src/ui/components/) small, avoid `useEffect`, group logic into [hooks](../src/hooks).\n- Always use the [logger](../src/util/logger.ts) instead of `console.log`.\n- Use commander to define [commands](../src/commands/BaseCommandOptions.ts)\n- `cn remote` is intended to act the same as `cn`'s TUI in almost every way. There is a test helper that allows us to run any e2e test in _both_ remote and normal modes to ensure this is true.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/otlp-metrics.md","content":"# OTLP Metrics Specification for Continue CLI\n\nThis document specifies the OpenTelemetry Protocol (OTLP) metrics that should be emitted by the Continue CLI to provide comprehensive observability and usage monitoring. The metrics are designed to be compatible with Claude Code dashboards for easy migration.\n\n## Overview\n\nThe Continue CLI should emit metrics that provide insights into:\n\n- Usage patterns and session analytics\n- Performance and reliability\n- Tool usage and effectiveness\n- Error rates and types\n- Resource utilization\n- Code modification tracking\n\n## Configuration\n\n### Environment Variables\n\n| Environment Variable            | Description                                                     | Example Values                       |\n| ------------------------------- | --------------------------------------------------------------- | ------------------------------------ |\n| `CONTINUE_METRICS_ENABLED`      | Enables OTEL telemetry collection (preferred, takes precedence) | `0` to disable, `1` to enable        |\n| `CONTINUE_CLI_ENABLE_TELEMETRY` | Enables OTEL telemetry collection (legacy, lower precedence)    | `0` to disable, `1` to enable        |\n| `OTEL_METRICS_EXPORTER`         | Metrics exporter type(s) (comma-separated)                      | `console`, `otlp`, `prometheus`      |\n| `OTEL_LOGS_EXPORTER`            | Logs/events exporter type(s) (comma-separated)                  | `console`, `otlp`                    |\n| `OTEL_EXPORTER_OTLP_PROTOCOL`   | Protocol for OTLP exporter (all signals)                        | `grpc`, `http/json`, `http/protobuf` |\n| `OTEL_EXPORTER_OTLP_ENDPOINT`   | OTLP collector endpoint (all signals)                           | `http://localhost:4317`              |\n| `OTEL_EXPORTER_OTLP_HEADERS`    | Authentication headers for OTLP                                 | `Authorization=Bearer token`         |\n| `OTEL_METRIC_EXPORT_INTERVAL`   | Export interval in milliseconds (default: 60000)                | `5000`, `60000`                      |\n| `OTEL_LOGS_EXPORT_INTERVAL`     | Logs export interval in milliseconds (default: 5000)            | `1000`, `10000`                      |\n| `OTEL_LOG_USER_PROMPTS`         | Enable logging of user prompt content (default: disabled)       | `1` to enable                        |\n\n### Metrics Cardinality Control\n\n| Environment Variable                | Description                                    | Default Value | Example to Disable |\n| ----------------------------------- | ---------------------------------------------- | ------------- | ------------------ |\n| `OTEL_METRICS_INCLUDE_SESSION_ID`   | Include session.id attribute in metrics        | `true`        | `false`            |\n| `OTEL_METRICS_INCLUDE_VERSION`      | Include app.version attribute in metrics       | `false`       | `true`             |\n| `OTEL_METRICS_INCLUDE_ACCOUNT_UUID` | Include user.account_uuid attribute in metrics | `true`        | `false`            |\n\n## Standard Attributes\n\nAll metrics and events share these standard attributes:\n\n| Attribute           | Description                                                   | Controlled By                                       |\n| ------------------- | ------------------------------------------------------------- | --------------------------------------------------- |\n| `session.id`        | Unique session identifier                                     | `OTEL_METRICS_INCLUDE_SESSION_ID` (default: true)   |\n| `app.version`       | Current Continue CLI version                                  | `OTEL_METRICS_INCLUDE_VERSION` (default: false)     |\n| `organization.id`   | Organization UUID (when authenticated)                        | Always included when available                      |\n| `user.account_uuid` | Account UUID (when authenticated)                             | `OTEL_METRICS_INCLUDE_ACCOUNT_UUID` (default: true) |\n| `terminal.type`     | Terminal type (e.g., `iTerm.app`, `vscode`, `cursor`, `tmux`) | Always included when detected                       |\n\n## Core Metrics\n\n### ‚úÖ `continue_cli_session_count`\n\n**Type:** Counter  \n**Unit:** `count`  \n**Description:** Count of CLI sessions started\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n\n**Implementation:** Track in `src/commands/chat.ts` when session starts\n\n---\n\n### ‚úÖ `continue_cli_lines_of_code_count`\n\n**Type:** Counter  \n**Unit:** `count`  \n**Description:** Count of lines of code modified\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `type`: (`\"added\"`, `\"removed\"`)\n\n**Implementation:** Track in `src/tools/writeFile.ts` by analyzing file diffs\n\n---\n\n### ‚úÖ `continue_cli_pull_request_count`\n\n**Type:** Counter  \n**Unit:** `count`  \n**Description:** Number of pull requests created\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n\n**Implementation:** Track when `runTerminalCommand` executes git/gh commands for PR creation\n\n---\n\n### ‚úÖ `continue_cli_commit_count`\n\n**Type:** Counter  \n**Unit:** `count`  \n**Description:** Number of git commits created\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n\n**Implementation:** Track when `runTerminalCommand` executes `git commit` commands\n\n---\n\n### ‚úÖ `continue_cli_cost_usage`\n\n**Type:** Counter  \n**Unit:** `USD`  \n**Description:** Cost of the Continue CLI session\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `model`: Model identifier (e.g., \"claude-3-5-sonnet-20241022\", \"gpt-4\")\n\n**Implementation:** Calculate costs based on token usage and model pricing in `src/streamChatResponse.ts`\n\n---\n\n### ‚úÖ `continue_cli_token_usage`\n\n**Type:** Counter  \n**Unit:** `tokens`  \n**Description:** Number of tokens used\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `type`: (`\"input\"`, `\"output\"`, `\"cacheRead\"`, `\"cacheCreation\"`)\n- `model`: Model identifier (e.g., \"claude-3-5-sonnet-20241022\", \"gpt-4\")\n\n**Implementation:** Track in `src/streamChatResponse.ts` after each API response\n\n---\n\n### ‚ùå `continue_cli_code_edit_tool_decision`\n\n**Type:** Counter  \n**Unit:** `count`  \n**Description:** Count of code editing tool permission decisions\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `tool`: Tool name (`\"writeFile\"`, `\"runTerminalCommand\"`, etc.)\n- `decision`: User decision (`\"accept\"`, `\"reject\"`)\n- `language`: Programming language of the edited file (e.g., `\"TypeScript\"`, `\"Python\"`, `\"JavaScript\"`, `\"Markdown\"`). Returns `\"unknown\"` for unrecognized file extensions.\n\n**Implementation:** Track in tool execution when user confirmation is required (if implemented)\n\n---\n\n### ‚úÖ `continue_cli_active_time_total`\n\n**Type:** Counter  \n**Unit:** `s`  \n**Description:** Total active time in seconds\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n\n**Implementation:** Track in TUI mode (`src/ui/TUIChat.tsx`) and standard chat mode, measuring time during active interactions\n\n## Core Events\n\n### ‚úÖ User Prompt Event\n\n**Event Name:** `continue_cli_user_prompt`\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `event.name`: `\"user_prompt\"`\n- `event.timestamp`: ISO 8601 timestamp\n- `prompt_length`: Length of the prompt\n- `prompt`: Prompt content (redacted by default, enable with `OTEL_LOG_USER_PROMPTS=1`)\n\n**Implementation:** Log in `src/streamChatResponse.ts` when user message is added to chat history\n\n---\n\n### ‚úÖ Tool Result Event\n\n**Event Name:** `continue_cli_tool_result`\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `event.name`: `\"tool_result\"`\n- `event.timestamp`: ISO 8601 timestamp\n- `tool_name`: Name of the tool\n- `success`: `\"true\"` or `\"false\"`\n- `duration_ms`: Execution time in milliseconds\n- `error`: Error message (if failed)\n- `decision`: Either `\"accept\"` or `\"reject\"` (if applicable)\n- `source`: Decision source - `\"config\"`, `\"user_permanent\"`, `\"user_temporary\"`, `\"user_abort\"`, or `\"user_reject\"` (if applicable)\n- `tool_parameters`: JSON string containing tool-specific parameters (when available)\n\n**Implementation:** Log in `src/tools/index.ts` `executeToolCall` function\n\n---\n\n### ‚úÖ API Request Event\n\n**Event Name:** `continue_cli_api_request`\n\n**Attributes:**\n\n- All [standard attributes](#standard-attributes)\n- `event.name`: `\"api_request\"`\n- `event.timestamp`: ISO 8601 timestamp\n- `model`: Model identifier\n- `duration_ms`: Request duration in milliseconds\n- `success`: `\"true\"` or `\"false\"`\n- `error`: Error message (if failed)\n- `input_tokens`: Number of input tokens\n- `output_tokens`: Number of output tokens\n- `cost_usd`: Request cost in USD\n\n**Implementation:** Log in `src/streamChatResponse.ts` for each API request\n\n## Additional Metrics\n\nThese metrics are unique to Continue CLI and provide additional insights without conflicting with compatibility:\n\n### Authentication Metrics\n\n#### ‚úÖ `continue_cli_auth_attempts`\n\n**Type:** Counter  \n**Unit:** `{attempt}`  \n**Description:** Authentication attempts\n\n**Labels:**\n\n- All [standard attributes](#standard-attributes)\n- `result`: `success` | `failure` | `cancelled`\n- `method`: `workos` | `token`\n\n**Implementation:** Track in `src/auth/workos.ts`\n\n### MCP Integration Metrics\n\n#### ‚ùå `continue_cli_mcp_connections`\n\n**Type:** Gauge  \n**Unit:** `{connection}`  \n**Description:** Active MCP connections\n\n**Labels:**\n\n- All [standard attributes](#standard-attributes)\n- `server_name`: MCP server identifier\n- `status`: `connected` | `disconnected` | `error`\n\n**Implementation:** Track in `src/mcp.ts`\n\n### Performance Metrics\n\n#### ‚ùå `continue_cli_startup_time`\n\n**Type:** Histogram  \n**Unit:** `ms`  \n**Description:** Time from CLI start to ready state\n\n**Labels:**\n\n- All [standard attributes](#standard-attributes)\n- `mode`: `tui` | `headless` | `standard`\n- `cold_start`: `true` | `false`\n\n**Implementation:** Track in `src/index.ts` and `src/commands/chat.ts`\n\n#### ‚úÖ `continue_cli_response_time`\n\n**Type:** Histogram  \n**Unit:** `ms`  \n**Description:** LLM response time metrics (supplements API request events)\n\n**Labels:**\n\n- All [standard attributes](#standard-attributes)\n- `model`: Model identifier\n- `metric_type`: `time_to_first_token` | `total_response_time`\n- `has_tools`: `true` | `false`\n\n**Implementation:** Track in `src/streamChatResponse.ts`\n\n## Implementation Guidelines\n\n### Migration from Claude Code Dashboards\n\nThe core metrics (`session_count`, `lines_of_code_count`, `token_usage`, `cost_usage`, etc.) use identical naming and attribute structures to Claude Code, allowing for easy dashboard migration by simply changing the metric prefix from `claude_code_*` to `continue_cli_*`.\n\n### Privacy Considerations\n\n- **No PII**: Avoid logging file paths, user content, or other personally identifiable information\n- **Redacted by Default**: User prompts are redacted unless `OTEL_LOG_USER_PROMPTS=1`\n- **Configurable Attributes**: Use cardinality control variables to manage data granularity\n- **Opt-out**: Provide mechanism to disable telemetry entirely\n\n### Resource Attributes\n\nAll metrics should include these resource attributes:\n\n- `service.name`: `continue-cli`\n- `service.version`: CLI version\n- `deployment.environment`: `development` | `production`\n- `os.type`: Operating system\n- `process.pid`: Process ID\n\n### Implementation Points\n\n1. **Session Tracking**: Initialize session ID and track lifecycle in `src/commands/chat.ts`\n2. **Token/Cost Tracking**: Implement in `src/streamChatResponse.ts` with model-specific pricing\n3. **Tool Usage**: Track in `src/tools/index.ts` and individual tool implementations\n4. **File Operations**: Implement diff analysis in `src/tools/writeFile.ts` for LOC tracking\n5. **Command Detection**: Parse git/gh commands in `src/tools/runTerminalCommand.ts`\n6. **Authentication**: Track auth flows in `src/auth/workos.ts`\n7. **Performance**: Add timing measurements throughout the application lifecycle\n\n## Implementation Status\n\n‚úÖ = Implemented  \n‚ùå = Not implemented yet\n\n**Core metrics:** 6/7 implemented (83%)\n**Core events:** 3/3 implemented (100%)  \n**Additional metrics:** 4/7 implemented (57%)\n\n**Missing implementations:**\n\n- `continue_cli_code_edit_tool_decision` - requires user confirmation UI\n- `continue_cli_mcp_connections` - needs MCP service monitoring\n- `continue_cli_startup_time` - needs startup time tracking\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/wire-format.md","content":"# HTTP Wire Protocol: `cn remote` ÔøΩ `cn serve`\n\nThis document describes the HTTP protocol used for communication between the `cn remote` client and `cn serve` server\n\n## Overview\n\nThe protocol uses a polling-based REST API where:\n\n- The server (`cn serve`) runs an Express HTTP server on port 3000\n- The client (`cn remote`) polls the server every 500ms for state updates\n- All communication uses JSON payloads\n\n## Endpoints\n\n### `GET /state`\n\nReturns the current chat state including message history and processing status.\n\n**Response:**\n\n```json\n{\n  \"chatHistory\": [\n    {\n      \"role\": \"user\" | \"assistant\" | \"system\",\n      \"content\": \"string\",\n      \"isStreaming\": boolean,\n      \"messageType\": \"tool-start\" | \"tool-result\" | \"tool-error\" | \"system\",\n      \"toolName\": \"string\",\n      \"toolResult\": \"string\"\n    }\n  ],\n  \"isProcessing\": boolean,\n  \"messageQueueLength\": number\n}\n```\n\n### `POST /message`\n\nSends a user message to the server. Messages are queued and processed sequentially.\n\n**Request Body:**\n\n```json\n{\n  \"message\": \"string\"\n}\n```\n\n**Response:**\n\n```json\n{\n  \"queued\": true,\n  \"position\": number,\n  \"willInterrupt\": boolean\n}\n```\n\n**Special Cases:**\n\n- Empty message (`\"\"`) interrupts current processing\n- Message `/exit` initiates server shutdown\n\n### `GET /diff`\n\nReturns the git diff between the current branch and main branch.\n\n**Response (Success):**\n\n```json\n{\n  \"diff\": \"string\"\n}\n```\n\n**Response (Error):**\n\n- 404: Not in a git repository\n- 500: Git command failed\n\n### `POST /exit`\n\nGracefully shuts down the server.\n\n**Response:**\n\n```json\n{\n  \"message\": \"Server shutting down\",\n  \"success\": true\n}\n```\n\n## Message Types\n\nMessages in the chat history can have different types:\n\n- **Regular messages**: Standard user/assistant messages\n- **Tool messages**: Messages with `messageType` set to:\n  - `tool-start`: Tool execution started\n  - `tool-result`: Tool execution completed\n  - `tool-error`: Tool execution failed\n  - `system`: System messages\n\n## Protocol Flow\n\n1. **Client starts**: Begins polling `GET /state` every 500ms\n2. **User sends message**: Client posts to `POST /message`\n3. **Server queues message**: Returns queue position\n4. **Server processes**: Updates state with streaming responses\n5. **Client displays**: Shows updates from state polling\n6. **Interruption**: Client sends empty message to interrupt\n7. **Exit**: Client sends `/exit` or `POST /exit` to shutdown\n\n## Implementation Files\n\n### Server Implementation\n\n- **Main server**: `src/commands/serve.ts:51-363`\n  - Express server setup\n  - Endpoint handlers\n  - State management\n  - Message processing\n\n### Client Implementation\n\n- **Remote chat hook**: `src/ui/hooks/useChat.ts:170-310`\n  - State polling logic\n  - Message sending\n  - Interrupt handling\n\n### Type Definitions\n\n- **Display message types**: `src/ui/types.ts:1-16`\n- **Server state interface**: `src/commands/serve.ts:20-33`\n\n### Testing\n\n- **Mock server**: `src/ui/__tests__/mockRemoteServer.ts`\n  - Complete mock implementation for testing\n  - Simulates streaming and message processing\n\n## Features\n\n### Message Queueing\n\n- Messages are queued and processed one at a time\n- Queue position returned on message submission\n- Supports interruption of current processing\n\n### Streaming Support\n\n- `isStreaming` flag indicates ongoing response\n- Character-by-character updates for real-time display\n\n### Auto-shutdown\n\n- Server shuts down after timeout (default: 300 seconds)\n- Configurable via `--timeout` flag\n\n### Error Handling\n\n- HTTP status codes for error conditions\n- Graceful error messages in responses\n\n## Security Considerations\n\n- Server binds to `127.0.0.1` (localhost only)\n- No authentication implemented (local use only)\n- File system access through tool execution\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/tty-less-support.md","content":"# TTY-less Environment Support\n\n## Overview\n\nThe Continue CLI supports running in TTY-less environments (environments without a terminal/TTY), which is essential for:\n\n- VSCode and IntelliJ extensions using the `run_terminal_command` tool\n- Docker containers without TTY allocation\n- CI/CD pipelines\n- Automated scripts and tools\n- Background processes\n\n## Architecture\n\n### Mode Separation\n\nThe CLI has two distinct execution modes with complete separation:\n\n1. **Interactive Mode (TUI)**: Requires a TTY, uses Ink for rendering\n2. **Headless Mode**: Works in TTY-less environments, outputs to stdout/stderr\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                        CLI Entry Point                       ‚îÇ\n‚îÇ                         (src/index.ts)                       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ\n            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n            ‚îÇ                         ‚îÇ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ Interactive    ‚îÇ       ‚îÇ   Headless      ‚îÇ\n    ‚îÇ Mode (TUI)     ‚îÇ       ‚îÇ   Mode (-p)     ‚îÇ\n    ‚îÇ                ‚îÇ       ‚îÇ                 ‚îÇ\n    ‚îÇ ‚Ä¢ Requires TTY ‚îÇ       ‚îÇ ‚Ä¢ No TTY needed ‚îÇ\n    ‚îÇ ‚Ä¢ Uses Ink     ‚îÇ       ‚îÇ ‚Ä¢ Stdin/stdout  ‚îÇ\n    ‚îÇ ‚Ä¢ Keyboard UI  ‚îÇ       ‚îÇ ‚Ä¢ One-shot exec ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Safeguards Implemented\n\n#### 1. **TTY Detection Utilities** (`src/util/cli.ts`)\n\n```typescript\n// Check if running in TTY-less environment\nexport function isTTYless(): boolean;\n\n// Check if environment supports interactive features\nexport function supportsInteractive(): boolean;\n\n// Check if prompt was supplied via CLI arguments\nexport function hasSuppliedPrompt(): boolean;\n```\n\n#### 2. **Stdin Reading Protection** (`src/util/stdin.ts`)\n\nPrevents stdin reading when:\n\n- In headless mode with supplied prompt\n- `FORCE_NO_TTY` environment variable is set\n- In test environments\n\nThis avoids blocking/hanging in TTY-less environments where stdin is not available or not readable.\n\n#### 3. **TUI Initialization Guards** (`src/ui/index.ts`)\n\nThe `startTUIChat()` function now includes multiple safeguards:\n\n- **Headless mode check**: Throws error if called in headless mode\n- **TTY-less check**: Throws error if no TTY is available\n- **Raw mode test**: Validates stdin supports raw mode (required by Ink)\n- **Explicit stdin/stdout**: Passes streams explicitly to Ink\n\n```typescript\n// Critical safeguard: Prevent TUI in headless mode\nif (isHeadlessMode()) {\n  throw new Error(\"Cannot start TUI in headless mode\");\n}\n\n// Critical safeguard: Prevent TUI in TTY-less environment\nif (isTTYless() && !customStdin) {\n  throw new Error(\"Cannot start TUI in TTY-less environment\");\n}\n```\n\n#### 4. **Headless Mode Validation** (`src/commands/chat.ts`)\n\nEnsures headless mode has all required inputs:\n\n```typescript\nif (!prompt) {\n  throw new Error(\"Headless mode requires a prompt\");\n}\n```\n\n#### 5. **Logger Configuration** (`src/util/logger.ts`)\n\nConfigures output handling for TTY-less environments:\n\n- Sets UTF-8 encoding\n- Leaves stdout/stderr buffering unchanged in headless mode.\n- Disables progress indicators\n\n## Usage Examples\n\n### From VSCode/IntelliJ Extension\n\n```typescript\n// Using the run_terminal_command tool\nconst command = 'cn -p \"Analyze the current git diff\"';\nconst result = await runTerminalCommand(command);\n```\n\n### From Docker Container\n\n```bash\n# Without TTY allocation (-t flag)\ndocker run --rm my-image cn -p \"Generate a README\"\n```\n\n### From CI/CD Pipeline\n\n```yaml\n- name: Run Continue CLI\n  run: |\n    cn -p \"Review code changes\" --format json\n```\n\n### From Automated Script\n\n```bash\n#!/bin/bash\n# Non-interactive script\ncn -p \"Generate commit message for current changes\" --silent\n```\n\n## Environment Variables\n\n- `FORCE_NO_TTY`: Forces TTY-less mode, prevents stdin reading\n- `CONTINUE_CLI_TEST`: Marks test environment, prevents stdin reading\n\n## Testing\n\n### TTY-less Test\n\n```typescript\nconst result = await runCLI(context, {\n  args: [\"-p\", \"Hello, world!\"],\n  env: {\n    FORCE_NO_TTY: \"true\",\n  },\n});\n```\n\n### Expected Behavior\n\n- ‚úÖ Should not hang on stdin\n- ‚úÖ Should not attempt to initialize Ink\n- ‚úÖ Should output results to stdout\n- ‚úÖ Should exit cleanly\n\n## Error Messages\n\n### Attempting TUI in TTY-less Environment\n\n```\nError: Cannot start TUI in TTY-less environment. No TTY available for interactive mode.\nFor non-interactive use, run with -p flag:\n  cn -p \"your prompt here\"\n```\n\n### Missing Prompt in Headless Mode\n\n```\nError: A prompt is required when using the -p/--print flag, unless --prompt or --agent is provided.\n\nUsage examples:\n  cn -p \"please review my current git diff\"\n  echo \"hello\" | cn -p\n  cn -p \"analyze the code in src/\"\n  cn -p --agent my-org/my-agent\n```\n\n## Troubleshooting\n\n### CLI Hangs in Docker/CI\n\n**Cause**: CLI attempting to read stdin in TTY-less environment\n\n**Solution**: Ensure using `-p` flag with a prompt:\n\n```bash\ncn -p \"your prompt\" --config config.yaml\n```\n\n### \"Cannot start TUI\" Error\n\n**Cause**: Attempting interactive mode in TTY-less environment\n\n**Solution**: Use headless mode:\n\n```bash\ncn -p \"your prompt\"\n```\n\n### Raw Mode Error\n\n**Cause**: Terminal doesn't support raw mode (required by Ink)\n\n**Solution**: Use headless mode instead of interactive mode\n\n## Design Principles\n\n1. **Fail Fast**: Detect environment early and fail with clear messages\n2. **Explicit Separation**: No code path should allow Ink to load in headless mode\n3. **No Blocking**: Never block on stdin in TTY-less environments\n4. **Clear Errors**: Provide actionable error messages with examples\n5. **Testing**: Comprehensive tests for TTY-less scenarios\n\n## Implementation Checklist\n\n- [x] Add TTY detection utilities\n- [x] Protect stdin reading in headless mode\n- [x] Guard TUI initialization\n- [x] Validate headless mode inputs\n- [x] Configure logger for TTY-less output\n- [x] Update test helpers\n- [x] Add TTY-less tests\n- [x] Document TTY-less support\n\n## Related Files\n\n- `src/util/cli.ts` - TTY detection utilities\n- `src/util/stdin.ts` - Stdin reading protection\n- `src/ui/index.ts` - TUI initialization guards\n- `src/commands/chat.ts` - Mode routing and validation\n- `src/util/logger.ts` - Output configuration\n- `src/test-helpers/cli-helpers.ts` - Test support\n- `src/e2e/headless-minimal.test.ts` - TTY-less tests\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/modes.md","content":"# Modes\n\nModes are a system for managing tool permissions in the CLI. They can be set via command-line flags at startup or switched dynamically during chat sessions. The following modes are available:\n\n## Available Modes\n\n### `normal` (default)\n\nThe default mode that follows configured permission policies from `permissions.yaml` and command-line overrides without any additional restrictions or mode-specific policies.\n\n- **UI Indicator:** No indicator shown (clean interface for default behavior)\n- **Current directory:** Visible in status bar for context\n- **Permission behavior:** Uses existing permission policies as configured\n- **Backward compatibility:** Existing configurations work unchanged\n\n### `plan`\n\nPlanning mode that **completely overrides all user permissions** to enforce read-only access with command execution. This mode prevents file modifications but allows command execution for analysis, regardless of user configuration.\n\n- **Command-line flag:** `--readonly` (for backward compatibility)\n- **UI Indicator:** `[plan]` shown in blue\n- **Current directory:** Hidden to save space and focus on analysis\n- **Permission override:** **Absolute** - excludes write tools (Write, Edit, etc.) and allows read tools (Read, Grep, LS, etc.) and Bash for command execution\n- **User config ignored:** Any user `--allow` flags for write tools are overridden\n\n### `auto`\n\nAuto mode that **completely overrides all user permissions** to allow everything without asking. This mode provides maximum automation by bypassing all permission policies and restrictions, regardless of user configuration.\n\n- **Command-line flag:** `--auto` (starts in auto mode)\n- **UI Indicator:** `[auto]` shown in green\n- **Current directory:** Hidden to save space and focus on automation\n- **Permission override:** **Absolute** - allows all tools with `*: allow` policy\n- **User config ignored:** Any user `--exclude` flags are overridden - everything is allowed\n\n## Usage\n\n### Command-Line Initialization\n\nModes can be set when starting the CLI:\n\n```bash\ncn --readonly \"Help me analyze this code\"  # Starts in plan mode\ncn --auto \"Fix all the linting errors\"     # Starts in auto mode\ncn \"Let me implement this feature\"         # Starts in normal mode (default)\n```\n\n### Dynamic Mode Switching\n\nUsers can switch modes during chat sessions using:\n\n**Keyboard Shortcut:**\n\n- **Shift+Tab** - Cycle through modes: normal ‚Üí plan ‚Üí auto ‚Üí (repeat)\n\n## Implementation\n\nModes are implemented through the permission system:\n\n- **ToolPermissionService**: Rectifies current mode and tool policies\n- **ModeIndicator**: UI component showing current mode (hidden for normal mode)\n- **Keyboard shortcuts**: Shift+Tab cycles through modes instantly\n- **Backward compatibility**: Existing `--readonly` flag maps to plan mode\n\n### Mode Policy Priority\n\n**Mode policies completely override all other configurations** when in plan or auto mode:\n\n**Plan and Auto modes:**\n\n1. **Mode policies** (absolute override - ignores everything else)\n\n**Normal mode only:**\n\n1. Command-line overrides (`--allow`, `--ask`, `--exclude`)\n2. Permission configuration from `permissions.yaml`\n3. Default tool policies\n\n### Mode-Specific Behaviors\n\n- **normal**: No mode policies applied, uses existing user configuration; shows current directory\n- **plan**: **Absolute override** - excludes write tools (Write, Edit), allows read tools (Read, Grep, LS) and Bash; hides current directory\n- **auto**: **Absolute override** - allows all tools with `*: allow` policy; hides current directory\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/mcp.md","content":"# Continue CLI Model Context Protocol (MCP) integration\n\n## Intro\n\nModel Context Protocol is a protocol for giving models access to resources and tools. MCP servers can run locally (stdio) or be remote (http/streamable/etc). See https://modelcontextprotocol.io/specification. The Continue CLI uses MCP to extend model's capabilities with MCP prompts and tools. MCP server configurations are stored in the `mcpServers` field of an assistant/config.yaml configuration.\n\n## Secret Resolution\n\nMCP server configurations often require secrets (API keys, tokens, etc.) referenced using template variables like `${{ secrets.API_KEY }}`. The CLI resolves these secrets in the following order:\n\n1. **Organization/Package Secrets**: First attempts to resolve secrets through the Continue API for organization or package-level secrets\n2. **Local Environment Variables**: Falls back to local environment variables if:\n   - The API secret exists but isn't accessible locally (e.g., in devboxes or restricted environments)\n   - The secret isn't found in organization/package secrets\n\nLocal environment variables are checked in this priority order:\n\n- `process.env` (runtime environment variables)\n- `~/.continue/.env`\n- `<workspace>/.continue/.env`\n- `<workspace>/.env`\n\nThis fallback mechanism ensures MCP servers can start successfully in environments where organization secrets aren't accessible, such as development containers or CI/CD pipelines, by allowing environment variables to provide the required credentials.\n\n## MCP Service\n\nThe CLI has an MCP Service should manage connections to MCP servers and provide state to the terminal app regarding MCP server connections.\nService initialization for MCP servers can take especially long, so it should not block service initialization. Instead, initialization should kick off server connections and then resolve.\nAll configured server connections can be initialized simultaneously using Promise.all\nEach server configuration should have its own connection and state separated, but the service should also have an easy way to retrieve all tools and prompts from all servers to pass to the model.\nServers should have a status \"idle\", \"connecting\", \"connected\", or \"error\". Only \"connected\" servers contribute prompts and tools.\nIf a server successfully connects but there is an error retrieving prompts or tools, it should have an array of warning messages that can be accessed in the terminal app.\nWhen the process exits, all servers should be shut down and disconnected.\nOnly MCP servers for the CURRENT config should be loaded\n\n- Logs from MCP servers should be routed through the winston logger, not the standard console.log (since this is a terminal app)\n\n## MCP slash command\n\nUsers can manage MCP connections using the /mcp slash command.\n/mcp shows a menu with the following options\n\n- \"No servers\" if there are no servers\n- Restart all servers\n- Stop all servers\n- View servers\n- Back (esc also goes back)\n\nView servers should open a submenu that has each server name with its status icon\n\n- red if error, yellow if warnings, green if connected, gray if idle\n- \"No Servers\" if there are none\n- Back (esc also goes back)\n\nFinally, selecting a server opens a sub-submenu that keeps the name/icon as header and lists\n\n- Back (esc also goes back)\n- Restart server\n- Stop server\n- warnings the server currently has\n- prompts the server contributes\n- tools the server contributes\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/ctrl-c-behavior.md","content":"# Ctrl+C Exit Behavior\n\n## Overview\n\nCtrl+C operates on two levels:\n\n1. **Menus/Selectors**: Immediate cancellation and return to previous state\n2. **Main Application**: Two-stage exit requiring double Ctrl+C within 1 second\n\n## Two-Stage Exit (Main Application)\n\n### First Ctrl+C\n\n- Shows temporary status message \"ctrl+c to exit\" in bottom-left status bar\n- Message automatically disappears after 1 second\n- Clears any text in the input field (if focused)\n- Does **not** exit the application\n\n### Second Ctrl+C (within 1 second)\n\n- Exits the application immediately\n- Performs graceful shutdown\n\n### Second Ctrl+C (after 1 second)\n\n- Treated as a new \"first Ctrl+C\"\n- Shows exit message again\n- User must press Ctrl+C twice within 1 second window to actually exit\n\n## Menu/Selector Cancellation\n\nCtrl+C immediately cancels and returns to the previous state in all interactive menus and selectors.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/spec/shell-mode.md","content":"Shell Mode (CLI TUI)\n\nOverview\n\n- Shell mode lets users run shell commands directly from the chat input by starting their input with an exclamation mark (!).\n- It is intended for quick terminal command execution without leaving the TUI.\n\nActivation\n\n- Shell mode is activated when the current input (trimmed) starts with !\n  - Example: \"!git status\" or \" !ls -la\" both activate Shell mode.\n- Visual indicator:\n  - Input border color changes to yellow.\n  - The input prompt indicator changes to a yellow \"$\".\n  - The input placeholder includes \"! for shell mode\".\n\nDeactivation / Exiting Shell Mode\n\n- Pressing Enter to submit the input exits shell mode immediately after submission, regardless of the command result.\n- Pressing Esc when the input (trimmed) is exactly ! clears the input and exits shell mode.\n- Editing the input so it no longer starts with ! also exits shell mode and restores normal input behavior.\n\nInteraction with other input helpers\n\n- When in shell mode (input starts with !):\n  - \"@\" file search suggestions are disabled.\n  - \"/\" slash command UI is disabled.\n- When in slash command mode (input starts with /):\n  - \"@\" file search suggestions are disabled.\n  - Enter submits the highlighted slash command directly (except /title, which requires Tab to select first).\n  - Tab selects the highlighted command without submitting.\n\nSubmission behavior\n\n- On submit (Enter) with a shell-mode input:\n  - The leading ! is removed and the remainder is treated as the shell command to run.\n  - The TUI immediately appends an assistant message representing a Shell tool call, with status set to calling, so users can see that the command is in progress.\n  - The shell command is executed asynchronously; when it completes, the tool call status is updated to done (or error) and the output is populated.\n\nExecution semantics\n\n- The command is executed in the same way as terminal tool commands, minus permissions, and inserted into the chat history the same.\n\nOutput handling\n\n- Stdout is streamed into memory; Stderr is captured and appended as a trailing \"Stderr: ...\" section on success.\n- If the process exits non-zero and Stderr contains content, the tool call is marked as error and the error text is shown.\n- Output is truncated to the first 5000 lines if exceeded.\n- Timeout behavior: If no output is received for 120 seconds (configurable in tests), the process is terminated and the result includes a note like:\n  \"[Command timed out after 120 seconds of no output]\".\n\nKeyboard behaviors (summary)\n\n- Enter: submit input. If in shell mode, exits shell mode after submission and shows the pending Shell tool call immediately.\n- Shift+Enter: new line.\n- Backslash (\\) at end-of-line: inserts a new line (line continuation) as usual.\n- Esc: if only ! (trimmed) is present, clears input and exits shell mode; otherwise cancels streaming or closes suggestions depending on context.\n\nScope / Modes\n\n- Shell mode applies to interactive (TUI/standard) CLI usage. It is not part of headless (-p/--print) processing.\n\nError handling\n\n- Command execution errors are captured and surfaced in the tool call as status error with human-readable error text (including Stderr when available).\n\nExamples\n\n- \"!git status\" ‚Üí shows a Shell tool call immediately, then populates with the git status output.\n- \"!echo hello\" ‚Üí shows a Shell tool call immediately, then output \"hello\".\n- \"!some-unknown-cmd\" ‚Üí shows a Shell tool call immediately, then sets status to error with an error message.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/AGENTS.md","content":"# AGENTS.md\n\nThis file provides guidance to AI coding agents when working with code in this repository.\n\n## Development Commands\n\n- **Build**: `npm run build` - Compiles TypeScript to JavaScript in dist/\n- **Test**: `npm test` - Runs Vitest tests with ESM support\n- **Type Check**: `npm run lint` - Runs TypeScript compiler without emitting files for type checking\n- **Format**: `npm run format` - Runs Prettier with --write flag to check + fix formatting\n- **Start**: `npm start` - Runs the built CLI from dist/index.js\n- **Development**: After building, test locally with `node dist/index.js`\n\n## Architecture Overview\n\nThis is a CLI tool for Continue Dev that provides an interactive AI-assisted development experience. The architecture consists of:\n\n### Core Components\n\n1. **Entry Point** (`src/index.ts`): Main CLI logic with two modes:\n\n   - **Headless mode**: Non-interactive mode for automation/CI\n   - **TUI mode**: Terminal User Interface using Ink/React\n   - **Standard mode**: Traditional readline-based chat interface\n\n2. **Authentication** (`src/auth/`): WorkOS-based authentication system\n\n   - `ensureAuth.ts`: Handles authentication flow\n   - `workos.ts`: WorkOS configuration and token management\n\n3. **Continue SDK Integration** (`src/continueSDK.ts`): Initializes the Continue SDK client with:\n\n   - API key authentication\n   - Assistant configuration (slug-based)\n   - Organization support\n\n4. **Terminal UI** (`src/ui/`): React/Ink-based TUI components\n\n   - `TUIChat.tsx`: Main chat interface component\n   - `UserInput.tsx`: Input handling with multi-line support\n   - `TextBuffer.ts`: Text display utilities\n\n5. **Tools System** (`src/tools/`): Built-in development tools including:\n\n   - File operations (read, write, list)\n   - Code search functionality\n   - Terminal command execution\n   - Diff viewing\n   - Exit tool (headless mode only)\n\n6. **MCP Integration** (`src/mcp.ts`): Model Context Protocol service for extended tool capabilities\n\n### Key Features\n\n- **Streaming Responses**: Real-time AI response streaming (`streamChatResponse.ts`)\n- **Slash Commands**: Built-in commands like `/help`, `/exit` (`slashCommands.ts`)\n- **Multi-mode Operation**: Supports TUI, headless, and standard chat modes\n- **Tool Integration**: Extensible tool system for development tasks\n\n### Testing Setup\n\n- Uses Vitest with TypeScript and ESM support\n- Configuration in `vitest.config.ts`\n- Tests should be written with `.test.ts` extension\n- No existing test files found - tests should be added when writing new functionality\n- Run tests using `npm run test path/or/pattern`\n\n### Build System\n\n- TypeScript compilation with declaration files\n- ESNext target with NodeNext module resolution\n- Outputs to `dist/` directory\n- Source maps and inline sources enabled\n- JSX support for React components\n- Relative import paths require explicit file extensions, e.g. 'from \"./test.js\"' instead of 'from \"./test\"'\n\n### Important rules\n\n- Whenever you create / update a test, you should run the test to be certain that it passes\n- If you ever create a PR, you should be sure to check the formatting and linting first with `npm run format` and `npm run lint` / `npm run lint:fix`.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/BUILD.md","content":"# Build Process Documentation\n\n## Overview\n\nThe Continue CLI uses esbuild to bundle the application along with local packages (`@continuedev/config-yaml` and `@continuedev/openai-adapters`) into a single distributable file. This ensures that users who install the CLI from npm don't need to worry about local file references.\n\n## Build Steps\n\n1. **Build packages**: `cd ../../ && node ./scripts/build-packages.js`\n2. **Install dependencies**: `npm install`\n3. **Build**: `npm run build`\n   - This first builds the local packages\n   - Then bundles everything with esbuild\n\n## How it Works\n\n### Bundling Strategy\n\nThe `build.mjs` script uses esbuild to:\n\n- Bundle all TypeScript/JavaScript code into a single ES module\n- Include local packages (`@continuedev/config-yaml`, `@continuedev/openai-adapters`) directly in the bundle\n- Keep problematic or native dependencies external (e.g., `@sentry/profiling-node`, `winston`, `express`)\n- Create a wrapper script (`dist/cn.js`) with the proper shebang for CLI execution\n\n### Key Features\n\n- **Local Package Bundling**: Local packages are bundled directly, avoiding `file:` reference issues\n- **Stub for Optional Dependencies**: `react-devtools-core` is stubbed to prevent runtime errors\n- **CommonJS Compatibility**: Adds `createRequire` to support packages that use dynamic requires\n- **Source Maps**: Generates source maps for debugging\n- **Metadata**: Creates `dist/meta.json` with bundle analysis information\n\n## Testing\n\n### Smoke Tests\n\nRun smoke tests to verify the build:\n\n```bash\nnpm run test:smoke\n```\n\nThe smoke tests verify:\n\n- Bundle files exist\n- CLI commands work (--version, --help)\n- Bundle size is reasonable (<20MB)\n- Local packages are properly bundled\n- No missing runtime dependencies\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Module not found errors**: Check if the module needs to be added to the external list in `build.mjs`\n2. **Native module issues**: Native modules should be marked as external\n3. **Bundle too large**: Review bundled packages in `dist/meta.json` and consider marking large packages as external\n\n### Analyzing the Bundle\n\nAfter building, check `dist/meta.json` to see:\n\n- What packages were bundled\n- Bundle size breakdown\n- Input/output file mappings\n\n## Publishing\n\nWhen publishing to npm:\n\n1. The bundled files in `dist/` are included\n2. Users install via `npm install -g @continuedev/cli`\n3. The `cn` command becomes available globally\n4. No local file references or missing dependencies\n"}
{"source":"github","repo":"continue","path":"extensions/cli/README.md","content":"# Continue CLI\n\nThe Continue CLI (`cn`) is a customizable command line coding agent.\n\n![Continue CLI Demo](./media/demo.gif)\n\n## Installation\n\n```bash\nnpm i -g @continuedev/cli\n```\n\n## Usage\n\n```bash\ncn\n```\n\n### Headless Mode\n\nHeadless mode (`-p` flag) runs without an interactive terminal UI, making it perfect for:\n\n- Scripts and automation\n- CI/CD pipelines\n- Docker containers\n- VSCode/IntelliJ extension integration\n- Environments without a TTY\n\n```bash\n# Basic usage\ncn -p \"Generate a conventional commit name for the current git changes.\"\n\n# With piped input\necho \"Review this code\" | cn -p\n\n# JSON output for scripting\ncn -p \"Analyze the code\" --format json\n\n# Silent mode (strips thinking tags)\ncn -p \"Write a README\" --silent\n```\n\n**TTY-less Environments**: Headless mode is designed to work in environments without a terminal (TTY), such as when called from VSCode/IntelliJ extensions using terminal commands. The CLI will not attempt to read stdin or initialize the interactive UI when running in headless mode with a supplied prompt.\n\n### Session Management\n\nThe CLI automatically saves your chat history for each terminal session. You can resume where you left off:\n\n```bash\n# Resume the last session in this terminal\ncn --resume\n\n# List recent sessions and choose one to resume\ncn ls\n\n# List sessions in JSON format (for scripting)\ncn ls --json\n```\n\n## Command Line Options\n\n- `-p`: Run in headless mode (no TUI)\n- `--config <path>`: Specify agent configuration path\n- `--resume`: Resume the last session for this terminal\n- `<prompt>`: Optional prompt to start with\n\n## Environment Variables\n\n- `CONTINUE_CLI_DISABLE_COMMIT_SIGNATURE`: Disable adding the Continue commit signature to generated commit messages\n- `FORCE_NO_TTY`: Force TTY-less mode, prevents stdin reading (useful for testing and automation)\n\n## Commands\n\n- `cn`: Start an interactive chat session\n- `cn ls`: List recent sessions with TUI selector to choose one to resume\n- `cn login`: Authenticate with Continue\n- `cn logout`: Sign out of current session\n- `cn remote`: Launch a remote instance\n- `cn serve`: Start HTTP server mode\n\n### Session Listing (`cn ls`)\n\nShows recent sessions, limited by screen height to ensure it fits on your terminal.\n\n- `--json`: Output in JSON format for scripting (always shows 10 sessions)\n\n## TTY-less Support\n\nThe CLI fully supports running in environments without a TTY (terminal):\n\n```bash\n# From Docker without TTY allocation\ndocker run --rm my-image cn -p \"Generate docs\"\n\n# From CI/CD pipeline\ncn -p \"Review changes\" --format json\n\n# From VSCode/IntelliJ extension terminal tool\ncn -p \"Analyze code\" --silent\n```\n\nThe CLI automatically detects TTY-less environments and adjusts its behavior:\n\n- Skips stdin reading when a prompt is supplied\n- Disables interactive UI components\n- Ensures clean stdout/stderr output\n\nFor more details, see [`spec/tty-less-support.md`](./spec/tty-less-support.md).\n"}
{"source":"github","repo":"continue","path":"extensions/cli/docs/artifact-uploads.md","content":"# Artifact Upload Architecture\n\n## Overview\n\nThe artifact upload feature enables Continue agents running in devboxes to upload arbitrary files (screenshots, videos, logs) to agent session storage for review and debugging purposes. The architecture uses a two-step presigned URL pattern for secure, performant uploads.\n\n## Architecture Pattern: Presigned URLs\n\n### Why Presigned URLs?\n\nThe artifact upload system uses **presigned URLs** for direct client-to-S3 uploads rather than proxying files through the backend. This design provides several benefits:\n\n1. **Security**: The backend controls who can upload, validates file types/sizes, and enforces storage limits before issuing a presigned URL. The agent cannot bypass these validations.\n\n2. **Performance**: Files upload directly from the devbox to S3, avoiding bandwidth costs and latency of routing through the backend API server.\n\n3. **Scalability**: The backend doesn't become a bottleneck for file uploads. S3 handles the heavy lifting of data transfer.\n\n4. **Simplicity**: Presigned URLs are time-limited (15 minutes), self-contained credentials that require no complex token management.\n\n## Two-Step Upload Flow\n\n### Step 1: Request Presigned URL\n\nThe agent requests a presigned upload URL from the backend:\n\n**Request:**\n\n```http\nPOST /agents/artifacts/upload-url\nAuthorization: Bearer <CONTINUE_API_KEY>\nContent-Type: application/json\n\n{\n  \"agentSessionId\": \"<session-id>\",\n  \"filename\": \"screenshot.png\",\n  \"contentType\": \"image/png\",\n  \"fileSize\": 1048576\n}\n```\n\n**Backend Validation:**\n\n- Authenticates the API key and verifies session ownership\n- Validates filename (no path traversal, allowed extension)\n- Validates file size against per-file limit (50MB default)\n- Validates content type against allowlist\n- Checks total session storage against limit (500MB default)\n\n**Response (if validation passes):**\n\n```json\n{\n  \"url\": \"https://s3.amazonaws.com/bucket/sessions/org/abc123/artifacts/screenshot.png?X-Amz-...\",\n  \"key\": \"sessions/org/abc123/artifacts/screenshot.png\",\n  \"expiresIn\": 900\n}\n```\n\n**Response (if validation fails):**\n\n```http\n400 Bad Request\n\n{\n  \"error\": \"File size exceeds maximum allowed (50MB)\"\n}\n```\n\n### Step 2: Upload to S3\n\nThe agent uploads the file directly to S3 using the presigned URL:\n\n**Request:**\n\n```http\nPUT <presigned-url>\nContent-Type: image/png\n<file-contents>\n```\n\nS3 validates the presigned URL signature and accepts the upload. The backend is not involved in this step.\n\n## Storage Organization\n\nArtifacts are stored in S3 with the following path structure:\n\n```\nsessions/\n  user/\n    <userId>/\n      <sessionId>/\n        artifacts/\n          screenshot.png\n          video.mp4\n          debug.log\n        session.json        # Session state (existing)\n        diff.txt           # Git diff (existing)\n  org/\n    <organizationId>/\n      <sessionId>/\n        artifacts/\n          ...\n```\n\nThis structure:\n\n- Maintains backward compatibility with existing `session.json` and `diff.txt` files\n- Isolates user/org data for security\n- Groups all session-related files together\n- Allows simple recursive deletion when a session is removed\n\n## File Type and Size Limits\n\n### Allowed File Types\n\nThe system validates both file extensions and MIME types:\n\n**Images:** `.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`\n**Videos:** `.mp4`, `.mov`, `.avi`, `.webm`\n**Text/Logs:** `.log`, `.txt`, `.json`, `.xml`, `.csv`, `.html`\n\nContent types are validated against an allowlist to prevent uploading executable files or other potentially dangerous content.\n\n### Size Limits\n\nTwo limits are enforced:\n\n1. **Per-File Limit:** 50MB (configurable via `ARTIFACT_MAX_FILE_SIZE_MB`)\n2. **Total Session Storage:** 500MB (configurable via `ARTIFACT_MAX_TOTAL_SIZE_MB`)\n\nThe backend calculates total storage by summing all files under the session's S3 prefix before issuing presigned URLs. This prevents a single session from consuming excessive storage.\n\n## CLI Usage\n\n### Using the UploadArtifact Tool (Recommended)\n\nThe `UploadArtifact` tool is available when running with the beta flag:\n\n```bash\ncn serve --id <agentSessionId> --beta-upload-artifact-tool\n```\n\nAgents can then use the built-in `UploadArtifact` tool to upload files:\n\n```typescript\n// The agent calls this tool with the file path\n{\n  \"name\": \"UploadArtifact\",\n  \"parameters\": {\n    \"filePath\": \"/tmp/screenshot.png\"\n  }\n}\n```\n\nThe tool will:\n\n- Validate the file exists and is an allowed type\n- Check file size limits (50MB max per file)\n- Upload to session storage\n- Return success message or detailed error\n\n**Tool Description:** \"Upload a file (screenshot, video, log) to the session artifacts for user review. Supported formats: images (png, jpg, jpeg, gif, webp), videos (mp4, mov, avi, webm), and text files (log, txt, json, xml, csv, html). Maximum file size: 50MB. **If an artifact with the same filename already exists, it will be overwritten with the new file.**\"\n\n**Requirements:**\n\n- Must run with `--id <agentSessionId>` (agent mode)\n- Must enable `--beta-upload-artifact-tool` flag\n- User must be authenticated (`cn login`)\n\n### Programmatic Upload (Service API)\n\nFor custom implementations, use the service directly:\n\n```typescript\nimport { services } from \"./services/index.js\";\n\nconst result = await services.artifactUpload.uploadArtifact({\n  agentSessionId: process.env.AGENT_SESSION_ID,\n  filePath: \"/tmp/screenshot.png\",\n  accessToken: process.env.CONTINUE_API_KEY,\n});\n\nif (result.success) {\n  console.log(`Uploaded: ${result.filename}`);\n} else {\n  console.error(`Failed: ${result.error}`);\n}\n```\n\n### Multiple Files\n\n```typescript\nconst results = await services.artifactUpload.uploadArtifacts(\n  process.env.AGENT_SESSION_ID,\n  [\"/tmp/screenshot1.png\", \"/tmp/screenshot2.png\", \"/tmp/debug.log\"],\n  process.env.CONTINUE_API_KEY,\n);\n\nresults.forEach((result) => {\n  console.log(`${result.filename}: ${result.success ? \"‚úì\" : \"‚úó\"}`);\n});\n```\n\n## Environment Variables\n\nThe CLI requires these environment variables for artifact uploads:\n\n- `CONTINUE_API_KEY`: Bearer token for backend authentication\n- `CONTINUE_API_BASE`: API base URL (defaults to `https://api.continue.dev/`)\n- `AGENT_SESSION_ID`: The current agent session identifier\n\nThese are automatically provided when running in Continue's devbox environment.\n\n## Error Handling\n\n### Validation Errors (400)\n\n- Invalid filename (path traversal, disallowed extension)\n- File too large (exceeds per-file limit)\n- Storage limit exceeded (session total > 500MB)\n- Invalid content type\n\n### Authentication Errors (401/403)\n\n- Missing or invalid API key\n- User doesn't own the agent session\n\n### Upload Errors (S3)\n\n- Network failure during upload\n- Presigned URL expired (15-minute timeout)\n- S3 service error\n\nAll errors are logged and returned with descriptive messages. Failed uploads don't crash the agent - they return an error result that the agent can handle gracefully.\n\n## Frontend Access\n\nThe frontend can list and download artifacts using:\n\n**List artifacts:**\n\n```http\nGET /agents/{agentSessionId}/artifacts\nAuthorization: Bearer <API_KEY>\n\nResponse:\n{\n  \"artifacts\": [\n    {\n      \"filename\": \"screenshot.png\",\n      \"size\": 1048576,\n      \"sizeFormatted\": \"1.0 MB\",\n      \"lastModified\": \"2025-12-08T10:30:00Z\"\n    }\n  ]\n}\n```\n\n**Download artifact:**\n\n```http\nGET /agents/{agentSessionId}/artifacts/{filename}/download\nAuthorization: Bearer <API_KEY>\n\nResponse:\n{\n  \"url\": \"https://s3.amazonaws.com/...\",\n  \"expiresIn\": 3600\n}\n```\n\nThe frontend then uses the presigned download URL to fetch the artifact directly from S3.\n\n## Security Considerations\n\n1. **Authentication**: All endpoints require valid API keys tied to user/org accounts\n2. **Authorization**: Session ownership is verified before issuing presigned URLs\n3. **Path Traversal Prevention**: Filenames are validated to prevent `../` attacks\n4. **Content Validation**: File types are restricted via extension and MIME type checks\n5. **Rate Limiting**: Storage limits prevent abuse (per-file and total session limits)\n6. **Time-Limited URLs**: Presigned URLs expire after 15 minutes for uploads, 1 hour for downloads\n7. **Storage Isolation**: Files are scoped to user/org prefixes, preventing cross-tenant access\n\n## Design Trade-offs\n\n### Chosen: No Database Tracking\n\n**Decision:** Store artifacts as files in S3 without tracking individual files in the database.\n\n**Rationale:**\n\n- Simpler implementation (no new database tables)\n- Files are the source of truth (no sync issues between DB and S3)\n- Fast listing via S3 API (ListObjects)\n- Automatic cleanup when deleting session folder\n\n**Trade-off:** Cannot query artifacts across sessions or track metadata without scanning S3.\n\n### Chosen: Overwrite on Name Collision\n\n**Decision:** If the same filename is uploaded twice, the last upload wins.\n\n**Rationale:**\n\n- Simpler than versioning or auto-renaming\n- Common use case: agent re-uploading updated screenshot\n- Reduces storage consumption\n\n**Trade-off:** No version history for artifacts.\n\n### Chosen: Direct S3 Upload (Presigned URLs)\n\n**Decision:** Use presigned URLs instead of proxying files through backend.\n\n**Rationale:**\n\n- Better performance (no backend bottleneck)\n- Lower cost (no backend bandwidth charges)\n- Proven pattern (already used in StorageSyncService)\n\n**Trade-off:** Backend cannot inspect file contents before upload (relies on validation at URL generation).\n\n## Future Enhancements\n\n- **Compression**: Automatically compress screenshots/videos before upload\n- **Retention Policies**: Auto-delete artifacts after N days\n- **Artifact Types**: Support more file types (PDFs, archives)\n- **Preview Generation**: Generate thumbnails for images/videos\n- **Streaming**: Support large file uploads with multipart upload\n- **Metadata**: Attach custom metadata to artifacts (tags, descriptions)\n\n---\n\nThis document describes the initial artifact upload implementation. Update it when the architecture evolves.\n"}
{"source":"github","repo":"continue","path":"extensions/cli/docs/storage-sync.md","content":"# Storage Sync Flow for `cn serve`\n\n## Overview\n\nThe `--id <storageId>` flag enables the `cn serve` command to periodically persist session state to an external Continue-managed storage bucket. On startup, the CLI exchanges the provided `storageId` for two pre-signed S3 URLs - one for `session.json` and one for `diff.txt` - and then pushes fresh copies of those files every 30 seconds.\n\nThis document captures the responsibilities for both the CLI and backend components so we can iterate on the feature together.\n\n## CLI Responsibilities\n\n- **Flag plumbing**: When `cn serve` is invoked with `--id <storageId>`, the CLI treats that value as an opaque identifier.\n- **API key auth**: The CLI attaches the user-level Continue API key (same mechanism we already use for other authenticated requests) to backend calls.\n- **Presign handshake**:\n  1. On startup, issue `POST https://api.continue.dev/agents/storage/presigned-url` with JSON payload `{ \"storageId\": \"<storageId>\" }`.\n  2. Expect a response payload containing two pre-signed `PUT` URLs and their target object keys:\n     ```json\n     {\n       \"session\": {\n         \"key\": \"sessions/<sessionId>/session.json\",\n         \"putUrl\": \"https://<s3-host>/...\"\n       },\n       \"diff\": {\n         \"key\": \"sessions/<sessionId>/diff.txt\",\n         \"putUrl\": \"https://<s3-host>/...\"\n       }\n     }\n     ```\n  3. If the call fails, log and continue without remote storage (no retries yet).\n- **Periodic uploads**:\n  - Every 30 seconds (configurable later), serialize the in-memory session to `session.json` and fetch the `/diff` payload to produce `diff.txt`.\n  - Upload both artifacts using their respective `PUT` URLs. For now we overwrite the same objects each cycle.\n  - Errors should be logged but non-fatal; the server keeps running.\n  - If the repo check fails (no git repo or missing `main`), `diff.txt` uploads an empty string and we log the condition once for debugging.\n\n## Backend Responsibilities\n\n- **Endpoint surface**: `POST /agents/storage/presigned-url` accepts a JSON body `{ \"storageId\": string }`.\n- **Authentication**: Leverage the caller's Continue API key (the request arrives with the standard `Authorization: Bearer <apiKey>` header). Apply normal auth/tenant validation so users can only request URLs tied to their account/org.\n- **URL issuance**:\n  - Resolve `storageId` into the desired S3 prefix (e.g., `sessions/<org>/<storageId>/`).\n  - Generate two short-lived pre-signed `PUT` URLs: one for `session.json`, one for `diff.txt`.\n  - Return both URLs and their keys in the response payload described above.\n- **Expiration**: URLs are issued with a 60-minute TTL. The CLI automatically refreshes them before expiry (see URL Refresh Strategy below).\n\n## URL Refresh Strategy\n\nPre-signed URLs are automatically refreshed using a dual-strategy approach:\n\n1. **Proactive Refresh**: URLs are refreshed at the 50-minute mark (10 minutes before expiry) to prevent disruption\n2. **Reactive Refresh**: If a 403 Forbidden error is detected (indicating expired URLs), an immediate refresh is triggered\n3. **Error Handling**: Upload errors are logged but non-fatal; the service continues running with automatic recovery\n\nThis ensures continuous operation during devbox suspension, network interruptions, and clock drift.\n\n## Open Questions & Future Enhancements\n\n- **Upload cadence**: The 30-second interval is hard-coded for now. Consider making it configurable in both CLI and backend policies.\n- **Error telemetry**: Decide if repeated upload failures should trip analytics or circuit breakers.\n- **Diff source**: `diff.txt` currently mirrors the `/diff` endpoint response. Confirm backend expectations for format and size limits.\n- **Security**: We might want to sign responses or enforce stricter scope on `storageId` mapping (e.g., require both org + storageId and validate ownership).\n\n---\n\nThis document should evolve alongside implementation details; update it whenever the API contract or client behavior changes.\n"}
{"source":"github","repo":"continue","path":"extensions/intellij/CONTRIBUTING.md","content":"# Contributing to Continue (JetBrains extension) <!-- omit in toc -->\n\nThis file is for contribution guidelines specific to the JetBrains extension. See the root [\n`CONTRIBUTING.md`](../../CONTRIBUTING.md) for general contribution guidelines.\n\n## Table of Contents <!-- omit in toc -->\n\n- [Architecture Overview](#architecture-overview)\n- [Environment Setup](#environment-setup)\n  - [IDE Installation](#ide-installation)\n  - [IDE configuration](#ide-configuration)\n    - [Recommended plugins](#recommended-plugins)\n  - [Node.js Requirements](#nodejs-requirements)\n  - [Install all dependencies](#install-all-dependencies)\n  - [Misc](#misc)\n- [Development Workflow](#development-workflow)\n  - [Running the extension in debug mode](#running-the-extension-in-debug-mode)\n  - [Accessing files in the `.continue` directory](#accessing-files-in-the-continue-directory)\n  - [Viewing logs](#viewing-logs)\n  - [Reloading changes](#reloading-changes)\n  - [Setting breakpoints](#setting-breakpoints)\n  - [Available Gradle tasks](#available-gradle-tasks)\n  - [Packaging](#packaging)\n    - [Installing the packaged extension](#installing-the-packaged-extension)\n- [Testing](#testing)\n  - [e2e testing](#e2e-testing)\n    - [Overview](#overview)\n    - [Setup](#setup)\n    - [Running the tests](#running-the-tests)\n    - [Identifying selectors](#identifying-selectors)\n    - [Rebuilding the extension](#rebuilding-the-extension)\n\n## Architecture Overview\n\nThis extension shares much of the code with the VS Code extension by utilizing shared code in the `core` directory and\npackaging it in a binary in the `binary` directory. Communication occurs over stdin/stdout.\n\n## Environment Setup\n\n### IDE Installation\n\nContinue is built with JDK version 17 (as specified in [`./build.gradle.kts`](./build.gradle.kts)), which can be\ndownloaded from [Oracle](https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html).\n\nWe recommend using IntelliJ IDEA, which you can download from\nthe [JetBrains website](https://www.jetbrains.com/idea/download).\n\nBoth Ultimate and Community (free) editions are suitable for this project, although Ultimate has better debugging (see\nnotes below).\n\n### IDE configuration\n\n- Enable code formatting on save: `Settings | Tools | Actions on Save | Reformat code`\n\n#### Recommended plugins\n\n- [Thread Access Info](https://plugins.jetbrains.com/plugin/16815-thread-access-info) - adds an extra debug panel\n  showing possible thread access violation (according to Intellij Platform SDK guidelines)\n- [File Expander](https://plugins.jetbrains.com/plugin/11940-file-expander) - allows you to easily preview archives as\n  directories (like `build/distributions/continue-*.zip`)\n\n### Node.js Requirements\n\nThis project requires Node.js version 20.19.0 (LTS) or higher. You have two options for installation:\n\n1. Download and install directly from [nodejs.org](https://nodejs.org/en/download).\n2. If you're using NVM (Node Version Manager), set the correct Node.js version for this project by running `nvm use` in\n   the project root.\n\n### Install all dependencies\n\n- Unix: `./scripts/install-dependencies.sh`\n- Windows: `.\\scripts\\install-dependencies.ps1`\n\n### Misc\n\n- Ensure that you have the Gradle plugin installed\n\n## Development Workflow\n\n### Running the extension in debug mode\n\nSelect the `Run Continue` task in the top right corner of the IDE and then select the \"Debug\" option.\n\n> In community edition, use `Run Continue (CE)` instead, which uses shell scripts instead of Ultimate-only node configs.\n> If you want to debug the core in CE, you'll need to quit the `Start Core Dev Server (CE)` process and run the core in\n> a\n> different environment that supports debugging, such as VS Code (Launch \"Core Binary\").\n\n![run-extension-screenshot](../../media/run-continue-intellij.png)\n\nThis should open a new instance on IntelliJ with the extension installed.\n\n### Accessing files in the `.continue` directory\n\nWhen running the `Start Core Dev Server` task, we set the location of your Continue directory to\n`./extensions/.continue-debug`. This is to\nallow for changes to your `config.json` and other files during development, without affecting your actual configuration.\n\n### Viewing logs\n\nWhen using the `Run Continue` task, we automatically tail both prompt logs and IDE logs.\n\n#### Viewing more IDE logs\n\nYou can selectively increase the log granularity (e.g., debug-level logs) as follows:\n\n- Navigate to `Help | Diagnostic Tools | Debug Log Settings...`\n- Add a line in the format: `com.intellij.diagnostic:debug`\n\nYou can find more information about this feature in [official docs](https://youtrack.jetbrains.com/articles/SUPPORT-A-43/How-to-enable-debug-logging-in-IntelliJ-IDEA).\n\n### Developing `build.plugin.kts`\n\nIf in doubt, check out the\nofficial [IntelliJ Platform Plugin Template](https://github.com/JetBrains/intellij-platform-plugin-template).\nThese templates are the most up-to-date examples of how to correctly customize the plugin build scripts.\n\nAlso, check out\nthe [useful recipes](https://plugins.jetbrains.com/docs/intellij/tools-intellij-platform-gradle-plugin-recipes.html)\nfor common problems.\n\n### Adding new extensions in `plugin.xml`\n\nThere's a tool called [JetBrains Platform Explorer](https://plugins.jetbrains.com/intellij-platform-explorer) that\naggregates plugin metadata and allows you to filter by specific\nextension points. If you're having trouble implementing a feature that's not officially documented,\nyou can learn from other open source plugins.\n\n### Reloading changes\n\n- `extensions/intellij`: Attempt to reload changed classes by selecting\n  _Run | Debugging Actions | Reload Changed Classes`_\n  - This will often fail on new imports, schema changes etc. In that case, you need to stop and restart the extension\n- `gui`: Changes will be reloaded automatically\n- `core`: Run `npm run build -- --os [darwin | linux | win32]` from the `binary` directory (requires\n  restarting the\n  `Start Core Dev Server` task)\n\n### Setting breakpoints\n\n- `extensions/intellij`: Breakpoints can be set in Intellij\n- `gui`: You'll need to set explicit `debugger` statements in the source code, or through the browser dev tools\n- `core`: Breakpoints can be set in Intellij (requires restarting the `Start Core Dev Server` task)\n  - If you have Community Edition installed, you won't be able to use breakpoints in IntelliJ. Instead, you can start\n    the `Core Binary` task in VS Code and set breakpoints in that IDE.\n\n### Available Gradle tasks\n\nTo see the list of Gradle tasks available, you can run the following:\n\n```shell\n./gradlew tasks\n```\n\nA handful of the most relevant tasks are outlined below:\n\n```shell\nbuild - Assembles and tests this project.\nclean - Deletes the build directory.\ndependencies - Displays all dependencies declared in root project 'continue-intellij-extension'\nrunIde - Runs the IDE instance with the developed plugin installed.\nverifyPluginConfiguration - Checks if Java and Kotlin compilers configuration meet IntelliJ SDK requirements\n```\n\n### Packaging\n\n- Unix: `./gradlew buildPlugin`\n- Windows: `./gradlew.bat buildPlugin`\n\nThis will generate a .zip file in `./build/distributions` with the version defined in [\n`./gradle.properties`](./gradle.properties)\n\n#### Installing the packaged extension\n\n- Navigate to the Plugins settings page (_Settings | Plugins_)\n- Click on the gear icon\n- Click _Install from disk_ and select the ZIP file in `./build/distributions`\n\n## Testing\n\nTest commands:\n\n- `./gradlew test` - to run **unit tests**\n- `./gradlew testIntegration` - to run **e2e tests**\n\n### About e2e tests\n\nThe e2e tests are written using [intellij-ide-starter](https://github.com/JetBrains/intellij-ide-starter).\nThe first run of the e2e tests may take a while because the required IDE needs\nto be downloaded. Note that these tests\nfully take control of your mouse while executing.\n\n#### Setup\n\nIf you are on macOS, you'll need to give IntelliJ permission to control your computer in order to run the e2e tests.\nOpen `System Settings > Privacy & Security > Accessibility` and toggle the switch for IntelliJ.\n\n#### Working with Intellij IDE Starter\n\nThe testing platform provides a rich DSL for most UI components in IntelliJ. However, if you want to interact with a\ncustom element, you can define your own XPath selector.\n\nTo do this, run an e2e test and visit [localhost:63343/api/remote-driver/](http://localhost:63343/api/remote-driver/) to\nview an HTML representation of the IDE's Swing component tree.\nSee [Integration Tests](https://plugins.jetbrains.com/docs/intellij/integration-tests-ui.html#searching-components) for\nmore details about this workflow.\n"}
{"source":"github","repo":"continue","path":"extensions/intellij/README.md","content":"<!-- Plugin description -->\n\n<h1 align=\"center\">Continue</h1>\n\n<div align=\"center\">\n\n<a target=\"_blank\" href=\"https://opensource.org/licenses/Apache-2.0\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://docs.continue.dev\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/Continue-docs-%23BE1B55.svg?logo=data:image/svg%2bxml;base64,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\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://discord.gg/vapESyrFmJ\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/discord-join-continue.svg?labelColor=191937&color=6F6FF7&logo=discord\" style=\"height: 22px;\" />\n</a>\n\n<p></p>\n\n<div align=\"center\">\n\n**Ship faster with Continuous AI**\n\n**The future of coding isn't writing more code. It's delegating the boring parts, so you can build the interesting stuff**\n\n</div>\n\nGet started in [Mission Control](https://hub.continue.dev/agents), [CLI (Headless Mode)](https://docs.continue.dev/cli/quick-start#headless-mode), or [CLI (TUI mode)](https://docs.continue.dev/cli/quick-start#tui-mode)\n\n## Agent\n\n[Agent](https://docs.continue.dev/features/agent/quick-start) to work on development tasks together with AI\n\n## Chat\n\n[Chat](https://docs.continue.dev/features/chat/quick-start) to ask general questions and clarify code sections\n\n## Edit\n\n[Edit](https://docs.continue.dev/features/edit/quick-start) to modify a code section without leaving your current file\n\n## Autocomplete\n\n[Autocomplete](https://docs.continue.dev/features/autocomplete/quick-start) to receive inline code suggestions as you type\n\n</div>\n\n## License\n\n[Apache 2.0 ¬© 2023-2025 Continue Dev, Inc.](./LICENSE)\n\n<!-- Plugin description end -->\n"}
{"source":"github","repo":"continue","path":"extensions/intellij/rules.md","content":"# Continue JetBrains Extension\n\n## Project Purpose\n\nJetBrains/IntelliJ extension for Continue AI code agent. Provides chat, autocomplete, inline edit, and agent features within JetBrains IDEs.\n\n## Architecture\n\n- **Language**: Kotlin (JDK 17), Gradle build\n- **Communication**: stdin/stdout message passing with core binary from `../../binary`\n- **UI**: Embeds React webview from `../../gui`\n- **Platform**: IntelliJ Platform Plugin (IDEA, PyCharm, WebStorm, etc.)\n\n## Key Source Structure\n\n```\nsrc/main/kotlin/com/github/continuedev/continueintellijextension/\n‚îú‚îÄ‚îÄ continue/         # Core integration (CoreMessenger, IntelliJIde, IdeProtocolClient)\n‚îú‚îÄ‚îÄ autocomplete/     # Code completion logic\n‚îú‚îÄ‚îÄ editor/          # Diff handling, inline edits\n‚îú‚îÄ‚îÄ toolWindow/      # Main UI panel\n‚îú‚îÄ‚îÄ services/        # Settings, plugin lifecycle\n‚îú‚îÄ‚îÄ actions/         # Keyboard shortcuts, menu actions\n‚îú‚îÄ‚îÄ protocol/        # Message type definitions\n‚îî‚îÄ‚îÄ constants/       # App constants, paths\n\nsrc/main/resources/\n‚îú‚îÄ‚îÄ META-INF/plugin.xml  # Plugin configuration\n‚îî‚îÄ‚îÄ webview/            # Embedded React UI assets\n```\n\n## Core Files\n\n- `IntelliJIde.kt`: Main IDE interface implementation\n- `CoreMessenger.kt`: Binary communication handler\n- `plugin.xml`: Plugin manifest and extension points\n- `build.gradle.kts`: Build configuration\n- `ContinuePluginService.kt`: Main service orchestrator\n\n## Message Protocol\n\nJSON messages between Extension ‚Üî Core ‚Üî GUI. Message types in `constants/MessageTypes.kt`. Extension relays messages between core binary and webview.\n\n## Testing\n\n- Unit tests: `src/test/kotlin/`\n- E2E tests: UI automation with intellij-ui-test-robot\n- Test command: `./gradlew test`\n- Debug: `runIde` Gradle task\n\n## Key Integration Points\n\n- File operations via IntelliJ VFS\n- Editor integration for diffs/autocomplete\n- Git operations for repository context\n- Settings via IntelliJ platform storage\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/e2e/README.md","content":"# E2E Tests\n\n## Setup\n\nWhen running e2e tests for the first time\n\n```bash\nnpm run e2e:all\n```\n\n## Run\n\nDepending on what code you update, you can use a faster loop to test your changes:\n\n- If you update the e2e test code and/or config.yaml/json, you can run `npm run e2e:quick`\n- If you update the extension code, you can run `npm run e2e:recompile`\n- If you update the gui code, you can run `npm run e2e:rebuild-gui`\n\n## Writing tests\n\nAll e2e tests are separated (by folder) into\n\n- `selectors` - functions that return elements\n- `actions` - functions that perform actions on the editor\n- `tests` - the actual tests, which are typically longer paths of functionality rather than individual actions\n\n### Why are my tests failing?\n\n- Did you place a `data-testid` on a React component instead of an actual HTML element?\n- Do you have a config.yaml locally but the test is failing when running with config.json in CI?\n- Is your `data-testid` or selector actually just wrong?\n- Are you inconsistently getting different behaviors? You can try adding a `TestUtils.waitForTimeout` between two events if you think it's caused by a race condition. Note that this may lead to flake down the road.\n- Alternatively, you can add a `TestUtils.waitForSuccess`\n\n### How is it connecting to the LLM?\n\n- This depends on the model provider written inside the `test-continue/config.json` and `test-continue/config.yaml`. If the model provider is `mock`, it will use `MockLLM`, and if it uses `test`, it will use `TestLLM`.\n\n### Quirks\n\n- For non-macOS systems, `codesign` is not available. You should run `npm run e2e:all-non-mac` instead of `npm run e2e:all`.\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/e2e/tests/TODO.md","content":"TODO testing scenarios\n\n- Highlighting code and pressing CMD+L _before_ the GUI was first opened. The highlighted code should be included as chat input.\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/vsc-extension-quickstart.md","content":"# Welcome to the Continue Visual Studio Code Extension\n\n## Get up and running straight away\n\n- Open the root of the continue repository in visual studio code\n- Press `F5` to open a new window with your extension loaded.\n- The continue in the launched vs code instance uses `extensions/.continue-debug` as configuration folder\n- Find output from your extension in the debug console.\n\n## Make changes\n\n- You can relaunch the extension from the debug toolbar after changing code.\n- You can also reload (`Ctrl+R` or `Cmd+R` on Mac) the VS Code window with your extension to load your changes.\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/CONTRIBUTING.md","content":"# Continue VS Code Extension\n\nThis is the Continue VS Code Extension. Its primary jobs are\n\n1. Implement the IDE side of the Continue IDE protocol, allowing a Continue server to interact natively in an IDE. This happens in `src/continueIdeClient.ts`.\n2. Open the Continue React app in a side panel. The React app's source code lives in the `gui` directory. The panel is opened by the `continue.openContinueGUI` command, as defined in `src/commands.ts`.\n\n# How to run the extension\n\nSee [Environment Setup](../../CONTRIBUTING.md#environment-setup)\n\n# How to run and debug tests\n\nAfter following the setup in [Environment Setup](../../CONTRIBUTING.md#environment-setup) you can run the `Extension (VSCode)` launch configuration in VS Code.\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/README.md","content":"<div align=\"center\">\n\n![Continue logo](media/readme.png)\n\n<div align=\"center\">\n\n<a target=\"_blank\" href=\"https://opensource.org/licenses/Apache-2.0\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://docs.continue.dev\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/Continue-docs-%23BE1B55.svg?logo=data:image/svg%2bxml;base64,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\" style=\"height: 22px;\" />\n</a>\n<a target=\"_blank\" href=\"https://discord.gg/vapESyrFmJ\" style=\"background:none\">\n    <img src=\"https://img.shields.io/badge/discord-join-continue.svg?labelColor=191937&color=6F6FF7&logo=discord\" style=\"height: 22px;\" />\n</a>\n\n<p></p>\n\n</div>\n\n<h1 align=\"center\">Continue</h1>\n\n<div align=\"center\">\n\n**Ship faster with Continuous AI**\n\n**Build and run custom agents across your IDE, terminal, and CI**\n\n</div>\n\n## Agent\n\n[Agent](https://docs.continue.dev/features/agent/quick-start) to work on development tasks together with AI\n\n![agent](docs/images/agent.gif)\n\n## Chat\n\n[Chat](https://docs.continue.dev/features/chat/quick-start) to ask general questions and clarify code sections\n\n![chat](docs/images/chat.gif)\n\n## Edit\n\n[Edit](https://docs.continue.dev/features/edit/quick-start) to modify a code section without leaving your current file\n\n![edit](docs/images/edit.gif)\n\n## Autocomplete\n\n[Autocomplete](https://docs.continue.dev/features/autocomplete/quick-start) to receive inline code suggestions as you type\n\n![autocomplete](docs/images/autocomplete.gif)\n\n</div>\n\n## License\n\n[Apache 2.0 ¬© 2023-2025 Continue Dev, Inc.](./LICENSE)\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/models/README.md","content":"all-MiniLM-L6-v2 is the sentence transformers model used with transformers.js to locally generate codebase embeddings.\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/models/all-MiniLM-L6-v2/README.md","content":"---\nlibrary_name: \"transformers.js\"\n---\n\nhttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 with ONNX weights to be compatible with Transformers.js.\n\n## Usage (Transformers.js)\n\nIf you haven't already, you can install the [Transformers.js](https://huggingface.co/docs/transformers.js) JavaScript library from [NPM](https://www.npmjs.com/package/@xenova/transformers) using:\n\n```bash\nnpm i @xenova/transformers\n```\n\nYou can then use the model to compute embeddings like this:\n\n```js\nimport { pipeline } from \"@xenova/transformers\";\n\n// Create a feature-extraction pipeline\nconst extractor = await pipeline(\n  \"feature-extraction\",\n  \"Xenova/all-MiniLM-L6-v2\",\n);\n\n// Compute sentence embeddings\nconst sentences = [\"This is an example sentence\", \"Each sentence is converted\"];\nconst output = await extractor(sentences, { pooling: \"mean\", normalize: true });\nconsole.log(output);\n// Tensor {\n//   dims: [ 2, 384 ],\n//   type: 'float32',\n//   data: Float32Array(768) [ 0.04592696577310562, 0.07328180968761444, ... ],\n//   size: 768\n// }\n```\n\nYou can convert this Tensor to a nested JavaScript array using `.tolist()`:\n\n```js\nconsole.log(output.tolist());\n// [\n//   [ 0.04592696577310562, 0.07328180968761444, 0.05400655046105385, ... ],\n//   [ 0.08188057690858841, 0.10760223120450974, -0.013241755776107311, ... ]\n// ]\n```\n\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using [ü§ó Optimum](https://huggingface.co/docs/optimum/index) and structuring your repo like this one (with ONNX weights located in a subfolder named `onnx`).\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/media/move-chat-panel-right.md","content":"# Move Chat to the right sidebar\n\n### This will prevent the Chat panel from covering your file explorer\n\n![Move Continue to right sidebar](./move-to-right-sidebar.gif)\n"}
{"source":"github","repo":"continue","path":"extensions/vscode/rules.md","content":"The `extensions/vscode` folder contains code for the \"Continue\" VS Code Extension. Continue is a coding extension which extends IDE functionality with AI.\nCore IDE functionality, such as reading files, is implemented using the `VSCodeIDE` class. Code to enable editing/diff streaming into the editor, code suggestion autocompletion, and more can also be found in this directory.\nShared code that has abstract logic for the extension is in `core/`. Avoid importing code from `core/` directly where possible. Core is designed to be bundalable as a binary. Use `core.invoke` to send messages to the\n\n- see `core/protocol/core.ts` for a list of commands VS Code can post/request to Core\n- see `core/portocol/ide.ts` for a list of commands/message types core can post/request to VS Code\n"}
