
{"source":"github","repo":"mcp-python-sdk","path":"examples/clients/conformance-auth-client/README.md","content":"# MCP Conformance Auth Client\n\nA Python OAuth client designed for use with the MCP conformance test framework.\n\n## Overview\n\nThis client implements OAuth authentication for MCP and is designed to work automatically with the conformance test framework without requiring user interaction. It programmatically fetches authorization URLs and extracts auth codes from redirects.\n\n## Installation\n\n```bash\ncd examples/clients/conformance-auth-client\nuv sync\n```\n\n## Usage with Conformance Tests\n\nRun the auth conformance tests against this Python client:\n\n```bash\n# From the conformance repository\nnpx @modelcontextprotocol/conformance client \\\n  --command \"uv run --directory /path/to/python-sdk/examples/clients/conformance-auth-client python -m mcp_conformance_auth_client\" \\\n  --scenario auth/basic-dcr\n```\n\nAvailable auth test scenarios:\n\n- `auth/basic-dcr` - Tests OAuth Dynamic Client Registration flow\n- `auth/basic-metadata-var1` - Tests OAuth with authorization metadata\n\n## How It Works\n\nUnlike interactive OAuth clients that open a browser for user authentication, this client:\n\n1. Receives the authorization URL from the OAuth provider\n2. Makes an HTTP request to that URL directly (without following redirects)\n3. Extracts the authorization code from the redirect response\n4. Uses the code to complete the OAuth token exchange\n\nThis allows the conformance test framework's mock OAuth server to automatically provide auth codes without human interaction.\n\n## Direct Usage\n\nYou can also run the client directly:\n\n```bash\nuv run python -m mcp_conformance_auth_client http://localhost:3000/mcp\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/clients/simple-auth-client/README.md","content":"# Simple Auth Client Example\n\nA demonstration of how to use the MCP Python SDK with OAuth authentication over streamable HTTP or SSE transport.\n\n## Features\n\n- OAuth 2.0 authentication with PKCE\n- Support for both StreamableHTTP and SSE transports\n- Interactive command-line interface\n\n## Installation\n\n```bash\ncd examples/clients/simple-auth-client\nuv sync --reinstall \n```\n\n## Usage\n\n### 1. Start an MCP server with OAuth support\n\n```bash\n# Example with mcp-simple-auth\ncd path/to/mcp-simple-auth\nuv run mcp-simple-auth --transport streamable-http --port 3001\n```\n\n### 2. Run the client\n\n```bash\nuv run mcp-simple-auth-client\n\n# Or with custom server URL\nMCP_SERVER_PORT=3001 uv run mcp-simple-auth-client\n\n# Use SSE transport\nMCP_TRANSPORT_TYPE=sse uv run mcp-simple-auth-client\n```\n\n### 3. Complete OAuth flow\n\nThe client will open your browser for authentication. After completing OAuth, you can use commands:\n\n- `list` - List available tools\n- `call <tool_name> [args]` - Call a tool with optional JSON arguments  \n- `quit` - Exit\n\n## Example\n\n```markdown\nðŸ” Simple MCP Auth Client\nConnecting to: http://localhost:3001\n\nPlease visit the following URL to authorize the application:\nhttp://localhost:3001/authorize?response_type=code&client_id=...\n\nâœ… Connected to MCP server at http://localhost:3001\n\nmcp> list\nðŸ“‹ Available tools:\n1. echo - Echo back the input text\n\nmcp> call echo {\"text\": \"Hello, world!\"}\nðŸ”§ Tool 'echo' result:\nHello, world!\n\nmcp> quit\nðŸ‘‹ Goodbye!\n```\n\n## Configuration\n\n- `MCP_SERVER_PORT` - Server URL (default: 8000)\n- `MCP_TRANSPORT_TYPE` - Transport type: `streamable-http` (default) or `sse`\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/clients/sse-polling-client/README.md","content":"# MCP SSE Polling Demo Client\n\nDemonstrates client-side auto-reconnect for the SSE polling pattern (SEP-1699).\n\n## Features\n\n- Connects to SSE polling demo server\n- Automatically reconnects when server closes SSE stream\n- Resumes from Last-Event-ID to avoid missing messages\n- Respects server-provided retry interval\n\n## Usage\n\n```bash\n# First start the server:\nuv run mcp-sse-polling-demo --port 3000\n\n# Then run this client:\nuv run mcp-sse-polling-client --url http://localhost:3000/mcp\n\n# Custom options:\nuv run mcp-sse-polling-client --url http://localhost:3000/mcp --items 20 --checkpoint-every 5\n```\n\n## Options\n\n- `--url`: Server URL (default: <http://localhost:3000/mcp>)\n- `--items`: Number of items to process (default: 10)\n- `--checkpoint-every`: Checkpoint interval (default: 3)\n- `--log-level`: Logging level (default: DEBUG)\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/clients/simple-task-client/README.md","content":"# Simple Task Client\n\nA minimal MCP client demonstrating polling for task results over streamable HTTP.\n\n## Running\n\nFirst, start the simple-task server in another terminal:\n\n```bash\ncd examples/servers/simple-task\nuv run mcp-simple-task\n```\n\nThen run the client:\n\n```bash\ncd examples/clients/simple-task-client\nuv run mcp-simple-task-client\n```\n\nUse `--url` to connect to a different server.\n\n## What it does\n\n1. Connects to the server via streamable HTTP\n2. Calls the `long_running_task` tool as a task\n3. Polls the task status until completion\n4. Retrieves and prints the result\n\n## Expected output\n\n```text\nAvailable tools: ['long_running_task']\n\nCalling tool as a task...\nTask created: <task-id>\n  Status: working - Starting work...\n  Status: working - Processing step 1...\n  Status: working - Processing step 2...\n  Status: completed -\n\nResult: Task completed!\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/clients/simple-task-interactive-client/README.md","content":"# Simple Interactive Task Client\n\nA minimal MCP client demonstrating responses to interactive tasks (elicitation and sampling).\n\n## Running\n\nFirst, start the interactive task server in another terminal:\n\n```bash\ncd examples/servers/simple-task-interactive\nuv run mcp-simple-task-interactive\n```\n\nThen run the client:\n\n```bash\ncd examples/clients/simple-task-interactive-client\nuv run mcp-simple-task-interactive-client\n```\n\nUse `--url` to connect to a different server.\n\n## What it does\n\n1. Connects to the server via streamable HTTP\n2. Calls `confirm_delete` - server asks for confirmation, client responds via terminal\n3. Calls `write_haiku` - server requests LLM completion, client returns a hardcoded haiku\n\n## Key concepts\n\n### Elicitation callback\n\n```python\nasync def elicitation_callback(context, params) -> ElicitResult:\n    # Handle user input request from server\n    return ElicitResult(action=\"accept\", content={\"confirm\": True})\n```\n\n### Sampling callback\n\n```python\nasync def sampling_callback(context, params) -> CreateMessageResult:\n    # Handle LLM completion request from server\n    return CreateMessageResult(model=\"...\", role=\"assistant\", content=...)\n```\n\n### Using call_tool_as_task\n\n```python\n# Call a tool as a task (returns immediately with task reference)\nresult = await session.experimental.call_tool_as_task(\"tool_name\", {\"arg\": \"value\"})\ntask_id = result.task.taskId\n\n# Get result - this delivers elicitation/sampling requests and blocks until complete\nfinal = await session.experimental.get_task_result(task_id, CallToolResult)\n```\n\n**Important**: The `get_task_result()` call is what triggers the delivery of elicitation\nand sampling requests to your callbacks. It blocks until the task completes and returns\nthe final result.\n\n## Expected output\n\n```text\nAvailable tools: ['confirm_delete', 'write_haiku']\n\n--- Demo 1: Elicitation ---\nCalling confirm_delete tool...\nTask created: <task-id>\n\n[Elicitation] Server asks: Are you sure you want to delete 'important.txt'?\nYour response (y/n): y\n[Elicitation] Responding with: confirm=True\nResult: Deleted 'important.txt'\n\n--- Demo 2: Sampling ---\nCalling write_haiku tool...\nTask created: <task-id>\n\n[Sampling] Server requests LLM completion for: Write a haiku about autumn leaves\n[Sampling] Responding with haiku\nResult:\nHaiku:\nCherry blossoms fall\nSoftly on the quiet pond\nSpring whispers goodbye\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/README.md","content":"# Python SDK Examples\n\nThis folders aims to provide simple examples of using the Python SDK. Please refer to the\n[servers repository](https://github.com/modelcontextprotocol/servers)\nfor real-world servers.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-streamablehttp-stateless/README.md","content":"# MCP Simple StreamableHttp Stateless Server Example\n\nA stateless MCP server example demonstrating the StreamableHttp transport without maintaining session state. This example is ideal for understanding how to deploy MCP servers in multi-node environments where requests can be routed to any instance.\n\n## Features\n\n- Uses the StreamableHTTP transport in stateless mode (mcp_session_id=None)\n- Each request creates a new ephemeral connection\n- No session state maintained between requests\n- Task lifecycle scoped to individual requests\n- Suitable for deployment in multi-node environments\n\n## Usage\n\nStart the server:\n\n```bash\n# Using default port 3000\nuv run mcp-simple-streamablehttp-stateless\n\n# Using custom port\nuv run mcp-simple-streamablehttp-stateless --port 3000\n\n# Custom logging level\nuv run mcp-simple-streamablehttp-stateless --log-level DEBUG\n\n# Enable JSON responses instead of SSE streams\nuv run mcp-simple-streamablehttp-stateless --json-response\n```\n\nThe server exposes a tool named \"start-notification-stream\" that accepts three arguments:\n\n- `interval`: Time between notifications in seconds (e.g., 1.0)\n- `count`: Number of notifications to send (e.g., 5)\n- `caller`: Identifier string for the caller\n\n## Client\n\nYou can connect to this server using an HTTP client. For now, only the TypeScript SDK has streamable HTTP client examples, or you can use [Inspector](https://github.com/modelcontextprotocol/inspector) for testing.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-auth/README.md","content":"# MCP OAuth Authentication Demo\n\nThis example demonstrates OAuth 2.0 authentication with the Model Context Protocol using **separate Authorization Server (AS) and Resource Server (RS)** to comply with the new RFC 9728 specification.\n\n---\n\n## Running the Servers\n\n### Step 1: Start Authorization Server\n\n```bash\n# Navigate to the simple-auth directory\ncd examples/servers/simple-auth\n\n# Start Authorization Server on port 9000\nuv run mcp-simple-auth-as --port=9000\n```\n\n**What it provides:**\n\n- OAuth 2.0 flows (registration, authorization, token exchange)\n- Simple credential-based authentication (no external provider needed)  \n- Token introspection endpoint for Resource Servers (`/introspect`)\n\n---\n\n### Step 2: Start Resource Server (MCP Server)\n\n```bash\n# In another terminal, navigate to the simple-auth directory\ncd examples/servers/simple-auth\n\n# Start Resource Server on port 8001, connected to Authorization Server\nuv run mcp-simple-auth-rs --port=8001 --auth-server=http://localhost:9000  --transport=streamable-http\n\n# With RFC 8707 strict resource validation (recommended for production)\nuv run mcp-simple-auth-rs --port=8001 --auth-server=http://localhost:9000  --transport=streamable-http --oauth-strict\n\n```\n\n### Step 3: Test with Client\n\n```bash\ncd examples/clients/simple-auth-client\n# Start client with streamable HTTP\nMCP_SERVER_PORT=8001 MCP_TRANSPORT_TYPE=streamable-http uv run mcp-simple-auth-client\n```\n\n## How It Works\n\n### RFC 9728 Discovery\n\n**Client â†’ Resource Server:**\n\n```bash\ncurl http://localhost:8001/.well-known/oauth-protected-resource\n```\n\n```json\n{\n  \"resource\": \"http://localhost:8001\",\n  \"authorization_servers\": [\"http://localhost:9000\"]\n}\n```\n\n**Client â†’ Authorization Server:**\n\n```bash\ncurl http://localhost:9000/.well-known/oauth-authorization-server\n```\n\n```json\n{\n  \"issuer\": \"http://localhost:9000\",\n  \"authorization_endpoint\": \"http://localhost:9000/authorize\",\n  \"token_endpoint\": \"http://localhost:9000/token\"\n}\n```\n\n## Legacy MCP Server as Authorization Server (Backwards Compatibility)\n\nFor backwards compatibility with older MCP implementations, a legacy server is provided that acts as an Authorization Server (following the old spec where MCP servers could optionally provide OAuth):\n\n### Running the Legacy Server\n\n```bash\n# Start legacy authorization server on port 8002\nuv run mcp-simple-auth-legacy --port=8002\n```\n\n**Differences from the new architecture:**\n\n- **MCP server acts as AS:** The MCP server itself provides OAuth endpoints (old spec behavior)\n- **No separate RS:** The server handles both authentication and MCP tools\n- **Local token validation:** Tokens are validated internally without introspection\n- **No RFC 9728 support:** Does not provide `/.well-known/oauth-protected-resource`\n- **Direct OAuth discovery:** OAuth metadata is at the MCP server's URL\n\n### Testing with Legacy Server\n\n```bash\n# Test with client (will automatically fall back to legacy discovery)\ncd examples/clients/simple-auth-client\nMCP_SERVER_PORT=8002 MCP_TRANSPORT_TYPE=streamable-http uv run mcp-simple-auth-client\n```\n\nThe client will:\n\n1. Try RFC 9728 discovery at `/.well-known/oauth-protected-resource` (404 on legacy server)\n2. Fall back to direct OAuth discovery at `/.well-known/oauth-authorization-server`\n3. Complete authentication with the MCP server acting as its own AS\n\nThis ensures existing MCP servers (which could optionally act as Authorization Servers under the old spec) continue to work while the ecosystem transitions to the new architecture where MCP servers are Resource Servers only.\n\n## Manual Testing\n\n### Test Discovery\n\n```bash\n# Test Resource Server discovery endpoint (new architecture)\ncurl -v http://localhost:8001/.well-known/oauth-protected-resource\n\n# Test Authorization Server metadata\ncurl -v http://localhost:9000/.well-known/oauth-authorization-server\n```\n\n### Test Token Introspection\n\n```bash\n# After getting a token through OAuth flow:\ncurl -X POST http://localhost:9000/introspect \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"token=your_access_token\"\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-streamablehttp/README.md","content":"# MCP Simple StreamableHttp Server Example\n\nA simple MCP server example demonstrating the StreamableHttp transport, which enables HTTP-based communication with MCP servers using streaming.\n\n## Features\n\n- Uses the StreamableHTTP transport for server-client communication\n- Supports REST API operations (POST, GET, DELETE) for `/mcp` endpoint\n- Task management with anyio task groups\n- Ability to send multiple notifications over time to the client\n- Proper resource cleanup and lifespan management\n- Resumability support via InMemoryEventStore\n\n## Usage\n\nStart the server on the default or custom port:\n\n```bash\n\n# Using custom port\nuv run mcp-simple-streamablehttp --port 3000\n\n# Custom logging level\nuv run mcp-simple-streamablehttp --log-level DEBUG\n\n# Enable JSON responses instead of SSE streams\nuv run mcp-simple-streamablehttp --json-response\n```\n\nThe server exposes a tool named \"start-notification-stream\" that accepts three arguments:\n\n- `interval`: Time between notifications in seconds (e.g., 1.0)\n- `count`: Number of notifications to send (e.g., 5)\n- `caller`: Identifier string for the caller\n\n## Resumability Support\n\nThis server includes resumability support through the InMemoryEventStore. This enables clients to:\n\n- Reconnect to the server after a disconnection\n- Resume event streaming from where they left off using the Last-Event-ID header\n\nThe server will:\n\n- Generate unique event IDs for each SSE message\n- Store events in memory for later replay\n- Replay missed events when a client reconnects with a Last-Event-ID header\n\nNote: The InMemoryEventStore is designed for demonstration purposes only. For production use, consider implementing a persistent storage solution.\n\n## Client\n\nYou can connect to this server using an HTTP client, for now only Typescript SDK has streamable HTTP client examples or you can use [Inspector](https://github.com/modelcontextprotocol/inspector)\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-task-interactive/README.md","content":"# Simple Interactive Task Server\n\nA minimal MCP server demonstrating interactive tasks with elicitation and sampling.\n\n## Running\n\n```bash\ncd examples/servers/simple-task-interactive\nuv run mcp-simple-task-interactive\n```\n\nThe server starts on `http://localhost:8000/mcp` by default. Use `--port` to change.\n\n## What it does\n\nThis server exposes two tools:\n\n### `confirm_delete` (demonstrates elicitation)\n\nAsks the user for confirmation before \"deleting\" a file.\n\n- Uses `task.elicit()` to request user input\n- Shows the elicitation flow: task -> input_required -> response -> complete\n\n### `write_haiku` (demonstrates sampling)\n\nAsks the LLM to write a haiku about a topic.\n\n- Uses `task.create_message()` to request LLM completion\n- Shows the sampling flow: task -> input_required -> response -> complete\n\n## Usage with the client\n\nIn one terminal, start the server:\n\n```bash\ncd examples/servers/simple-task-interactive\nuv run mcp-simple-task-interactive\n```\n\nIn another terminal, run the interactive client:\n\n```bash\ncd examples/clients/simple-task-interactive-client\nuv run mcp-simple-task-interactive-client\n```\n\n## Expected server output\n\nWhen a client connects and calls the tools, you'll see:\n\n```text\nStarting server on http://localhost:8000/mcp\n\n[Server] confirm_delete called for 'important.txt'\n[Server] Task created: <task-id>\n[Server] Sending elicitation request to client...\n[Server] Received elicitation response: action=accept, content={'confirm': True}\n[Server] Completing task with result: Deleted 'important.txt'\n\n[Server] write_haiku called for topic 'autumn leaves'\n[Server] Task created: <task-id>\n[Server] Sending sampling request to client...\n[Server] Received sampling response: Cherry blossoms fall\nSoftly on the quiet pon...\n[Server] Completing task with haiku\n```\n\n## Key concepts\n\n1. **ServerTaskContext**: Provides `elicit()` and `create_message()` for user interaction\n2. **run_task()**: Spawns background work, auto-completes/fails, returns immediately\n3. **TaskResultHandler**: Delivers queued messages and routes responses\n4. **Response routing**: Responses are routed back to waiting resolvers\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-pagination/README.md","content":"# MCP Simple Pagination\n\nA simple MCP server demonstrating pagination for tools, resources, and prompts using cursor-based pagination.\n\n## Usage\n\nStart the server using either stdio (default) or SSE transport:\n\n```bash\n# Using stdio transport (default)\nuv run mcp-simple-pagination\n\n# Using SSE transport on custom port\nuv run mcp-simple-pagination --transport sse --port 8000\n```\n\nThe server exposes:\n\n- 25 tools (paginated, 5 per page)\n- 30 resources (paginated, 10 per page)\n- 20 prompts (paginated, 7 per page)\n\nEach paginated list returns a `nextCursor` when more pages are available. Use this cursor in subsequent requests to retrieve the next page.\n\n## Example\n\nUsing the MCP client, you can retrieve paginated items like this using the STDIO transport:\n\n```python\nimport asyncio\nfrom mcp.client.session import ClientSession\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\n\n\nasync def main():\n    async with stdio_client(\n        StdioServerParameters(command=\"uv\", args=[\"run\", \"mcp-simple-pagination\"])\n    ) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # Get first page of tools\n            tools_page1 = await session.list_tools()\n            print(f\"First page: {len(tools_page1.tools)} tools\")\n            print(f\"Next cursor: {tools_page1.nextCursor}\")\n\n            # Get second page using cursor\n            if tools_page1.nextCursor:\n                tools_page2 = await session.list_tools(cursor=tools_page1.nextCursor)\n                print(f\"Second page: {len(tools_page2.tools)} tools\")\n\n            # Similarly for resources\n            resources_page1 = await session.list_resources()\n            print(f\"First page: {len(resources_page1.resources)} resources\")\n\n            # And for prompts\n            prompts_page1 = await session.list_prompts()\n            print(f\"First page: {len(prompts_page1.prompts)} prompts\")\n\n\nasyncio.run(main())\n```\n\n## Pagination Details\n\nThe server uses simple numeric indices as cursors for demonstration purposes. In production scenarios, you might use:\n\n- Database offsets or row IDs\n- Timestamps for time-based pagination\n- Opaque tokens encoding pagination state\n\nThe pagination implementation demonstrates:\n\n- Handling `None` cursor for the first page\n- Returning `nextCursor` when more data exists\n- Gracefully handling invalid cursors\n- Different page sizes for different resource types\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/everything-server/README.md","content":"# MCP Everything Server\n\nA comprehensive MCP server implementing all protocol features for conformance testing.\n\n## Overview\n\nThe Everything Server is a reference implementation that demonstrates all features of the Model Context Protocol (MCP). It is designed to be used with the [MCP Conformance Test Framework](https://github.com/modelcontextprotocol/conformance) to validate MCP client and server implementations.\n\n## Installation\n\nFrom the python-sdk root directory:\n\n```bash\nuv sync --frozen\n```\n\n## Usage\n\n### Running the Server\n\nStart the server with default settings (port 3001):\n\n```bash\nuv run -m mcp_everything_server\n```\n\nOr with custom options:\n\n```bash\nuv run -m mcp_everything_server --port 3001 --log-level DEBUG\n```\n\nThe server will be available at: `http://localhost:3001/mcp`\n\n### Command-Line Options\n\n- `--port` - Port to listen on (default: 3001)\n- `--log-level` - Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)\n\n## Running Conformance Tests\n\nSee the [MCP Conformance Test Framework](https://github.com/modelcontextprotocol/conformance) for instructions on running conformance tests against this server.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/sse-polling-demo/README.md","content":"# MCP SSE Polling Demo Server\n\nDemonstrates the SSE polling pattern with server-initiated stream close for long-running tasks (SEP-1699).\n\n## Features\n\n- Priming events (automatic with EventStore)\n- Server-initiated stream close via `close_sse_stream()` callback\n- Client auto-reconnect with Last-Event-ID\n- Progress notifications during long-running tasks\n- Configurable retry interval\n\n## Usage\n\n```bash\n# Start server on default port\nuv run mcp-sse-polling-demo --port 3000\n\n# Custom retry interval (milliseconds)\nuv run mcp-sse-polling-demo --port 3000 --retry-interval 100\n```\n\n## Tool: process_batch\n\nProcesses items with periodic checkpoints that trigger SSE stream closes:\n\n- `items`: Number of items to process (1-100, default: 10)\n- `checkpoint_every`: Close stream after this many items (1-20, default: 3)\n\n## Client\n\nUse the companion `mcp-sse-polling-client` to test:\n\n```bash\nuv run mcp-sse-polling-client --url http://localhost:3000/mcp\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-tool/README.md","content":"\nA simple MCP server that exposes a website fetching tool.\n\n## Usage\n\nStart the server using either stdio (default) or SSE transport:\n\n```bash\n# Using stdio transport (default)\nuv run mcp-simple-tool\n\n# Using SSE transport on custom port\nuv run mcp-simple-tool --transport sse --port 8000\n```\n\nThe server exposes a tool named \"fetch\" that accepts one required argument:\n\n- `url`: The URL of the website to fetch\n\n## Example\n\nUsing the MCP client, you can use the tool like this using the STDIO transport:\n\n```python\nimport asyncio\nfrom mcp.client.session import ClientSession\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\n\n\nasync def main():\n    async with stdio_client(\n        StdioServerParameters(command=\"uv\", args=[\"run\", \"mcp-simple-tool\"])\n    ) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # List available tools\n            tools = await session.list_tools()\n            print(tools)\n\n            # Call the fetch tool\n            result = await session.call_tool(\"fetch\", {\"url\": \"https://example.com\"})\n            print(result)\n\n\nasyncio.run(main())\n\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-resource/README.md","content":"# MCP Simple Resource\n\nA simple MCP server that exposes sample text files as resources.\n\n## Usage\n\nStart the server using either stdio (default) or SSE transport:\n\n```bash\n# Using stdio transport (default)\nuv run mcp-simple-resource\n\n# Using SSE transport on custom port\nuv run mcp-simple-resource --transport sse --port 8000\n```\n\nThe server exposes some basic text file resources that can be read by clients.\n\n## Example\n\nUsing the MCP client, you can retrieve resources like this using the STDIO transport:\n\n```python\nimport asyncio\nfrom mcp.types import AnyUrl\nfrom mcp.client.session import ClientSession\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\n\n\nasync def main():\n    async with stdio_client(\n        StdioServerParameters(command=\"uv\", args=[\"run\", \"mcp-simple-resource\"])\n    ) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # List available resources\n            resources = await session.list_resources()\n            print(resources)\n\n            # Get a specific resource\n            resource = await session.read_resource(AnyUrl(\"file:///greeting.txt\"))\n            print(resource)\n\n\nasyncio.run(main())\n\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-task/README.md","content":"# Simple Task Server\n\nA minimal MCP server demonstrating the experimental tasks feature over streamable HTTP.\n\n## Running\n\n```bash\ncd examples/servers/simple-task\nuv run mcp-simple-task\n```\n\nThe server starts on `http://localhost:8000/mcp` by default. Use `--port` to change.\n\n## What it does\n\nThis server exposes a single tool `long_running_task` that:\n\n1. Must be called as a task (with `task` metadata in the request)\n2. Takes ~3 seconds to complete\n3. Sends status updates during execution\n4. Returns a result when complete\n\n## Usage with the client\n\nIn one terminal, start the server:\n\n```bash\ncd examples/servers/simple-task\nuv run mcp-simple-task\n```\n\nIn another terminal, run the client:\n\n```bash\ncd examples/clients/simple-task-client\nuv run mcp-simple-task-client\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"examples/servers/simple-prompt/README.md","content":"# MCP Simple Prompt\n\nA simple MCP server that exposes a customizable prompt template with optional context and topic parameters.\n\n## Usage\n\nStart the server using either stdio (default) or SSE transport:\n\n```bash\n# Using stdio transport (default)\nuv run mcp-simple-prompt\n\n# Using SSE transport on custom port\nuv run mcp-simple-prompt --transport sse --port 8000\n```\n\nThe server exposes a prompt named \"simple\" that accepts two optional arguments:\n\n- `context`: Additional context to consider\n- `topic`: Specific topic to focus on\n\n## Example\n\nUsing the MCP client, you can retrieve the prompt like this using the STDIO transport:\n\n```python\nimport asyncio\nfrom mcp.client.session import ClientSession\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\n\n\nasync def main():\n    async with stdio_client(\n        StdioServerParameters(command=\"uv\", args=[\"run\", \"mcp-simple-prompt\"])\n    ) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # List available prompts\n            prompts = await session.list_prompts()\n            print(prompts)\n\n            # Get the prompt with arguments\n            prompt = await session.get_prompt(\n                \"simple\",\n                {\n                    \"context\": \"User is a software developer\",\n                    \"topic\": \"Python async programming\",\n                },\n            )\n            print(prompt)\n\n\nasyncio.run(main())\n```\n"}
{"source":"github","repo":"mcp-python-sdk","path":"CLAUDE.md","content":"# Development Guidelines\n\nThis document contains critical information about working with this codebase. Follow these guidelines precisely.\n\n## Core Development Rules\n\n1. Package Management\n   - ONLY use uv, NEVER pip\n   - Installation: `uv add package`\n   - Running tools: `uv run tool`\n   - Upgrading: `uv add --dev package --upgrade-package package`\n   - FORBIDDEN: `uv pip install`, `@latest` syntax\n\n2. Code Quality\n   - Type hints required for all code\n   - Public APIs must have docstrings\n   - Functions must be focused and small\n   - Follow existing patterns exactly\n   - Line length: 120 chars maximum\n\n3. Testing Requirements\n   - Framework: `uv run --frozen pytest`\n   - Async testing: use anyio, not asyncio\n   - Coverage: test edge cases and errors\n   - New features require tests\n   - Bug fixes require regression tests\n\n- For commits fixing bugs or adding features based on user reports add:\n\n  ```bash\n  git commit --trailer \"Reported-by:<name>\"\n  ```\n\n  Where `<name>` is the name of the user.\n\n- For commits related to a Github issue, add\n\n  ```bash\n  git commit --trailer \"Github-Issue:#<number>\"\n  ```\n\n- NEVER ever mention a `co-authored-by` or similar aspects. In particular, never\n  mention the tool used to create the commit message or PR.\n\n## Pull Requests\n\n- Create a detailed message of what changed. Focus on the high level description of\n  the problem it tries to solve, and how it is solved. Don't go into the specifics of the\n  code unless it adds clarity.\n\n- NEVER ever mention a `co-authored-by` or similar aspects. In particular, never\n  mention the tool used to create the commit message or PR.\n\n## Python Tools\n\n## Code Formatting\n\n1. Ruff\n   - Format: `uv run --frozen ruff format .`\n   - Check: `uv run --frozen ruff check .`\n   - Fix: `uv run --frozen ruff check . --fix`\n   - Critical issues:\n     - Line length (88 chars)\n     - Import sorting (I001)\n     - Unused imports\n   - Line wrapping:\n     - Strings: use parentheses\n     - Function calls: multi-line with proper indent\n     - Imports: split into multiple lines\n\n2. Type Checking\n   - Tool: `uv run --frozen pyright`\n   - Requirements:\n     - Explicit None checks for Optional\n     - Type narrowing for strings\n     - Version warnings can be ignored if checks pass\n\n3. Pre-commit\n   - Config: `.pre-commit-config.yaml`\n   - Runs: on git commit\n   - Tools: Prettier (YAML/JSON), Ruff (Python)\n   - Ruff updates:\n     - Check PyPI versions\n     - Update config rev\n     - Commit config first\n\n## Error Resolution\n\n1. CI Failures\n   - Fix order:\n     1. Formatting\n     2. Type errors\n     3. Linting\n   - Type errors:\n     - Get full line context\n     - Check Optional types\n     - Add type narrowing\n     - Verify function signatures\n\n2. Common Issues\n   - Line length:\n     - Break strings with parentheses\n     - Multi-line function calls\n     - Split imports\n   - Types:\n     - Add None checks\n     - Narrow string types\n     - Match existing patterns\n   - Pytest:\n     - If the tests aren't finding the anyio pytest mark, try adding PYTEST_DISABLE_PLUGIN_AUTOLOAD=\"\"\n       to the start of the pytest run command eg:\n       `PYTEST_DISABLE_PLUGIN_AUTOLOAD=\"\" uv run --frozen pytest`\n\n3. Best Practices\n   - Check git status before commits\n   - Run formatters before type checks\n   - Keep changes minimal\n   - Follow existing patterns\n   - Document public APIs\n   - Test thoroughly\n\n## Exception Handling\n\n- **Always use `logger.exception()` instead of `logger.error()` when catching exceptions**\n  - Don't include the exception in the message: `logger.exception(\"Failed\")` not `logger.exception(f\"Failed: {e}\")`\n- **Catch specific exceptions** where possible:\n  - File ops: `except (OSError, PermissionError):`\n  - JSON: `except json.JSONDecodeError:`\n  - Network: `except (ConnectionError, TimeoutError):`\n- **Only catch `Exception` for**:\n  - Top-level handlers that must not crash\n  - Cleanup blocks (log at debug level)\n"}
{"source":"github","repo":"mcp-python-sdk","path":"SECURITY.md","content":"# Security Policy\n\nThank you for helping us keep the SDKs and systems they interact with secure.\n\n## Reporting Security Issues\n\nThis SDK is maintained by [Anthropic](https://www.anthropic.com/) as part of the Model Context Protocol project.\n\nThe security of our systems and user data is Anthropicâ€™s top priority. We appreciate the work of security researchers acting in good faith in identifying and reporting potential vulnerabilities.\n\nOur security program is managed on HackerOne and we ask that any validated vulnerability in this functionality be reported through their [submission form](https://hackerone.com/anthropic-vdp/reports/new?type=team&report_type=vulnerability).\n\n## Vulnerability Disclosure Program\n\nOur Vulnerability Program Guidelines are defined on our [HackerOne program page](https://hackerone.com/anthropic-vdp).\n"}
{"source":"github","repo":"mcp-python-sdk","path":"CODE_OF_CONDUCT.md","content":"# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n<mcp-coc@anthropic.com>.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\n<https://www.contributor-covenant.org/version/2/0/code_of_conduct.html>.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\n<https://www.contributor-covenant.org/faq>. Translations are available at\n<https://www.contributor-covenant.org/translations>.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"CONTRIBUTING.md","content":"# Contributing\n\nThank you for your interest in contributing to the MCP Python SDK! This document provides guidelines and instructions for contributing.\n\n## Development Setup\n\n1. Make sure you have Python 3.10+ installed\n2. Install [uv](https://docs.astral.sh/uv/getting-started/installation/)\n3. Fork the repository\n4. Clone your fork: `git clone https://github.com/YOUR-USERNAME/python-sdk.git`\n5. Install dependencies:\n\n```bash\nuv sync --frozen --all-extras --dev\n```\n\n6. Set up pre-commit hooks:\n\n```bash\nuv tool install pre-commit --with pre-commit-uv --force-reinstall\n```\n\n## Development Workflow\n\n1. Choose the correct branch for your changes:\n   - For bug fixes to a released version: use the latest release branch (e.g. v1.1.x for 1.1.3)\n   - For new features: use the main branch (which will become the next minor/major version)\n   - If unsure, ask in an issue first\n\n2. Create a new branch from your chosen base branch\n\n3. Make your changes\n\n4. Ensure tests pass:\n\n```bash\nuv run pytest\n```\n\n5. Run type checking:\n\n```bash\nuv run pyright\n```\n\n6. Run linting:\n\n```bash\nuv run ruff check .\nuv run ruff format .\n```\n\n7. Update README snippets if you modified example code:\n\n```bash\nuv run scripts/update_readme_snippets.py\n```\n\n8. (Optional) Run pre-commit hooks on all files:\n\n```bash\npre-commit run --all-files\n```\n\n9. Submit a pull request to the same branch you branched from\n\n## Code Style\n\n- We use `ruff` for linting and formatting\n- Follow PEP 8 style guidelines\n- Add type hints to all functions\n- Include docstrings for public APIs\n\n## Pull Request Process\n\n1. Update documentation as needed\n2. Add tests for new functionality\n3. Ensure CI passes\n4. Maintainers will review your code\n5. Address review feedback\n\n## Code of Conduct\n\nPlease note that this project is released with a [Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.\n\n## License\n\nBy contributing, you agree that your contributions will be licensed under the MIT License.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"README.md","content":"# MCP Python SDK\n\n<div align=\"center\">\n\n<strong>Python implementation of the Model Context Protocol (MCP)</strong>\n\n[![PyPI][pypi-badge]][pypi-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Python Version][python-badge]][python-url]\n[![Documentation][docs-badge]][docs-url]\n[![Protocol][protocol-badge]][protocol-url]\n[![Specification][spec-badge]][spec-url]\n\n</div>\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [MCP Python SDK](#mcp-python-sdk)\n  - [Overview](#overview)\n  - [Installation](#installation)\n    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)\n    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)\n  - [Quickstart](#quickstart)\n  - [What is MCP?](#what-is-mcp)\n  - [Core Concepts](#core-concepts)\n    - [Server](#server)\n    - [Resources](#resources)\n    - [Tools](#tools)\n      - [Structured Output](#structured-output)\n    - [Prompts](#prompts)\n    - [Images](#images)\n    - [Context](#context)\n      - [Getting Context in Functions](#getting-context-in-functions)\n      - [Context Properties and Methods](#context-properties-and-methods)\n    - [Completions](#completions)\n    - [Elicitation](#elicitation)\n    - [Sampling](#sampling)\n    - [Logging and Notifications](#logging-and-notifications)\n    - [Authentication](#authentication)\n    - [FastMCP Properties](#fastmcp-properties)\n    - [Session Properties and Methods](#session-properties-and-methods)\n    - [Request Context Properties](#request-context-properties)\n  - [Running Your Server](#running-your-server)\n    - [Development Mode](#development-mode)\n    - [Claude Desktop Integration](#claude-desktop-integration)\n    - [Direct Execution](#direct-execution)\n    - [Streamable HTTP Transport](#streamable-http-transport)\n      - [CORS Configuration for Browser-Based Clients](#cors-configuration-for-browser-based-clients)\n    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)\n      - [StreamableHTTP servers](#streamablehttp-servers)\n        - [Basic mounting](#basic-mounting)\n        - [Host-based routing](#host-based-routing)\n        - [Multiple servers with path configuration](#multiple-servers-with-path-configuration)\n        - [Path configuration at initialization](#path-configuration-at-initialization)\n      - [SSE servers](#sse-servers)\n  - [Advanced Usage](#advanced-usage)\n    - [Low-Level Server](#low-level-server)\n      - [Structured Output Support](#structured-output-support)\n    - [Pagination (Advanced)](#pagination-advanced)\n    - [Writing MCP Clients](#writing-mcp-clients)\n    - [Client Display Utilities](#client-display-utilities)\n    - [OAuth Authentication for Clients](#oauth-authentication-for-clients)\n    - [Parsing Tool Results](#parsing-tool-results)\n    - [MCP Primitives](#mcp-primitives)\n    - [Server Capabilities](#server-capabilities)\n  - [Documentation](#documentation)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg\n[pypi-url]: https://pypi.org/project/mcp/\n[mit-badge]: https://img.shields.io/pypi/l/mcp.svg\n[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE\n[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg\n[python-url]: https://www.python.org/downloads/\n[docs-badge]: https://img.shields.io/badge/docs-python--sdk-blue.svg\n[docs-url]: https://modelcontextprotocol.github.io/python-sdk/\n[protocol-badge]: https://img.shields.io/badge/protocol-modelcontextprotocol.io-blue.svg\n[protocol-url]: https://modelcontextprotocol.io\n[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg\n[spec-url]: https://modelcontextprotocol.io/specification/latest\n\n## Overview\n\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\n\n- Build MCP clients that can connect to any MCP server\n- Create MCP servers that expose resources, prompts and tools\n- Use standard transports like stdio, SSE, and Streamable HTTP\n- Handle all MCP protocol messages and lifecycle events\n\n## Installation\n\n### Adding MCP to your python project\n\nWe recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects.\n\nIf you haven't created a uv-managed project yet, create one:\n\n   ```bash\n   uv init mcp-server-demo\n   cd mcp-server-demo\n   ```\n\n   Then add MCP to your project dependencies:\n\n   ```bash\n   uv add \"mcp[cli]\"\n   ```\n\nAlternatively, for projects using pip for dependencies:\n\n```bash\npip install \"mcp[cli]\"\n```\n\n### Running the standalone MCP development tools\n\nTo run the mcp command with uv:\n\n```bash\nuv run mcp\n```\n\n## Quickstart\n\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\n<!-- snippet-source examples/snippets/servers/fastmcp_quickstart.py -->\n```python\n\"\"\"\nFastMCP quickstart example.\n\nRun from the repository root:\n    uv run examples/snippets/servers/fastmcp_quickstart.py\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\", json_response=True)\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n# Add a prompt\n@mcp.prompt()\ndef greet_user(name: str, style: str = \"friendly\") -> str:\n    \"\"\"Generate a greeting prompt\"\"\"\n    styles = {\n        \"friendly\": \"Please write a warm, friendly greeting\",\n        \"formal\": \"Please write a formal, professional greeting\",\n        \"casual\": \"Please write a casual, relaxed greeting\",\n    }\n\n    return f\"{styles.get(style, styles['friendly'])} for someone named {name}.\"\n\n\n# Run with streamable HTTP transport\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n\n_Full example: [examples/snippets/servers/fastmcp_quickstart.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/fastmcp_quickstart.py)_\n<!-- /snippet-source -->\n\nYou can install this server in [Claude Code](https://docs.claude.com/en/docs/claude-code/mcp) and interact with it right away. First, run the server:\n\n```bash\nuv run --with mcp examples/snippets/servers/fastmcp_quickstart.py\n```\n\nThen add it to Claude Code:\n\n```bash\nclaude mcp add --transport http my-server http://localhost:8000/mcp\n```\n\nAlternatively, you can test it with the MCP Inspector. Start the server as above, then in a separate terminal:\n\n```bash\nnpx -y @modelcontextprotocol/inspector\n```\n\nIn the inspector UI, connect to `http://localhost:8000/mcp`.\n\n## What is MCP?\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\n\n- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context)\n- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\n- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)\n- And more!\n\n## Core Concepts\n\n### Server\n\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\n<!-- snippet-source examples/snippets/servers/lifespan_example.py -->\n```python\n\"\"\"Example showing lifespan support for startup/shutdown with strong typing.\"\"\"\n\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\n\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\n\n# Mock database class for example\nclass Database:\n    \"\"\"Mock database class for example.\"\"\"\n\n    @classmethod\n    async def connect(cls) -> \"Database\":\n        \"\"\"Connect to database.\"\"\"\n        return cls()\n\n    async def disconnect(self) -> None:\n        \"\"\"Disconnect from database.\"\"\"\n        pass\n\n    def query(self) -> str:\n        \"\"\"Execute a query.\"\"\"\n        return \"Query result\"\n\n\n@dataclass\nclass AppContext:\n    \"\"\"Application context with typed dependencies.\"\"\"\n\n    db: Database\n\n\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    \"\"\"Manage application lifecycle with type-safe context.\"\"\"\n    # Initialize on startup\n    db = await Database.connect()\n    try:\n        yield AppContext(db=db)\n    finally:\n        # Cleanup on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nmcp = FastMCP(\"My App\", lifespan=app_lifespan)\n\n\n# Access type-safe lifespan context in tools\n@mcp.tool()\ndef query_db(ctx: Context[ServerSession, AppContext]) -> str:\n    \"\"\"Tool that uses initialized resources.\"\"\"\n    db = ctx.request_context.lifespan_context.db\n    return db.query()\n```\n\n_Full example: [examples/snippets/servers/lifespan_example.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lifespan_example.py)_\n<!-- /snippet-source -->\n\n### Resources\n\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\n\n<!-- snippet-source examples/snippets/servers/basic_resource.py -->\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(name=\"Resource Example\")\n\n\n@mcp.resource(\"file://documents/{name}\")\ndef read_document(name: str) -> str:\n    \"\"\"Read a document by name.\"\"\"\n    # This would normally read from disk\n    return f\"Content of {name}\"\n\n\n@mcp.resource(\"config://settings\")\ndef get_settings() -> str:\n    \"\"\"Get application settings.\"\"\"\n    return \"\"\"{\n  \"theme\": \"dark\",\n  \"language\": \"en\",\n  \"debug\": false\n}\"\"\"\n```\n\n_Full example: [examples/snippets/servers/basic_resource.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_resource.py)_\n<!-- /snippet-source -->\n\n### Tools\n\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n\n<!-- snippet-source examples/snippets/servers/basic_tool.py -->\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(name=\"Tool Example\")\n\n\n@mcp.tool()\ndef sum(a: int, b: int) -> int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\n\n@mcp.tool()\ndef get_weather(city: str, unit: str = \"celsius\") -> str:\n    \"\"\"Get weather for a city.\"\"\"\n    # This would normally call a weather API\n    return f\"Weather in {city}: 22degrees{unit[0].upper()}\"\n```\n\n_Full example: [examples/snippets/servers/basic_tool.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_tool.py)_\n<!-- /snippet-source -->\n\nTools can optionally receive a Context object by including a parameter with the `Context` type annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:\n\n<!-- snippet-source examples/snippets/servers/tool_progress.py -->\n```python\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\nmcp = FastMCP(name=\"Progress Example\")\n\n\n@mcp.tool()\nasync def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -> str:\n    \"\"\"Execute a task with progress updates.\"\"\"\n    await ctx.info(f\"Starting: {task_name}\")\n\n    for i in range(steps):\n        progress = (i + 1) / steps\n        await ctx.report_progress(\n            progress=progress,\n            total=1.0,\n            message=f\"Step {i + 1}/{steps}\",\n        )\n        await ctx.debug(f\"Completed step {i + 1}\")\n\n    return f\"Task '{task_name}' completed\"\n```\n\n_Full example: [examples/snippets/servers/tool_progress.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/tool_progress.py)_\n<!-- /snippet-source -->\n\n#### Structured Output\n\nTools will return structured results by default, if their return type\nannotation is compatible. Otherwise, they will return unstructured results.\n\nStructured output supports these return types:\n\n- Pydantic models (BaseModel subclasses)\n- TypedDicts\n- Dataclasses and other classes with type hints\n- `dict[str, T]` (where T is any JSON-serializable type)\n- Primitive types (str, int, float, bool, bytes, None) - wrapped in `{\"result\": value}`\n- Generic types (list, tuple, Union, Optional, etc.) - wrapped in `{\"result\": value}`\n\nClasses without type hints cannot be serialized for structured output. Only\nclasses with properly annotated attributes will be converted to Pydantic models\nfor schema generation and validation.\n\nStructured results are automatically validated against the output schema\ngenerated from the annotation. This ensures the tool returns well-typed,\nvalidated data that clients can easily process.\n\n**Note:** For backward compatibility, unstructured results are also\nreturned. Unstructured results are provided for backward compatibility\nwith previous versions of the MCP specification, and are quirks-compatible\nwith previous versions of FastMCP in the current version of the SDK.\n\n**Note:** In cases where a tool function's return type annotation\ncauses the tool to be classified as structured _and this is undesirable_,\nthe  classification can be suppressed by passing `structured_output=False`\nto the `@tool` decorator.\n\n##### Advanced: Direct CallToolResult\n\nFor full control over tool responses including the `_meta` field (for passing data to client applications without exposing it to the model), you can return `CallToolResult` directly:\n\n<!-- snippet-source examples/snippets/servers/direct_call_tool_result.py -->\n```python\n\"\"\"Example showing direct CallToolResult return for advanced control.\"\"\"\n\nfrom typing import Annotated\n\nfrom pydantic import BaseModel\n\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.types import CallToolResult, TextContent\n\nmcp = FastMCP(\"CallToolResult Example\")\n\n\nclass ValidationModel(BaseModel):\n    \"\"\"Model for validating structured output.\"\"\"\n\n    status: str\n    data: dict[str, int]\n\n\n@mcp.tool()\ndef advanced_tool() -> CallToolResult:\n    \"\"\"Return CallToolResult directly for full control including _meta field.\"\"\"\n    return CallToolResult(\n        content=[TextContent(type=\"text\", text=\"Response visible to the model\")],\n        _meta={\"hidden\": \"data for client applications only\"},\n    )\n\n\n@mcp.tool()\ndef validated_tool() -> Annotated[CallToolResult, ValidationModel]:\n    \"\"\"Return CallToolResult with structured output validation.\"\"\"\n    return CallToolResult(\n        content=[TextContent(type=\"text\", text=\"Validated response\")],\n        structuredContent={\"status\": \"success\", \"data\": {\"result\": 42}},\n        _meta={\"internal\": \"metadata\"},\n    )\n\n\n@mcp.tool()\ndef empty_result_tool() -> CallToolResult:\n    \"\"\"For empty results, return CallToolResult with empty content.\"\"\"\n    return CallToolResult(content=[])\n```\n\n_Full example: [examples/snippets/servers/direct_call_tool_result.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/direct_call_tool_result.py)_\n<!-- /snippet-source -->\n\n**Important:** `CallToolResult` must always be returned (no `Optional` or `Union`). For empty results, use `CallToolResult(content=[])`. For optional simple types, use `str | None` without `CallToolResult`.\n\n<!-- snippet-source examples/snippets/servers/structured_output.py -->\n```python\n\"\"\"Example showing structured output with tools.\"\"\"\n\nfrom typing import TypedDict\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Structured Output Example\")\n\n\n# Using Pydantic models for rich structured data\nclass WeatherData(BaseModel):\n    \"\"\"Weather information structure.\"\"\"\n\n    temperature: float = Field(description=\"Temperature in Celsius\")\n    humidity: float = Field(description=\"Humidity percentage\")\n    condition: str\n    wind_speed: float\n\n\n@mcp.tool()\ndef get_weather(city: str) -> WeatherData:\n    \"\"\"Get weather for a city - returns structured data.\"\"\"\n    # Simulated weather data\n    return WeatherData(\n        temperature=22.5,\n        humidity=45.0,\n        condition=\"sunny\",\n        wind_speed=5.2,\n    )\n\n\n# Using TypedDict for simpler structures\nclass LocationInfo(TypedDict):\n    latitude: float\n    longitude: float\n    name: str\n\n\n@mcp.tool()\ndef get_location(address: str) -> LocationInfo:\n    \"\"\"Get location coordinates\"\"\"\n    return LocationInfo(latitude=51.5074, longitude=-0.1278, name=\"London, UK\")\n\n\n# Using dict[str, Any] for flexible schemas\n@mcp.tool()\ndef get_statistics(data_type: str) -> dict[str, float]:\n    \"\"\"Get various statistics\"\"\"\n    return {\"mean\": 42.5, \"median\": 40.0, \"std_dev\": 5.2}\n\n\n# Ordinary classes with type hints work for structured output\nclass UserProfile:\n    name: str\n    age: int\n    email: str | None = None\n\n    def __init__(self, name: str, age: int, email: str | None = None):\n        self.name = name\n        self.age = age\n        self.email = email\n\n\n@mcp.tool()\ndef get_user(user_id: str) -> UserProfile:\n    \"\"\"Get user profile - returns structured data\"\"\"\n    return UserProfile(name=\"Alice\", age=30, email=\"alice@example.com\")\n\n\n# Classes WITHOUT type hints cannot be used for structured output\nclass UntypedConfig:\n    def __init__(self, setting1, setting2):  # type: ignore[reportMissingParameterType]\n        self.setting1 = setting1\n        self.setting2 = setting2\n\n\n@mcp.tool()\ndef get_config() -> UntypedConfig:\n    \"\"\"This returns unstructured output - no schema generated\"\"\"\n    return UntypedConfig(\"value1\", \"value2\")\n\n\n# Lists and other types are wrapped automatically\n@mcp.tool()\ndef list_cities() -> list[str]:\n    \"\"\"Get a list of cities\"\"\"\n    return [\"London\", \"Paris\", \"Tokyo\"]\n    # Returns: {\"result\": [\"London\", \"Paris\", \"Tokyo\"]}\n\n\n@mcp.tool()\ndef get_temperature(city: str) -> float:\n    \"\"\"Get temperature as a simple float\"\"\"\n    return 22.5\n    # Returns: {\"result\": 22.5}\n```\n\n_Full example: [examples/snippets/servers/structured_output.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/structured_output.py)_\n<!-- /snippet-source -->\n\n### Prompts\n\nPrompts are reusable templates that help LLMs interact with your server effectively:\n\n<!-- snippet-source examples/snippets/servers/basic_prompt.py -->\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base\n\nmcp = FastMCP(name=\"Prompt Example\")\n\n\n@mcp.prompt(title=\"Code Review\")\ndef review_code(code: str) -> str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n\n@mcp.prompt(title=\"Debug Assistant\")\ndef debug_error(error: str) -> list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n```\n\n_Full example: [examples/snippets/servers/basic_prompt.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_prompt.py)_\n<!-- /snippet-source -->\n\n### Icons\n\nMCP servers can provide icons for UI display. Icons can be added to the server implementation, tools, resources, and prompts:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Icon\n\n# Create an icon from a file path or URL\nicon = Icon(\n    src=\"icon.png\",\n    mimeType=\"image/png\",\n    sizes=\"64x64\"\n)\n\n# Add icons to server\nmcp = FastMCP(\n    \"My Server\",\n    website_url=\"https://example.com\",\n    icons=[icon]\n)\n\n# Add icons to tools, resources, and prompts\n@mcp.tool(icons=[icon])\ndef my_tool():\n    \"\"\"Tool with an icon.\"\"\"\n    return \"result\"\n\n@mcp.resource(\"demo://resource\", icons=[icon])\ndef my_resource():\n    \"\"\"Resource with an icon.\"\"\"\n    return \"content\"\n```\n\n_Full example: [examples/fastmcp/icons_demo.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/fastmcp/icons_demo.py)_\n\n### Images\n\nFastMCP provides an `Image` class that automatically handles image data:\n\n<!-- snippet-source examples/snippets/servers/images.py -->\n```python\n\"\"\"Example showing image handling with FastMCP.\"\"\"\n\nfrom PIL import Image as PILImage\n\nfrom mcp.server.fastmcp import FastMCP, Image\n\nmcp = FastMCP(\"Image Example\")\n\n\n@mcp.tool()\ndef create_thumbnail(image_path: str) -> Image:\n    \"\"\"Create a thumbnail from an image\"\"\"\n    img = PILImage.open(image_path)\n    img.thumbnail((100, 100))\n    return Image(data=img.tobytes(), format=\"png\")\n```\n\n_Full example: [examples/snippets/servers/images.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/images.py)_\n<!-- /snippet-source -->\n\n### Context\n\nThe Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.\n\n#### Getting Context in Functions\n\nTo use context in a tool or resource function, add a parameter with the `Context` type annotation:\n\n```python\nfrom mcp.server.fastmcp import Context, FastMCP\n\nmcp = FastMCP(name=\"Context Example\")\n\n\n@mcp.tool()\nasync def my_tool(x: int, ctx: Context) -> str:\n    \"\"\"Tool that uses context capabilities.\"\"\"\n    # The context parameter can have any name as long as it's type-annotated\n    return await process_with_context(x, ctx)\n```\n\n#### Context Properties and Methods\n\nThe Context object provides the following capabilities:\n\n- `ctx.request_id` - Unique ID for the current request\n- `ctx.client_id` - Client ID if available\n- `ctx.fastmcp` - Access to the FastMCP server instance (see [FastMCP Properties](#fastmcp-properties))\n- `ctx.session` - Access to the underlying session for advanced communication (see [Session Properties and Methods](#session-properties-and-methods))\n- `ctx.request_context` - Access to request-specific data and lifespan resources (see [Request Context Properties](#request-context-properties))\n- `await ctx.debug(message)` - Send debug log message\n- `await ctx.info(message)` - Send info log message  \n- `await ctx.warning(message)` - Send warning log message\n- `await ctx.error(message)` - Send error log message\n- `await ctx.log(level, message, logger_name=None)` - Send log with custom level\n- `await ctx.report_progress(progress, total=None, message=None)` - Report operation progress\n- `await ctx.read_resource(uri)` - Read a resource by URI\n- `await ctx.elicit(message, schema)` - Request additional information from user with validation\n\n<!-- snippet-source examples/snippets/servers/tool_progress.py -->\n```python\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\nmcp = FastMCP(name=\"Progress Example\")\n\n\n@mcp.tool()\nasync def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -> str:\n    \"\"\"Execute a task with progress updates.\"\"\"\n    await ctx.info(f\"Starting: {task_name}\")\n\n    for i in range(steps):\n        progress = (i + 1) / steps\n        await ctx.report_progress(\n            progress=progress,\n            total=1.0,\n            message=f\"Step {i + 1}/{steps}\",\n        )\n        await ctx.debug(f\"Completed step {i + 1}\")\n\n    return f\"Task '{task_name}' completed\"\n```\n\n_Full example: [examples/snippets/servers/tool_progress.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/tool_progress.py)_\n<!-- /snippet-source -->\n\n### Completions\n\nMCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:\n\nClient usage:\n\n<!-- snippet-source examples/snippets/clients/completion_client.py -->\n```python\n\"\"\"\ncd to the `examples/snippets` directory and run:\n    uv run completion-client\n\"\"\"\n\nimport asyncio\nimport os\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom mcp.types import PromptReference, ResourceTemplateReference\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"uv\",  # Using uv to run the server\n    args=[\"run\", \"server\", \"completion\", \"stdio\"],  # Server with completion support\n    env={\"UV_INDEX\": os.environ.get(\"UV_INDEX\", \"\")},\n)\n\n\nasync def run():\n    \"\"\"Run the completion client example.\"\"\"\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available resource templates\n            templates = await session.list_resource_templates()\n            print(\"Available resource templates:\")\n            for template in templates.resourceTemplates:\n                print(f\"  - {template.uriTemplate}\")\n\n            # List available prompts\n            prompts = await session.list_prompts()\n            print(\"\\nAvailable prompts:\")\n            for prompt in prompts.prompts:\n                print(f\"  - {prompt.name}\")\n\n            # Complete resource template arguments\n            if templates.resourceTemplates:\n                template = templates.resourceTemplates[0]\n                print(f\"\\nCompleting arguments for resource template: {template.uriTemplate}\")\n\n                # Complete without context\n                result = await session.complete(\n                    ref=ResourceTemplateReference(type=\"ref/resource\", uri=template.uriTemplate),\n                    argument={\"name\": \"owner\", \"value\": \"model\"},\n                )\n                print(f\"Completions for 'owner' starting with 'model': {result.completion.values}\")\n\n                # Complete with context - repo suggestions based on owner\n                result = await session.complete(\n                    ref=ResourceTemplateReference(type=\"ref/resource\", uri=template.uriTemplate),\n                    argument={\"name\": \"repo\", \"value\": \"\"},\n                    context_arguments={\"owner\": \"modelcontextprotocol\"},\n                )\n                print(f\"Completions for 'repo' with owner='modelcontextprotocol': {result.completion.values}\")\n\n            # Complete prompt arguments\n            if prompts.prompts:\n                prompt_name = prompts.prompts[0].name\n                print(f\"\\nCompleting arguments for prompt: {prompt_name}\")\n\n                result = await session.complete(\n                    ref=PromptReference(type=\"ref/prompt\", name=prompt_name),\n                    argument={\"name\": \"style\", \"value\": \"\"},\n                )\n                print(f\"Completions for 'style' argument: {result.completion.values}\")\n\n\ndef main():\n    \"\"\"Entry point for the completion client.\"\"\"\n    asyncio.run(run())\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n_Full example: [examples/snippets/clients/completion_client.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/clients/completion_client.py)_\n<!-- /snippet-source -->\n### Elicitation\n\nRequest additional information from users. This example shows an Elicitation during a Tool Call:\n\n<!-- snippet-source examples/snippets/servers/elicitation.py -->\n```python\n\"\"\"Elicitation examples demonstrating form and URL mode elicitation.\n\nForm mode elicitation collects structured, non-sensitive data through a schema.\nURL mode elicitation directs users to external URLs for sensitive operations\nlike OAuth flows, credential collection, or payment processing.\n\"\"\"\n\nimport uuid\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\nfrom mcp.shared.exceptions import UrlElicitationRequiredError\nfrom mcp.types import ElicitRequestURLParams\n\nmcp = FastMCP(name=\"Elicitation Example\")\n\n\nclass BookingPreferences(BaseModel):\n    \"\"\"Schema for collecting user preferences.\"\"\"\n\n    checkAlternative: bool = Field(description=\"Would you like to check another date?\")\n    alternativeDate: str = Field(\n        default=\"2024-12-26\",\n        description=\"Alternative date (YYYY-MM-DD)\",\n    )\n\n\n@mcp.tool()\nasync def book_table(date: str, time: str, party_size: int, ctx: Context[ServerSession, None]) -> str:\n    \"\"\"Book a table with date availability check.\n\n    This demonstrates form mode elicitation for collecting non-sensitive user input.\n    \"\"\"\n    # Check if date is available\n    if date == \"2024-12-25\":\n        # Date unavailable - ask user for alternative\n        result = await ctx.elicit(\n            message=(f\"No tables available for {party_size} on {date}. Would you like to try another date?\"),\n            schema=BookingPreferences,\n        )\n\n        if result.action == \"accept\" and result.data:\n            if result.data.checkAlternative:\n                return f\"[SUCCESS] Booked for {result.data.alternativeDate}\"\n            return \"[CANCELLED] No booking made\"\n        return \"[CANCELLED] Booking cancelled\"\n\n    # Date available\n    return f\"[SUCCESS] Booked for {date} at {time}\"\n\n\n@mcp.tool()\nasync def secure_payment(amount: float, ctx: Context[ServerSession, None]) -> str:\n    \"\"\"Process a secure payment requiring URL confirmation.\n\n    This demonstrates URL mode elicitation using ctx.elicit_url() for\n    operations that require out-of-band user interaction.\n    \"\"\"\n    elicitation_id = str(uuid.uuid4())\n\n    result = await ctx.elicit_url(\n        message=f\"Please confirm payment of ${amount:.2f}\",\n        url=f\"https://payments.example.com/confirm?amount={amount}&id={elicitation_id}\",\n        elicitation_id=elicitation_id,\n    )\n\n    if result.action == \"accept\":\n        # In a real app, the payment confirmation would happen out-of-band\n        # and you'd verify the payment status from your backend\n        return f\"Payment of ${amount:.2f} initiated - check your browser to complete\"\n    elif result.action == \"decline\":\n        return \"Payment declined by user\"\n    return \"Payment cancelled\"\n\n\n@mcp.tool()\nasync def connect_service(service_name: str, ctx: Context[ServerSession, None]) -> str:\n    \"\"\"Connect to a third-party service requiring OAuth authorization.\n\n    This demonstrates the \"throw error\" pattern using UrlElicitationRequiredError.\n    Use this pattern when the tool cannot proceed without user authorization.\n    \"\"\"\n    elicitation_id = str(uuid.uuid4())\n\n    # Raise UrlElicitationRequiredError to signal that the client must complete\n    # a URL elicitation before this request can be processed.\n    # The MCP framework will convert this to a -32042 error response.\n    raise UrlElicitationRequiredError(\n        [\n            ElicitRequestURLParams(\n                mode=\"url\",\n                message=f\"Authorization required to connect to {service_name}\",\n                url=f\"https://{service_name}.example.com/oauth/authorize?elicit={elicitation_id}\",\n                elicitationId=elicitation_id,\n            )\n        ]\n    )\n```\n\n_Full example: [examples/snippets/servers/elicitation.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/elicitation.py)_\n<!-- /snippet-source -->\n\nElicitation schemas support default values for all field types. Default values are automatically included in the JSON schema sent to clients, allowing them to pre-populate forms.\n\nThe `elicit()` method returns an `ElicitationResult` with:\n\n- `action`: \"accept\", \"decline\", or \"cancel\"\n- `data`: The validated response (only when accepted)\n- `validation_error`: Any validation error message\n\n### Sampling\n\nTools can interact with LLMs through sampling (generating text):\n\n<!-- snippet-source examples/snippets/servers/sampling.py -->\n```python\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\nfrom mcp.types import SamplingMessage, TextContent\n\nmcp = FastMCP(name=\"Sampling Example\")\n\n\n@mcp.tool()\nasync def generate_poem(topic: str, ctx: Context[ServerSession, None]) -> str:\n    \"\"\"Generate a poem using LLM sampling.\"\"\"\n    prompt = f\"Write a short poem about {topic}\"\n\n    result = await ctx.session.create_message(\n        messages=[\n            SamplingMessage(\n                role=\"user\",\n                content=TextContent(type=\"text\", text=prompt),\n            )\n        ],\n        max_tokens=100,\n    )\n\n    # Since we're not passing tools param, result.content is single content\n    if result.content.type == \"text\":\n        return result.content.text\n    return str(result.content)\n```\n\n_Full example: [examples/snippets/servers/sampling.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/sampling.py)_\n<!-- /snippet-source -->\n\n### Logging and Notifications\n\nTools can send logs and notifications through the context:\n\n<!-- snippet-source examples/snippets/servers/notifications.py -->\n```python\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\nmcp = FastMCP(name=\"Notifications Example\")\n\n\n@mcp.tool()\nasync def process_data(data: str, ctx: Context[ServerSession, None]) -> str:\n    \"\"\"Process data with logging.\"\"\"\n    # Different log levels\n    await ctx.debug(f\"Debug: Processing '{data}'\")\n    await ctx.info(\"Info: Starting processing\")\n    await ctx.warning(\"Warning: This is experimental\")\n    await ctx.error(\"Error: (This is just a demo)\")\n\n    # Notify about resource changes\n    await ctx.session.send_resource_list_changed()\n\n    return f\"Processed: {data}\"\n```\n\n_Full example: [examples/snippets/servers/notifications.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/notifications.py)_\n<!-- /snippet-source -->\n\n### Authentication\n\nAuthentication can be used by servers that want to expose tools accessing protected resources.\n\n`mcp.server.auth` implements OAuth 2.1 resource server functionality, where MCP servers act as Resource Servers (RS) that validate tokens issued by separate Authorization Servers (AS). This follows the [MCP authorization specification](https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization) and implements RFC 9728 (Protected Resource Metadata) for AS discovery.\n\nMCP servers can use authentication by providing an implementation of the `TokenVerifier` protocol:\n\n<!-- snippet-source examples/snippets/servers/oauth_server.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uv run examples/snippets/servers/oauth_server.py\n\"\"\"\n\nfrom pydantic import AnyHttpUrl\n\nfrom mcp.server.auth.provider import AccessToken, TokenVerifier\nfrom mcp.server.auth.settings import AuthSettings\nfrom mcp.server.fastmcp import FastMCP\n\n\nclass SimpleTokenVerifier(TokenVerifier):\n    \"\"\"Simple token verifier for demonstration.\"\"\"\n\n    async def verify_token(self, token: str) -> AccessToken | None:\n        pass  # This is where you would implement actual token validation\n\n\n# Create FastMCP instance as a Resource Server\nmcp = FastMCP(\n    \"Weather Service\",\n    json_response=True,\n    # Token verifier for authentication\n    token_verifier=SimpleTokenVerifier(),\n    # Auth settings for RFC 9728 Protected Resource Metadata\n    auth=AuthSettings(\n        issuer_url=AnyHttpUrl(\"https://auth.example.com\"),  # Authorization Server URL\n        resource_server_url=AnyHttpUrl(\"http://localhost:3001\"),  # This server's URL\n        required_scopes=[\"user\"],\n    ),\n)\n\n\n@mcp.tool()\nasync def get_weather(city: str = \"London\") -> dict[str, str]:\n    \"\"\"Get weather data for a city\"\"\"\n    return {\n        \"city\": city,\n        \"temperature\": \"22\",\n        \"condition\": \"Partly cloudy\",\n        \"humidity\": \"65%\",\n    }\n\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n\n_Full example: [examples/snippets/servers/oauth_server.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/oauth_server.py)_\n<!-- /snippet-source -->\n\nFor a complete example with separate Authorization Server and Resource Server implementations, see [`examples/servers/simple-auth/`](examples/servers/simple-auth/).\n\n**Architecture:**\n\n- **Authorization Server (AS)**: Handles OAuth flows, user authentication, and token issuance\n- **Resource Server (RS)**: Your MCP server that validates tokens and serves protected resources\n- **Client**: Discovers AS through RFC 9728, obtains tokens, and uses them with the MCP server\n\nSee [TokenVerifier](src/mcp/server/auth/provider.py) for more details on implementing token validation.\n\n### FastMCP Properties\n\nThe FastMCP server instance accessible via `ctx.fastmcp` provides access to server configuration and metadata:\n\n- `ctx.fastmcp.name` - The server's name as defined during initialization\n- `ctx.fastmcp.instructions` - Server instructions/description provided to clients\n- `ctx.fastmcp.website_url` - Optional website URL for the server\n- `ctx.fastmcp.icons` - Optional list of icons for UI display\n- `ctx.fastmcp.settings` - Complete server configuration object containing:\n  - `debug` - Debug mode flag\n  - `log_level` - Current logging level\n  - `host` and `port` - Server network configuration\n  - `mount_path`, `sse_path`, `streamable_http_path` - Transport paths\n  - `stateless_http` - Whether the server operates in stateless mode\n  - And other configuration options\n\n```python\n@mcp.tool()\ndef server_info(ctx: Context) -> dict:\n    \"\"\"Get information about the current server.\"\"\"\n    return {\n        \"name\": ctx.fastmcp.name,\n        \"instructions\": ctx.fastmcp.instructions,\n        \"debug_mode\": ctx.fastmcp.settings.debug,\n        \"log_level\": ctx.fastmcp.settings.log_level,\n        \"host\": ctx.fastmcp.settings.host,\n        \"port\": ctx.fastmcp.settings.port,\n    }\n```\n\n### Session Properties and Methods\n\nThe session object accessible via `ctx.session` provides advanced control over client communication:\n\n- `ctx.session.client_params` - Client initialization parameters and declared capabilities\n- `await ctx.session.send_log_message(level, data, logger)` - Send log messages with full control\n- `await ctx.session.create_message(messages, max_tokens)` - Request LLM sampling/completion\n- `await ctx.session.send_progress_notification(token, progress, total, message)` - Direct progress updates\n- `await ctx.session.send_resource_updated(uri)` - Notify clients that a specific resource changed\n- `await ctx.session.send_resource_list_changed()` - Notify clients that the resource list changed\n- `await ctx.session.send_tool_list_changed()` - Notify clients that the tool list changed\n- `await ctx.session.send_prompt_list_changed()` - Notify clients that the prompt list changed\n\n```python\n@mcp.tool()\nasync def notify_data_update(resource_uri: str, ctx: Context) -> str:\n    \"\"\"Update data and notify clients of the change.\"\"\"\n    # Perform data update logic here\n    \n    # Notify clients that this specific resource changed\n    await ctx.session.send_resource_updated(AnyUrl(resource_uri))\n    \n    # If this affects the overall resource list, notify about that too\n    await ctx.session.send_resource_list_changed()\n    \n    return f\"Updated {resource_uri} and notified clients\"\n```\n\n### Request Context Properties\n\nThe request context accessible via `ctx.request_context` contains request-specific information and resources:\n\n- `ctx.request_context.lifespan_context` - Access to resources initialized during server startup\n  - Database connections, configuration objects, shared services\n  - Type-safe access to resources defined in your server's lifespan function\n- `ctx.request_context.meta` - Request metadata from the client including:\n  - `progressToken` - Token for progress notifications\n  - Other client-provided metadata\n- `ctx.request_context.request` - The original MCP request object for advanced processing\n- `ctx.request_context.request_id` - Unique identifier for this request\n\n```python\n# Example with typed lifespan context\n@dataclass\nclass AppContext:\n    db: Database\n    config: AppConfig\n\n@mcp.tool()\ndef query_with_config(query: str, ctx: Context) -> str:\n    \"\"\"Execute a query using shared database and configuration.\"\"\"\n    # Access typed lifespan context\n    app_ctx: AppContext = ctx.request_context.lifespan_context\n    \n    # Use shared resources\n    connection = app_ctx.db\n    settings = app_ctx.config\n    \n    # Execute query with configuration\n    result = connection.execute(query, timeout=settings.query_timeout)\n    return str(result)\n```\n\n_Full lifespan example: [examples/snippets/servers/lifespan_example.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lifespan_example.py)_\n\n## Running Your Server\n\n### Development Mode\n\nThe fastest way to test and debug your server is with the MCP Inspector:\n\n```bash\nuv run mcp dev server.py\n\n# Add dependencies\nuv run mcp dev server.py --with pandas --with numpy\n\n# Mount local code\nuv run mcp dev server.py --with-editable .\n```\n\n### Claude Desktop Integration\n\nOnce your server is ready, install it in Claude Desktop:\n\n```bash\nuv run mcp install server.py\n\n# Custom name\nuv run mcp install server.py --name \"My Analytics Server\"\n\n# Environment variables\nuv run mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nuv run mcp install server.py -f .env\n```\n\n### Direct Execution\n\nFor advanced scenarios like custom deployments:\n\n<!-- snippet-source examples/snippets/servers/direct_execution.py -->\n```python\n\"\"\"Example showing direct execution of an MCP server.\n\nThis is the simplest way to run an MCP server directly.\ncd to the `examples/snippets` directory and run:\n    uv run direct-execution-server\n    or\n    python servers/direct_execution.py\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef hello(name: str = \"World\") -> str:\n    \"\"\"Say hello to someone.\"\"\"\n    return f\"Hello, {name}!\"\n\n\ndef main():\n    \"\"\"Entry point for the direct execution server.\"\"\"\n    mcp.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n_Full example: [examples/snippets/servers/direct_execution.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/direct_execution.py)_\n<!-- /snippet-source -->\n\nRun it with:\n\n```bash\npython servers/direct_execution.py\n# or\nuv run mcp run servers/direct_execution.py\n```\n\nNote that `uv run mcp run` or `uv run mcp dev` only supports server using FastMCP and not the low-level server variant.\n\n### Streamable HTTP Transport\n\n> **Note**: Streamable HTTP transport is the recommended transport for production deployments. Use `stateless_http=True` and `json_response=True` for optimal scalability.\n\n<!-- snippet-source examples/snippets/servers/streamable_config.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uv run examples/snippets/servers/streamable_config.py\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Stateless server with JSON responses (recommended)\nmcp = FastMCP(\"StatelessServer\", stateless_http=True, json_response=True)\n\n# Other configuration options:\n# Stateless server with SSE streaming responses\n# mcp = FastMCP(\"StatelessServer\", stateless_http=True)\n\n# Stateful server with session persistence\n# mcp = FastMCP(\"StatefulServer\")\n\n\n# Add a simple tool to demonstrate the server\n@mcp.tool()\ndef greet(name: str = \"World\") -> str:\n    \"\"\"Greet someone by name.\"\"\"\n    return f\"Hello, {name}!\"\n\n\n# Run server with streamable_http transport\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n\n_Full example: [examples/snippets/servers/streamable_config.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/streamable_config.py)_\n<!-- /snippet-source -->\n\nYou can mount multiple FastMCP servers in a Starlette application:\n\n<!-- snippet-source examples/snippets/servers/streamable_starlette_mount.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uvicorn examples.snippets.servers.streamable_starlette_mount:app --reload\n\"\"\"\n\nimport contextlib\n\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the Echo server\necho_mcp = FastMCP(name=\"EchoServer\", stateless_http=True, json_response=True)\n\n\n@echo_mcp.tool()\ndef echo(message: str) -> str:\n    \"\"\"A simple echo tool\"\"\"\n    return f\"Echo: {message}\"\n\n\n# Create the Math server\nmath_mcp = FastMCP(name=\"MathServer\", stateless_http=True, json_response=True)\n\n\n@math_mcp.tool()\ndef add_two(n: int) -> int:\n    \"\"\"Tool to add two to the input\"\"\"\n    return n + 2\n\n\n# Create a combined lifespan to manage both session managers\n@contextlib.asynccontextmanager\nasync def lifespan(app: Starlette):\n    async with contextlib.AsyncExitStack() as stack:\n        await stack.enter_async_context(echo_mcp.session_manager.run())\n        await stack.enter_async_context(math_mcp.session_manager.run())\n        yield\n\n\n# Create the Starlette app and mount the MCP servers\napp = Starlette(\n    routes=[\n        Mount(\"/echo\", echo_mcp.streamable_http_app()),\n        Mount(\"/math\", math_mcp.streamable_http_app()),\n    ],\n    lifespan=lifespan,\n)\n\n# Note: Clients connect to http://localhost:8000/echo/mcp and http://localhost:8000/math/mcp\n# To mount at the root of each path (e.g., /echo instead of /echo/mcp):\n# echo_mcp.settings.streamable_http_path = \"/\"\n# math_mcp.settings.streamable_http_path = \"/\"\n```\n\n_Full example: [examples/snippets/servers/streamable_starlette_mount.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/streamable_starlette_mount.py)_\n<!-- /snippet-source -->\n\nFor low level server with Streamable HTTP implementations, see:\n\n- Stateful server: [`examples/servers/simple-streamablehttp/`](examples/servers/simple-streamablehttp/)\n- Stateless server: [`examples/servers/simple-streamablehttp-stateless/`](examples/servers/simple-streamablehttp-stateless/)\n\nThe streamable HTTP transport supports:\n\n- Stateful and stateless operation modes\n- Resumability with event stores\n- JSON or SSE response formats\n- Better scalability for multi-node deployments\n\n#### CORS Configuration for Browser-Based Clients\n\nIf you'd like your server to be accessible by browser-based MCP clients, you'll need to configure CORS headers. The `Mcp-Session-Id` header must be exposed for browser clients to access it:\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.middleware.cors import CORSMiddleware\n\n# Create your Starlette app first\nstarlette_app = Starlette(routes=[...])\n\n# Then wrap it with CORS middleware\nstarlette_app = CORSMiddleware(\n    starlette_app,\n    allow_origins=[\"*\"],  # Configure appropriately for production\n    allow_methods=[\"GET\", \"POST\", \"DELETE\"],  # MCP streamable HTTP methods\n    expose_headers=[\"Mcp-Session-Id\"],\n)\n```\n\nThis configuration is necessary because:\n\n- The MCP streamable HTTP transport uses the `Mcp-Session-Id` header for session management\n- Browsers restrict access to response headers unless explicitly exposed via CORS\n- Without this configuration, browser-based clients won't be able to read the session ID from initialization responses\n\n### Mounting to an Existing ASGI Server\n\nBy default, SSE servers are mounted at `/sse` and Streamable HTTP servers are mounted at `/mcp`. You can customize these paths using the methods described below.\n\nFor more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).\n\n#### StreamableHTTP servers\n\nYou can mount the StreamableHTTP server to an existing ASGI server using the `streamable_http_app` method. This allows you to integrate the StreamableHTTP server with other ASGI applications.\n\n##### Basic mounting\n\n<!-- snippet-source examples/snippets/servers/streamable_http_basic_mounting.py -->\n```python\n\"\"\"\nBasic example showing how to mount StreamableHTTP server in Starlette.\n\nRun from the repository root:\n    uvicorn examples.snippets.servers.streamable_http_basic_mounting:app --reload\n\"\"\"\n\nimport contextlib\n\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create MCP server\nmcp = FastMCP(\"My App\", json_response=True)\n\n\n@mcp.tool()\ndef hello() -> str:\n    \"\"\"A simple hello tool\"\"\"\n    return \"Hello from MCP!\"\n\n\n# Create a lifespan context manager to run the session manager\n@contextlib.asynccontextmanager\nasync def lifespan(app: Starlette):\n    async with mcp.session_manager.run():\n        yield\n\n\n# Mount the StreamableHTTP server to the existing ASGI server\napp = Starlette(\n    routes=[\n        Mount(\"/\", app=mcp.streamable_http_app()),\n    ],\n    lifespan=lifespan,\n)\n```\n\n_Full example: [examples/snippets/servers/streamable_http_basic_mounting.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/streamable_http_basic_mounting.py)_\n<!-- /snippet-source -->\n\n##### Host-based routing\n\n<!-- snippet-source examples/snippets/servers/streamable_http_host_mounting.py -->\n```python\n\"\"\"\nExample showing how to mount StreamableHTTP server using Host-based routing.\n\nRun from the repository root:\n    uvicorn examples.snippets.servers.streamable_http_host_mounting:app --reload\n\"\"\"\n\nimport contextlib\n\nfrom starlette.applications import Starlette\nfrom starlette.routing import Host\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create MCP server\nmcp = FastMCP(\"MCP Host App\", json_response=True)\n\n\n@mcp.tool()\ndef domain_info() -> str:\n    \"\"\"Get domain-specific information\"\"\"\n    return \"This is served from mcp.acme.corp\"\n\n\n# Create a lifespan context manager to run the session manager\n@contextlib.asynccontextmanager\nasync def lifespan(app: Starlette):\n    async with mcp.session_manager.run():\n        yield\n\n\n# Mount using Host-based routing\napp = Starlette(\n    routes=[\n        Host(\"mcp.acme.corp\", app=mcp.streamable_http_app()),\n    ],\n    lifespan=lifespan,\n)\n```\n\n_Full example: [examples/snippets/servers/streamable_http_host_mounting.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/streamable_http_host_mounting.py)_\n<!-- /snippet-source -->\n\n##### Multiple servers with path configuration\n\n<!-- snippet-source examples/snippets/servers/streamable_http_multiple_servers.py -->\n```python\n\"\"\"\nExample showing how to mount multiple StreamableHTTP servers with path configuration.\n\nRun from the repository root:\n    uvicorn examples.snippets.servers.streamable_http_multiple_servers:app --reload\n\"\"\"\n\nimport contextlib\n\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create multiple MCP servers\napi_mcp = FastMCP(\"API Server\", json_response=True)\nchat_mcp = FastMCP(\"Chat Server\", json_response=True)\n\n\n@api_mcp.tool()\ndef api_status() -> str:\n    \"\"\"Get API status\"\"\"\n    return \"API is running\"\n\n\n@chat_mcp.tool()\ndef send_message(message: str) -> str:\n    \"\"\"Send a chat message\"\"\"\n    return f\"Message sent: {message}\"\n\n\n# Configure servers to mount at the root of each path\n# This means endpoints will be at /api and /chat instead of /api/mcp and /chat/mcp\napi_mcp.settings.streamable_http_path = \"/\"\nchat_mcp.settings.streamable_http_path = \"/\"\n\n\n# Create a combined lifespan to manage both session managers\n@contextlib.asynccontextmanager\nasync def lifespan(app: Starlette):\n    async with contextlib.AsyncExitStack() as stack:\n        await stack.enter_async_context(api_mcp.session_manager.run())\n        await stack.enter_async_context(chat_mcp.session_manager.run())\n        yield\n\n\n# Mount the servers\napp = Starlette(\n    routes=[\n        Mount(\"/api\", app=api_mcp.streamable_http_app()),\n        Mount(\"/chat\", app=chat_mcp.streamable_http_app()),\n    ],\n    lifespan=lifespan,\n)\n```\n\n_Full example: [examples/snippets/servers/streamable_http_multiple_servers.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/streamable_http_multiple_servers.py)_\n<!-- /snippet-source -->\n\n##### Path configuration at initialization\n\n<!-- snippet-source examples/snippets/servers/streamable_http_path_config.py -->\n```python\n\"\"\"\nExample showing path configuration during FastMCP initialization.\n\nRun from the repository root:\n    uvicorn examples.snippets.servers.streamable_http_path_config:app --reload\n\"\"\"\n\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Configure streamable_http_path during initialization\n# This server will mount at the root of wherever it's mounted\nmcp_at_root = FastMCP(\n    \"My Server\",\n    json_response=True,\n    streamable_http_path=\"/\",\n)\n\n\n@mcp_at_root.tool()\ndef process_data(data: str) -> str:\n    \"\"\"Process some data\"\"\"\n    return f\"Processed: {data}\"\n\n\n# Mount at /process - endpoints will be at /process instead of /process/mcp\napp = Starlette(\n    routes=[\n        Mount(\"/process\", app=mcp_at_root.streamable_http_app()),\n    ]\n)\n```\n\n_Full example: [examples/snippets/servers/streamable_http_path_config.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/streamable_http_path_config.py)_\n<!-- /snippet-source -->\n\n#### SSE servers\n\n> **Note**: SSE transport is being superseded by [Streamable HTTP transport](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http).\n\nYou can mount the SSE server to an existing ASGI server using the `sse_app` method. This allows you to integrate the SSE server with other ASGI applications.\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount, Host\nfrom mcp.server.fastmcp import FastMCP\n\n\nmcp = FastMCP(\"My App\")\n\n# Mount the SSE server to the existing ASGI server\napp = Starlette(\n    routes=[\n        Mount('/', app=mcp.sse_app()),\n    ]\n)\n\n# or dynamically mount as host\napp.router.routes.append(Host('mcp.acme.corp', app=mcp.sse_app()))\n```\n\nWhen mounting multiple MCP servers under different paths, you can configure the mount path in several ways:\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\nfrom mcp.server.fastmcp import FastMCP\n\n# Create multiple MCP servers\ngithub_mcp = FastMCP(\"GitHub API\")\nbrowser_mcp = FastMCP(\"Browser\")\ncurl_mcp = FastMCP(\"Curl\")\nsearch_mcp = FastMCP(\"Search\")\n\n# Method 1: Configure mount paths via settings (recommended for persistent configuration)\ngithub_mcp.settings.mount_path = \"/github\"\nbrowser_mcp.settings.mount_path = \"/browser\"\n\n# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)\n# This approach doesn't modify the server's settings permanently\n\n# Create Starlette app with multiple mounted servers\napp = Starlette(\n    routes=[\n        # Using settings-based configuration\n        Mount(\"/github\", app=github_mcp.sse_app()),\n        Mount(\"/browser\", app=browser_mcp.sse_app()),\n        # Using direct mount path parameter\n        Mount(\"/curl\", app=curl_mcp.sse_app(\"/curl\")),\n        Mount(\"/search\", app=search_mcp.sse_app(\"/search\")),\n    ]\n)\n\n# Method 3: For direct execution, you can also pass the mount path to run()\nif __name__ == \"__main__\":\n    search_mcp.run(transport=\"sse\", mount_path=\"/search\")\n```\n\nFor more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).\n\n## Advanced Usage\n\n### Low-Level Server\n\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\n<!-- snippet-source examples/snippets/servers/lowlevel/lifespan.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uv run examples/snippets/servers/lowlevel/lifespan.py\n\"\"\"\n\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\nfrom typing import Any\n\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\n\n# Mock database class for example\nclass Database:\n    \"\"\"Mock database class for example.\"\"\"\n\n    @classmethod\n    async def connect(cls) -> \"Database\":\n        \"\"\"Connect to database.\"\"\"\n        print(\"Database connected\")\n        return cls()\n\n    async def disconnect(self) -> None:\n        \"\"\"Disconnect from database.\"\"\"\n        print(\"Database disconnected\")\n\n    async def query(self, query_str: str) -> list[dict[str, str]]:\n        \"\"\"Execute a query.\"\"\"\n        # Simulate database query\n        return [{\"id\": \"1\", \"name\": \"Example\", \"query\": query_str}]\n\n\n@asynccontextmanager\nasync def server_lifespan(_server: Server) -> AsyncIterator[dict[str, Any]]:\n    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n    # Initialize resources on startup\n    db = await Database.connect()\n    try:\n        yield {\"db\": db}\n    finally:\n        # Clean up on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nserver = Server(\"example-server\", lifespan=server_lifespan)\n\n\n@server.list_tools()\nasync def handle_list_tools() -> list[types.Tool]:\n    \"\"\"List available tools.\"\"\"\n    return [\n        types.Tool(\n            name=\"query_db\",\n            description=\"Query the database\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"SQL query to execute\"}},\n                \"required\": [\"query\"],\n            },\n        )\n    ]\n\n\n@server.call_tool()\nasync def query_db(name: str, arguments: dict[str, Any]) -> list[types.TextContent]:\n    \"\"\"Handle database query tool call.\"\"\"\n    if name != \"query_db\":\n        raise ValueError(f\"Unknown tool: {name}\")\n\n    # Access lifespan context\n    ctx = server.request_context\n    db = ctx.lifespan_context[\"db\"]\n\n    # Execute query\n    results = await db.query(arguments[\"query\"])\n\n    return [types.TextContent(type=\"text\", text=f\"Query results: {results}\")]\n\n\nasync def run():\n    \"\"\"Run the server with lifespan management.\"\"\"\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example-server\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\n_Full example: [examples/snippets/servers/lowlevel/lifespan.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lowlevel/lifespan.py)_\n<!-- /snippet-source -->\n\nThe lifespan API provides:\n\n- A way to initialize resources when the server starts and clean them up when it stops\n- Access to initialized resources through the request context in handlers\n- Type-safe context passing between lifespan and request handlers\n\n<!-- snippet-source examples/snippets/servers/lowlevel/basic.py -->\n```python\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/basic.py\n\"\"\"\n\nimport asyncio\n\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\n# Create a server instance\nserver = Server(\"example-server\")\n\n\n@server.list_prompts()\nasync def handle_list_prompts() -> list[types.Prompt]:\n    \"\"\"List available prompts.\"\"\"\n    return [\n        types.Prompt(\n            name=\"example-prompt\",\n            description=\"An example prompt template\",\n            arguments=[types.PromptArgument(name=\"arg1\", description=\"Example argument\", required=True)],\n        )\n    ]\n\n\n@server.get_prompt()\nasync def handle_get_prompt(name: str, arguments: dict[str, str] | None) -> types.GetPromptResult:\n    \"\"\"Get a specific prompt by name.\"\"\"\n    if name != \"example-prompt\":\n        raise ValueError(f\"Unknown prompt: {name}\")\n\n    arg1_value = (arguments or {}).get(\"arg1\", \"default\")\n\n    return types.GetPromptResult(\n        description=\"Example prompt\",\n        messages=[\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(type=\"text\", text=f\"Example prompt text with argument: {arg1_value}\"),\n            )\n        ],\n    )\n\n\nasync def run():\n    \"\"\"Run the basic low-level server.\"\"\"\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run())\n```\n\n_Full example: [examples/snippets/servers/lowlevel/basic.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lowlevel/basic.py)_\n<!-- /snippet-source -->\n\nCaution: The `uv run mcp run` and `uv run mcp dev` tool doesn't support low-level server.\n\n#### Structured Output Support\n\nThe low-level server supports structured output for tools, allowing you to return both human-readable content and machine-readable structured data. Tools can define an `outputSchema` to validate their structured output:\n\n<!-- snippet-source examples/snippets/servers/lowlevel/structured_output.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uv run examples/snippets/servers/lowlevel/structured_output.py\n\"\"\"\n\nimport asyncio\nfrom typing import Any\n\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\nserver = Server(\"example-server\")\n\n\n@server.list_tools()\nasync def list_tools() -> list[types.Tool]:\n    \"\"\"List available tools with structured output schemas.\"\"\"\n    return [\n        types.Tool(\n            name=\"get_weather\",\n            description=\"Get current weather for a city\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"City name\"}},\n                \"required\": [\"city\"],\n            },\n            outputSchema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"temperature\": {\"type\": \"number\", \"description\": \"Temperature in Celsius\"},\n                    \"condition\": {\"type\": \"string\", \"description\": \"Weather condition\"},\n                    \"humidity\": {\"type\": \"number\", \"description\": \"Humidity percentage\"},\n                    \"city\": {\"type\": \"string\", \"description\": \"City name\"},\n                },\n                \"required\": [\"temperature\", \"condition\", \"humidity\", \"city\"],\n            },\n        )\n    ]\n\n\n@server.call_tool()\nasync def call_tool(name: str, arguments: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Handle tool calls with structured output.\"\"\"\n    if name == \"get_weather\":\n        city = arguments[\"city\"]\n\n        # Simulated weather data - in production, call a weather API\n        weather_data = {\n            \"temperature\": 22.5,\n            \"condition\": \"partly cloudy\",\n            \"humidity\": 65,\n            \"city\": city,  # Include the requested city\n        }\n\n        # low-level server will validate structured output against the tool's\n        # output schema, and additionally serialize it into a TextContent block\n        # for backwards compatibility with pre-2025-06-18 clients.\n        return weather_data\n    else:\n        raise ValueError(f\"Unknown tool: {name}\")\n\n\nasync def run():\n    \"\"\"Run the structured output server.\"\"\"\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"structured-output-example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run())\n```\n\n_Full example: [examples/snippets/servers/lowlevel/structured_output.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lowlevel/structured_output.py)_\n<!-- /snippet-source -->\n\nTools can return data in four ways:\n\n1. **Content only**: Return a list of content blocks (default behavior before spec revision 2025-06-18)\n2. **Structured data only**: Return a dictionary that will be serialized to JSON (Introduced in spec revision 2025-06-18)\n3. **Both**: Return a tuple of (content, structured_data) preferred option to use for backwards compatibility\n4. **Direct CallToolResult**: Return `CallToolResult` directly for full control (including `_meta` field)\n\nWhen an `outputSchema` is defined, the server automatically validates the structured output against the schema. This ensures type safety and helps catch errors early.\n\n##### Returning CallToolResult Directly\n\nFor full control over the response including the `_meta` field (for passing data to client applications without exposing it to the model), return `CallToolResult` directly:\n\n<!-- snippet-source examples/snippets/servers/lowlevel/direct_call_tool_result.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uv run examples/snippets/servers/lowlevel/direct_call_tool_result.py\n\"\"\"\n\nimport asyncio\nfrom typing import Any\n\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\nserver = Server(\"example-server\")\n\n\n@server.list_tools()\nasync def list_tools() -> list[types.Tool]:\n    \"\"\"List available tools.\"\"\"\n    return [\n        types.Tool(\n            name=\"advanced_tool\",\n            description=\"Tool with full control including _meta field\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\"message\": {\"type\": \"string\"}},\n                \"required\": [\"message\"],\n            },\n        )\n    ]\n\n\n@server.call_tool()\nasync def handle_call_tool(name: str, arguments: dict[str, Any]) -> types.CallToolResult:\n    \"\"\"Handle tool calls by returning CallToolResult directly.\"\"\"\n    if name == \"advanced_tool\":\n        message = str(arguments.get(\"message\", \"\"))\n        return types.CallToolResult(\n            content=[types.TextContent(type=\"text\", text=f\"Processed: {message}\")],\n            structuredContent={\"result\": \"success\", \"message\": message},\n            _meta={\"hidden\": \"data for client applications only\"},\n        )\n\n    raise ValueError(f\"Unknown tool: {name}\")\n\n\nasync def run():\n    \"\"\"Run the server.\"\"\"\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run())\n```\n\n_Full example: [examples/snippets/servers/lowlevel/direct_call_tool_result.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lowlevel/direct_call_tool_result.py)_\n<!-- /snippet-source -->\n\n**Note:** When returning `CallToolResult`, you bypass the automatic content/structured conversion. You must construct the complete response yourself.\n\n### Pagination (Advanced)\n\nFor servers that need to handle large datasets, the low-level server provides paginated versions of list operations. This is an optional optimization - most servers won't need pagination unless they're dealing with hundreds or thousands of items.\n\n#### Server-side Implementation\n\n<!-- snippet-source examples/snippets/servers/pagination_example.py -->\n```python\n\"\"\"\nExample of implementing pagination with MCP server decorators.\n\"\"\"\n\nfrom pydantic import AnyUrl\n\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n# Initialize the server\nserver = Server(\"paginated-server\")\n\n# Sample data to paginate\nITEMS = [f\"Item {i}\" for i in range(1, 101)]  # 100 items\n\n\n@server.list_resources()\nasync def list_resources_paginated(request: types.ListResourcesRequest) -> types.ListResourcesResult:\n    \"\"\"List resources with pagination support.\"\"\"\n    page_size = 10\n\n    # Extract cursor from request params\n    cursor = request.params.cursor if request.params is not None else None\n\n    # Parse cursor to get offset\n    start = 0 if cursor is None else int(cursor)\n    end = start + page_size\n\n    # Get page of resources\n    page_items = [\n        types.Resource(uri=AnyUrl(f\"resource://items/{item}\"), name=item, description=f\"Description for {item}\")\n        for item in ITEMS[start:end]\n    ]\n\n    # Determine next cursor\n    next_cursor = str(end) if end < len(ITEMS) else None\n\n    return types.ListResourcesResult(resources=page_items, nextCursor=next_cursor)\n```\n\n_Full example: [examples/snippets/servers/pagination_example.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/pagination_example.py)_\n<!-- /snippet-source -->\n\n#### Client-side Consumption\n\n<!-- snippet-source examples/snippets/clients/pagination_client.py -->\n```python\n\"\"\"\nExample of consuming paginated MCP endpoints from a client.\n\"\"\"\n\nimport asyncio\n\nfrom mcp.client.session import ClientSession\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\nfrom mcp.types import PaginatedRequestParams, Resource\n\n\nasync def list_all_resources() -> None:\n    \"\"\"Fetch all resources using pagination.\"\"\"\n    async with stdio_client(StdioServerParameters(command=\"uv\", args=[\"run\", \"mcp-simple-pagination\"])) as (\n        read,\n        write,\n    ):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            all_resources: list[Resource] = []\n            cursor = None\n\n            while True:\n                # Fetch a page of resources\n                result = await session.list_resources(params=PaginatedRequestParams(cursor=cursor))\n                all_resources.extend(result.resources)\n\n                print(f\"Fetched {len(result.resources)} resources\")\n\n                # Check if there are more pages\n                if result.nextCursor:\n                    cursor = result.nextCursor\n                else:\n                    break\n\n            print(f\"Total resources: {len(all_resources)}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(list_all_resources())\n```\n\n_Full example: [examples/snippets/clients/pagination_client.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/clients/pagination_client.py)_\n<!-- /snippet-source -->\n\n#### Key Points\n\n- **Cursors are opaque strings** - the server defines the format (numeric offsets, timestamps, etc.)\n- **Return `nextCursor=None`** when there are no more pages\n- **Backward compatible** - clients that don't support pagination will still work (they'll just get the first page)\n- **Flexible page sizes** - Each endpoint can define its own page size based on data characteristics\n\nSee the [simple-pagination example](examples/servers/simple-pagination) for a complete implementation.\n\n### Writing MCP Clients\n\nThe SDK provides a high-level client interface for connecting to MCP servers using various [transports](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports):\n\n<!-- snippet-source examples/snippets/clients/stdio_client.py -->\n```python\n\"\"\"\ncd to the `examples/snippets/clients` directory and run:\n    uv run client\n\"\"\"\n\nimport asyncio\nimport os\n\nfrom pydantic import AnyUrl\n\nfrom mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\nfrom mcp.shared.context import RequestContext\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"uv\",  # Using uv to run the server\n    args=[\"run\", \"server\", \"fastmcp_quickstart\", \"stdio\"],  # We're already in snippets dir\n    env={\"UV_INDEX\": os.environ.get(\"UV_INDEX\", \"\")},\n)\n\n\n# Optional: create a sampling callback\nasync def handle_sampling_message(\n    context: RequestContext[ClientSession, None], params: types.CreateMessageRequestParams\n) -> types.CreateMessageResult:\n    print(f\"Sampling request: {params.messages}\")\n    return types.CreateMessageResult(\n        role=\"assistant\",\n        content=types.TextContent(\n            type=\"text\",\n            text=\"Hello, world! from model\",\n        ),\n        model=\"gpt-3.5-turbo\",\n        stopReason=\"endTurn\",\n    )\n\n\nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write, sampling_callback=handle_sampling_message) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available prompts\n            prompts = await session.list_prompts()\n            print(f\"Available prompts: {[p.name for p in prompts.prompts]}\")\n\n            # Get a prompt (greet_user prompt from fastmcp_quickstart)\n            if prompts.prompts:\n                prompt = await session.get_prompt(\"greet_user\", arguments={\"name\": \"Alice\", \"style\": \"friendly\"})\n                print(f\"Prompt result: {prompt.messages[0].content}\")\n\n            # List available resources\n            resources = await session.list_resources()\n            print(f\"Available resources: {[r.uri for r in resources.resources]}\")\n\n            # List available tools\n            tools = await session.list_tools()\n            print(f\"Available tools: {[t.name for t in tools.tools]}\")\n\n            # Read a resource (greeting resource from fastmcp_quickstart)\n            resource_content = await session.read_resource(AnyUrl(\"greeting://World\"))\n            content_block = resource_content.contents[0]\n            if isinstance(content_block, types.TextContent):\n                print(f\"Resource content: {content_block.text}\")\n\n            # Call a tool (add tool from fastmcp_quickstart)\n            result = await session.call_tool(\"add\", arguments={\"a\": 5, \"b\": 3})\n            result_unstructured = result.content[0]\n            if isinstance(result_unstructured, types.TextContent):\n                print(f\"Tool result: {result_unstructured.text}\")\n            result_structured = result.structuredContent\n            print(f\"Structured tool result: {result_structured}\")\n\n\ndef main():\n    \"\"\"Entry point for the client script.\"\"\"\n    asyncio.run(run())\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n_Full example: [examples/snippets/clients/stdio_client.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/clients/stdio_client.py)_\n<!-- /snippet-source -->\n\nClients can also connect using [Streamable HTTP transport](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http):\n\n<!-- snippet-source examples/snippets/clients/streamable_basic.py -->\n```python\n\"\"\"\nRun from the repository root:\n    uv run examples/snippets/clients/streamable_basic.py\n\"\"\"\n\nimport asyncio\n\nfrom mcp import ClientSession\nfrom mcp.client.streamable_http import streamable_http_client\n\n\nasync def main():\n    # Connect to a streamable HTTP server\n    async with streamable_http_client(\"http://localhost:8000/mcp\") as (\n        read_stream,\n        write_stream,\n        _,\n    ):\n        # Create a session using the client streams\n        async with ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n            # List available tools\n            tools = await session.list_tools()\n            print(f\"Available tools: {[tool.name for tool in tools.tools]}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n_Full example: [examples/snippets/clients/streamable_basic.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/clients/streamable_basic.py)_\n<!-- /snippet-source -->\n\n### Client Display Utilities\n\nWhen building MCP clients, the SDK provides utilities to help display human-readable names for tools, resources, and prompts:\n\n<!-- snippet-source examples/snippets/clients/display_utilities.py -->\n```python\n\"\"\"\ncd to the `examples/snippets` directory and run:\n    uv run display-utilities-client\n\"\"\"\n\nimport asyncio\nimport os\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom mcp.shared.metadata_utils import get_display_name\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"uv\",  # Using uv to run the server\n    args=[\"run\", \"server\", \"fastmcp_quickstart\", \"stdio\"],\n    env={\"UV_INDEX\": os.environ.get(\"UV_INDEX\", \"\")},\n)\n\n\nasync def display_tools(session: ClientSession):\n    \"\"\"Display available tools with human-readable names\"\"\"\n    tools_response = await session.list_tools()\n\n    for tool in tools_response.tools:\n        # get_display_name() returns the title if available, otherwise the name\n        display_name = get_display_name(tool)\n        print(f\"Tool: {display_name}\")\n        if tool.description:\n            print(f\"   {tool.description}\")\n\n\nasync def display_resources(session: ClientSession):\n    \"\"\"Display available resources with human-readable names\"\"\"\n    resources_response = await session.list_resources()\n\n    for resource in resources_response.resources:\n        display_name = get_display_name(resource)\n        print(f\"Resource: {display_name} ({resource.uri})\")\n\n    templates_response = await session.list_resource_templates()\n    for template in templates_response.resourceTemplates:\n        display_name = get_display_name(template)\n        print(f\"Resource Template: {display_name}\")\n\n\nasync def run():\n    \"\"\"Run the display utilities example.\"\"\"\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            print(\"=== Available Tools ===\")\n            await display_tools(session)\n\n            print(\"\\n=== Available Resources ===\")\n            await display_resources(session)\n\n\ndef main():\n    \"\"\"Entry point for the display utilities client.\"\"\"\n    asyncio.run(run())\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n_Full example: [examples/snippets/clients/display_utilities.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/clients/display_utilities.py)_\n<!-- /snippet-source -->\n\nThe `get_display_name()` function implements the proper precedence rules for displaying names:\n\n- For tools: `title` > `annotations.title` > `name`\n- For other objects: `title` > `name`\n\nThis ensures your client UI shows the most user-friendly names that servers provide.\n\n### OAuth Authentication for Clients\n\nThe SDK includes [authorization support](https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization) for connecting to protected MCP servers:\n\n<!-- snippet-source examples/snippets/clients/oauth_client.py -->\n```python\n\"\"\"\nBefore running, specify running MCP RS server URL.\nTo spin up RS server locally, see\n    examples/servers/simple-auth/README.md\n\ncd to the `examples/snippets` directory and run:\n    uv run oauth-client\n\"\"\"\n\nimport asyncio\nfrom urllib.parse import parse_qs, urlparse\n\nimport httpx\nfrom pydantic import AnyUrl\n\nfrom mcp import ClientSession\nfrom mcp.client.auth import OAuthClientProvider, TokenStorage\nfrom mcp.client.streamable_http import streamable_http_client\nfrom mcp.shared.auth import OAuthClientInformationFull, OAuthClientMetadata, OAuthToken\n\n\nclass InMemoryTokenStorage(TokenStorage):\n    \"\"\"Demo In-memory token storage implementation.\"\"\"\n\n    def __init__(self):\n        self.tokens: OAuthToken | None = None\n        self.client_info: OAuthClientInformationFull | None = None\n\n    async def get_tokens(self) -> OAuthToken | None:\n        \"\"\"Get stored tokens.\"\"\"\n        return self.tokens\n\n    async def set_tokens(self, tokens: OAuthToken) -> None:\n        \"\"\"Store tokens.\"\"\"\n        self.tokens = tokens\n\n    async def get_client_info(self) -> OAuthClientInformationFull | None:\n        \"\"\"Get stored client information.\"\"\"\n        return self.client_info\n\n    async def set_client_info(self, client_info: OAuthClientInformationFull) -> None:\n        \"\"\"Store client information.\"\"\"\n        self.client_info = client_info\n\n\nasync def handle_redirect(auth_url: str) -> None:\n    print(f\"Visit: {auth_url}\")\n\n\nasync def handle_callback() -> tuple[str, str | None]:\n    callback_url = input(\"Paste callback URL: \")\n    params = parse_qs(urlparse(callback_url).query)\n    return params[\"code\"][0], params.get(\"state\", [None])[0]\n\n\nasync def main():\n    \"\"\"Run the OAuth client example.\"\"\"\n    oauth_auth = OAuthClientProvider(\n        server_url=\"http://localhost:8001\",\n        client_metadata=OAuthClientMetadata(\n            client_name=\"Example MCP Client\",\n            redirect_uris=[AnyUrl(\"http://localhost:3000/callback\")],\n            grant_types=[\"authorization_code\", \"refresh_token\"],\n            response_types=[\"code\"],\n            scope=\"user\",\n        ),\n        storage=InMemoryTokenStorage(),\n        redirect_handler=handle_redirect,\n        callback_handler=handle_callback,\n    )\n\n    async with httpx.AsyncClient(auth=oauth_auth, follow_redirects=True) as custom_client:\n        async with streamable_http_client(\"http://localhost:8001/mcp\", http_client=custom_client) as (read, write, _):\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n\n                tools = await session.list_tools()\n                print(f\"Available tools: {[tool.name for tool in tools.tools]}\")\n\n                resources = await session.list_resources()\n                print(f\"Available resources: {[r.uri for r in resources.resources]}\")\n\n\ndef run():\n    asyncio.run(main())\n\n\nif __name__ == \"__main__\":\n    run()\n```\n\n_Full example: [examples/snippets/clients/oauth_client.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/clients/oauth_client.py)_\n<!-- /snippet-source -->\n\nFor a complete working example, see [`examples/clients/simple-auth-client/`](examples/clients/simple-auth-client/).\n\n### Parsing Tool Results\n\nWhen calling tools through MCP, the `CallToolResult` object contains the tool's response in a structured format. Understanding how to parse this result is essential for properly handling tool outputs.\n\n```python\n\"\"\"examples/snippets/clients/parsing_tool_results.py\"\"\"\n\nimport asyncio\n\nfrom mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\n\n\nasync def parse_tool_results():\n    \"\"\"Demonstrates how to parse different types of content in CallToolResult.\"\"\"\n    server_params = StdioServerParameters(\n        command=\"python\", args=[\"path/to/mcp_server.py\"]\n    )\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            # Example 1: Parsing text content\n            result = await session.call_tool(\"get_data\", {\"format\": \"text\"})\n            for content in result.content:\n                if isinstance(content, types.TextContent):\n                    print(f\"Text: {content.text}\")\n\n            # Example 2: Parsing structured content from JSON tools\n            result = await session.call_tool(\"get_user\", {\"id\": \"123\"})\n            if hasattr(result, \"structuredContent\") and result.structuredContent:\n                # Access structured data directly\n                user_data = result.structuredContent\n                print(f\"User: {user_data.get('name')}, Age: {user_data.get('age')}\")\n\n            # Example 3: Parsing embedded resources\n            result = await session.call_tool(\"read_config\", {})\n            for content in result.content:\n                if isinstance(content, types.EmbeddedResource):\n                    resource = content.resource\n                    if isinstance(resource, types.TextResourceContents):\n                        print(f\"Config from {resource.uri}: {resource.text}\")\n                    elif isinstance(resource, types.BlobResourceContents):\n                        print(f\"Binary data from {resource.uri}\")\n\n            # Example 4: Parsing image content\n            result = await session.call_tool(\"generate_chart\", {\"data\": [1, 2, 3]})\n            for content in result.content:\n                if isinstance(content, types.ImageContent):\n                    print(f\"Image ({content.mimeType}): {len(content.data)} bytes\")\n\n            # Example 5: Handling errors\n            result = await session.call_tool(\"failing_tool\", {})\n            if result.isError:\n                print(\"Tool execution failed!\")\n                for content in result.content:\n                    if isinstance(content, types.TextContent):\n                        print(f\"Error: {content.text}\")\n\n\nasync def main():\n    await parse_tool_results()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### MCP Primitives\n\nThe MCP protocol defines three core primitives that servers can implement:\n\n| Primitive | Control               | Description                                         | Example Use                  |\n|-----------|-----------------------|-----------------------------------------------------|------------------------------|\n| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |\n| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |\n| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |\n\n### Server Capabilities\n\nMCP servers declare capabilities during initialization:\n\n| Capability   | Feature Flag                 | Description                        |\n|--------------|------------------------------|------------------------------------|\n| `prompts`    | `listChanged`                | Prompt template management         |\n| `resources`  | `subscribe`<br/>`listChanged`| Resource exposure and updates      |\n| `tools`      | `listChanged`                | Tool discovery and execution       |\n| `logging`    | -                            | Server logging configuration       |\n| `completions`| -                            | Argument completion suggestions    |\n\n## Documentation\n\n- [API Reference](https://modelcontextprotocol.github.io/python-sdk/api/)\n- [Experimental Features (Tasks)](https://modelcontextprotocol.github.io/python-sdk/experimental/tasks/)\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Model Context Protocol specification](https://modelcontextprotocol.io/specification/latest)\n- [Officially supported servers](https://github.com/modelcontextprotocol/servers)\n\n## Contributing\n\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the [contributing guide](CONTRIBUTING.md) to get started.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/testing.md","content":"# Testing MCP Servers\n\nIf you call yourself a developer, you will want to test your MCP server.\nThe Python SDK offers the `create_connected_server_and_client_session` function to create a session\nusing an in-memory transport. I know, I know, the name is too long... We are working on improving it.\n\nAnyway, let's assume you have a simple server with a single tool:\n\n```python title=\"server.py\"\nfrom mcp.server import FastMCP\n\napp = FastMCP(\"Calculator\")\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers.\"\"\"  # (1)!\n    return a + b\n```\n\n1. The docstring is automatically added as the description of the tool.\n\nTo run the below test, you'll need to install the following dependencies:\n\n=== \"pip\"\n    ```bash\n    pip install inline-snapshot pytest\n    ```\n\n=== \"uv\"\n    ```bash\n    uv add inline-snapshot pytest\n    ```\n\n!!! info\n    I think [`pytest`](https://docs.pytest.org/en/stable/) is a pretty standard testing framework,\n    so I won't go into details here.\n\n    The [`inline-snapshot`](https://15r10nk.github.io/inline-snapshot/latest/) is a library that allows\n    you to take snapshots of the output of your tests. Which makes it easier to create tests for your\n    server - you don't need to use it, but we are spreading the word for best practices.\n\n```python title=\"test_server.py\"\nfrom collections.abc import AsyncGenerator\n\nimport pytest\nfrom inline_snapshot import snapshot\nfrom mcp.client.session import ClientSession\nfrom mcp.shared.memory import create_connected_server_and_client_session\nfrom mcp.types import CallToolResult, TextContent\n\nfrom server import app\n\n\n@pytest.fixture\ndef anyio_backend():  # (1)!\n    return \"asyncio\"\n\n\n@pytest.fixture\nasync def client_session() -> AsyncGenerator[ClientSession]:\n    async with create_connected_server_and_client_session(app, raise_exceptions=True) as _session:\n        yield _session\n\n\n@pytest.mark.anyio\nasync def test_call_add_tool(client_session: ClientSession):\n    result = await client_session.call_tool(\"add\", {\"a\": 1, \"b\": 2})\n    assert result == snapshot(\n        CallToolResult(\n            content=[TextContent(type=\"text\", text=\"3\")],\n            structuredContent={\"result\": 3},\n        )\n    )\n```\n\n1. If you are using `trio`, you should set `\"trio\"` as the `anyio_backend`. Check more information in the [anyio documentation](https://anyio.readthedocs.io/en/stable/testing.html#specifying-the-backends-to-run-on).\n\nThere you go! You can now extend your tests to cover more scenarios.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/experimental/tasks-server.md","content":"# Server Task Implementation\n\n!!! warning \"Experimental\"\n\n    Tasks are an experimental feature. The API may change without notice.\n\nThis guide covers implementing task support in MCP servers, from basic setup to advanced patterns like elicitation and sampling within tasks.\n\n## Quick Start\n\nThe simplest way to add task support:\n\n```python\nfrom mcp.server import Server\nfrom mcp.server.experimental.task_context import ServerTaskContext\nfrom mcp.types import CallToolResult, CreateTaskResult, TextContent, Tool, ToolExecution, TASK_REQUIRED\n\nserver = Server(\"my-server\")\nserver.experimental.enable_tasks()  # Registers all task handlers automatically\n\n@server.list_tools()\nasync def list_tools():\n    return [\n        Tool(\n            name=\"process_data\",\n            description=\"Process data asynchronously\",\n            inputSchema={\"type\": \"object\", \"properties\": {\"input\": {\"type\": \"string\"}}},\n            execution=ToolExecution(taskSupport=TASK_REQUIRED),\n        )\n    ]\n\n@server.call_tool()\nasync def handle_tool(name: str, arguments: dict) -> CallToolResult | CreateTaskResult:\n    if name == \"process_data\":\n        return await handle_process_data(arguments)\n    return CallToolResult(content=[TextContent(type=\"text\", text=f\"Unknown: {name}\")], isError=True)\n\nasync def handle_process_data(arguments: dict) -> CreateTaskResult:\n    ctx = server.request_context\n    ctx.experimental.validate_task_mode(TASK_REQUIRED)\n\n    async def work(task: ServerTaskContext) -> CallToolResult:\n        await task.update_status(\"Processing...\")\n        result = arguments.get(\"input\", \"\").upper()\n        return CallToolResult(content=[TextContent(type=\"text\", text=result)])\n\n    return await ctx.experimental.run_task(work)\n```\n\nThat's it. `enable_tasks()` automatically:\n\n- Creates an in-memory task store\n- Registers handlers for `tasks/get`, `tasks/result`, `tasks/list`, `tasks/cancel`\n- Updates server capabilities\n\n## Tool Declaration\n\nTools declare task support via the `execution.taskSupport` field:\n\n```python\nfrom mcp.types import Tool, ToolExecution, TASK_REQUIRED, TASK_OPTIONAL, TASK_FORBIDDEN\n\nTool(\n    name=\"my_tool\",\n    inputSchema={\"type\": \"object\"},\n    execution=ToolExecution(taskSupport=TASK_REQUIRED),  # or TASK_OPTIONAL, TASK_FORBIDDEN\n)\n```\n\n| Value | Meaning |\n|-------|---------|\n| `TASK_REQUIRED` | Tool **must** be called as a task |\n| `TASK_OPTIONAL` | Tool supports both sync and task execution |\n| `TASK_FORBIDDEN` | Tool **cannot** be called as a task (default) |\n\nValidate the request matches your tool's requirements:\n\n```python\n@server.call_tool()\nasync def handle_tool(name: str, arguments: dict):\n    ctx = server.request_context\n\n    if name == \"required_task_tool\":\n        ctx.experimental.validate_task_mode(TASK_REQUIRED)  # Raises if not task mode\n        return await handle_as_task(arguments)\n\n    elif name == \"optional_task_tool\":\n        if ctx.experimental.is_task:\n            return await handle_as_task(arguments)\n        else:\n            return handle_sync(arguments)\n```\n\n## The run_task Pattern\n\n`run_task()` is the recommended way to execute task work:\n\n```python\nasync def handle_my_tool(arguments: dict) -> CreateTaskResult:\n    ctx = server.request_context\n    ctx.experimental.validate_task_mode(TASK_REQUIRED)\n\n    async def work(task: ServerTaskContext) -> CallToolResult:\n        # Your work here\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"Done\")])\n\n    return await ctx.experimental.run_task(work)\n```\n\n**What `run_task()` does:**\n\n1. Creates a task in the store\n2. Spawns your work function in the background\n3. Returns `CreateTaskResult` immediately\n4. Auto-completes the task when your function returns\n5. Auto-fails the task if your function raises\n\n**The `ServerTaskContext` provides:**\n\n- `task.task_id` - The task identifier\n- `task.update_status(message)` - Update progress\n- `task.complete(result)` - Explicitly complete (usually automatic)\n- `task.fail(error)` - Explicitly fail\n- `task.is_cancelled` - Check if cancellation requested\n\n## Status Updates\n\nKeep clients informed of progress:\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    await task.update_status(\"Starting...\")\n\n    for i, item in enumerate(items):\n        await task.update_status(f\"Processing {i+1}/{len(items)}\")\n        await process_item(item)\n\n    await task.update_status(\"Finalizing...\")\n    return CallToolResult(content=[TextContent(type=\"text\", text=\"Complete\")])\n```\n\nStatus messages appear in `tasks/get` responses, letting clients show progress to users.\n\n## Elicitation Within Tasks\n\nTasks can request user input via elicitation. This transitions the task to `input_required` status.\n\n### Form Elicitation\n\nCollect structured data from the user:\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    await task.update_status(\"Waiting for confirmation...\")\n\n    result = await task.elicit(\n        message=\"Delete these files?\",\n        requestedSchema={\n            \"type\": \"object\",\n            \"properties\": {\n                \"confirm\": {\"type\": \"boolean\"},\n                \"reason\": {\"type\": \"string\"},\n            },\n            \"required\": [\"confirm\"],\n        },\n    )\n\n    if result.action == \"accept\" and result.content.get(\"confirm\"):\n        # User confirmed\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"Files deleted\")])\n    else:\n        # User declined or cancelled\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"Cancelled\")])\n```\n\n### URL Elicitation\n\nDirect users to external URLs for OAuth, payments, or other out-of-band flows:\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    await task.update_status(\"Waiting for OAuth...\")\n\n    result = await task.elicit_url(\n        message=\"Please authorize with GitHub\",\n        url=\"https://github.com/login/oauth/authorize?client_id=...\",\n        elicitation_id=\"oauth-github-123\",\n    )\n\n    if result.action == \"accept\":\n        # User completed OAuth flow\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"Connected to GitHub\")])\n    else:\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"OAuth cancelled\")])\n```\n\n## Sampling Within Tasks\n\nTasks can request LLM completions from the client:\n\n```python\nfrom mcp.types import SamplingMessage, TextContent\n\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    await task.update_status(\"Generating response...\")\n\n    result = await task.create_message(\n        messages=[\n            SamplingMessage(\n                role=\"user\",\n                content=TextContent(type=\"text\", text=\"Write a haiku about coding\"),\n            )\n        ],\n        max_tokens=100,\n    )\n\n    haiku = result.content.text if isinstance(result.content, TextContent) else \"Error\"\n    return CallToolResult(content=[TextContent(type=\"text\", text=haiku)])\n```\n\nSampling supports additional parameters:\n\n```python\nresult = await task.create_message(\n    messages=[...],\n    max_tokens=500,\n    system_prompt=\"You are a helpful assistant\",\n    temperature=0.7,\n    stop_sequences=[\"\\n\\n\"],\n    model_preferences=ModelPreferences(hints=[ModelHint(name=\"claude-3\")]),\n)\n```\n\n## Cancellation Support\n\nCheck for cancellation in long-running work:\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    for i in range(1000):\n        if task.is_cancelled:\n            # Clean up and exit\n            return CallToolResult(content=[TextContent(type=\"text\", text=\"Cancelled\")])\n\n        await task.update_status(f\"Step {i}/1000\")\n        await process_step(i)\n\n    return CallToolResult(content=[TextContent(type=\"text\", text=\"Complete\")])\n```\n\nThe SDK's default cancel handler updates the task status. Your work function should check `is_cancelled` periodically.\n\n## Custom Task Store\n\nFor production, implement `TaskStore` with persistent storage:\n\n```python\nfrom mcp.shared.experimental.tasks.store import TaskStore\nfrom mcp.types import Task, TaskMetadata, Result\n\nclass RedisTaskStore(TaskStore):\n    def __init__(self, redis_client):\n        self.redis = redis_client\n\n    async def create_task(self, metadata: TaskMetadata, task_id: str | None = None) -> Task:\n        # Create and persist task\n        ...\n\n    async def get_task(self, task_id: str) -> Task | None:\n        # Retrieve task from Redis\n        ...\n\n    async def update_task(self, task_id: str, status: str | None = None, ...) -> Task:\n        # Update and persist\n        ...\n\n    async def store_result(self, task_id: str, result: Result) -> None:\n        # Store result in Redis\n        ...\n\n    async def get_result(self, task_id: str) -> Result | None:\n        # Retrieve result\n        ...\n\n    # ... implement remaining methods\n```\n\nUse your custom store:\n\n```python\nstore = RedisTaskStore(redis_client)\nserver.experimental.enable_tasks(store=store)\n```\n\n## Complete Example\n\nA server with multiple task-supporting tools:\n\n```python\nfrom mcp.server import Server\nfrom mcp.server.experimental.task_context import ServerTaskContext\nfrom mcp.types import (\n    CallToolResult, CreateTaskResult, TextContent, Tool, ToolExecution,\n    SamplingMessage, TASK_REQUIRED,\n)\n\nserver = Server(\"task-demo\")\nserver.experimental.enable_tasks()\n\n\n@server.list_tools()\nasync def list_tools():\n    return [\n        Tool(\n            name=\"confirm_action\",\n            description=\"Requires user confirmation\",\n            inputSchema={\"type\": \"object\", \"properties\": {\"action\": {\"type\": \"string\"}}},\n            execution=ToolExecution(taskSupport=TASK_REQUIRED),\n        ),\n        Tool(\n            name=\"generate_text\",\n            description=\"Generate text via LLM\",\n            inputSchema={\"type\": \"object\", \"properties\": {\"prompt\": {\"type\": \"string\"}}},\n            execution=ToolExecution(taskSupport=TASK_REQUIRED),\n        ),\n    ]\n\n\nasync def handle_confirm_action(arguments: dict) -> CreateTaskResult:\n    ctx = server.request_context\n    ctx.experimental.validate_task_mode(TASK_REQUIRED)\n\n    action = arguments.get(\"action\", \"unknown action\")\n\n    async def work(task: ServerTaskContext) -> CallToolResult:\n        result = await task.elicit(\n            message=f\"Confirm: {action}?\",\n            requestedSchema={\n                \"type\": \"object\",\n                \"properties\": {\"confirm\": {\"type\": \"boolean\"}},\n                \"required\": [\"confirm\"],\n            },\n        )\n\n        if result.action == \"accept\" and result.content.get(\"confirm\"):\n            return CallToolResult(content=[TextContent(type=\"text\", text=f\"Executed: {action}\")])\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"Cancelled\")])\n\n    return await ctx.experimental.run_task(work)\n\n\nasync def handle_generate_text(arguments: dict) -> CreateTaskResult:\n    ctx = server.request_context\n    ctx.experimental.validate_task_mode(TASK_REQUIRED)\n\n    prompt = arguments.get(\"prompt\", \"Hello\")\n\n    async def work(task: ServerTaskContext) -> CallToolResult:\n        await task.update_status(\"Generating...\")\n\n        result = await task.create_message(\n            messages=[SamplingMessage(role=\"user\", content=TextContent(type=\"text\", text=prompt))],\n            max_tokens=200,\n        )\n\n        text = result.content.text if isinstance(result.content, TextContent) else \"Error\"\n        return CallToolResult(content=[TextContent(type=\"text\", text=text)])\n\n    return await ctx.experimental.run_task(work)\n\n\n@server.call_tool()\nasync def handle_tool(name: str, arguments: dict) -> CallToolResult | CreateTaskResult:\n    if name == \"confirm_action\":\n        return await handle_confirm_action(arguments)\n    elif name == \"generate_text\":\n        return await handle_generate_text(arguments)\n    return CallToolResult(content=[TextContent(type=\"text\", text=f\"Unknown: {name}\")], isError=True)\n```\n\n## Error Handling in Tasks\n\nTasks handle errors automatically, but you can also fail explicitly:\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    try:\n        result = await risky_operation()\n        return CallToolResult(content=[TextContent(type=\"text\", text=result)])\n    except PermissionError:\n        await task.fail(\"Access denied - insufficient permissions\")\n        raise\n    except TimeoutError:\n        await task.fail(\"Operation timed out after 30 seconds\")\n        raise\n```\n\nWhen `run_task()` catches an exception, it automatically:\n\n1. Marks the task as `failed`\n2. Sets `statusMessage` to the exception message\n3. Propagates the exception (which is caught by the task group)\n\nFor custom error messages, call `task.fail()` before raising.\n\n## HTTP Transport Example\n\nFor web applications, use the Streamable HTTP transport:\n\n```python\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\n\nimport uvicorn\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\n\nfrom mcp.server import Server\nfrom mcp.server.experimental.task_context import ServerTaskContext\nfrom mcp.server.streamable_http_manager import StreamableHTTPSessionManager\nfrom mcp.types import (\n    CallToolResult, CreateTaskResult, TextContent, Tool, ToolExecution, TASK_REQUIRED,\n)\n\n\nserver = Server(\"http-task-server\")\nserver.experimental.enable_tasks()\n\n\n@server.list_tools()\nasync def list_tools():\n    return [\n        Tool(\n            name=\"long_operation\",\n            description=\"A long-running operation\",\n            inputSchema={\"type\": \"object\", \"properties\": {\"duration\": {\"type\": \"number\"}}},\n            execution=ToolExecution(taskSupport=TASK_REQUIRED),\n        )\n    ]\n\n\nasync def handle_long_operation(arguments: dict) -> CreateTaskResult:\n    ctx = server.request_context\n    ctx.experimental.validate_task_mode(TASK_REQUIRED)\n\n    duration = arguments.get(\"duration\", 5)\n\n    async def work(task: ServerTaskContext) -> CallToolResult:\n        import anyio\n        for i in range(int(duration)):\n            await task.update_status(f\"Step {i+1}/{int(duration)}\")\n            await anyio.sleep(1)\n        return CallToolResult(content=[TextContent(type=\"text\", text=f\"Completed after {duration}s\")])\n\n    return await ctx.experimental.run_task(work)\n\n\n@server.call_tool()\nasync def handle_tool(name: str, arguments: dict) -> CallToolResult | CreateTaskResult:\n    if name == \"long_operation\":\n        return await handle_long_operation(arguments)\n    return CallToolResult(content=[TextContent(type=\"text\", text=f\"Unknown: {name}\")], isError=True)\n\n\ndef create_app():\n    session_manager = StreamableHTTPSessionManager(app=server)\n\n    @asynccontextmanager\n    async def lifespan(app: Starlette) -> AsyncIterator[None]:\n        async with session_manager.run():\n            yield\n\n    return Starlette(\n        routes=[Mount(\"/mcp\", app=session_manager.handle_request)],\n        lifespan=lifespan,\n    )\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(create_app(), host=\"127.0.0.1\", port=8000)\n```\n\n## Testing Task Servers\n\nTest task functionality with the SDK's testing utilities:\n\n```python\nimport pytest\nimport anyio\nfrom mcp.client.session import ClientSession\nfrom mcp.types import CallToolResult\n\n\n@pytest.mark.anyio\nasync def test_task_tool():\n    server_to_client_send, server_to_client_receive = anyio.create_memory_object_stream(10)\n    client_to_server_send, client_to_server_receive = anyio.create_memory_object_stream(10)\n\n    async def run_server():\n        await server.run(\n            client_to_server_receive,\n            server_to_client_send,\n            server.create_initialization_options(),\n        )\n\n    async def run_client():\n        async with ClientSession(server_to_client_receive, client_to_server_send) as session:\n            await session.initialize()\n\n            # Call the tool as a task\n            result = await session.experimental.call_tool_as_task(\"my_tool\", {\"arg\": \"value\"})\n            task_id = result.task.taskId\n            assert result.task.status == \"working\"\n\n            # Poll until complete\n            async for status in session.experimental.poll_task(task_id):\n                if status.status in (\"completed\", \"failed\"):\n                    break\n\n            # Get result\n            final = await session.experimental.get_task_result(task_id, CallToolResult)\n            assert len(final.content) > 0\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(run_server)\n        tg.start_soon(run_client)\n```\n\n## Best Practices\n\n### Keep Work Functions Focused\n\n```python\n# Good: focused work function\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    await task.update_status(\"Validating...\")\n    validate_input(arguments)\n\n    await task.update_status(\"Processing...\")\n    result = await process_data(arguments)\n\n    return CallToolResult(content=[TextContent(type=\"text\", text=result)])\n```\n\n### Check Cancellation in Loops\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    results = []\n    for item in large_dataset:\n        if task.is_cancelled:\n            return CallToolResult(content=[TextContent(type=\"text\", text=\"Cancelled\")])\n\n        results.append(await process(item))\n\n    return CallToolResult(content=[TextContent(type=\"text\", text=str(results))])\n```\n\n### Use Meaningful Status Messages\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    await task.update_status(\"Connecting to database...\")\n    db = await connect()\n\n    await task.update_status(\"Fetching records (0/1000)...\")\n    for i, record in enumerate(records):\n        if i % 100 == 0:\n            await task.update_status(f\"Processing records ({i}/1000)...\")\n        await process(record)\n\n    await task.update_status(\"Finalizing results...\")\n    return CallToolResult(content=[TextContent(type=\"text\", text=\"Done\")])\n```\n\n### Handle Elicitation Responses\n\n```python\nasync def work(task: ServerTaskContext) -> CallToolResult:\n    result = await task.elicit(message=\"Continue?\", requestedSchema={...})\n\n    match result.action:\n        case \"accept\":\n            # User accepted, process content\n            return await process_accepted(result.content)\n        case \"decline\":\n            # User explicitly declined\n            return CallToolResult(content=[TextContent(type=\"text\", text=\"User declined\")])\n        case \"cancel\":\n            # User cancelled the elicitation\n            return CallToolResult(content=[TextContent(type=\"text\", text=\"Cancelled\")])\n```\n\n## Next Steps\n\n- [Client Usage](tasks-client.md) - Learn how clients interact with task servers\n- [Tasks Overview](tasks.md) - Review lifecycle and concepts\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/experimental/index.md","content":"# Experimental Features\n\n!!! warning \"Experimental APIs\"\n\n    The features in this section are experimental and may change without notice.\n    They track the evolving MCP specification and are not yet stable.\n\nThis section documents experimental features in the MCP Python SDK. These features\nimplement draft specifications that are still being refined.\n\n## Available Experimental Features\n\n### [Tasks](tasks.md)\n\nTasks enable asynchronous execution of MCP operations. Instead of waiting for a\nlong-running operation to complete, the server returns a task reference immediately.\nClients can then poll for status updates and retrieve results when ready.\n\nTasks are useful for:\n\n- **Long-running computations** that would otherwise block\n- **Batch operations** that process many items\n- **Interactive workflows** that require user input (elicitation) or LLM assistance (sampling)\n\n## Using Experimental APIs\n\nExperimental features are accessed via the `.experimental` property:\n\n```python\n# Server-side\n@server.experimental.get_task()\nasync def handle_get_task(request: GetTaskRequest) -> GetTaskResult:\n    ...\n\n# Client-side\nresult = await session.experimental.call_tool_as_task(\"tool_name\", {\"arg\": \"value\"})\n```\n\n## Providing Feedback\n\nSince these features are experimental, feedback is especially valuable. If you encounter\nissues or have suggestions, please open an issue on the\n[python-sdk repository](https://github.com/modelcontextprotocol/python-sdk/issues).\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/experimental/tasks.md","content":"# Tasks\n\n!!! warning \"Experimental\"\n\n    Tasks are an experimental feature tracking the draft MCP specification.\n    The API may change without notice.\n\nTasks enable asynchronous request handling in MCP. Instead of blocking until an operation completes, the receiver creates a task, returns immediately, and the requestor polls for the result.\n\n## When to Use Tasks\n\nTasks are designed for operations that:\n\n- Take significant time (seconds to minutes)\n- Need progress updates during execution\n- Require user input mid-execution (elicitation, sampling)\n- Should run without blocking the requestor\n\nCommon use cases:\n\n- Long-running data processing\n- Multi-step workflows with user confirmation\n- LLM-powered operations requiring sampling\n- OAuth flows requiring user browser interaction\n\n## Task Lifecycle\n\n```text\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   working   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚            â”‚            â”‚\n              â–¼            â–¼            â–¼\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚ completed  â”‚  â”‚  failed   â”‚  â”‚ cancelled â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              â–²\n              â”‚\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚ input_required  â”‚â—„â”€â”€â”€â”€â”€â”€â”\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n              â”‚                â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n| Status | Description |\n|--------|-------------|\n| `working` | Task is being processed |\n| `input_required` | Receiver needs input from requestor (elicitation/sampling) |\n| `completed` | Task finished successfully |\n| `failed` | Task encountered an error |\n| `cancelled` | Task was cancelled by requestor |\n\nTerminal states (`completed`, `failed`, `cancelled`) are finalâ€”tasks cannot transition out of them.\n\n## Bidirectional Flow\n\nTasks work in both directions:\n\n**Client â†’ Server** (most common):\n\n```text\nClient                              Server\n  â”‚                                    â”‚\n  â”‚â”€â”€ tools/call (task) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Creates task\n  â”‚<â”€â”€ CreateTaskResult â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n  â”‚                                    â”‚\n  â”‚â”€â”€ tasks/get â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚\n  â”‚<â”€â”€ status: working â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n  â”‚                                    â”‚ ... work continues ...\n  â”‚â”€â”€ tasks/get â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚\n  â”‚<â”€â”€ status: completed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n  â”‚                                    â”‚\n  â”‚â”€â”€ tasks/result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚\n  â”‚<â”€â”€ CallToolResult â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n```\n\n**Server â†’ Client** (for elicitation/sampling):\n\n```text\nServer                              Client\n  â”‚                                    â”‚\n  â”‚â”€â”€ elicitation/create (task) â”€â”€â”€â”€â”€â”€>â”‚ Creates task\n  â”‚<â”€â”€ CreateTaskResult â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n  â”‚                                    â”‚\n  â”‚â”€â”€ tasks/get â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚\n  â”‚<â”€â”€ status: working â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n  â”‚                                    â”‚ ... user interaction ...\n  â”‚â”€â”€ tasks/get â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚\n  â”‚<â”€â”€ status: completed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n  â”‚                                    â”‚\n  â”‚â”€â”€ tasks/result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚\n  â”‚<â”€â”€ ElicitResult â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n```\n\n## Key Concepts\n\n### Task Metadata\n\nWhen augmenting a request with task execution, include `TaskMetadata`:\n\n```python\nfrom mcp.types import TaskMetadata\n\ntask = TaskMetadata(ttl=60000)  # TTL in milliseconds\n```\n\nThe `ttl` (time-to-live) specifies how long the task and result are retained after completion.\n\n### Task Store\n\nServers persist task state in a `TaskStore`. The SDK provides `InMemoryTaskStore` for development:\n\n```python\nfrom mcp.shared.experimental.tasks import InMemoryTaskStore\n\nstore = InMemoryTaskStore()\n```\n\nFor production, implement `TaskStore` with a database or distributed cache.\n\n### Capabilities\n\nBoth servers and clients declare task support through capabilities:\n\n**Server capabilities:**\n\n- `tasks.requests.tools.call` - Server accepts task-augmented tool calls\n\n**Client capabilities:**\n\n- `tasks.requests.sampling.createMessage` - Client accepts task-augmented sampling\n- `tasks.requests.elicitation.create` - Client accepts task-augmented elicitation\n\nThe SDK manages these automatically when you enable task support.\n\n## Quick Example\n\n**Server** (simplified API):\n\n```python\nfrom mcp.server import Server\nfrom mcp.server.experimental.task_context import ServerTaskContext\nfrom mcp.types import CallToolResult, TextContent, TASK_REQUIRED\n\nserver = Server(\"my-server\")\nserver.experimental.enable_tasks()  # One-line setup\n\n@server.call_tool()\nasync def handle_tool(name: str, arguments: dict):\n    ctx = server.request_context\n    ctx.experimental.validate_task_mode(TASK_REQUIRED)\n\n    async def work(task: ServerTaskContext):\n        await task.update_status(\"Processing...\")\n        # ... do work ...\n        return CallToolResult(content=[TextContent(type=\"text\", text=\"Done!\")])\n\n    return await ctx.experimental.run_task(work)\n```\n\n**Client:**\n\n```python\nfrom mcp.client.session import ClientSession\nfrom mcp.types import CallToolResult\n\nasync with ClientSession(read, write) as session:\n    await session.initialize()\n\n    # Call tool as task\n    result = await session.experimental.call_tool_as_task(\"my_tool\", {\"arg\": \"value\"})\n    task_id = result.task.taskId\n\n    # Poll until done\n    async for status in session.experimental.poll_task(task_id):\n        print(f\"Status: {status.status}\")\n\n    # Get result\n    final = await session.experimental.get_task_result(task_id, CallToolResult)\n```\n\n## Next Steps\n\n- [Server Implementation](tasks-server.md) - Build task-supporting servers\n- [Client Usage](tasks-client.md) - Call and poll tasks from clients\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/experimental/tasks-client.md","content":"# Client Task Usage\n\n!!! warning \"Experimental\"\n\n    Tasks are an experimental feature. The API may change without notice.\n\nThis guide covers calling task-augmented tools from clients, handling the `input_required` status, and advanced patterns like receiving task requests from servers.\n\n## Quick Start\n\nCall a tool as a task and poll for the result:\n\n```python\nfrom mcp.client.session import ClientSession\nfrom mcp.types import CallToolResult\n\nasync with ClientSession(read, write) as session:\n    await session.initialize()\n\n    # Call tool as task\n    result = await session.experimental.call_tool_as_task(\n        \"process_data\",\n        {\"input\": \"hello\"},\n        ttl=60000,\n    )\n    task_id = result.task.taskId\n\n    # Poll until complete\n    async for status in session.experimental.poll_task(task_id):\n        print(f\"Status: {status.status} - {status.statusMessage or ''}\")\n\n    # Get result\n    final = await session.experimental.get_task_result(task_id, CallToolResult)\n    print(f\"Result: {final.content[0].text}\")\n```\n\n## Calling Tools as Tasks\n\nUse `call_tool_as_task()` to invoke a tool with task augmentation:\n\n```python\nresult = await session.experimental.call_tool_as_task(\n    \"my_tool\",           # Tool name\n    {\"arg\": \"value\"},    # Arguments\n    ttl=60000,           # Time-to-live in milliseconds\n    meta={\"key\": \"val\"}, # Optional metadata\n)\n\ntask_id = result.task.taskId\nprint(f\"Task: {task_id}, Status: {result.task.status}\")\n```\n\nThe response is a `CreateTaskResult` containing:\n\n- `task.taskId` - Unique identifier for polling\n- `task.status` - Initial status (usually `\"working\"`)\n- `task.pollInterval` - Suggested polling interval (milliseconds)\n- `task.ttl` - Time-to-live for results\n- `task.createdAt` - Creation timestamp\n\n## Polling with poll_task\n\nThe `poll_task()` async iterator polls until the task reaches a terminal state:\n\n```python\nasync for status in session.experimental.poll_task(task_id):\n    print(f\"Status: {status.status}\")\n    if status.statusMessage:\n        print(f\"Progress: {status.statusMessage}\")\n```\n\nIt automatically:\n\n- Respects the server's suggested `pollInterval`\n- Stops when status is `completed`, `failed`, or `cancelled`\n- Yields each status for progress display\n\n### Handling input_required\n\nWhen a task needs user input (elicitation), it transitions to `input_required`. You must call `get_task_result()` to receive and respond to the elicitation:\n\n```python\nasync for status in session.experimental.poll_task(task_id):\n    print(f\"Status: {status.status}\")\n\n    if status.status == \"input_required\":\n        # This delivers the elicitation and waits for completion\n        final = await session.experimental.get_task_result(task_id, CallToolResult)\n        break\n```\n\nThe elicitation callback (set during session creation) handles the actual user interaction.\n\n## Elicitation Callbacks\n\nTo handle elicitation requests from the server, provide a callback when creating the session:\n\n```python\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nasync def handle_elicitation(context, params: ElicitRequestParams) -> ElicitResult:\n    # Display the message to the user\n    print(f\"Server asks: {params.message}\")\n\n    # Collect user input (this is a simplified example)\n    response = input(\"Your response (y/n): \")\n    confirmed = response.lower() == \"y\"\n\n    return ElicitResult(\n        action=\"accept\",\n        content={\"confirm\": confirmed},\n    )\n\nasync with ClientSession(\n    read,\n    write,\n    elicitation_callback=handle_elicitation,\n) as session:\n    await session.initialize()\n    # ... call tasks that may require elicitation\n```\n\n## Sampling Callbacks\n\nSimilarly, handle sampling requests with a callback:\n\n```python\nfrom mcp.types import CreateMessageRequestParams, CreateMessageResult, TextContent\n\nasync def handle_sampling(context, params: CreateMessageRequestParams) -> CreateMessageResult:\n    # In a real implementation, call your LLM here\n    prompt = params.messages[-1].content.text if params.messages else \"\"\n\n    # Return a mock response\n    return CreateMessageResult(\n        role=\"assistant\",\n        content=TextContent(type=\"text\", text=f\"Response to: {prompt}\"),\n        model=\"my-model\",\n    )\n\nasync with ClientSession(\n    read,\n    write,\n    sampling_callback=handle_sampling,\n) as session:\n    # ...\n```\n\n## Retrieving Results\n\nOnce a task completes, retrieve the result:\n\n```python\nif status.status == \"completed\":\n    result = await session.experimental.get_task_result(task_id, CallToolResult)\n    for content in result.content:\n        if hasattr(content, \"text\"):\n            print(content.text)\n\nelif status.status == \"failed\":\n    print(f\"Task failed: {status.statusMessage}\")\n\nelif status.status == \"cancelled\":\n    print(\"Task was cancelled\")\n```\n\nThe result type matches the original request:\n\n- `tools/call` â†’ `CallToolResult`\n- `sampling/createMessage` â†’ `CreateMessageResult`\n- `elicitation/create` â†’ `ElicitResult`\n\n## Cancellation\n\nCancel a running task:\n\n```python\ncancel_result = await session.experimental.cancel_task(task_id)\nprint(f\"Cancelled, status: {cancel_result.status}\")\n```\n\nNote: Cancellation is cooperativeâ€”the server must check for and handle cancellation.\n\n## Listing Tasks\n\nView all tasks on the server:\n\n```python\nresult = await session.experimental.list_tasks()\nfor task in result.tasks:\n    print(f\"{task.taskId}: {task.status}\")\n\n# Handle pagination\nwhile result.nextCursor:\n    result = await session.experimental.list_tasks(cursor=result.nextCursor)\n    for task in result.tasks:\n        print(f\"{task.taskId}: {task.status}\")\n```\n\n## Advanced: Client as Task Receiver\n\nServers can send task-augmented requests to clients. This is useful when the server needs the client to perform async work (like complex sampling or user interaction).\n\n### Declaring Client Capabilities\n\nRegister task handlers to declare what task-augmented requests your client accepts:\n\n```python\nfrom mcp.client.experimental.task_handlers import ExperimentalTaskHandlers\nfrom mcp.types import (\n    CreateTaskResult, GetTaskResult, GetTaskPayloadResult,\n    TaskMetadata, ElicitRequestParams,\n)\nfrom mcp.shared.experimental.tasks import InMemoryTaskStore\n\n# Client-side task store\nclient_store = InMemoryTaskStore()\n\nasync def handle_augmented_elicitation(context, params: ElicitRequestParams, task_metadata: TaskMetadata):\n    \"\"\"Handle task-augmented elicitation from server.\"\"\"\n    # Create a task for this elicitation\n    task = await client_store.create_task(task_metadata)\n\n    # Start async work (e.g., show UI, wait for user)\n    async def complete_elicitation():\n        # ... do async work ...\n        result = ElicitResult(action=\"accept\", content={\"confirm\": True})\n        await client_store.store_result(task.taskId, result)\n        await client_store.update_task(task.taskId, status=\"completed\")\n\n    context.session._task_group.start_soon(complete_elicitation)\n\n    # Return task reference immediately\n    return CreateTaskResult(task=task)\n\nasync def handle_get_task(context, params):\n    \"\"\"Handle tasks/get from server.\"\"\"\n    task = await client_store.get_task(params.taskId)\n    return GetTaskResult(\n        taskId=task.taskId,\n        status=task.status,\n        statusMessage=task.statusMessage,\n        createdAt=task.createdAt,\n        lastUpdatedAt=task.lastUpdatedAt,\n        ttl=task.ttl,\n        pollInterval=100,\n    )\n\nasync def handle_get_task_result(context, params):\n    \"\"\"Handle tasks/result from server.\"\"\"\n    result = await client_store.get_result(params.taskId)\n    return GetTaskPayloadResult.model_validate(result.model_dump())\n\ntask_handlers = ExperimentalTaskHandlers(\n    augmented_elicitation=handle_augmented_elicitation,\n    get_task=handle_get_task,\n    get_task_result=handle_get_task_result,\n)\n\nasync with ClientSession(\n    read,\n    write,\n    experimental_task_handlers=task_handlers,\n) as session:\n    # Client now accepts task-augmented elicitation from server\n    await session.initialize()\n```\n\nThis enables flows where:\n\n1. Client calls a task-augmented tool\n2. Server's tool work calls `task.elicit_as_task()`\n3. Client receives task-augmented elicitation\n4. Client creates its own task, does async work\n5. Server polls client's task\n6. Eventually both tasks complete\n\n## Complete Example\n\nA client that handles all task scenarios:\n\n```python\nimport anyio\nfrom mcp.client.session import ClientSession\nfrom mcp.client.stdio import stdio_client\nfrom mcp.types import CallToolResult, ElicitRequestParams, ElicitResult\n\n\nasync def elicitation_callback(context, params: ElicitRequestParams) -> ElicitResult:\n    print(f\"\\n[Elicitation] {params.message}\")\n    response = input(\"Confirm? (y/n): \")\n    return ElicitResult(action=\"accept\", content={\"confirm\": response.lower() == \"y\"})\n\n\nasync def main():\n    async with stdio_client(command=\"python\", args=[\"server.py\"]) as (read, write):\n        async with ClientSession(\n            read,\n            write,\n            elicitation_callback=elicitation_callback,\n        ) as session:\n            await session.initialize()\n\n            # List available tools\n            tools = await session.list_tools()\n            print(\"Tools:\", [t.name for t in tools.tools])\n\n            # Call a task-augmented tool\n            print(\"\\nCalling task tool...\")\n            result = await session.experimental.call_tool_as_task(\n                \"confirm_action\",\n                {\"action\": \"delete files\"},\n            )\n            task_id = result.task.taskId\n            print(f\"Task created: {task_id}\")\n\n            # Poll and handle input_required\n            async for status in session.experimental.poll_task(task_id):\n                print(f\"Status: {status.status}\")\n\n                if status.status == \"input_required\":\n                    final = await session.experimental.get_task_result(task_id, CallToolResult)\n                    print(f\"Result: {final.content[0].text}\")\n                    break\n\n            if status.status == \"completed\":\n                final = await session.experimental.get_task_result(task_id, CallToolResult)\n                print(f\"Result: {final.content[0].text}\")\n\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n```\n\n## Error Handling\n\nHandle task errors gracefully:\n\n```python\nfrom mcp.shared.exceptions import McpError\n\ntry:\n    result = await session.experimental.call_tool_as_task(\"my_tool\", args)\n    task_id = result.task.taskId\n\n    async for status in session.experimental.poll_task(task_id):\n        if status.status == \"failed\":\n            raise RuntimeError(f\"Task failed: {status.statusMessage}\")\n\n    final = await session.experimental.get_task_result(task_id, CallToolResult)\n\nexcept McpError as e:\n    print(f\"MCP error: {e.error.message}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n## Next Steps\n\n- [Server Implementation](tasks-server.md) - Build task-supporting servers\n- [Tasks Overview](tasks.md) - Review lifecycle and concepts\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/index.md","content":"# MCP Python SDK\n\nThe **Model Context Protocol (MCP)** allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction.\n\nThis Python SDK implements the full MCP specification, making it easy to:\n\n- **Build MCP servers** that expose resources, prompts, and tools\n- **Create MCP clients** that can connect to any MCP server\n- **Use standard transports** like stdio, SSE, and Streamable HTTP\n\nIf you want to read more about the specification, please visit the [MCP documentation](https://modelcontextprotocol.io).\n\n## Quick Example\n\nHere's a simple MCP server that exposes a tool, resource, and prompt:\n\n```python title=\"server.py\"\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Test Server\", json_response=True)\n\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n@mcp.prompt()\ndef greet_user(name: str, style: str = \"friendly\") -> str:\n    \"\"\"Generate a greeting prompt\"\"\"\n    return f\"Write a {style} greeting for someone named {name}.\"\n\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n\nRun the server:\n\n```bash\nuv run --with mcp server.py\n```\n\nThen open the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) and connect to `http://localhost:8000/mcp`:\n\n```bash\nnpx -y @modelcontextprotocol/inspector\n```\n\n## Getting Started\n\n<!-- TODO(Marcelo): automatically generate the follow references with a header on each of those files. -->\n1. **[Install](installation.md)** the MCP SDK\n2. **[Learn concepts](concepts.md)** - understand the three primitives and architecture\n3. **[Explore authorization](authorization.md)** - add security to your servers\n4. **[Use low-level APIs](low-level-server.md)** - for advanced customization\n\n## API Reference\n\nFull API documentation is available in the [API Reference](api.md).\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/low-level-server.md","content":"# Low-Level Server\n\n!!! warning \"Under Construction\"\n\n    This page is currently being written. Check back soon for complete documentation.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/concepts.md","content":"# Concepts\n\n!!! warning \"Under Construction\"\n\n    This page is currently being written. Check back soon for complete documentation.\n\n<!--\n  - Server vs Client\n  - Three primitives (tools, resources, prompts)\n  - Transports (stdio, SSE, streamable HTTP)\n  - Context and sessions\n  - Lifecycle and state\n -->\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/authorization.md","content":"# Authorization\n\n!!! warning \"Under Construction\"\n\n    This page is currently being written. Check back soon for complete documentation.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/api.md","content":"::: mcp\n"}
{"source":"github","repo":"mcp-python-sdk","path":"docs/installation.md","content":"# Installation\n\nThe Python SDK is available on PyPI as [`mcp`](https://pypi.org/project/mcp/) so installation is as simple as:\n\n=== \"pip\"\n\n    ```bash\n    pip install mcp\n    ```\n=== \"uv\"\n\n    ```bash\n    uv add mcp\n    ```\n\nThe following dependencies are automatically installed:\n\n- [`httpx`](https://pypi.org/project/httpx/): HTTP client to handle HTTP Streamable and SSE transports.\n- [`httpx-sse`](https://pypi.org/project/httpx-sse/): HTTP client to handle SSE transport.\n- [`pydantic`](https://pypi.org/project/pydantic/): Types, JSON schema generation, data validation, and [more](https://docs.pydantic.dev/latest/).\n- [`starlette`](https://pypi.org/project/starlette/): Web framework used to build the HTTP transport endpoints.\n- [`python-multipart`](https://pypi.org/project/python-multipart/): Handle HTTP body parsing.\n- [`sse-starlette`](https://pypi.org/project/sse-starlette/): Server-Sent Events for Starlette, used to build the SSE transport endpoint.\n- [`pydantic-settings`](https://pypi.org/project/pydantic-settings/): Settings management used in FastMCP.\n- [`uvicorn`](https://pypi.org/project/uvicorn/): ASGI server used to run the HTTP transport endpoints.\n- [`jsonschema`](https://pypi.org/project/jsonschema/): JSON schema validation.\n- [`pywin32`](https://pypi.org/project/pywin32/): Windows specific dependencies for the CLI tools.\n\nThis package has the following optional groups:\n\n- `cli`: Installs `typer` and `python-dotenv` for the MCP CLI tools.\n"}
{"source":"github","repo":"mcp-python-sdk","path":"RELEASE.md","content":"# Release Process\n\n## Bumping Dependencies\n\n1. Change dependency version in `pyproject.toml`\n2. Upgrade lock with `uv lock --resolution lowest-direct`\n\n## Major or Minor Release\n\nCreate a GitHub release via UI with the tag being `vX.Y.Z` where `X.Y.Z` is the version,\nand the release title being the same. Then ask someone to review the release.\n\nThe package version will be set automatically from the tag.\n"}
