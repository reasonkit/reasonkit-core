{"id": "aider_71161aea57605c434dea6da38932ec44", "source": "aider", "path": "/home/zyxsys/RK-PROJECT/reasonkit-core/data/docs/raw/aider/benchmark/README.md", "title": "README", "content": "\n# Aider benchmark harness\n\nAider uses benchmarks to quantitatively measure how well it works\nwith various LLMs.\nThis directory holds the harness and tools needed to run the benchmarking suite.\n\n## Background\n\nThe benchmark is based on the [Exercism](https://github.com/exercism/python) coding exercises.\nThis\nbenchmark evaluates how effectively aider and LLMs can translate a\nnatural language coding request into executable code saved into\nfiles that pass unit tests.\nIt provides an end-to-end evaluation of not just\nthe LLM's coding ability, but also its capacity to *edit existing code*\nand *format those code edits* so that aider can save the\nedits to the local source files.\n\nSee [this writeup for a longer discussion about the benchmark](https://aider.chat/2024/12/21/polyglot.html).\n\nThe benchmark is intended to be run *inside a docker container*.\nThis is because the benchmarking harness will be\ntaking code written by an LLM\nand executing it without any human review or supervision!\nThe LLM could generate dangerous python that harms your system, like this: `import os; os.system(\"sudo rm -rf /\")`.\nRunning inside a docker container helps limit the damage that could be done.\n\n## Usage\n\nThere are 3 main tasks involved in benchmarking aider:\n\n1. Install and setup for benchmarking.\n\n2. Run the benchmark to measure performance across all the exercises.\n\n3. Generate a summary report of how many of the exercises succeeded or failed.\n\n### Setup for benchmarking\n\nFirst, prepare all the groundwork for running the benchmarks.\nThese steps only need to be done once.\n\n```\n# Clone the aider repo\ngit clone https://github.com/Aider-AI/aider.git\n\n# Create the scratch dir to hold benchmarking results inside the main aider dir:\ncd aider\nmkdir tmp.benchmarks\n\n# Clone the repo with the exercises\ngit clone https://github.com/Aider-AI/polyglot-benchmark tmp.benchmarks/polyglot-benchmark\n\n# Build the docker container\n./benchmark/docker_build.sh\n```\n\n### Running the benchmark\n\nLaunch the docker container and run the benchmark inside it:\n\n```\n# Launch the docker container\n./benchmark/docker.sh\n\n# Inside the container, install aider as a development build.\n# This way you're running the code that you cloned above, including any local changes.\npip install -e .[dev]\n\n# Run the benchmark:\n./benchmark/benchmark.py a-helpful-name-for-this-run --model gpt-3.5-turbo --edit-format whole --threads 10 --exercises-dir polyglot-benchmark\n```\n\nThe above will create a folder `tmp.benchmarks/YYYY-MM-DD-HH-MM-SS--a-helpful-name-for-this-run` with benchmarking results.\nRun like this, the script will run all the exercises in a random order.\n\nYou can run `./benchmark/benchmark.py --help` for a list of all the arguments, but here are the most useful to keep in mind:\n\n- `--model` is the name of the model, same as you would pass directly to `aider`.\n- `--edit-format` is the name of the edit format, same as you would pass directly to `aider`. When working with an experimental LLM, I recommend starting with `whole`\n- `--threads` specifies how many exercises to benchmark in parallel. Start with a single thread if you are working out the kinks on your benchmarking setup or working with a new model, etc. Once you are getting reliable results, you can speed up the process by running with more threads. 10 works well against the OpenAI APIs.\n- `--num-tests` specifies how many of the tests to run before stopping. This is another way to start gently as you debug your benchmarking setup.\n- `--keywords` filters the tests to run to only the ones whose name match the supplied argument (similar to `pytest -k xxxx`).\n- `--read-model-settings=<filename.yml>` specify model settings, see here: https://aider.chat/docs/config/adv-model-settings.html#model-settings\n\n### Benchmark report\n\nYou can generate stats about any benchmark, including ones which are still running.\nYou don't need to run this inside the docker container, as it is just\ncollecting stats not executing unsafe python.\n\n```\n# Generate stats for a specific benchmarking directory\n./benchmark/benchmark.py --stats tmp.benchmarks/YYYY-MM-DD-HH-MM-SS--a-helpful-name-for-this-run\n```\n\nThe benchmark report is a yaml record with statistics about the run:\n\n```yaml\n- dirname: 2024-07-04-14-32-08--claude-3.5-sonnet-diff-continue\n  test_cases: 225\n  model: claude-3.5-sonnet\n  edit_format: diff\n  commit_hash: 35f21b5\n  pass_rate_1: 57.1\n  pass_rate_2: 77.4\n  percent_cases_well_formed: 99.2\n  error_outputs: 23\n  num_malformed_responses: 4\n  num_with_malformed_responses: 1\n  user_asks: 2\n  lazy_comments: 0\n  syntax_errors: 1\n  indentation_errors: 0\n  exhausted_context_windows: 0\n  test_timeouts: 1\n  command: aider --sonnet\n  date: 2024-07-04\n  versions: 0.42.1-dev\n  seconds_per_case: 17.6\n  total_cost: 3.6346\n```\n\nThe key statistics are the `pass_rate_#` entries, which report the\npercent of the tasks which had all tests passing.\nThere will be multiple of these pass rate stats,\ndepending on the value of the `--tries` parameter.\n\nThe yaml also includes all the settings which were in effect for the benchmark run.\nIt also reports the git hash of the repo at the time that the benchmark was\nrun, with `(dirty)` if there were uncommitted changes.\nIt's good practice to commit the repo before starting a benchmark run.\nThis way the `model`, `edit_format` and `commit_hash`\nshould be enough to reliably reproduce any benchmark run.\n\nYou can see examples of the benchmark report yaml in the\n[aider leaderboard data files](https://github.com/Aider-AI/aider/blob/main/aider/website/_data/).\n\n\n## Limitations, notes\n\n- Contributions of benchmark results are welcome! Submit results by opening a PR with edits to the\n[aider leaderboard data files](https://github.com/Aider-AI/aider/blob/main/aider/website/_data/).\n- These scripts are not intended for use by typical aider end users.\n- Some of these tools are written as `bash` scripts, so it will be hard to use them on Windows.\n", "type": "documentation", "indexed_at": "2026-01-04T03:32:42+01:00"}
